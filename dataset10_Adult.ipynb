{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list=['age','workclass','fnlwgt','education','education-num','marital-status','occupation',\n",
    "           'relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",names=name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>160187</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>209642</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>45781</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>159449</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "5   37            Private  284582     Masters             14   \n",
       "6   49            Private  160187         9th              5   \n",
       "7   52   Self-emp-not-inc  209642     HS-grad              9   \n",
       "8   31            Private   45781     Masters             14   \n",
       "9   42            Private  159449   Bachelors             13   \n",
       "\n",
       "           marital-status          occupation    relationship    race  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black   \n",
       "5      Married-civ-spouse     Exec-managerial            Wife   White   \n",
       "6   Married-spouse-absent       Other-service   Not-in-family   Black   \n",
       "7      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "8           Never-married      Prof-specialty   Not-in-family   White   \n",
       "9      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "\n",
       "       sex  capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0     Male          2174             0              40   United-States   <=50K  \n",
       "1     Male             0             0              13   United-States   <=50K  \n",
       "2     Male             0             0              40   United-States   <=50K  \n",
       "3     Male             0             0              40   United-States   <=50K  \n",
       "4   Female             0             0              40            Cuba   <=50K  \n",
       "5   Female             0             0              40   United-States   <=50K  \n",
       "6   Female             0             0              16         Jamaica   <=50K  \n",
       "7     Male             0             0              45   United-States    >50K  \n",
       "8   Female         14084             0              50   United-States    >50K  \n",
       "9     Male          5178             0              40   United-States    >50K  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAAPYCAYAAABXLAn+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11608e750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "cols = 5\n",
    "rows = math.ceil(float(train.shape[1]) / cols)\n",
    "for i, column in enumerate(train.columns):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    ax.set_title(column)\n",
    "    if train.dtypes[column] == np.object:\n",
    "        train[column].value_counts().plot(kind=\"bar\", axes=ax)\n",
    "    else:\n",
    "        train[column].hist(axes=ax)\n",
    "        plt.xticks(rotation=\"vertical\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " United-States    0.895857\n",
       " Mexico           0.019748\n",
       " ?                0.017905\n",
       " Philippines      0.006081\n",
       " Germany          0.004207\n",
       " Canada           0.003716\n",
       " Puerto-Rico      0.003501\n",
       " El-Salvador      0.003255\n",
       " India            0.003071\n",
       " Cuba             0.002918\n",
       "Name: native-country, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train[\"native-country\"].value_counts() / train.shape[0]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " White                 0.854274\n",
       " Black                 0.095943\n",
       " Asian-Pac-Islander    0.031909\n",
       " Amer-Indian-Eskimo    0.009551\n",
       " Other                 0.008323\n",
       "Name: race, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train[\"race\"].value_counts() / train.shape[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education-num     0\n",
       "marital-status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "native-country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "train['income'] = le.fit_transform(train['income'].values)\n",
    "train['workclass'] = le.fit_transform(train['workclass'].values)\n",
    "train['sex']=le.fit_transform(train['sex'].values)\n",
    "train['native-country']=le.fit_transform(train['native-country'].values)\n",
    "train['occupation']=le.fit_transform(train['occupation'].values)\n",
    "train['relationship']=le.fit_transform(train['relationship'].values)\n",
    "train['marital-status']=le.fit_transform(train['marital-status'].values)\n",
    "train['race']=le.fit_transform(train['race'].values)\n",
    "\n",
    "train=train.drop('education',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cate_attrs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fbccbbefd910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcate_attrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdummies_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdummies_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummies_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdummies_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cate_attrs' is not defined"
     ]
    }
   ],
   "source": [
    "for i in cate_attrs:\n",
    "    dummies_df = pd.get_dummies(train[i])\n",
    "    dummies_df = dummies_df.rename(columns=lambda x: i+'_'+str(x))\n",
    "    train = pd.concat([train,dummies_df],axis=1)\n",
    "    train = train.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>284582</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>160187</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>209642</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>45781</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>159449</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>280464</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>141297</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>122272</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>205019</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>121772</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>245487</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>176756</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>186824</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>28887</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>292175</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  workclass  fnlwgt  education-num  marital-status  occupation  \\\n",
       "0    39          7   77516             13               4           1   \n",
       "1    50          6   83311             13               2           4   \n",
       "2    38          4  215646              9               0           6   \n",
       "3    53          4  234721              7               2           6   \n",
       "4    28          4  338409             13               2          10   \n",
       "5    37          4  284582             14               2           4   \n",
       "6    49          4  160187              5               3           8   \n",
       "7    52          6  209642              9               2           4   \n",
       "8    31          4   45781             14               4          10   \n",
       "9    42          4  159449             13               2           4   \n",
       "10   37          4  280464             10               2           4   \n",
       "11   30          7  141297             13               2          10   \n",
       "12   23          4  122272             13               4           1   \n",
       "13   32          4  205019             12               4          12   \n",
       "14   40          4  121772             11               2           3   \n",
       "15   34          4  245487              4               2          14   \n",
       "16   25          6  176756              9               4           5   \n",
       "17   32          4  186824              9               4           7   \n",
       "18   38          4   28887              7               2          12   \n",
       "19   43          6  292175             14               0           4   \n",
       "\n",
       "    relationship  race  sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0              1     4    1          2174             0              40   \n",
       "1              0     4    1             0             0              13   \n",
       "2              1     4    1             0             0              40   \n",
       "3              0     2    1             0             0              40   \n",
       "4              5     2    0             0             0              40   \n",
       "5              5     4    0             0             0              40   \n",
       "6              1     2    0             0             0              16   \n",
       "7              0     4    1             0             0              45   \n",
       "8              1     4    0         14084             0              50   \n",
       "9              0     4    1          5178             0              40   \n",
       "10             0     2    1             0             0              80   \n",
       "11             0     1    1             0             0              40   \n",
       "12             3     4    0             0             0              30   \n",
       "13             1     2    1             0             0              50   \n",
       "14             0     1    1             0             0              40   \n",
       "15             0     0    1             0             0              45   \n",
       "16             3     4    1             0             0              35   \n",
       "17             4     4    1             0             0              40   \n",
       "18             0     4    1             0             0              50   \n",
       "19             4     4    0             0             0              45   \n",
       "\n",
       "    native-country  income  \n",
       "0               39       0  \n",
       "1               39       0  \n",
       "2               39       0  \n",
       "3               39       0  \n",
       "4                5       0  \n",
       "5               39       0  \n",
       "6               23       0  \n",
       "7               39       1  \n",
       "8               39       1  \n",
       "9               39       1  \n",
       "10              39       1  \n",
       "11              19       1  \n",
       "12              39       0  \n",
       "13              39       0  \n",
       "14               0       1  \n",
       "15              26       0  \n",
       "16              39       0  \n",
       "17              39       0  \n",
       "18              39       0  \n",
       "19              39       1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt  education-num  marital-status  occupation  \\\n",
       "0   39          7   77516             13               4           1   \n",
       "1   50          6   83311             13               2           4   \n",
       "2   38          4  215646              9               0           6   \n",
       "3   53          4  234721              7               2           6   \n",
       "4   28          4  338409             13               2          10   \n",
       "\n",
       "   relationship  race  sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0             1     4    1          2174             0              40   \n",
       "1             0     4    1             0             0              13   \n",
       "2             1     4    1             0             0              40   \n",
       "3             0     2    1             0             0              40   \n",
       "4             5     2    0             0             0              40   \n",
       "\n",
       "   native-country  \n",
       "0              39  \n",
       "1              39  \n",
       "2              39  \n",
       "3              39  \n",
       "4               5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=train.drop(\"income\",axis=1)\n",
    "outcomes=train[\"income\"].values\n",
    "features.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, outcomes, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranom Forest         84.41 (+/-) 0.48 \n"
     ]
    }
   ],
   "source": [
    "model=RandomForestClassifier(n_estimators=5)\n",
    "kfold = KFold(n_splits=10, random_state=0)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "results=[\"Ranom Forest\",cv_result.mean(),cv_result.std()]\n",
    "\n",
    "print('{:20s} {:2.2f} (+/-) {:2.2f} '.format(results[0] , results[1] * 100, results[2] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best accuracy : ', 0.8543407043407043)\n",
      "('Best parameters :', {'max_features': None, 'n_estimators': 200, 'max_depth': 5})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "paramaters = [\n",
    "             {'n_estimators' : [100, 200, 300, 500, 1000], \n",
    "              'max_features' : ['auto','log2',None],\n",
    "              'max_depth':[3,4,5],\n",
    "             }                                       \n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train,Y_train)\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5983  222]\n",
      " [ 991  945]]\n",
      "85.1001105515\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.96      0.91      6205\n",
      "          1       0.81      0.49      0.61      1936\n",
      "\n",
      "avg / total       0.85      0.85      0.84      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from matplotlib import pylab\n",
    "\n",
    "final_model = RandomForestClassifier(n_estimators=200,max_features='auto',bootstrap=True,oob_score=True,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5918  287]\n",
      " [ 909 1027]]\n",
      "85.3089301069\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91      6205\n",
      "          1       0.78      0.53      0.63      1936\n",
      "\n",
      "avg / total       0.85      0.85      0.84      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=200,max_features=None,bootstrap=True,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "\n",
    "\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5915  290]\n",
      " [ 907 1029]]\n",
      "85.2966466036\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91      6205\n",
      "          1       0.78      0.53      0.63      1936\n",
      "\n",
      "avg / total       0.85      0.85      0.84      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=1,max_features=None,bootstrap=False,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "\n",
    "\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest by xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5849  356]\n",
      " [ 667 1269]]\n",
      "87.43397617\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.94      0.92      6205\n",
      "          1       0.78      0.66      0.71      1936\n",
      "\n",
      "avg / total       0.87      0.87      0.87      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=200,num_boost_round=1,max_depth=6,subsample=0.632,colsample_bytree=0.4)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagged tree by Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5808  397]\n",
      " [ 677 1259]]\n",
      "86.807517504\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.94      0.92      6205\n",
      "          1       0.76      0.65      0.70      1936\n",
      "\n",
      "avg / total       0.86      0.87      0.86      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=200,num_boost_round=1,max_depth=6,subsample=0.632,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree by Xgoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5893  312]\n",
      " [ 906 1030]]\n",
      "85.0386930353\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91      6205\n",
      "          1       0.77      0.53      0.63      1936\n",
      "\n",
      "avg / total       0.84      0.85      0.84      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=1,num_boost_round=1,max_depth=6,subsample=1,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,History\n",
    "from keras.layers import Dense, Activation, Dropout,Input\n",
    "from keras import optimizers\n",
    "history = History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(len(np.unique(Y_train)), activation='softmax'))\n",
    "    \n",
    "m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19536 samples, validate on 4884 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.53896, saving model to best.model\n",
      "0s - loss: 0.6608 - acc: 0.7016 - val_loss: 0.5390 - val_acc: 0.7697\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.53896 to 0.53352, saving model to best.model\n",
      "0s - loss: 0.5859 - acc: 0.7410 - val_loss: 0.5335 - val_acc: 0.7697\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5680 - acc: 0.7538 - val_loss: 0.5369 - val_acc: 0.7697\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5623 - acc: 0.7556 - val_loss: 0.5360 - val_acc: 0.7697\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7562 - val_loss: 0.5343 - val_acc: 0.7697\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5572 - acc: 0.7567 - val_loss: 0.5357 - val_acc: 0.7701\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5583 - acc: 0.7564 - val_loss: 0.5367 - val_acc: 0.7701\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5586 - acc: 0.7573 - val_loss: 0.5374 - val_acc: 0.7717\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.53352 to 0.53345, saving model to best.model\n",
      "0s - loss: 0.5547 - acc: 0.7581 - val_loss: 0.5334 - val_acc: 0.7727\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.53345 to 0.52995, saving model to best.model\n",
      "0s - loss: 0.5530 - acc: 0.7592 - val_loss: 0.5299 - val_acc: 0.7729\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7611 - val_loss: 0.5309 - val_acc: 0.7735\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.52995 to 0.52829, saving model to best.model\n",
      "0s - loss: 0.5489 - acc: 0.7630 - val_loss: 0.5283 - val_acc: 0.7744\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.52829 to 0.52303, saving model to best.model\n",
      "0s - loss: 0.5444 - acc: 0.7669 - val_loss: 0.5230 - val_acc: 0.7776\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5466 - acc: 0.7663 - val_loss: 0.5310 - val_acc: 0.7744\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7643 - val_loss: 0.5302 - val_acc: 0.7744\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7641 - val_loss: 0.5294 - val_acc: 0.7744\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7643 - val_loss: 0.5312 - val_acc: 0.7776\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7650 - val_loss: 0.5278 - val_acc: 0.7776\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7676 - val_loss: 0.5263 - val_acc: 0.7776\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7671 - val_loss: 0.5261 - val_acc: 0.7776\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.52303 to 0.51653, saving model to best.model\n",
      "0s - loss: 0.5410 - acc: 0.7708 - val_loss: 0.5165 - val_acc: 0.7776\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7693 - val_loss: 0.5284 - val_acc: 0.7776\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5453 - acc: 0.7670 - val_loss: 0.5268 - val_acc: 0.7776\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7690 - val_loss: 0.5242 - val_acc: 0.7776\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7707 - val_loss: 0.5250 - val_acc: 0.7776\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7692 - val_loss: 0.5247 - val_acc: 0.7776\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5388 - acc: 0.7708 - val_loss: 0.5218 - val_acc: 0.7776\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5356 - acc: 0.7736 - val_loss: 0.5225 - val_acc: 0.7776\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7734 - val_loss: 0.5198 - val_acc: 0.7776\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5312 - acc: 0.7774 - val_loss: 0.5168 - val_acc: 0.7776\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5350 - acc: 0.7753 - val_loss: 0.5196 - val_acc: 0.7776\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7722 - val_loss: 0.5331 - val_acc: 0.7744\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5441 - acc: 0.7645 - val_loss: 0.5279 - val_acc: 0.7776\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7685 - val_loss: 0.5259 - val_acc: 0.7776\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7691 - val_loss: 0.5273 - val_acc: 0.7776\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7677 - val_loss: 0.5263 - val_acc: 0.7776\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7681 - val_loss: 0.5268 - val_acc: 0.7776\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7685 - val_loss: 0.5281 - val_acc: 0.7776\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7678 - val_loss: 0.5276 - val_acc: 0.7776\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7675 - val_loss: 0.5283 - val_acc: 0.7776\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7687 - val_loss: 0.5251 - val_acc: 0.7776\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7692 - val_loss: 0.5271 - val_acc: 0.7776\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7692 - val_loss: 0.5273 - val_acc: 0.7776\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7685 - val_loss: 0.5267 - val_acc: 0.7776\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7689 - val_loss: 0.5278 - val_acc: 0.7776\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7685 - val_loss: 0.5267 - val_acc: 0.7776\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7684 - val_loss: 0.5261 - val_acc: 0.7776\n"
     ]
    }
   ],
   "source": [
    "hist=m.fit(\n",
    "    # Feature matrix\n",
    "    X_train.values, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=256, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEII4SL+H8ID1nIcS/zDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129b49f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.load_weights(\"best.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in m.predict(X_test.values).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6199    6]\n",
      " [1853   83]]\n",
      "77.1649674487\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      1.00      0.87      6205\n",
      "          1       0.93      0.04      0.08      1936\n",
      "\n",
      "avg / total       0.81      0.77      0.68      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19536 samples, validate on 4884 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.38085, saving model to best.model\n",
      "1s - loss: 0.4451 - acc: 0.7903 - val_loss: 0.3809 - val_acc: 0.8331\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.38085 to 0.36844, saving model to best.model\n",
      "1s - loss: 0.3959 - acc: 0.8167 - val_loss: 0.3684 - val_acc: 0.8378\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "1s - loss: 0.3852 - acc: 0.8203 - val_loss: 0.3759 - val_acc: 0.8196\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.36844 to 0.36618, saving model to best.model\n",
      "1s - loss: 0.3764 - acc: 0.8250 - val_loss: 0.3662 - val_acc: 0.8327\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.36618 to 0.35934, saving model to best.model\n",
      "1s - loss: 0.3719 - acc: 0.8259 - val_loss: 0.3593 - val_acc: 0.8415\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.35934 to 0.35134, saving model to best.model\n",
      "1s - loss: 0.3664 - acc: 0.8268 - val_loss: 0.3513 - val_acc: 0.8454\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "1s - loss: 0.3647 - acc: 0.8282 - val_loss: 0.3946 - val_acc: 0.8092\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.35134 to 0.35043, saving model to best.model\n",
      "1s - loss: 0.3644 - acc: 0.8285 - val_loss: 0.3504 - val_acc: 0.8450\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.35043 to 0.35039, saving model to best.model\n",
      "1s - loss: 0.3620 - acc: 0.8287 - val_loss: 0.3504 - val_acc: 0.8468\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.3620 - acc: 0.8277 - val_loss: 0.3512 - val_acc: 0.8466\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.3591 - acc: 0.8308 - val_loss: 0.3549 - val_acc: 0.8376\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.3576 - acc: 0.8285 - val_loss: 0.3933 - val_acc: 0.8043\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.35039 to 0.34679, saving model to best.model\n",
      "1s - loss: 0.3574 - acc: 0.8289 - val_loss: 0.3468 - val_acc: 0.8485\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.3562 - acc: 0.8317 - val_loss: 0.3547 - val_acc: 0.8446\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "1s - loss: 0.3534 - acc: 0.8313 - val_loss: 0.3481 - val_acc: 0.8485\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.34679 to 0.34570, saving model to best.model\n",
      "1s - loss: 0.3525 - acc: 0.8326 - val_loss: 0.3457 - val_acc: 0.8466\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "1s - loss: 0.3502 - acc: 0.8324 - val_loss: 0.3489 - val_acc: 0.8477\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "1s - loss: 0.3484 - acc: 0.8324 - val_loss: 0.3468 - val_acc: 0.8511\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.34570 to 0.33891, saving model to best.model\n",
      "1s - loss: 0.3480 - acc: 0.8321 - val_loss: 0.3389 - val_acc: 0.8485\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "1s - loss: 0.3473 - acc: 0.8326 - val_loss: 0.3414 - val_acc: 0.8499\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.33891 to 0.33705, saving model to best.model\n",
      "1s - loss: 0.3452 - acc: 0.8334 - val_loss: 0.3371 - val_acc: 0.8518\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "1s - loss: 0.3453 - acc: 0.8339 - val_loss: 0.3397 - val_acc: 0.8499\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "1s - loss: 0.3445 - acc: 0.8346 - val_loss: 0.3388 - val_acc: 0.8456\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "1s - loss: 0.3462 - acc: 0.8335 - val_loss: 0.3378 - val_acc: 0.8495\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.33705 to 0.33521, saving model to best.model\n",
      "1s - loss: 0.3425 - acc: 0.8354 - val_loss: 0.3352 - val_acc: 0.8536\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "1s - loss: 0.3434 - acc: 0.8345 - val_loss: 0.3397 - val_acc: 0.8495\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "1s - loss: 0.3425 - acc: 0.8351 - val_loss: 0.3383 - val_acc: 0.8530\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "1s - loss: 0.3424 - acc: 0.8359 - val_loss: 0.3353 - val_acc: 0.8536\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.3408 - acc: 0.8369 - val_loss: 0.3357 - val_acc: 0.8526\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.33521 to 0.33485, saving model to best.model\n",
      "1s - loss: 0.3412 - acc: 0.8343 - val_loss: 0.3349 - val_acc: 0.8530\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "1s - loss: 0.3416 - acc: 0.8358 - val_loss: 0.3350 - val_acc: 0.8524\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "1s - loss: 0.3411 - acc: 0.8373 - val_loss: 0.3456 - val_acc: 0.8473\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "1s - loss: 0.3414 - acc: 0.8322 - val_loss: 0.3378 - val_acc: 0.8436\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "1s - loss: 0.3396 - acc: 0.8361 - val_loss: 0.3382 - val_acc: 0.8434\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "1s - loss: 0.3399 - acc: 0.8355 - val_loss: 0.3382 - val_acc: 0.8401\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "1s - loss: 0.3401 - acc: 0.8359 - val_loss: 0.3380 - val_acc: 0.8514\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.33485 to 0.33474, saving model to best.model\n",
      "1s - loss: 0.3393 - acc: 0.8366 - val_loss: 0.3347 - val_acc: 0.8524\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.33474 to 0.33357, saving model to best.model\n",
      "1s - loss: 0.3407 - acc: 0.8361 - val_loss: 0.3336 - val_acc: 0.8505\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "1s - loss: 0.3396 - acc: 0.8351 - val_loss: 0.3354 - val_acc: 0.8530\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "1s - loss: 0.3402 - acc: 0.8345 - val_loss: 0.3343 - val_acc: 0.8528\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "1s - loss: 0.3396 - acc: 0.8364 - val_loss: 0.3336 - val_acc: 0.8532\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "1s - loss: 0.3394 - acc: 0.8371 - val_loss: 0.3391 - val_acc: 0.8534\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.33357 to 0.33233, saving model to best.model\n",
      "1s - loss: 0.3389 - acc: 0.8360 - val_loss: 0.3323 - val_acc: 0.8530\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "1s - loss: 0.3379 - acc: 0.8384 - val_loss: 0.3345 - val_acc: 0.8450\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "1s - loss: 0.3376 - acc: 0.8368 - val_loss: 0.3414 - val_acc: 0.8464\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "1s - loss: 0.3378 - acc: 0.8377 - val_loss: 0.3373 - val_acc: 0.8522\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "1s - loss: 0.3375 - acc: 0.8387 - val_loss: 0.3354 - val_acc: 0.8505\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "1s - loss: 0.3371 - acc: 0.8361 - val_loss: 0.3358 - val_acc: 0.8532\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "1s - loss: 0.3370 - acc: 0.8378 - val_loss: 0.3338 - val_acc: 0.8485\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "1s - loss: 0.3369 - acc: 0.8370 - val_loss: 0.3361 - val_acc: 0.8536\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "2s - loss: 0.3362 - acc: 0.8383 - val_loss: 0.3345 - val_acc: 0.8534\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "1s - loss: 0.3372 - acc: 0.8395 - val_loss: 0.3336 - val_acc: 0.8487\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "2s - loss: 0.3368 - acc: 0.8400 - val_loss: 0.3342 - val_acc: 0.8536\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.33233 to 0.33165, saving model to best.model\n",
      "2s - loss: 0.3362 - acc: 0.8389 - val_loss: 0.3317 - val_acc: 0.8520\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "1s - loss: 0.3368 - acc: 0.8394 - val_loss: 0.3357 - val_acc: 0.8497\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "1s - loss: 0.3365 - acc: 0.8390 - val_loss: 0.3321 - val_acc: 0.8499\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "1s - loss: 0.3362 - acc: 0.8390 - val_loss: 0.3346 - val_acc: 0.8511\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "1s - loss: 0.3362 - acc: 0.8388 - val_loss: 0.3367 - val_acc: 0.8497\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "1s - loss: 0.3361 - acc: 0.8395 - val_loss: 0.3320 - val_acc: 0.8516\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "1s - loss: 0.3349 - acc: 0.8388 - val_loss: 0.3340 - val_acc: 0.8493\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "1s - loss: 0.3364 - acc: 0.8382 - val_loss: 0.3331 - val_acc: 0.8503\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "1s - loss: 0.3357 - acc: 0.8383 - val_loss: 0.3323 - val_acc: 0.8524\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "1s - loss: 0.3353 - acc: 0.8365 - val_loss: 0.3357 - val_acc: 0.8475\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "1s - loss: 0.3346 - acc: 0.8392 - val_loss: 0.3341 - val_acc: 0.8518\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "1s - loss: 0.3350 - acc: 0.8390 - val_loss: 0.3320 - val_acc: 0.8495\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "1s - loss: 0.3349 - acc: 0.8397 - val_loss: 0.3477 - val_acc: 0.8460\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.3348 - acc: 0.8388 - val_loss: 0.3338 - val_acc: 0.8503\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "1s - loss: 0.3338 - acc: 0.8396 - val_loss: 0.3343 - val_acc: 0.8511\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "1s - loss: 0.3337 - acc: 0.8392 - val_loss: 0.3376 - val_acc: 0.8532\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "1s - loss: 0.3340 - acc: 0.8391 - val_loss: 0.3349 - val_acc: 0.8483\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "1s - loss: 0.3345 - acc: 0.8387 - val_loss: 0.3372 - val_acc: 0.8497\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.3335 - acc: 0.8398 - val_loss: 0.3341 - val_acc: 0.8509\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.3346 - acc: 0.8393 - val_loss: 0.3357 - val_acc: 0.8524\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.3330 - acc: 0.8414 - val_loss: 0.3337 - val_acc: 0.8511\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.3341 - acc: 0.8382 - val_loss: 0.3334 - val_acc: 0.8505\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "1s - loss: 0.3328 - acc: 0.8406 - val_loss: 0.3395 - val_acc: 0.8503\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "2s - loss: 0.3325 - acc: 0.8421 - val_loss: 0.3333 - val_acc: 0.8493\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "2s - loss: 0.3330 - acc: 0.8397 - val_loss: 0.3338 - val_acc: 0.8534\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "2s - loss: 0.3332 - acc: 0.8384 - val_loss: 0.3500 - val_acc: 0.8411\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.3321 - acc: 0.8393 - val_loss: 0.3320 - val_acc: 0.8491\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,History\n",
    "from keras.models import Model\n",
    "import keras\n",
    "history = History()\n",
    "\n",
    "\n",
    "input_1 = Input(shape=(1,))\n",
    "input_2 = Input(shape=(1,))\n",
    "input_3 = Input(shape=(1,))\n",
    "input_4 = Input(shape=(1,))\n",
    "input_5 = Input(shape=(1,))\n",
    "input_6 = Input(shape=(1,))\n",
    "input_7 = Input(shape=(1,))\n",
    "input_8 = Input(shape=(1,))\n",
    "input_9 = Input(shape=(1,))\n",
    "input_10 = Input(shape=(1,))\n",
    "input_11 = Input(shape=(1,))\n",
    "input_12 = Input(shape=(1,))\n",
    "input_13 = Input(shape=(1,))\n",
    "\n",
    "\n",
    "\n",
    "hidden_1_1 = Dense(32, activation='sigmoid')(input_1)\n",
    "hidden_1_2 = Dense(32, activation='sigmoid')(input_1)\n",
    "hidden_1_3 = Dense(32, activation='sigmoid')(input_1)\n",
    "hidden_2 = Dense(32, activation='sigmoid')(input_2)\n",
    "hidden_3_1 = Dense(32, activation='sigmoid')(input_3)\n",
    "hidden_3_2 = Dense(32, activation='sigmoid')(input_3)\n",
    "hidden_3_3 = Dense(32, activation='sigmoid')(input_3)\n",
    "hidden_4 = Dense(32, activation='sigmoid')(input_4)\n",
    "hidden_5 = Dense(32, activation='sigmoid')(input_5)\n",
    "hidden_6 = Dense(32, activation='sigmoid')(input_6)\n",
    "hidden_7 = Dense(32, activation='sigmoid')(input_7)\n",
    "hidden_8 = Dense(32, activation='sigmoid')(input_8)\n",
    "hidden_9 = Dense(32, activation='sigmoid')(input_9)\n",
    "hidden_10 = Dense(32, activation='sigmoid')(input_10)\n",
    "hidden_11 = Dense(32, activation='sigmoid')(input_11)\n",
    "hidden_12 = Dense(32, activation='sigmoid')(input_12)\n",
    "hidden_13 = Dense(32, activation='sigmoid')(input_13)\n",
    "\n",
    "\n",
    "value_list=[X_train[['age']].values,\n",
    "            X_train[['workclass']].values,\n",
    "            X_train[['fnlwgt']].values,\n",
    "            X_train[['education-num']].values,\n",
    "            X_train[['marital-status']].values,\n",
    "            X_train[['occupation']].values,\n",
    "            X_train[['relationship']].values,\n",
    "            X_train[['race']].values,\n",
    "            X_train[['sex']].values,\n",
    "            X_train[['capital-gain']].values,\n",
    "            X_train[['capital-loss']].values,\n",
    "            X_train[['hours-per-week']].values,\n",
    "            X_train[['native-country']].values\n",
    "\n",
    "           ]\n",
    "\n",
    "value_list_test=[X_test[['age']].values,\n",
    "                X_test[['workclass']].values,\n",
    "                X_test[['fnlwgt']].values,\n",
    "                X_test[['education-num']].values,\n",
    "                X_test[['marital-status']].values,\n",
    "                X_test[['occupation']].values,\n",
    "                X_test[['relationship']].values,\n",
    "                X_test[['race']].values,\n",
    "                X_test[['sex']].values,\n",
    "                X_test[['capital-gain']].values,\n",
    "                X_test[['capital-loss']].values,\n",
    "                X_test[['hours-per-week']].values,\n",
    "                X_test[['native-country']].values\n",
    "                ]\n",
    "\n",
    "x = keras.layers.concatenate([hidden_1_1,hidden_1_2,hidden_1_3, hidden_2,hidden_3_1,hidden_3_2,hidden_3_3,\n",
    "                              hidden_4,hidden_5,hidden_6,hidden_7,\n",
    "                              hidden_8,hidden_9,hidden_10,hidden_11,hidden_12,hidden_13])\n",
    "\n",
    "x = Dense(96, activation='sigmoid')(x)\n",
    "output = Dense(len(np.unique(Y_train)), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[input_1,input_2,input_3,input_4,input_5,input_6,input_7,input_8,\n",
    "                      input_9,input_10,input_11,input_12,input_13], outputs=[output])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "hist=model.fit(\n",
    "    # Feature matrix\n",
    "    value_list, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a2c22d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.model\")\n",
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in model.predict(value_list_test).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5769  436]\n",
      " [ 875 1061]]\n",
      "83.8963272325\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.93      0.90      6205\n",
      "          1       0.71      0.55      0.62      1936\n",
      "\n",
      "avg / total       0.83      0.84      0.83      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list=['age','workclass','fnlwgt','education','education-num','marital-status','occupation',\n",
    "           'relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",names=name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>160187</td>\n",
       "      <td>9th</td>\n",
       "      <td>5</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>209642</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>45781</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>159449</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "5   37            Private  284582     Masters             14   \n",
       "6   49            Private  160187         9th              5   \n",
       "7   52   Self-emp-not-inc  209642     HS-grad              9   \n",
       "8   31            Private   45781     Masters             14   \n",
       "9   42            Private  159449   Bachelors             13   \n",
       "\n",
       "           marital-status          occupation    relationship    race  \\\n",
       "0           Never-married        Adm-clerical   Not-in-family   White   \n",
       "1      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "2                Divorced   Handlers-cleaners   Not-in-family   White   \n",
       "3      Married-civ-spouse   Handlers-cleaners         Husband   Black   \n",
       "4      Married-civ-spouse      Prof-specialty            Wife   Black   \n",
       "5      Married-civ-spouse     Exec-managerial            Wife   White   \n",
       "6   Married-spouse-absent       Other-service   Not-in-family   Black   \n",
       "7      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "8           Never-married      Prof-specialty   Not-in-family   White   \n",
       "9      Married-civ-spouse     Exec-managerial         Husband   White   \n",
       "\n",
       "       sex  capital-gain  capital-loss  hours-per-week  native-country  income  \n",
       "0     Male          2174             0              40   United-States   <=50K  \n",
       "1     Male             0             0              13   United-States   <=50K  \n",
       "2     Male             0             0              40   United-States   <=50K  \n",
       "3     Male             0             0              40   United-States   <=50K  \n",
       "4   Female             0             0              40            Cuba   <=50K  \n",
       "5   Female             0             0              40   United-States   <=50K  \n",
       "6   Female             0             0              16         Jamaica   <=50K  \n",
       "7     Male             0             0              45   United-States    >50K  \n",
       "8   Female         14084             0              50   United-States    >50K  \n",
       "9     Male          5178             0              40   United-States    >50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "train['income'] = le.fit_transform(train['income'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_attrs = ['age', 'fnlwgt', 'capital-gain','capital-loss']\n",
    "cate_attrs = ['workclass', 'education', 'marital-status', 'occupation', \n",
    "                  'relationship', 'race','native-country','sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in cate_attrs:\n",
    "    dummies_df = pd.get_dummies(train[i])\n",
    "    dummies_df = dummies_df.rename(columns=lambda x: i+'_'+str(x))\n",
    "    train = pd.concat([train,dummies_df],axis=1)\n",
    "    train = train.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass_ ?</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_ Scotland</th>\n",
       "      <th>native-country_ South</th>\n",
       "      <th>native-country_ Taiwan</th>\n",
       "      <th>native-country_ Thailand</th>\n",
       "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_ United-States</th>\n",
       "      <th>native-country_ Vietnam</th>\n",
       "      <th>native-country_ Yugoslavia</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>sex_ Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>284582</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>160187</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>209642</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>45781</td>\n",
       "      <td>14</td>\n",
       "      <td>14084</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>159449</td>\n",
       "      <td>13</td>\n",
       "      <td>5178</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   39   77516             13          2174             0              40   \n",
       "1   50   83311             13             0             0              13   \n",
       "2   38  215646              9             0             0              40   \n",
       "3   53  234721              7             0             0              40   \n",
       "4   28  338409             13             0             0              40   \n",
       "5   37  284582             14             0             0              40   \n",
       "6   49  160187              5             0             0              16   \n",
       "7   52  209642              9             0             0              45   \n",
       "8   31   45781             14         14084             0              50   \n",
       "9   42  159449             13          5178             0              40   \n",
       "\n",
       "   income  workclass_ ?  workclass_ Federal-gov  workclass_ Local-gov  \\\n",
       "0       0             0                       0                     0   \n",
       "1       0             0                       0                     0   \n",
       "2       0             0                       0                     0   \n",
       "3       0             0                       0                     0   \n",
       "4       0             0                       0                     0   \n",
       "5       0             0                       0                     0   \n",
       "6       0             0                       0                     0   \n",
       "7       1             0                       0                     0   \n",
       "8       1             0                       0                     0   \n",
       "9       1             0                       0                     0   \n",
       "\n",
       "     ...      native-country_ Scotland  native-country_ South  \\\n",
       "0    ...                             0                      0   \n",
       "1    ...                             0                      0   \n",
       "2    ...                             0                      0   \n",
       "3    ...                             0                      0   \n",
       "4    ...                             0                      0   \n",
       "5    ...                             0                      0   \n",
       "6    ...                             0                      0   \n",
       "7    ...                             0                      0   \n",
       "8    ...                             0                      0   \n",
       "9    ...                             0                      0   \n",
       "\n",
       "   native-country_ Taiwan  native-country_ Thailand  \\\n",
       "0                       0                         0   \n",
       "1                       0                         0   \n",
       "2                       0                         0   \n",
       "3                       0                         0   \n",
       "4                       0                         0   \n",
       "5                       0                         0   \n",
       "6                       0                         0   \n",
       "7                       0                         0   \n",
       "8                       0                         0   \n",
       "9                       0                         0   \n",
       "\n",
       "   native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
       "0                                0                              1   \n",
       "1                                0                              1   \n",
       "2                                0                              1   \n",
       "3                                0                              1   \n",
       "4                                0                              0   \n",
       "5                                0                              1   \n",
       "6                                0                              0   \n",
       "7                                0                              1   \n",
       "8                                0                              1   \n",
       "9                                0                              1   \n",
       "\n",
       "   native-country_ Vietnam  native-country_ Yugoslavia  sex_ Female  sex_ Male  \n",
       "0                        0                           0            0          1  \n",
       "1                        0                           0            0          1  \n",
       "2                        0                           0            0          1  \n",
       "3                        0                           0            0          1  \n",
       "4                        0                           0            1          0  \n",
       "5                        0                           0            1          0  \n",
       "6                        0                           0            1          0  \n",
       "7                        0                           0            0          1  \n",
       "8                        0                           0            1          0  \n",
       "9                        0                           0            0          1  \n",
       "\n",
       "[10 rows x 109 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(features, outcomes, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranom Forest         83.97 (+/-) 0.64 \n"
     ]
    }
   ],
   "source": [
    "model=RandomForestClassifier(n_estimators=5)\n",
    "kfold = KFold(n_splits=10, random_state=0)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "results=[\"Ranom Forest\",cv_result.mean(),cv_result.std()]\n",
    "\n",
    "print('{:20s} {:2.2f} (+/-) {:2.2f} '.format(results[0] , results[1] * 100, results[2] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5918  287]\n",
      " [ 909 1027]]\n",
      "85.3089301069\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91      6205\n",
      "          1       0.78      0.53      0.63      1936\n",
      "\n",
      "avg / total       0.85      0.85      0.84      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from matplotlib import pylab\n",
    "\n",
    "final_model = RandomForestClassifier(n_estimators=200,max_features=None,bootstrap=True,oob_score=True,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5915  290]\n",
      " [ 908 1028]]\n",
      "85.2843631004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91      6205\n",
      "          1       0.78      0.53      0.63      1936\n",
      "\n",
      "avg / total       0.85      0.85      0.84      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from matplotlib import pylab\n",
    "\n",
    "final_model = RandomForestClassifier(n_estimators=200,max_features=None,bootstrap=False,oob_score=False,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(len(np.unique(Y_train)), activation='softmax'))\n",
    "    \n",
    "m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19536 samples, validate on 4884 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.53871, saving model to best.model\n",
      "0s - loss: 0.6025 - acc: 0.7325 - val_loss: 0.5387 - val_acc: 0.7697\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5671 - acc: 0.7544 - val_loss: 0.5432 - val_acc: 0.7697\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5637 - acc: 0.7552 - val_loss: 0.5396 - val_acc: 0.7697\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5612 - acc: 0.7553 - val_loss: 0.5415 - val_acc: 0.7697\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5604 - acc: 0.7553 - val_loss: 0.5441 - val_acc: 0.7697\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5596 - acc: 0.7553 - val_loss: 0.5413 - val_acc: 0.7697\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5583 - acc: 0.7553 - val_loss: 0.5438 - val_acc: 0.7697\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5584 - acc: 0.7553 - val_loss: 0.5426 - val_acc: 0.7697\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5581 - acc: 0.7553 - val_loss: 0.5392 - val_acc: 0.7697\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.53871 to 0.53860, saving model to best.model\n",
      "0s - loss: 0.5586 - acc: 0.7553 - val_loss: 0.5386 - val_acc: 0.7697\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5580 - acc: 0.7553 - val_loss: 0.5400 - val_acc: 0.7697\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7553 - val_loss: 0.5392 - val_acc: 0.7697\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.53860 to 0.53844, saving model to best.model\n",
      "0s - loss: 0.5582 - acc: 0.7553 - val_loss: 0.5384 - val_acc: 0.7697\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.53844 to 0.53772, saving model to best.model\n",
      "0s - loss: 0.5580 - acc: 0.7553 - val_loss: 0.5377 - val_acc: 0.7697\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.53772 to 0.53460, saving model to best.model\n",
      "0s - loss: 0.5556 - acc: 0.7554 - val_loss: 0.5346 - val_acc: 0.7697\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7564 - val_loss: 0.5367 - val_acc: 0.7711\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7584 - val_loss: 0.5346 - val_acc: 0.7735\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7612 - val_loss: 0.5368 - val_acc: 0.7744\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.53460 to 0.52174, saving model to best.model\n",
      "0s - loss: 0.5468 - acc: 0.7639 - val_loss: 0.5217 - val_acc: 0.7744\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7713 - val_loss: 0.5228 - val_acc: 0.7766\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7717 - val_loss: 0.5277 - val_acc: 0.7766\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7640 - val_loss: 0.5323 - val_acc: 0.7766\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7627 - val_loss: 0.5354 - val_acc: 0.7766\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7626 - val_loss: 0.5307 - val_acc: 0.7766\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7633 - val_loss: 0.5322 - val_acc: 0.7766\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7641 - val_loss: 0.5317 - val_acc: 0.7766\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7668 - val_loss: 0.5278 - val_acc: 0.7766\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5447 - acc: 0.7668 - val_loss: 0.5278 - val_acc: 0.7766\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7682 - val_loss: 0.5302 - val_acc: 0.7766\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7669 - val_loss: 0.5283 - val_acc: 0.7766\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7676 - val_loss: 0.5278 - val_acc: 0.7766\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7677 - val_loss: 0.5268 - val_acc: 0.7766\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7678 - val_loss: 0.5265 - val_acc: 0.7766\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7687 - val_loss: 0.5271 - val_acc: 0.7766\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7661 - val_loss: 0.5303 - val_acc: 0.7766\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5450 - acc: 0.7664 - val_loss: 0.5296 - val_acc: 0.7766\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7675 - val_loss: 0.5653 - val_acc: 0.7766\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5709 - acc: 0.7476 - val_loss: 0.5384 - val_acc: 0.7707\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7572 - val_loss: 0.5374 - val_acc: 0.7711\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7578 - val_loss: 0.5363 - val_acc: 0.7725\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7581 - val_loss: 0.5374 - val_acc: 0.7725\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7598 - val_loss: 0.5352 - val_acc: 0.7725\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7595 - val_loss: 0.5381 - val_acc: 0.7725\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7596 - val_loss: 0.5339 - val_acc: 0.7744\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7600 - val_loss: 0.5338 - val_acc: 0.7744\n"
     ]
    }
   ],
   "source": [
    "hist=m.fit(\n",
    "    # Feature matrix\n",
    "    X_train.values, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=256, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126240b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.load_weights(\"best.model\")\n",
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in m.predict(X_test.values).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6204    1]\n",
      " [1892   44]]\n",
      "76.747328338\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      1.00      0.87      6205\n",
      "          1       0.98      0.02      0.04      1936\n",
      "\n",
      "avg / total       0.82      0.77      0.67      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train,data_val=train_test_split(train,test_size=0.25, random_state=10)\n",
    "X_val=data_val.drop(['income'], axis=1).values\n",
    "y_val=data_val['income'].ravel()\n",
    "\n",
    "def train_nn_simple(data_train,X_val,y_val):\n",
    "    \n",
    "\n",
    "    data_train_new=data_train.sample(frac=0.632,replace=True)\n",
    "    X_train=data_train_new.drop(['income'], axis=1).values\n",
    "    y_train=data_train_new['income'].ravel()\n",
    "    \n",
    "    m = Sequential()\n",
    "    m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "    \n",
    "    m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    m.fit(\n",
    "    # Feature matrix\n",
    "    X_train, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=256, \n",
    "    )\n",
    "    m.load_weights(\"best.model\")\n",
    "    mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    "    )\n",
    "    y_pred = [mapping[pred] for pred in m.predict(X_val).argmax(axis=1)]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55450, saving model to best.model\n",
      "0s - loss: 0.6064 - acc: 0.7319 - val_loss: 0.5545 - val_acc: 0.7568\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5718 - acc: 0.7528 - val_loss: 0.5569 - val_acc: 0.7568\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55450 to 0.55274, saving model to best.model\n",
      "0s - loss: 0.5638 - acc: 0.7559 - val_loss: 0.5527 - val_acc: 0.7568\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5618 - acc: 0.7560 - val_loss: 0.5543 - val_acc: 0.7568\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.55274 to 0.55262, saving model to best.model\n",
      "0s - loss: 0.5601 - acc: 0.7559 - val_loss: 0.5526 - val_acc: 0.7568\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.55262 to 0.55150, saving model to best.model\n",
      "0s - loss: 0.5569 - acc: 0.7559 - val_loss: 0.5515 - val_acc: 0.7568\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.55150 to 0.54989, saving model to best.model\n",
      "0s - loss: 0.5559 - acc: 0.7561 - val_loss: 0.5499 - val_acc: 0.7568\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5584 - acc: 0.7560 - val_loss: 0.5539 - val_acc: 0.7568\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5581 - acc: 0.7559 - val_loss: 0.5526 - val_acc: 0.7568\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5563 - acc: 0.7560 - val_loss: 0.5539 - val_acc: 0.7568\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5588 - acc: 0.7559 - val_loss: 0.5541 - val_acc: 0.7568\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5579 - acc: 0.7559 - val_loss: 0.5540 - val_acc: 0.7568\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5566 - acc: 0.7560 - val_loss: 0.5538 - val_acc: 0.7568\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7559 - val_loss: 0.5542 - val_acc: 0.7568\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5577 - acc: 0.7559 - val_loss: 0.5546 - val_acc: 0.7568\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7561 - val_loss: 0.5540 - val_acc: 0.7568\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5583 - acc: 0.7561 - val_loss: 0.5543 - val_acc: 0.7568\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7560 - val_loss: 0.5541 - val_acc: 0.7568\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5581 - acc: 0.7560 - val_loss: 0.5555 - val_acc: 0.7568\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5576 - acc: 0.7562 - val_loss: 0.5543 - val_acc: 0.7568\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7563 - val_loss: 0.5536 - val_acc: 0.7568\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5571 - acc: 0.7565 - val_loss: 0.5535 - val_acc: 0.7570\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5554 - acc: 0.7575 - val_loss: 0.5538 - val_acc: 0.7573\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5563 - acc: 0.7572 - val_loss: 0.5532 - val_acc: 0.7580\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7583 - val_loss: 0.5524 - val_acc: 0.7580\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7581 - val_loss: 0.5531 - val_acc: 0.7580\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7589 - val_loss: 0.5514 - val_acc: 0.7580\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7589 - val_loss: 0.5513 - val_acc: 0.7602\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7599 - val_loss: 0.5504 - val_acc: 0.7609\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7600 - val_loss: 0.5502 - val_acc: 0.7602\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54989 to 0.54989, saving model to best.model\n",
      "0s - loss: 0.5526 - acc: 0.7598 - val_loss: 0.5499 - val_acc: 0.7609\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7598 - val_loss: 0.5519 - val_acc: 0.7604\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7603 - val_loss: 0.5507 - val_acc: 0.7609\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54989 to 0.54985, saving model to best.model\n",
      "0s - loss: 0.5508 - acc: 0.7604 - val_loss: 0.5498 - val_acc: 0.7609\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54985 to 0.54918, saving model to best.model\n",
      "0s - loss: 0.5525 - acc: 0.7602 - val_loss: 0.5492 - val_acc: 0.7609\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7604 - val_loss: 0.5501 - val_acc: 0.7609\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54918 to 0.54904, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7598 - val_loss: 0.5490 - val_acc: 0.7609\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54904 to 0.54878, saving model to best.model\n",
      "0s - loss: 0.5504 - acc: 0.7607 - val_loss: 0.5488 - val_acc: 0.7612\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7605 - val_loss: 0.5488 - val_acc: 0.7612\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.54878 to 0.54873, saving model to best.model\n",
      "0s - loss: 0.5499 - acc: 0.7610 - val_loss: 0.5487 - val_acc: 0.7612\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7606 - val_loss: 0.5493 - val_acc: 0.7612\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7609 - val_loss: 0.5488 - val_acc: 0.7612\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54873 to 0.54873, saving model to best.model\n",
      "0s - loss: 0.5510 - acc: 0.7608 - val_loss: 0.5487 - val_acc: 0.7612\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.54873 to 0.54863, saving model to best.model\n",
      "0s - loss: 0.5497 - acc: 0.7609 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7612 - val_loss: 0.5487 - val_acc: 0.7612\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7609 - val_loss: 0.5491 - val_acc: 0.7612\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.54863 to 0.54851, saving model to best.model\n",
      "0s - loss: 0.5500 - acc: 0.7613 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.54851 to 0.54850, saving model to best.model\n",
      "0s - loss: 0.5494 - acc: 0.7613 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7612 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.54850 to 0.54848, saving model to best.model\n",
      "0s - loss: 0.5494 - acc: 0.7613 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7617 - val_loss: 0.5487 - val_acc: 0.7612\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7613 - val_loss: 0.5492 - val_acc: 0.7612\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7611 - val_loss: 0.5494 - val_acc: 0.7612\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.54848 to 0.54843, saving model to best.model\n",
      "0s - loss: 0.5493 - acc: 0.7613 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7614 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5494 - acc: 0.7612 - val_loss: 0.5497 - val_acc: 0.7612\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7614 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7613 - val_loss: 0.5489 - val_acc: 0.7612\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.54843 to 0.54841, saving model to best.model\n",
      "0s - loss: 0.5498 - acc: 0.7613 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.54841 to 0.54841, saving model to best.model\n",
      "0s - loss: 0.5487 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.54841 to 0.54833, saving model to best.model\n",
      "0s - loss: 0.5494 - acc: 0.7613 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7613 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.54833 to 0.54831, saving model to best.model\n",
      "0s - loss: 0.5487 - acc: 0.7615 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7615 - val_loss: 0.5488 - val_acc: 0.7612\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7616 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7616 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7609 - val_loss: 0.5488 - val_acc: 0.7612\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7611 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7615 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7616 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7615 - val_loss: 0.5487 - val_acc: 0.7612\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7615 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7614 - val_loss: 0.5489 - val_acc: 0.7612\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7612 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.54831 to 0.54826, saving model to best.model\n",
      "0s - loss: 0.5483 - acc: 0.7615 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7612 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7615 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7616 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7612 - val_loss: 0.5489 - val_acc: 0.7612\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7618 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7612 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7615 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7612 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7618 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7614 - val_loss: 0.5487 - val_acc: 0.7612\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7614 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7617 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7617\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7615 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7614 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.54826 to 0.54821, saving model to best.model\n",
      "0s - loss: 0.5486 - acc: 0.7613 - val_loss: 0.5482 - val_acc: 0.7612\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.54821 to 0.54821, saving model to best.model\n",
      "0s - loss: 0.5481 - acc: 0.7615 - val_loss: 0.5482 - val_acc: 0.7617\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7612 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7613 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.54821 to 0.54818, saving model to best.model\n",
      "0s - loss: 0.5485 - acc: 0.7613 - val_loss: 0.5482 - val_acc: 0.7612\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7613 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7617 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7614 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7615 - val_loss: 0.5485 - val_acc: 0.7612\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7614 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7612 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7616 - val_loss: 0.5482 - val_acc: 0.7612\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7613 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7616 - val_loss: 0.5482 - val_acc: 0.7612\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7612 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7613 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7613 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7615 - val_loss: 0.5482 - val_acc: 0.7617\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.54818 to 0.54809, saving model to best.model\n",
      "0s - loss: 0.5481 - acc: 0.7613 - val_loss: 0.5481 - val_acc: 0.7617\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7616 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7615 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7612 - val_loss: 0.5482 - val_acc: 0.7612\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7615 - val_loss: 0.5481 - val_acc: 0.7617\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7612 - val_loss: 0.5482 - val_acc: 0.7617\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7616 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7616 - val_loss: 0.5481 - val_acc: 0.7617\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7617 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7612 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7613 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7615 - val_loss: 0.5481 - val_acc: 0.7617\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7612 - val_loss: 0.5482 - val_acc: 0.7612\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7618 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7613 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7616 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7615 - val_loss: 0.5482 - val_acc: 0.7612\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7613 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7616 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7615 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7614 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7613 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7617 - val_loss: 0.5482 - val_acc: 0.7612\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7616 - val_loss: 0.5483 - val_acc: 0.7612\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55316, saving model to best.model\n",
      "0s - loss: 0.6965 - acc: 0.6890 - val_loss: 0.5532 - val_acc: 0.7624\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55316 to 0.54769, saving model to best.model\n",
      "0s - loss: 0.5952 - acc: 0.7377 - val_loss: 0.5477 - val_acc: 0.7624\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.54769 to 0.54644, saving model to best.model\n",
      "0s - loss: 0.5744 - acc: 0.7556 - val_loss: 0.5464 - val_acc: 0.7624\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5605 - acc: 0.7607 - val_loss: 0.5475 - val_acc: 0.7624\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5587 - acc: 0.7617 - val_loss: 0.5477 - val_acc: 0.7624\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5547 - acc: 0.7620 - val_loss: 0.5475 - val_acc: 0.7624\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7618 - val_loss: 0.5477 - val_acc: 0.7624\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5539 - acc: 0.7620 - val_loss: 0.5480 - val_acc: 0.7624\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7620 - val_loss: 0.5480 - val_acc: 0.7624\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7620 - val_loss: 0.5491 - val_acc: 0.7624\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7620 - val_loss: 0.5484 - val_acc: 0.7624\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7620 - val_loss: 0.5471 - val_acc: 0.7624\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7620 - val_loss: 0.5472 - val_acc: 0.7624\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.54644 to 0.54640, saving model to best.model\n",
      "0s - loss: 0.5509 - acc: 0.7620 - val_loss: 0.5464 - val_acc: 0.7624\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7620 - val_loss: 0.5480 - val_acc: 0.7624\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7620 - val_loss: 0.5480 - val_acc: 0.7624\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7620 - val_loss: 0.5476 - val_acc: 0.7624\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7620 - val_loss: 0.5488 - val_acc: 0.7624\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7620 - val_loss: 0.5474 - val_acc: 0.7624\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7620 - val_loss: 0.5473 - val_acc: 0.7624\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7620 - val_loss: 0.5471 - val_acc: 0.7624\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7620 - val_loss: 0.5482 - val_acc: 0.7624\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7620 - val_loss: 0.5478 - val_acc: 0.7624\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7621 - val_loss: 0.5467 - val_acc: 0.7624\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54640 to 0.54624, saving model to best.model\n",
      "0s - loss: 0.5493 - acc: 0.7622 - val_loss: 0.5462 - val_acc: 0.7624\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54624 to 0.54579, saving model to best.model\n",
      "0s - loss: 0.5496 - acc: 0.7630 - val_loss: 0.5458 - val_acc: 0.7629\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7638 - val_loss: 0.5468 - val_acc: 0.7648\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54579 to 0.54513, saving model to best.model\n",
      "0s - loss: 0.5470 - acc: 0.7642 - val_loss: 0.5451 - val_acc: 0.7648\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54513 to 0.54402, saving model to best.model\n",
      "0s - loss: 0.5462 - acc: 0.7649 - val_loss: 0.5440 - val_acc: 0.7648\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5466 - acc: 0.7655 - val_loss: 0.5447 - val_acc: 0.7648\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54402 to 0.54307, saving model to best.model\n",
      "0s - loss: 0.5463 - acc: 0.7659 - val_loss: 0.5431 - val_acc: 0.7653\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54307 to 0.54281, saving model to best.model\n",
      "0s - loss: 0.5444 - acc: 0.7666 - val_loss: 0.5428 - val_acc: 0.7653\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7664 - val_loss: 0.5432 - val_acc: 0.7653\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7663 - val_loss: 0.5445 - val_acc: 0.7665\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54281 to 0.54176, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7673 - val_loss: 0.5418 - val_acc: 0.7665\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.54176 to 0.54146, saving model to best.model\n",
      "0s - loss: 0.5443 - acc: 0.7672 - val_loss: 0.5415 - val_acc: 0.7682\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54146 to 0.54130, saving model to best.model\n",
      "0s - loss: 0.5436 - acc: 0.7671 - val_loss: 0.5413 - val_acc: 0.7682\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54130 to 0.54122, saving model to best.model\n",
      "0s - loss: 0.5435 - acc: 0.7679 - val_loss: 0.5412 - val_acc: 0.7682\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7703 - val_loss: 0.5414 - val_acc: 0.7682\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7674 - val_loss: 0.5413 - val_acc: 0.7682\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.54122 to 0.54109, saving model to best.model\n",
      "0s - loss: 0.5427 - acc: 0.7683 - val_loss: 0.5411 - val_acc: 0.7682\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.54109 to 0.54088, saving model to best.model\n",
      "0s - loss: 0.5425 - acc: 0.7681 - val_loss: 0.5409 - val_acc: 0.7682\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54088 to 0.54074, saving model to best.model\n",
      "0s - loss: 0.5428 - acc: 0.7684 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.54074 to 0.54060, saving model to best.model\n",
      "0s - loss: 0.5431 - acc: 0.7681 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7678 - val_loss: 0.5410 - val_acc: 0.7682\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.54060 to 0.54059, saving model to best.model\n",
      "0s - loss: 0.5424 - acc: 0.7681 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7678 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7686 - val_loss: 0.5409 - val_acc: 0.7682\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.54059 to 0.54048, saving model to best.model\n",
      "0s - loss: 0.5416 - acc: 0.7683 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.54048 to 0.54044, saving model to best.model\n",
      "0s - loss: 0.5416 - acc: 0.7686 - val_loss: 0.5404 - val_acc: 0.7682\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.54044 to 0.54042, saving model to best.model\n",
      "0s - loss: 0.5423 - acc: 0.7684 - val_loss: 0.5404 - val_acc: 0.7682\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7681 - val_loss: 0.5415 - val_acc: 0.7682\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7689 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7686 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7680 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7683 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7684 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7685 - val_loss: 0.5412 - val_acc: 0.7682\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7684 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7681 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7686 - val_loss: 0.5404 - val_acc: 0.7682\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7689 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7688 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.54042 to 0.54041, saving model to best.model\n",
      "0s - loss: 0.5413 - acc: 0.7688 - val_loss: 0.5404 - val_acc: 0.7682\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7687 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7689 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7686 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7691 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7689 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7689 - val_loss: 0.5418 - val_acc: 0.7682\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7688 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7694 - val_loss: 0.5422 - val_acc: 0.7682\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7684 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7685 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7685 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7681 - val_loss: 0.5418 - val_acc: 0.7682\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7682 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7685 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7686 - val_loss: 0.5414 - val_acc: 0.7682\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7690 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7688 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7686 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7681 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7691 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7686 - val_loss: 0.5409 - val_acc: 0.7682\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.54041 to 0.54041, saving model to best.model\n",
      "0s - loss: 0.5403 - acc: 0.7686 - val_loss: 0.5404 - val_acc: 0.7682\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7691 - val_loss: 0.5405 - val_acc: 0.7682\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7686 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7686 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7687 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7685 - val_loss: 0.5410 - val_acc: 0.7682\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7690 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7685 - val_loss: 0.5411 - val_acc: 0.7682\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7685 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7688 - val_loss: 0.5412 - val_acc: 0.7682\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7687 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7691 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7683 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7684 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7682 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7687 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7684 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7687 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7683 - val_loss: 0.5406 - val_acc: 0.7682\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7686 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7692 - val_loss: 0.5410 - val_acc: 0.7682\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7691 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7686 - val_loss: 0.5409 - val_acc: 0.7677\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7686 - val_loss: 0.5410 - val_acc: 0.7672\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7683 - val_loss: 0.5420 - val_acc: 0.7665\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7676 - val_loss: 0.5420 - val_acc: 0.7665\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.56882, saving model to best.model\n",
      "0s - loss: 0.6197 - acc: 0.7210 - val_loss: 0.5688 - val_acc: 0.7444\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5779 - acc: 0.7494 - val_loss: 0.5704 - val_acc: 0.7444\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5688 - acc: 0.7537 - val_loss: 0.5691 - val_acc: 0.7444\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.56882 to 0.56824, saving model to best.model\n",
      "0s - loss: 0.5651 - acc: 0.7538 - val_loss: 0.5682 - val_acc: 0.7444\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5623 - acc: 0.7538 - val_loss: 0.5683 - val_acc: 0.7444\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.56824 to 0.56807, saving model to best.model\n",
      "0s - loss: 0.5630 - acc: 0.7538 - val_loss: 0.5681 - val_acc: 0.7444\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.56807 to 0.56766, saving model to best.model\n",
      "0s - loss: 0.5613 - acc: 0.7538 - val_loss: 0.5677 - val_acc: 0.7444\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7538 - val_loss: 0.5680 - val_acc: 0.7444\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.56766 to 0.56732, saving model to best.model\n",
      "0s - loss: 0.5597 - acc: 0.7538 - val_loss: 0.5673 - val_acc: 0.7444\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.56732 to 0.56729, saving model to best.model\n",
      "0s - loss: 0.5607 - acc: 0.7538 - val_loss: 0.5673 - val_acc: 0.7444\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.56729 to 0.56707, saving model to best.model\n",
      "0s - loss: 0.5600 - acc: 0.7538 - val_loss: 0.5671 - val_acc: 0.7444\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5598 - acc: 0.7538 - val_loss: 0.5677 - val_acc: 0.7444\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.56707 to 0.56665, saving model to best.model\n",
      "0s - loss: 0.5597 - acc: 0.7538 - val_loss: 0.5666 - val_acc: 0.7444\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5592 - acc: 0.7538 - val_loss: 0.5675 - val_acc: 0.7444\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.56665 to 0.56585, saving model to best.model\n",
      "0s - loss: 0.5591 - acc: 0.7538 - val_loss: 0.5659 - val_acc: 0.7444\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5588 - acc: 0.7541 - val_loss: 0.5659 - val_acc: 0.7444\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.56585 to 0.56533, saving model to best.model\n",
      "0s - loss: 0.5579 - acc: 0.7541 - val_loss: 0.5653 - val_acc: 0.7444\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5610 - acc: 0.7541 - val_loss: 0.5665 - val_acc: 0.7444\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5593 - acc: 0.7541 - val_loss: 0.5666 - val_acc: 0.7447\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.56533 to 0.56519, saving model to best.model\n",
      "0s - loss: 0.5577 - acc: 0.7542 - val_loss: 0.5652 - val_acc: 0.7449\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.56519 to 0.56516, saving model to best.model\n",
      "0s - loss: 0.5586 - acc: 0.7545 - val_loss: 0.5652 - val_acc: 0.7454\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.56516 to 0.56424, saving model to best.model\n",
      "0s - loss: 0.5574 - acc: 0.7555 - val_loss: 0.5642 - val_acc: 0.7464\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.56424 to 0.56344, saving model to best.model\n",
      "0s - loss: 0.5565 - acc: 0.7559 - val_loss: 0.5634 - val_acc: 0.7473\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7575 - val_loss: 0.5635 - val_acc: 0.7490\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.56344 to 0.56155, saving model to best.model\n",
      "0s - loss: 0.5552 - acc: 0.7578 - val_loss: 0.5615 - val_acc: 0.7490\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7590 - val_loss: 0.5620 - val_acc: 0.7490\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.56155 to 0.56119, saving model to best.model\n",
      "0s - loss: 0.5551 - acc: 0.7584 - val_loss: 0.5612 - val_acc: 0.7490\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.56119 to 0.56107, saving model to best.model\n",
      "0s - loss: 0.5556 - acc: 0.7578 - val_loss: 0.5611 - val_acc: 0.7490\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7592 - val_loss: 0.5611 - val_acc: 0.7490\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.56107 to 0.56029, saving model to best.model\n",
      "0s - loss: 0.5528 - acc: 0.7593 - val_loss: 0.5603 - val_acc: 0.7524\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7600 - val_loss: 0.5624 - val_acc: 0.7524\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.56029 to 0.55931, saving model to best.model\n",
      "0s - loss: 0.5539 - acc: 0.7599 - val_loss: 0.5593 - val_acc: 0.7524\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.55931 to 0.55877, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7606 - val_loss: 0.5588 - val_acc: 0.7524\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7609 - val_loss: 0.5603 - val_acc: 0.7524\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.55877 to 0.55856, saving model to best.model\n",
      "0s - loss: 0.5511 - acc: 0.7606 - val_loss: 0.5586 - val_acc: 0.7524\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7607 - val_loss: 0.5605 - val_acc: 0.7524\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7604 - val_loss: 0.5606 - val_acc: 0.7524\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.55856 to 0.55839, saving model to best.model\n",
      "0s - loss: 0.5507 - acc: 0.7608 - val_loss: 0.5584 - val_acc: 0.7524\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7610 - val_loss: 0.5593 - val_acc: 0.7524\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7612 - val_loss: 0.5589 - val_acc: 0.7524\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.55839 to 0.55823, saving model to best.model\n",
      "0s - loss: 0.5512 - acc: 0.7604 - val_loss: 0.5582 - val_acc: 0.7524\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7611 - val_loss: 0.5583 - val_acc: 0.7524\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7609 - val_loss: 0.5583 - val_acc: 0.7524\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7612 - val_loss: 0.5587 - val_acc: 0.7524\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.55823 to 0.55821, saving model to best.model\n",
      "0s - loss: 0.5499 - acc: 0.7608 - val_loss: 0.5582 - val_acc: 0.7524\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7610 - val_loss: 0.5585 - val_acc: 0.7524\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5502 - acc: 0.7612 - val_loss: 0.5589 - val_acc: 0.7524\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7610 - val_loss: 0.5598 - val_acc: 0.7524\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.55821 to 0.55811, saving model to best.model\n",
      "0s - loss: 0.5508 - acc: 0.7610 - val_loss: 0.5581 - val_acc: 0.7524\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7609 - val_loss: 0.5583 - val_acc: 0.7524\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7606 - val_loss: 0.5581 - val_acc: 0.7524\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.55811 to 0.55786, saving model to best.model\n",
      "0s - loss: 0.5500 - acc: 0.7609 - val_loss: 0.5579 - val_acc: 0.7527\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7608 - val_loss: 0.5579 - val_acc: 0.7524\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7609 - val_loss: 0.5579 - val_acc: 0.7524\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7609 - val_loss: 0.5600 - val_acc: 0.7524\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7607 - val_loss: 0.5581 - val_acc: 0.7524\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7610 - val_loss: 0.5597 - val_acc: 0.7524\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7610 - val_loss: 0.5580 - val_acc: 0.7524\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7610 - val_loss: 0.5580 - val_acc: 0.7524\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7611 - val_loss: 0.5579 - val_acc: 0.7524\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7609 - val_loss: 0.5584 - val_acc: 0.7527\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7610 - val_loss: 0.5583 - val_acc: 0.7524\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7612 - val_loss: 0.5595 - val_acc: 0.7524\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7608 - val_loss: 0.5590 - val_acc: 0.7524\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7614 - val_loss: 0.5580 - val_acc: 0.7524\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5494 - acc: 0.7610 - val_loss: 0.5579 - val_acc: 0.7527\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7610 - val_loss: 0.5581 - val_acc: 0.7524\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7611 - val_loss: 0.5579 - val_acc: 0.7527\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7609 - val_loss: 0.5587 - val_acc: 0.7524\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7610 - val_loss: 0.5582 - val_acc: 0.7524\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7613 - val_loss: 0.5584 - val_acc: 0.7524\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.55786 to 0.55783, saving model to best.model\n",
      "0s - loss: 0.5495 - acc: 0.7610 - val_loss: 0.5578 - val_acc: 0.7527\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7609 - val_loss: 0.5583 - val_acc: 0.7527\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7613 - val_loss: 0.5579 - val_acc: 0.7524\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7605 - val_loss: 0.5581 - val_acc: 0.7524\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7613 - val_loss: 0.5581 - val_acc: 0.7527\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7612 - val_loss: 0.5581 - val_acc: 0.7524\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7613 - val_loss: 0.5584 - val_acc: 0.7524\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7607 - val_loss: 0.5582 - val_acc: 0.7524\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5501 - acc: 0.7609 - val_loss: 0.5583 - val_acc: 0.7524\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7612 - val_loss: 0.5582 - val_acc: 0.7524\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7613 - val_loss: 0.5583 - val_acc: 0.7524\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7607 - val_loss: 0.5581 - val_acc: 0.7524\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7610 - val_loss: 0.5586 - val_acc: 0.7524\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7608 - val_loss: 0.5586 - val_acc: 0.7524\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7609 - val_loss: 0.5580 - val_acc: 0.7524\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7608 - val_loss: 0.5584 - val_acc: 0.7524\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7607 - val_loss: 0.5582 - val_acc: 0.7524\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7612 - val_loss: 0.5582 - val_acc: 0.7524\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7612 - val_loss: 0.5582 - val_acc: 0.7524\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7608 - val_loss: 0.5584 - val_acc: 0.7524\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7609 - val_loss: 0.5580 - val_acc: 0.7524\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7612 - val_loss: 0.5584 - val_acc: 0.7524\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7610 - val_loss: 0.5586 - val_acc: 0.7524\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7612 - val_loss: 0.5580 - val_acc: 0.7524\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7608 - val_loss: 0.5584 - val_acc: 0.7524\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7605 - val_loss: 0.5579 - val_acc: 0.7524\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7608 - val_loss: 0.5580 - val_acc: 0.7527\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55221, saving model to best.model\n",
      "0s - loss: 0.6915 - acc: 0.6953 - val_loss: 0.5522 - val_acc: 0.7634\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55221 to 0.54699, saving model to best.model\n",
      "0s - loss: 0.5940 - acc: 0.7432 - val_loss: 0.5470 - val_acc: 0.7634\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5710 - acc: 0.7582 - val_loss: 0.5477 - val_acc: 0.7634\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.54699 to 0.54575, saving model to best.model\n",
      "0s - loss: 0.5580 - acc: 0.7627 - val_loss: 0.5458 - val_acc: 0.7634\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.54575 to 0.54499, saving model to best.model\n",
      "0s - loss: 0.5559 - acc: 0.7641 - val_loss: 0.5450 - val_acc: 0.7634\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7640 - val_loss: 0.5465 - val_acc: 0.7634\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7640 - val_loss: 0.5469 - val_acc: 0.7634\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7641 - val_loss: 0.5467 - val_acc: 0.7634\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7641 - val_loss: 0.5469 - val_acc: 0.7634\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7641 - val_loss: 0.5466 - val_acc: 0.7634\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7641 - val_loss: 0.5466 - val_acc: 0.7634\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7641 - val_loss: 0.5466 - val_acc: 0.7634\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7641 - val_loss: 0.5483 - val_acc: 0.7634\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7641 - val_loss: 0.5465 - val_acc: 0.7634\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7641 - val_loss: 0.5463 - val_acc: 0.7634\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7641 - val_loss: 0.5471 - val_acc: 0.7634\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7644 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7645 - val_loss: 0.5457 - val_acc: 0.7634\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7652 - val_loss: 0.5454 - val_acc: 0.7643\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7655 - val_loss: 0.5451 - val_acc: 0.7643\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7657 - val_loss: 0.5450 - val_acc: 0.7648\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.54499 to 0.54436, saving model to best.model\n",
      "0s - loss: 0.5472 - acc: 0.7656 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.54436 to 0.54399, saving model to best.model\n",
      "0s - loss: 0.5457 - acc: 0.7663 - val_loss: 0.5440 - val_acc: 0.7658\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54399 to 0.54382, saving model to best.model\n",
      "0s - loss: 0.5459 - acc: 0.7664 - val_loss: 0.5438 - val_acc: 0.7660\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54382 to 0.54363, saving model to best.model\n",
      "0s - loss: 0.5448 - acc: 0.7667 - val_loss: 0.5436 - val_acc: 0.7660\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7667 - val_loss: 0.5450 - val_acc: 0.7660\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7671 - val_loss: 0.5452 - val_acc: 0.7660\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54363 to 0.54344, saving model to best.model\n",
      "0s - loss: 0.5455 - acc: 0.7673 - val_loss: 0.5434 - val_acc: 0.7660\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5443 - acc: 0.7673 - val_loss: 0.5437 - val_acc: 0.7660\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54344 to 0.54275, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7675 - val_loss: 0.5428 - val_acc: 0.7660\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5437 - acc: 0.7675 - val_loss: 0.5428 - val_acc: 0.7660\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54275 to 0.54239, saving model to best.model\n",
      "0s - loss: 0.5444 - acc: 0.7674 - val_loss: 0.5424 - val_acc: 0.7660\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7675 - val_loss: 0.5424 - val_acc: 0.7660\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54239 to 0.54230, saving model to best.model\n",
      "0s - loss: 0.5443 - acc: 0.7677 - val_loss: 0.5423 - val_acc: 0.7660\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7681 - val_loss: 0.5428 - val_acc: 0.7660\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.54230 to 0.54199, saving model to best.model\n",
      "0s - loss: 0.5435 - acc: 0.7678 - val_loss: 0.5420 - val_acc: 0.7675\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54199 to 0.54194, saving model to best.model\n",
      "0s - loss: 0.5439 - acc: 0.7680 - val_loss: 0.5419 - val_acc: 0.7675\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54194 to 0.54165, saving model to best.model\n",
      "0s - loss: 0.5430 - acc: 0.7678 - val_loss: 0.5417 - val_acc: 0.7675\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.54165 to 0.54138, saving model to best.model\n",
      "0s - loss: 0.5427 - acc: 0.7679 - val_loss: 0.5414 - val_acc: 0.7675\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7679 - val_loss: 0.5422 - val_acc: 0.7675\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7680 - val_loss: 0.5414 - val_acc: 0.7675\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7681 - val_loss: 0.5415 - val_acc: 0.7675\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54138 to 0.54088, saving model to best.model\n",
      "0s - loss: 0.5428 - acc: 0.7681 - val_loss: 0.5409 - val_acc: 0.7677\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7680 - val_loss: 0.5409 - val_acc: 0.7675\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7679 - val_loss: 0.5412 - val_acc: 0.7675\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7683 - val_loss: 0.5415 - val_acc: 0.7675\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7685 - val_loss: 0.5411 - val_acc: 0.7675\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.54088 to 0.54056, saving model to best.model\n",
      "0s - loss: 0.5426 - acc: 0.7684 - val_loss: 0.5406 - val_acc: 0.7677\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.54056 to 0.54050, saving model to best.model\n",
      "0s - loss: 0.5423 - acc: 0.7683 - val_loss: 0.5405 - val_acc: 0.7687\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.54050 to 0.54045, saving model to best.model\n",
      "0s - loss: 0.5425 - acc: 0.7683 - val_loss: 0.5405 - val_acc: 0.7687\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.54045 to 0.54039, saving model to best.model\n",
      "0s - loss: 0.5418 - acc: 0.7685 - val_loss: 0.5404 - val_acc: 0.7687\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.54039 to 0.54025, saving model to best.model\n",
      "0s - loss: 0.5423 - acc: 0.7683 - val_loss: 0.5402 - val_acc: 0.7687\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.54025 to 0.54016, saving model to best.model\n",
      "0s - loss: 0.5410 - acc: 0.7688 - val_loss: 0.5402 - val_acc: 0.7687\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.54016 to 0.54016, saving model to best.model\n",
      "0s - loss: 0.5417 - acc: 0.7685 - val_loss: 0.5402 - val_acc: 0.7687\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.54016 to 0.54007, saving model to best.model\n",
      "0s - loss: 0.5420 - acc: 0.7687 - val_loss: 0.5401 - val_acc: 0.7687\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.54007 to 0.54003, saving model to best.model\n",
      "0s - loss: 0.5410 - acc: 0.7689 - val_loss: 0.5400 - val_acc: 0.7687\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7686 - val_loss: 0.5407 - val_acc: 0.7687\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.54003 to 0.54002, saving model to best.model\n",
      "0s - loss: 0.5415 - acc: 0.7688 - val_loss: 0.5400 - val_acc: 0.7687\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.54002 to 0.53976, saving model to best.model\n",
      "0s - loss: 0.5411 - acc: 0.7686 - val_loss: 0.5398 - val_acc: 0.7687\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7687 - val_loss: 0.5413 - val_acc: 0.7687\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7687 - val_loss: 0.5400 - val_acc: 0.7687\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.53976 to 0.53969, saving model to best.model\n",
      "0s - loss: 0.5419 - acc: 0.7685 - val_loss: 0.5397 - val_acc: 0.7687\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7685 - val_loss: 0.5399 - val_acc: 0.7687\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7685 - val_loss: 0.5401 - val_acc: 0.7687\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7690 - val_loss: 0.5398 - val_acc: 0.7687\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7685 - val_loss: 0.5403 - val_acc: 0.7687\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7687 - val_loss: 0.5408 - val_acc: 0.7687\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7686 - val_loss: 0.5399 - val_acc: 0.7687\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.53969 to 0.53965, saving model to best.model\n",
      "0s - loss: 0.5405 - acc: 0.7689 - val_loss: 0.5397 - val_acc: 0.7687\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7689 - val_loss: 0.5403 - val_acc: 0.7687\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7688 - val_loss: 0.5397 - val_acc: 0.7687\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7688 - val_loss: 0.5399 - val_acc: 0.7687\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.53965 to 0.53954, saving model to best.model\n",
      "0s - loss: 0.5408 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7688 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7688 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7686 - val_loss: 0.5401 - val_acc: 0.7687\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7688 - val_loss: 0.5398 - val_acc: 0.7687\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7689 - val_loss: 0.5403 - val_acc: 0.7687\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7689 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7685 - val_loss: 0.5405 - val_acc: 0.7687\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7688 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7686 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7690 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7689 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7688 - val_loss: 0.5398 - val_acc: 0.7687\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7687 - val_loss: 0.5397 - val_acc: 0.7687\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7688 - val_loss: 0.5398 - val_acc: 0.7687\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7687 - val_loss: 0.5397 - val_acc: 0.7687\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.53954 to 0.53951, saving model to best.model\n",
      "0s - loss: 0.5404 - acc: 0.7690 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7687 - val_loss: 0.5397 - val_acc: 0.7687\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7689 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7688 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.53951 to 0.53947, saving model to best.model\n",
      "0s - loss: 0.5405 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7690 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7689 - val_loss: 0.5400 - val_acc: 0.7687\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7686 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7688 - val_loss: 0.5399 - val_acc: 0.7687\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7689 - val_loss: 0.5400 - val_acc: 0.7687\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7687 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7688 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7686 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7687 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7688 - val_loss: 0.5396 - val_acc: 0.7687\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.53947 to 0.53946, saving model to best.model\n",
      "0s - loss: 0.5397 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7690 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.53946 to 0.53946, saving model to best.model\n",
      "0s - loss: 0.5399 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7690 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7690 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.53946 to 0.53945, saving model to best.model\n",
      "0s - loss: 0.5395 - acc: 0.7689 - val_loss: 0.5394 - val_acc: 0.7687\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7686 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7687 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7685 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7691 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7686 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7691 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7687 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7687 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7687 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7686 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7690 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7691 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7691 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7687 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7686 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7689 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7687 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55632, saving model to best.model\n",
      "0s - loss: 0.6186 - acc: 0.7259 - val_loss: 0.5563 - val_acc: 0.7578\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55632 to 0.55317, saving model to best.model\n",
      "0s - loss: 0.5695 - acc: 0.7596 - val_loss: 0.5532 - val_acc: 0.7578\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55317 to 0.55313, saving model to best.model\n",
      "0s - loss: 0.5582 - acc: 0.7618 - val_loss: 0.5531 - val_acc: 0.7578\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5541 - acc: 0.7621 - val_loss: 0.5553 - val_acc: 0.7578\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7621 - val_loss: 0.5535 - val_acc: 0.7578\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.55313 to 0.55308, saving model to best.model\n",
      "0s - loss: 0.5518 - acc: 0.7621 - val_loss: 0.5531 - val_acc: 0.7578\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7621 - val_loss: 0.5531 - val_acc: 0.7578\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.55308 to 0.55306, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7621 - val_loss: 0.5531 - val_acc: 0.7578\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7621 - val_loss: 0.5567 - val_acc: 0.7578\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.55306 to 0.55259, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7621 - val_loss: 0.5526 - val_acc: 0.7578\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.55259 to 0.55235, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7621 - val_loss: 0.5523 - val_acc: 0.7578\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.55235 to 0.55181, saving model to best.model\n",
      "0s - loss: 0.5496 - acc: 0.7621 - val_loss: 0.5518 - val_acc: 0.7578\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.55181 to 0.55161, saving model to best.model\n",
      "0s - loss: 0.5497 - acc: 0.7623 - val_loss: 0.5516 - val_acc: 0.7578\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.55161 to 0.55131, saving model to best.model\n",
      "0s - loss: 0.5493 - acc: 0.7623 - val_loss: 0.5513 - val_acc: 0.7578\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.55131 to 0.54961, saving model to best.model\n",
      "0s - loss: 0.5480 - acc: 0.7630 - val_loss: 0.5496 - val_acc: 0.7583\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7637 - val_loss: 0.5517 - val_acc: 0.7595\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7646 - val_loss: 0.5501 - val_acc: 0.7600\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.54961 to 0.54875, saving model to best.model\n",
      "0s - loss: 0.5467 - acc: 0.7655 - val_loss: 0.5488 - val_acc: 0.7612\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7657 - val_loss: 0.5498 - val_acc: 0.7614\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7661 - val_loss: 0.5510 - val_acc: 0.7614\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.54875 to 0.54743, saving model to best.model\n",
      "0s - loss: 0.5455 - acc: 0.7666 - val_loss: 0.5474 - val_acc: 0.7617\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.54743 to 0.54710, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7672 - val_loss: 0.5471 - val_acc: 0.7626\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7676 - val_loss: 0.5474 - val_acc: 0.7626\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54710 to 0.54677, saving model to best.model\n",
      "0s - loss: 0.5451 - acc: 0.7680 - val_loss: 0.5468 - val_acc: 0.7626\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54677 to 0.54607, saving model to best.model\n",
      "0s - loss: 0.5435 - acc: 0.7677 - val_loss: 0.5461 - val_acc: 0.7631\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54607 to 0.54442, saving model to best.model\n",
      "0s - loss: 0.5428 - acc: 0.7683 - val_loss: 0.5444 - val_acc: 0.7631\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54442 to 0.54411, saving model to best.model\n",
      "0s - loss: 0.5421 - acc: 0.7691 - val_loss: 0.5441 - val_acc: 0.7653\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54411 to 0.54384, saving model to best.model\n",
      "0s - loss: 0.5423 - acc: 0.7689 - val_loss: 0.5438 - val_acc: 0.7660\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7702 - val_loss: 0.5477 - val_acc: 0.7660\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54384 to 0.54295, saving model to best.model\n",
      "0s - loss: 0.5410 - acc: 0.7700 - val_loss: 0.5430 - val_acc: 0.7660\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7712 - val_loss: 0.5431 - val_acc: 0.7660\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54295 to 0.54267, saving model to best.model\n",
      "0s - loss: 0.5404 - acc: 0.7708 - val_loss: 0.5427 - val_acc: 0.7660\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7709 - val_loss: 0.5427 - val_acc: 0.7660\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7709 - val_loss: 0.5441 - val_acc: 0.7660\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7706 - val_loss: 0.5437 - val_acc: 0.7655\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7702 - val_loss: 0.5442 - val_acc: 0.7655\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7706 - val_loss: 0.5452 - val_acc: 0.7655\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7701 - val_loss: 0.5445 - val_acc: 0.7655\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7703 - val_loss: 0.5435 - val_acc: 0.7655\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7702 - val_loss: 0.5447 - val_acc: 0.7655\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7699 - val_loss: 0.5436 - val_acc: 0.7655\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7706 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7703 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7705 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7709 - val_loss: 0.5444 - val_acc: 0.7655\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7700 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7700 - val_loss: 0.5438 - val_acc: 0.7655\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7705 - val_loss: 0.5438 - val_acc: 0.7655\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7702 - val_loss: 0.5439 - val_acc: 0.7655\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7699 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7703 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7705 - val_loss: 0.5441 - val_acc: 0.7655\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7705 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7695 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7704 - val_loss: 0.5436 - val_acc: 0.7655\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7704 - val_loss: 0.5436 - val_acc: 0.7655\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.54267 to 0.54221, saving model to best.model\n",
      "0s - loss: 0.5388 - acc: 0.7703 - val_loss: 0.5422 - val_acc: 0.7655\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.54221 to 0.54129, saving model to best.model\n",
      "0s - loss: 0.5364 - acc: 0.7719 - val_loss: 0.5413 - val_acc: 0.7655\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7722 - val_loss: 0.5452 - val_acc: 0.7638\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7694 - val_loss: 0.5458 - val_acc: 0.7631\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7695 - val_loss: 0.5463 - val_acc: 0.7631\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7694 - val_loss: 0.5465 - val_acc: 0.7631\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7686 - val_loss: 0.5461 - val_acc: 0.7631\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7688 - val_loss: 0.5459 - val_acc: 0.7631\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7693 - val_loss: 0.5461 - val_acc: 0.7631\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7691 - val_loss: 0.5463 - val_acc: 0.7631\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7694 - val_loss: 0.5460 - val_acc: 0.7631\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7695 - val_loss: 0.5465 - val_acc: 0.7631\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7695 - val_loss: 0.5460 - val_acc: 0.7631\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7688 - val_loss: 0.5457 - val_acc: 0.7631\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7697 - val_loss: 0.5464 - val_acc: 0.7631\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7692 - val_loss: 0.5455 - val_acc: 0.7631\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7692 - val_loss: 0.5459 - val_acc: 0.7631\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7694 - val_loss: 0.5457 - val_acc: 0.7631\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7697 - val_loss: 0.5458 - val_acc: 0.7631\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7691 - val_loss: 0.5458 - val_acc: 0.7631\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7692 - val_loss: 0.5455 - val_acc: 0.7631\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7691 - val_loss: 0.5459 - val_acc: 0.7631\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7694 - val_loss: 0.5454 - val_acc: 0.7631\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7697 - val_loss: 0.5455 - val_acc: 0.7631\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7692 - val_loss: 0.5454 - val_acc: 0.7631\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7691 - val_loss: 0.5459 - val_acc: 0.7631\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7691 - val_loss: 0.5459 - val_acc: 0.7631\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7690 - val_loss: 0.5460 - val_acc: 0.7631\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54017, saving model to best.model\n",
      "0s - loss: 0.6583 - acc: 0.7027 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5873 - acc: 0.7421 - val_loss: 0.5405 - val_acc: 0.7672\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.54017 to 0.53926, saving model to best.model\n",
      "0s - loss: 0.5686 - acc: 0.7548 - val_loss: 0.5393 - val_acc: 0.7672\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7599 - val_loss: 0.5409 - val_acc: 0.7672\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5597 - acc: 0.7602 - val_loss: 0.5410 - val_acc: 0.7672\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5570 - acc: 0.7601 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7603 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7603 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7604 - val_loss: 0.5403 - val_acc: 0.7672\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7604 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7605 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7606 - val_loss: 0.5397 - val_acc: 0.7672\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.53926 to 0.53656, saving model to best.model\n",
      "0s - loss: 0.5490 - acc: 0.7612 - val_loss: 0.5366 - val_acc: 0.7675\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.53656 to 0.53520, saving model to best.model\n",
      "0s - loss: 0.5490 - acc: 0.7624 - val_loss: 0.5352 - val_acc: 0.7687\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.53520 to 0.53144, saving model to best.model\n",
      "0s - loss: 0.5452 - acc: 0.7632 - val_loss: 0.5314 - val_acc: 0.7711\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.53144 to 0.53000, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7661 - val_loss: 0.5300 - val_acc: 0.7719\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.53000 to 0.52867, saving model to best.model\n",
      "0s - loss: 0.5409 - acc: 0.7665 - val_loss: 0.5287 - val_acc: 0.7719\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.52867 to 0.52719, saving model to best.model\n",
      "0s - loss: 0.5395 - acc: 0.7674 - val_loss: 0.5272 - val_acc: 0.7728\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7623 - val_loss: 0.5391 - val_acc: 0.7687\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7629 - val_loss: 0.5397 - val_acc: 0.7687\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7629 - val_loss: 0.5394 - val_acc: 0.7702\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7637 - val_loss: 0.5391 - val_acc: 0.7709\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7638 - val_loss: 0.5378 - val_acc: 0.7709\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7649 - val_loss: 0.5375 - val_acc: 0.7711\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7648 - val_loss: 0.5375 - val_acc: 0.7711\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5456 - acc: 0.7648 - val_loss: 0.5368 - val_acc: 0.7711\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7655 - val_loss: 0.5369 - val_acc: 0.7719\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7655 - val_loss: 0.5364 - val_acc: 0.7719\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5463 - acc: 0.7663 - val_loss: 0.5368 - val_acc: 0.7719\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7652 - val_loss: 0.5374 - val_acc: 0.7719\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5453 - acc: 0.7658 - val_loss: 0.5364 - val_acc: 0.7719\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7657 - val_loss: 0.5359 - val_acc: 0.7719\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7657 - val_loss: 0.5364 - val_acc: 0.7719\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7662 - val_loss: 0.5357 - val_acc: 0.7719\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7661 - val_loss: 0.5358 - val_acc: 0.7724\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7661 - val_loss: 0.5360 - val_acc: 0.7724\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5443 - acc: 0.7665 - val_loss: 0.5360 - val_acc: 0.7724\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5445 - acc: 0.7664 - val_loss: 0.5356 - val_acc: 0.7724\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5445 - acc: 0.7664 - val_loss: 0.5364 - val_acc: 0.7728\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7662 - val_loss: 0.5359 - val_acc: 0.7724\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5441 - acc: 0.7659 - val_loss: 0.5356 - val_acc: 0.7724\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7663 - val_loss: 0.5352 - val_acc: 0.7728\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5449 - acc: 0.7663 - val_loss: 0.5362 - val_acc: 0.7728\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7664 - val_loss: 0.5357 - val_acc: 0.7728\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54942, saving model to best.model\n",
      "0s - loss: 0.6120 - acc: 0.7287 - val_loss: 0.5494 - val_acc: 0.7612\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54942 to 0.54840, saving model to best.model\n",
      "0s - loss: 0.5709 - acc: 0.7545 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5646 - acc: 0.7561 - val_loss: 0.5488 - val_acc: 0.7612\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5619 - acc: 0.7561 - val_loss: 0.5509 - val_acc: 0.7612\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5594 - acc: 0.7561 - val_loss: 0.5490 - val_acc: 0.7612\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5580 - acc: 0.7561 - val_loss: 0.5497 - val_acc: 0.7612\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5590 - acc: 0.7561 - val_loss: 0.5494 - val_acc: 0.7612\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5576 - acc: 0.7561 - val_loss: 0.5486 - val_acc: 0.7612\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5585 - acc: 0.7561 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5584 - acc: 0.7561 - val_loss: 0.5522 - val_acc: 0.7612\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5587 - acc: 0.7561 - val_loss: 0.5496 - val_acc: 0.7612\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5571 - acc: 0.7561 - val_loss: 0.5490 - val_acc: 0.7612\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.54840 to 0.54836, saving model to best.model\n",
      "0s - loss: 0.5560 - acc: 0.7561 - val_loss: 0.5484 - val_acc: 0.7612\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.54836 to 0.54667, saving model to best.model\n",
      "0s - loss: 0.5558 - acc: 0.7562 - val_loss: 0.5467 - val_acc: 0.7612\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.54667 to 0.54529, saving model to best.model\n",
      "0s - loss: 0.5559 - acc: 0.7562 - val_loss: 0.5453 - val_acc: 0.7612\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.54529 to 0.54187, saving model to best.model\n",
      "0s - loss: 0.5529 - acc: 0.7568 - val_loss: 0.5419 - val_acc: 0.7624\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7583 - val_loss: 0.5449 - val_acc: 0.7626\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7581 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7596 - val_loss: 0.5421 - val_acc: 0.7660\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.54187 to 0.54125, saving model to best.model\n",
      "0s - loss: 0.5522 - acc: 0.7601 - val_loss: 0.5413 - val_acc: 0.7672\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.54125 to 0.53964, saving model to best.model\n",
      "0s - loss: 0.5514 - acc: 0.7613 - val_loss: 0.5396 - val_acc: 0.7672\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.53964 to 0.53872, saving model to best.model\n",
      "0s - loss: 0.5486 - acc: 0.7623 - val_loss: 0.5387 - val_acc: 0.7677\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7633 - val_loss: 0.5389 - val_acc: 0.7677\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.53872 to 0.53864, saving model to best.model\n",
      "0s - loss: 0.5491 - acc: 0.7638 - val_loss: 0.5386 - val_acc: 0.7677\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.53864 to 0.53750, saving model to best.model\n",
      "0s - loss: 0.5474 - acc: 0.7643 - val_loss: 0.5375 - val_acc: 0.7677\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7647 - val_loss: 0.5377 - val_acc: 0.7677\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7650 - val_loss: 0.5378 - val_acc: 0.7685\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.53750 to 0.53708, saving model to best.model\n",
      "0s - loss: 0.5463 - acc: 0.7644 - val_loss: 0.5371 - val_acc: 0.7685\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.53708 to 0.53570, saving model to best.model\n",
      "0s - loss: 0.5462 - acc: 0.7652 - val_loss: 0.5357 - val_acc: 0.7685\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5468 - acc: 0.7647 - val_loss: 0.5377 - val_acc: 0.7685\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7646 - val_loss: 0.5377 - val_acc: 0.7685\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7653 - val_loss: 0.5364 - val_acc: 0.7685\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5458 - acc: 0.7657 - val_loss: 0.5370 - val_acc: 0.7685\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7648 - val_loss: 0.5362 - val_acc: 0.7685\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7652 - val_loss: 0.5370 - val_acc: 0.7685\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5468 - acc: 0.7646 - val_loss: 0.5374 - val_acc: 0.7685\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7656 - val_loss: 0.5369 - val_acc: 0.7685\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5449 - acc: 0.7657 - val_loss: 0.5370 - val_acc: 0.7685\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5449 - acc: 0.7658 - val_loss: 0.5364 - val_acc: 0.7685\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7661 - val_loss: 0.5363 - val_acc: 0.7685\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5456 - acc: 0.7654 - val_loss: 0.5370 - val_acc: 0.7685\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7656 - val_loss: 0.5368 - val_acc: 0.7685\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5447 - acc: 0.7655 - val_loss: 0.5374 - val_acc: 0.7685\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5443 - acc: 0.7659 - val_loss: 0.5364 - val_acc: 0.7685\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5437 - acc: 0.7666 - val_loss: 0.5372 - val_acc: 0.7685\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7654 - val_loss: 0.5358 - val_acc: 0.7685\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7661 - val_loss: 0.5367 - val_acc: 0.7685\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7667 - val_loss: 0.5367 - val_acc: 0.7685\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7652 - val_loss: 0.5370 - val_acc: 0.7685\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5449 - acc: 0.7654 - val_loss: 0.5377 - val_acc: 0.7685\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7657 - val_loss: 0.5365 - val_acc: 0.7685\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7660 - val_loss: 0.5365 - val_acc: 0.7685\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5456 - acc: 0.7651 - val_loss: 0.5368 - val_acc: 0.7685\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.53570 to 0.53501, saving model to best.model\n",
      "0s - loss: 0.5436 - acc: 0.7659 - val_loss: 0.5350 - val_acc: 0.7685\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7657 - val_loss: 0.5358 - val_acc: 0.7685\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7656 - val_loss: 0.5364 - val_acc: 0.7685\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7660 - val_loss: 0.5351 - val_acc: 0.7685\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7660 - val_loss: 0.5365 - val_acc: 0.7685\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7660 - val_loss: 0.5354 - val_acc: 0.7685\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7652 - val_loss: 0.5365 - val_acc: 0.7685\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7664 - val_loss: 0.5372 - val_acc: 0.7685\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5453 - acc: 0.7653 - val_loss: 0.5366 - val_acc: 0.7685\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7661 - val_loss: 0.5374 - val_acc: 0.7685\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7661 - val_loss: 0.5370 - val_acc: 0.7685\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7659 - val_loss: 0.5361 - val_acc: 0.7685\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7658 - val_loss: 0.5361 - val_acc: 0.7685\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7663 - val_loss: 0.5371 - val_acc: 0.7685\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7665 - val_loss: 0.5362 - val_acc: 0.7685\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5447 - acc: 0.7656 - val_loss: 0.5368 - val_acc: 0.7685\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5648 - acc: 0.7534 - val_loss: 0.5494 - val_acc: 0.7672\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7585 - val_loss: 0.5464 - val_acc: 0.7643\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5534 - acc: 0.7585 - val_loss: 0.5459 - val_acc: 0.7648\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7596 - val_loss: 0.5445 - val_acc: 0.7660\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7603 - val_loss: 0.5429 - val_acc: 0.7665\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5501 - acc: 0.7605 - val_loss: 0.5421 - val_acc: 0.7665\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7610 - val_loss: 0.5436 - val_acc: 0.7672\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7615 - val_loss: 0.5423 - val_acc: 0.7672\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7622 - val_loss: 0.5413 - val_acc: 0.7672\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7625 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7628 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55914, saving model to best.model\n",
      "0s - loss: 0.6011 - acc: 0.7374 - val_loss: 0.5591 - val_acc: 0.7522\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55914 to 0.55840, saving model to best.model\n",
      "0s - loss: 0.5628 - acc: 0.7599 - val_loss: 0.5584 - val_acc: 0.7522\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5563 - acc: 0.7626 - val_loss: 0.5596 - val_acc: 0.7522\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7626 - val_loss: 0.5591 - val_acc: 0.7522\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7626 - val_loss: 0.5592 - val_acc: 0.7522\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5502 - acc: 0.7626 - val_loss: 0.5600 - val_acc: 0.7522\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7626 - val_loss: 0.5589 - val_acc: 0.7522\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7626 - val_loss: 0.5606 - val_acc: 0.7522\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.55840 to 0.55822, saving model to best.model\n",
      "0s - loss: 0.5509 - acc: 0.7626 - val_loss: 0.5582 - val_acc: 0.7522\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.55822 to 0.55551, saving model to best.model\n",
      "0s - loss: 0.5485 - acc: 0.7626 - val_loss: 0.5555 - val_acc: 0.7522\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.55551 to 0.55382, saving model to best.model\n",
      "0s - loss: 0.5475 - acc: 0.7627 - val_loss: 0.5538 - val_acc: 0.7522\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.55382 to 0.55298, saving model to best.model\n",
      "0s - loss: 0.5483 - acc: 0.7628 - val_loss: 0.5530 - val_acc: 0.7522\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.55298 to 0.54941, saving model to best.model\n",
      "0s - loss: 0.5434 - acc: 0.7632 - val_loss: 0.5494 - val_acc: 0.7527\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5466 - acc: 0.7656 - val_loss: 0.5582 - val_acc: 0.7527\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7632 - val_loss: 0.5571 - val_acc: 0.7522\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7631 - val_loss: 0.5570 - val_acc: 0.7527\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7633 - val_loss: 0.5575 - val_acc: 0.7534\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7632 - val_loss: 0.5578 - val_acc: 0.7527\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7638 - val_loss: 0.5577 - val_acc: 0.7534\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7640 - val_loss: 0.5574 - val_acc: 0.7541\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7648 - val_loss: 0.5560 - val_acc: 0.7546\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7654 - val_loss: 0.5551 - val_acc: 0.7551\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7657 - val_loss: 0.5553 - val_acc: 0.7566\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5463 - acc: 0.7658 - val_loss: 0.5542 - val_acc: 0.7566\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7666 - val_loss: 0.5535 - val_acc: 0.7570\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5451 - acc: 0.7669 - val_loss: 0.5547 - val_acc: 0.7570\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7667 - val_loss: 0.5528 - val_acc: 0.7580\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7673 - val_loss: 0.5534 - val_acc: 0.7580\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7678 - val_loss: 0.5533 - val_acc: 0.7580\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7680 - val_loss: 0.5522 - val_acc: 0.7580\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7684 - val_loss: 0.5519 - val_acc: 0.7580\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7682 - val_loss: 0.5517 - val_acc: 0.7580\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7679 - val_loss: 0.5518 - val_acc: 0.7580\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7683 - val_loss: 0.5531 - val_acc: 0.7590\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7617 - val_loss: 0.5577 - val_acc: 0.7546\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5468 - acc: 0.7651 - val_loss: 0.5575 - val_acc: 0.7551\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7663 - val_loss: 0.5540 - val_acc: 0.7566\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7669 - val_loss: 0.5539 - val_acc: 0.7566\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7666 - val_loss: 0.5537 - val_acc: 0.7566\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54974, saving model to best.model\n",
      "0s - loss: 0.6623 - acc: 0.6959 - val_loss: 0.5497 - val_acc: 0.7663\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54974 to 0.54321, saving model to best.model\n",
      "0s - loss: 0.5964 - acc: 0.7345 - val_loss: 0.5432 - val_acc: 0.7663\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5782 - acc: 0.7485 - val_loss: 0.5440 - val_acc: 0.7663\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5703 - acc: 0.7508 - val_loss: 0.5452 - val_acc: 0.7663\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5690 - acc: 0.7511 - val_loss: 0.5452 - val_acc: 0.7663\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5669 - acc: 0.7511 - val_loss: 0.5456 - val_acc: 0.7663\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5648 - acc: 0.7511 - val_loss: 0.5438 - val_acc: 0.7663\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5639 - acc: 0.7511 - val_loss: 0.5448 - val_acc: 0.7663\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5643 - acc: 0.7511 - val_loss: 0.5444 - val_acc: 0.7663\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5653 - acc: 0.7511 - val_loss: 0.5456 - val_acc: 0.7663\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.54321 to 0.54268, saving model to best.model\n",
      "0s - loss: 0.5638 - acc: 0.7511 - val_loss: 0.5427 - val_acc: 0.7663\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.54268 to 0.54158, saving model to best.model\n",
      "0s - loss: 0.5624 - acc: 0.7511 - val_loss: 0.5416 - val_acc: 0.7663\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.54158 to 0.53945, saving model to best.model\n",
      "0s - loss: 0.5617 - acc: 0.7511 - val_loss: 0.5395 - val_acc: 0.7663\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5606 - acc: 0.7512 - val_loss: 0.5416 - val_acc: 0.7663\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5614 - acc: 0.7513 - val_loss: 0.5425 - val_acc: 0.7663\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5615 - acc: 0.7513 - val_loss: 0.5439 - val_acc: 0.7663\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5613 - acc: 0.7519 - val_loss: 0.5428 - val_acc: 0.7663\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5606 - acc: 0.7519 - val_loss: 0.5420 - val_acc: 0.7670\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5615 - acc: 0.7529 - val_loss: 0.5398 - val_acc: 0.7672\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5588 - acc: 0.7533 - val_loss: 0.5415 - val_acc: 0.7697\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.53945 to 0.53810, saving model to best.model\n",
      "0s - loss: 0.5575 - acc: 0.7548 - val_loss: 0.5381 - val_acc: 0.7702\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.53810 to 0.53786, saving model to best.model\n",
      "0s - loss: 0.5580 - acc: 0.7559 - val_loss: 0.5379 - val_acc: 0.7702\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.53786 to 0.53710, saving model to best.model\n",
      "0s - loss: 0.5573 - acc: 0.7562 - val_loss: 0.5371 - val_acc: 0.7704\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7569 - val_loss: 0.5371 - val_acc: 0.7704\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.53710 to 0.53568, saving model to best.model\n",
      "0s - loss: 0.5554 - acc: 0.7586 - val_loss: 0.5357 - val_acc: 0.7704\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5552 - acc: 0.7585 - val_loss: 0.5367 - val_acc: 0.7728\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.53568 to 0.53533, saving model to best.model\n",
      "0s - loss: 0.5549 - acc: 0.7582 - val_loss: 0.5353 - val_acc: 0.7728\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5550 - acc: 0.7595 - val_loss: 0.5365 - val_acc: 0.7731\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.53533 to 0.53421, saving model to best.model\n",
      "0s - loss: 0.5538 - acc: 0.7588 - val_loss: 0.5342 - val_acc: 0.7728\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.53421 to 0.53415, saving model to best.model\n",
      "0s - loss: 0.5538 - acc: 0.7597 - val_loss: 0.5341 - val_acc: 0.7731\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7604 - val_loss: 0.5363 - val_acc: 0.7731\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.53415 to 0.53376, saving model to best.model\n",
      "0s - loss: 0.5540 - acc: 0.7593 - val_loss: 0.5338 - val_acc: 0.7731\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7606 - val_loss: 0.5339 - val_acc: 0.7731\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53376 to 0.53356, saving model to best.model\n",
      "0s - loss: 0.5519 - acc: 0.7604 - val_loss: 0.5336 - val_acc: 0.7731\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7598 - val_loss: 0.5345 - val_acc: 0.7731\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7602 - val_loss: 0.5349 - val_acc: 0.7731\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.53356 to 0.53299, saving model to best.model\n",
      "0s - loss: 0.5525 - acc: 0.7601 - val_loss: 0.5330 - val_acc: 0.7731\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7601 - val_loss: 0.5349 - val_acc: 0.7731\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.53299 to 0.53240, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7604 - val_loss: 0.5324 - val_acc: 0.7731\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7612 - val_loss: 0.5335 - val_acc: 0.7731\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7610 - val_loss: 0.5357 - val_acc: 0.7731\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7602 - val_loss: 0.5339 - val_acc: 0.7731\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7604 - val_loss: 0.5346 - val_acc: 0.7731\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7610 - val_loss: 0.5347 - val_acc: 0.7731\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7602 - val_loss: 0.5335 - val_acc: 0.7731\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7609 - val_loss: 0.5345 - val_acc: 0.7731\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7605 - val_loss: 0.5330 - val_acc: 0.7731\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.53240 to 0.53240, saving model to best.model\n",
      "0s - loss: 0.5504 - acc: 0.7604 - val_loss: 0.5324 - val_acc: 0.7731\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.53240 to 0.53238, saving model to best.model\n",
      "0s - loss: 0.5525 - acc: 0.7600 - val_loss: 0.5324 - val_acc: 0.7731\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7607 - val_loss: 0.5355 - val_acc: 0.7731\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7604 - val_loss: 0.5332 - val_acc: 0.7731\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7618 - val_loss: 0.5328 - val_acc: 0.7731\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7604 - val_loss: 0.5332 - val_acc: 0.7731\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7604 - val_loss: 0.5332 - val_acc: 0.7731\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.53238 to 0.53192, saving model to best.model\n",
      "0s - loss: 0.5498 - acc: 0.7613 - val_loss: 0.5319 - val_acc: 0.7731\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.53192 to 0.53177, saving model to best.model\n",
      "0s - loss: 0.5492 - acc: 0.7610 - val_loss: 0.5318 - val_acc: 0.7731\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7607 - val_loss: 0.5326 - val_acc: 0.7731\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7606 - val_loss: 0.5335 - val_acc: 0.7731\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.53177 to 0.53143, saving model to best.model\n",
      "0s - loss: 0.5507 - acc: 0.7605 - val_loss: 0.5314 - val_acc: 0.7731\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7615 - val_loss: 0.5327 - val_acc: 0.7731\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7613 - val_loss: 0.5326 - val_acc: 0.7731\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7610 - val_loss: 0.5329 - val_acc: 0.7731\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7611 - val_loss: 0.5331 - val_acc: 0.7731\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7606 - val_loss: 0.5329 - val_acc: 0.7731\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7607 - val_loss: 0.5328 - val_acc: 0.7731\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7610 - val_loss: 0.5333 - val_acc: 0.7731\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7609 - val_loss: 0.5340 - val_acc: 0.7731\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7603 - val_loss: 0.5320 - val_acc: 0.7731\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7606 - val_loss: 0.5331 - val_acc: 0.7731\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7618 - val_loss: 0.5327 - val_acc: 0.7770\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7603 - val_loss: 0.5324 - val_acc: 0.7731\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7610 - val_loss: 0.5324 - val_acc: 0.7731\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7609 - val_loss: 0.5340 - val_acc: 0.7731\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7618 - val_loss: 0.5320 - val_acc: 0.7731\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7609 - val_loss: 0.5322 - val_acc: 0.7770\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7603 - val_loss: 0.5320 - val_acc: 0.7731\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7613 - val_loss: 0.5315 - val_acc: 0.7731\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7607 - val_loss: 0.5320 - val_acc: 0.7731\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5502 - acc: 0.7606 - val_loss: 0.5317 - val_acc: 0.7731\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7613 - val_loss: 0.5327 - val_acc: 0.7731\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7616 - val_loss: 0.5322 - val_acc: 0.7731\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5502 - acc: 0.7607 - val_loss: 0.5329 - val_acc: 0.7731\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7609 - val_loss: 0.5329 - val_acc: 0.7731\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7607 - val_loss: 0.5319 - val_acc: 0.7770\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7609 - val_loss: 0.5341 - val_acc: 0.7731\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.56196, saving model to best.model\n",
      "0s - loss: 0.6337 - acc: 0.7162 - val_loss: 0.5620 - val_acc: 0.7534\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.56196 to 0.55835, saving model to best.model\n",
      "0s - loss: 0.5829 - acc: 0.7521 - val_loss: 0.5584 - val_acc: 0.7534\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5685 - acc: 0.7571 - val_loss: 0.5587 - val_acc: 0.7534\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.55835 to 0.55815, saving model to best.model\n",
      "0s - loss: 0.5632 - acc: 0.7581 - val_loss: 0.5582 - val_acc: 0.7534\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5598 - acc: 0.7582 - val_loss: 0.5582 - val_acc: 0.7534\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5577 - acc: 0.7582 - val_loss: 0.5582 - val_acc: 0.7534\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5578 - acc: 0.7582 - val_loss: 0.5586 - val_acc: 0.7534\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5574 - acc: 0.7582 - val_loss: 0.5583 - val_acc: 0.7534\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5559 - acc: 0.7582 - val_loss: 0.5587 - val_acc: 0.7534\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7582 - val_loss: 0.5594 - val_acc: 0.7534\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5565 - acc: 0.7582 - val_loss: 0.5583 - val_acc: 0.7534\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.55815 to 0.55749, saving model to best.model\n",
      "0s - loss: 0.5560 - acc: 0.7582 - val_loss: 0.5575 - val_acc: 0.7534\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5559 - acc: 0.7582 - val_loss: 0.5578 - val_acc: 0.7534\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5546 - acc: 0.7582 - val_loss: 0.5586 - val_acc: 0.7534\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5564 - acc: 0.7582 - val_loss: 0.5575 - val_acc: 0.7534\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.55749 to 0.55731, saving model to best.model\n",
      "0s - loss: 0.5556 - acc: 0.7582 - val_loss: 0.5573 - val_acc: 0.7534\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7582 - val_loss: 0.5575 - val_acc: 0.7534\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.55731 to 0.55699, saving model to best.model\n",
      "0s - loss: 0.5538 - acc: 0.7582 - val_loss: 0.5570 - val_acc: 0.7534\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7582 - val_loss: 0.5577 - val_acc: 0.7534\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.55699 to 0.55691, saving model to best.model\n",
      "0s - loss: 0.5557 - acc: 0.7582 - val_loss: 0.5569 - val_acc: 0.7534\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5552 - acc: 0.7582 - val_loss: 0.5573 - val_acc: 0.7534\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.55691 to 0.55652, saving model to best.model\n",
      "0s - loss: 0.5547 - acc: 0.7582 - val_loss: 0.5565 - val_acc: 0.7534\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.55652 to 0.55603, saving model to best.model\n",
      "0s - loss: 0.5537 - acc: 0.7582 - val_loss: 0.5560 - val_acc: 0.7534\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.55603 to 0.55567, saving model to best.model\n",
      "0s - loss: 0.5530 - acc: 0.7582 - val_loss: 0.5557 - val_acc: 0.7534\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.55567 to 0.55538, saving model to best.model\n",
      "0s - loss: 0.5530 - acc: 0.7583 - val_loss: 0.5554 - val_acc: 0.7534\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.55538 to 0.55483, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7588 - val_loss: 0.5548 - val_acc: 0.7536\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.55483 to 0.55413, saving model to best.model\n",
      "0s - loss: 0.5525 - acc: 0.7595 - val_loss: 0.5541 - val_acc: 0.7544\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.55413 to 0.55239, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7597 - val_loss: 0.5524 - val_acc: 0.7556\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7607 - val_loss: 0.5536 - val_acc: 0.7580\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.55239 to 0.55152, saving model to best.model\n",
      "0s - loss: 0.5510 - acc: 0.7615 - val_loss: 0.5515 - val_acc: 0.7583\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.55152 to 0.55044, saving model to best.model\n",
      "0s - loss: 0.5495 - acc: 0.7615 - val_loss: 0.5504 - val_acc: 0.7585\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.55044 to 0.54603, saving model to best.model\n",
      "0s - loss: 0.5480 - acc: 0.7634 - val_loss: 0.5460 - val_acc: 0.7585\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7589 - val_loss: 0.5565 - val_acc: 0.7544\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7604 - val_loss: 0.5540 - val_acc: 0.7556\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7604 - val_loss: 0.5534 - val_acc: 0.7580\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7612 - val_loss: 0.5526 - val_acc: 0.7580\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7554 - val_loss: 0.5580 - val_acc: 0.7539\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7583 - val_loss: 0.5577 - val_acc: 0.7539\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7589 - val_loss: 0.5574 - val_acc: 0.7539\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7588 - val_loss: 0.5571 - val_acc: 0.7541\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7589 - val_loss: 0.5572 - val_acc: 0.7544\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5534 - acc: 0.7589 - val_loss: 0.5566 - val_acc: 0.7544\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7594 - val_loss: 0.5574 - val_acc: 0.7544\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7591 - val_loss: 0.5567 - val_acc: 0.7544\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5534 - acc: 0.7595 - val_loss: 0.5565 - val_acc: 0.7544\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7599 - val_loss: 0.5572 - val_acc: 0.7546\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7593 - val_loss: 0.5559 - val_acc: 0.7549\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7600 - val_loss: 0.5557 - val_acc: 0.7558\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7598 - val_loss: 0.5564 - val_acc: 0.7558\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7599 - val_loss: 0.5550 - val_acc: 0.7556\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7604 - val_loss: 0.5549 - val_acc: 0.7556\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7599 - val_loss: 0.5547 - val_acc: 0.7556\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7607 - val_loss: 0.5548 - val_acc: 0.7578\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7608 - val_loss: 0.5542 - val_acc: 0.7578\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7605 - val_loss: 0.5542 - val_acc: 0.7578\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7606 - val_loss: 0.5538 - val_acc: 0.7578\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7606 - val_loss: 0.5534 - val_acc: 0.7580\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7603 - val_loss: 0.5534 - val_acc: 0.7580\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55663, saving model to best.model\n",
      "0s - loss: 0.6248 - acc: 0.7244 - val_loss: 0.5566 - val_acc: 0.7549\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5717 - acc: 0.7592 - val_loss: 0.5567 - val_acc: 0.7549\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55663 to 0.55594, saving model to best.model\n",
      "0s - loss: 0.5553 - acc: 0.7666 - val_loss: 0.5559 - val_acc: 0.7549\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.55594 to 0.55589, saving model to best.model\n",
      "0s - loss: 0.5512 - acc: 0.7669 - val_loss: 0.5559 - val_acc: 0.7549\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7669 - val_loss: 0.5565 - val_acc: 0.7549\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7669 - val_loss: 0.5563 - val_acc: 0.7549\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5458 - acc: 0.7669 - val_loss: 0.5562 - val_acc: 0.7549\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7669 - val_loss: 0.5561 - val_acc: 0.7549\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7669 - val_loss: 0.5574 - val_acc: 0.7549\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.55589 to 0.55557, saving model to best.model\n",
      "0s - loss: 0.5453 - acc: 0.7669 - val_loss: 0.5556 - val_acc: 0.7549\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.55557 to 0.55399, saving model to best.model\n",
      "0s - loss: 0.5448 - acc: 0.7669 - val_loss: 0.5540 - val_acc: 0.7549\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7669 - val_loss: 0.5557 - val_acc: 0.7549\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7669 - val_loss: 0.5564 - val_acc: 0.7549\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5450 - acc: 0.7669 - val_loss: 0.5561 - val_acc: 0.7549\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7669 - val_loss: 0.5567 - val_acc: 0.7549\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5455 - acc: 0.7669 - val_loss: 0.5562 - val_acc: 0.7549\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5455 - acc: 0.7669 - val_loss: 0.5567 - val_acc: 0.7549\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7669 - val_loss: 0.5565 - val_acc: 0.7549\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7669 - val_loss: 0.5562 - val_acc: 0.7549\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7669 - val_loss: 0.5568 - val_acc: 0.7549\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7669 - val_loss: 0.5561 - val_acc: 0.7549\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5441 - acc: 0.7669 - val_loss: 0.5571 - val_acc: 0.7549\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7669 - val_loss: 0.5560 - val_acc: 0.7549\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7669 - val_loss: 0.5560 - val_acc: 0.7549\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5441 - acc: 0.7669 - val_loss: 0.5562 - val_acc: 0.7549\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7669 - val_loss: 0.5574 - val_acc: 0.7549\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7671 - val_loss: 0.5556 - val_acc: 0.7549\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5447 - acc: 0.7672 - val_loss: 0.5561 - val_acc: 0.7549\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7672 - val_loss: 0.5574 - val_acc: 0.7551\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5441 - acc: 0.7672 - val_loss: 0.5558 - val_acc: 0.7551\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7678 - val_loss: 0.5558 - val_acc: 0.7553\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7678 - val_loss: 0.5564 - val_acc: 0.7553\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7680 - val_loss: 0.5549 - val_acc: 0.7553\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7683 - val_loss: 0.5548 - val_acc: 0.7556\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7686 - val_loss: 0.5562 - val_acc: 0.7558\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7688 - val_loss: 0.5548 - val_acc: 0.7558\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7691 - val_loss: 0.5542 - val_acc: 0.7561\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55165, saving model to best.model\n",
      "0s - loss: 0.6259 - acc: 0.7209 - val_loss: 0.5517 - val_acc: 0.7619\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55165 to 0.54888, saving model to best.model\n",
      "0s - loss: 0.5742 - acc: 0.7524 - val_loss: 0.5489 - val_acc: 0.7619\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.54888 to 0.54851, saving model to best.model\n",
      "0s - loss: 0.5642 - acc: 0.7591 - val_loss: 0.5485 - val_acc: 0.7619\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.54851 to 0.54840, saving model to best.model\n",
      "0s - loss: 0.5586 - acc: 0.7597 - val_loss: 0.5484 - val_acc: 0.7619\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5593 - acc: 0.7598 - val_loss: 0.5507 - val_acc: 0.7619\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7598 - val_loss: 0.5492 - val_acc: 0.7619\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.54840 to 0.54834, saving model to best.model\n",
      "0s - loss: 0.5561 - acc: 0.7598 - val_loss: 0.5483 - val_acc: 0.7619\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.54834 to 0.54779, saving model to best.model\n",
      "0s - loss: 0.5550 - acc: 0.7598 - val_loss: 0.5478 - val_acc: 0.7619\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7598 - val_loss: 0.5486 - val_acc: 0.7619\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7598 - val_loss: 0.5503 - val_acc: 0.7619\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7598 - val_loss: 0.5484 - val_acc: 0.7619\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7598 - val_loss: 0.5490 - val_acc: 0.7619\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5535 - acc: 0.7598 - val_loss: 0.5483 - val_acc: 0.7619\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7598 - val_loss: 0.5484 - val_acc: 0.7619\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7598 - val_loss: 0.5485 - val_acc: 0.7619\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7598 - val_loss: 0.5483 - val_acc: 0.7619\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7598 - val_loss: 0.5483 - val_acc: 0.7619\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7598 - val_loss: 0.5478 - val_acc: 0.7619\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7598 - val_loss: 0.5500 - val_acc: 0.7619\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.54779 to 0.54737, saving model to best.model\n",
      "0s - loss: 0.5525 - acc: 0.7598 - val_loss: 0.5474 - val_acc: 0.7619\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.54737 to 0.54726, saving model to best.model\n",
      "0s - loss: 0.5527 - acc: 0.7598 - val_loss: 0.5473 - val_acc: 0.7619\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.54726 to 0.54723, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7599 - val_loss: 0.5472 - val_acc: 0.7619\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7598 - val_loss: 0.5475 - val_acc: 0.7619\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7603 - val_loss: 0.5474 - val_acc: 0.7629\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54723 to 0.54575, saving model to best.model\n",
      "0s - loss: 0.5512 - acc: 0.7606 - val_loss: 0.5458 - val_acc: 0.7638\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54575 to 0.54486, saving model to best.model\n",
      "0s - loss: 0.5508 - acc: 0.7620 - val_loss: 0.5449 - val_acc: 0.7646\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54486 to 0.54474, saving model to best.model\n",
      "0s - loss: 0.5501 - acc: 0.7620 - val_loss: 0.5447 - val_acc: 0.7655\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54474 to 0.54415, saving model to best.model\n",
      "0s - loss: 0.5493 - acc: 0.7629 - val_loss: 0.5442 - val_acc: 0.7672\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54415 to 0.54292, saving model to best.model\n",
      "0s - loss: 0.5482 - acc: 0.7638 - val_loss: 0.5429 - val_acc: 0.7672\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54292 to 0.54254, saving model to best.model\n",
      "0s - loss: 0.5485 - acc: 0.7637 - val_loss: 0.5425 - val_acc: 0.7672\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54254 to 0.54208, saving model to best.model\n",
      "0s - loss: 0.5486 - acc: 0.7638 - val_loss: 0.5421 - val_acc: 0.7672\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7643 - val_loss: 0.5433 - val_acc: 0.7672\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7644 - val_loss: 0.5421 - val_acc: 0.7672\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7648 - val_loss: 0.5434 - val_acc: 0.7672\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54208 to 0.54144, saving model to best.model\n",
      "0s - loss: 0.5472 - acc: 0.7647 - val_loss: 0.5414 - val_acc: 0.7672\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.54144 to 0.54139, saving model to best.model\n",
      "0s - loss: 0.5470 - acc: 0.7648 - val_loss: 0.5414 - val_acc: 0.7672\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54139 to 0.54131, saving model to best.model\n",
      "0s - loss: 0.5462 - acc: 0.7650 - val_loss: 0.5413 - val_acc: 0.7672\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54131 to 0.54122, saving model to best.model\n",
      "0s - loss: 0.5460 - acc: 0.7652 - val_loss: 0.5412 - val_acc: 0.7672\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7656 - val_loss: 0.5415 - val_acc: 0.7672\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5458 - acc: 0.7655 - val_loss: 0.5412 - val_acc: 0.7672\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.54122 to 0.54089, saving model to best.model\n",
      "0s - loss: 0.5449 - acc: 0.7654 - val_loss: 0.5409 - val_acc: 0.7672\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7656 - val_loss: 0.5412 - val_acc: 0.7672\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54089 to 0.54085, saving model to best.model\n",
      "0s - loss: 0.5454 - acc: 0.7654 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7660 - val_loss: 0.5410 - val_acc: 0.7672\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.54085 to 0.54068, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7658 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7657 - val_loss: 0.5417 - val_acc: 0.7672\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.54068 to 0.54048, saving model to best.model\n",
      "0s - loss: 0.5449 - acc: 0.7656 - val_loss: 0.5405 - val_acc: 0.7672\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7660 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7659 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7657 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7657 - val_loss: 0.5406 - val_acc: 0.7672\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7657 - val_loss: 0.5427 - val_acc: 0.7672\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.54048 to 0.54037, saving model to best.model\n",
      "0s - loss: 0.5444 - acc: 0.7660 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5450 - acc: 0.7660 - val_loss: 0.5405 - val_acc: 0.7672\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7656 - val_loss: 0.5405 - val_acc: 0.7672\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7658 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7664 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5447 - acc: 0.7658 - val_loss: 0.5406 - val_acc: 0.7672\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7663 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.54037 to 0.54016, saving model to best.model\n",
      "0s - loss: 0.5434 - acc: 0.7664 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7658 - val_loss: 0.5409 - val_acc: 0.7672\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7656 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7658 - val_loss: 0.5406 - val_acc: 0.7672\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7661 - val_loss: 0.5403 - val_acc: 0.7672\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7660 - val_loss: 0.5407 - val_acc: 0.7672\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7659 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5447 - acc: 0.7655 - val_loss: 0.5406 - val_acc: 0.7672\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5443 - acc: 0.7659 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7659 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7664 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7659 - val_loss: 0.5415 - val_acc: 0.7672\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.54016 to 0.54003, saving model to best.model\n",
      "0s - loss: 0.5438 - acc: 0.7657 - val_loss: 0.5400 - val_acc: 0.7690\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7658 - val_loss: 0.5405 - val_acc: 0.7672\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7660 - val_loss: 0.5406 - val_acc: 0.7672\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7660 - val_loss: 0.5403 - val_acc: 0.7672\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7662 - val_loss: 0.5405 - val_acc: 0.7672\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.54003 to 0.53993, saving model to best.model\n",
      "0s - loss: 0.5430 - acc: 0.7664 - val_loss: 0.5399 - val_acc: 0.7672\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5443 - acc: 0.7657 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7663 - val_loss: 0.5400 - val_acc: 0.7672\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.53993 to 0.53984, saving model to best.model\n",
      "0s - loss: 0.5438 - acc: 0.7658 - val_loss: 0.5398 - val_acc: 0.7672\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7660 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7659 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5437 - acc: 0.7659 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7663 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.53984 to 0.53970, saving model to best.model\n",
      "0s - loss: 0.5436 - acc: 0.7659 - val_loss: 0.5397 - val_acc: 0.7690\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7658 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7660 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7660 - val_loss: 0.5399 - val_acc: 0.7672\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7658 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7661 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7662 - val_loss: 0.5397 - val_acc: 0.7690\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7659 - val_loss: 0.5403 - val_acc: 0.7672\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7663 - val_loss: 0.5405 - val_acc: 0.7672\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7661 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.53970 to 0.53943, saving model to best.model\n",
      "0s - loss: 0.5433 - acc: 0.7661 - val_loss: 0.5394 - val_acc: 0.7690\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7660 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7659 - val_loss: 0.5400 - val_acc: 0.7672\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7661 - val_loss: 0.5397 - val_acc: 0.7690\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7663 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7661 - val_loss: 0.5403 - val_acc: 0.7672\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7660 - val_loss: 0.5403 - val_acc: 0.7672\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7661 - val_loss: 0.5398 - val_acc: 0.7690\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7664 - val_loss: 0.5396 - val_acc: 0.7690\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7658 - val_loss: 0.5398 - val_acc: 0.7690\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7658 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7656 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7661 - val_loss: 0.5394 - val_acc: 0.7690\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7659 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7660 - val_loss: 0.5399 - val_acc: 0.7690\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7661 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7658 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7663 - val_loss: 0.5398 - val_acc: 0.7690\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7661 - val_loss: 0.5397 - val_acc: 0.7690\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7663 - val_loss: 0.5400 - val_acc: 0.7672\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7657 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7664 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7663 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7660 - val_loss: 0.5402 - val_acc: 0.7672\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7661 - val_loss: 0.5399 - val_acc: 0.7672\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7660 - val_loss: 0.5404 - val_acc: 0.7672\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7664 - val_loss: 0.5395 - val_acc: 0.7690\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.56616, saving model to best.model\n",
      "0s - loss: 0.6524 - acc: 0.7035 - val_loss: 0.5662 - val_acc: 0.7459\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.56616 to 0.56601, saving model to best.model\n",
      "0s - loss: 0.5903 - acc: 0.7426 - val_loss: 0.5660 - val_acc: 0.7459\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.56601 to 0.56558, saving model to best.model\n",
      "0s - loss: 0.5732 - acc: 0.7519 - val_loss: 0.5656 - val_acc: 0.7459\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.56558 to 0.56443, saving model to best.model\n",
      "0s - loss: 0.5663 - acc: 0.7537 - val_loss: 0.5644 - val_acc: 0.7459\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5645 - acc: 0.7539 - val_loss: 0.5649 - val_acc: 0.7459\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5613 - acc: 0.7539 - val_loss: 0.5666 - val_acc: 0.7459\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5617 - acc: 0.7539 - val_loss: 0.5650 - val_acc: 0.7459\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5599 - acc: 0.7539 - val_loss: 0.5652 - val_acc: 0.7459\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.56443 to 0.56389, saving model to best.model\n",
      "0s - loss: 0.5607 - acc: 0.7539 - val_loss: 0.5639 - val_acc: 0.7459\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.56389 to 0.56328, saving model to best.model\n",
      "0s - loss: 0.5586 - acc: 0.7540 - val_loss: 0.5633 - val_acc: 0.7459\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.56328 to 0.56285, saving model to best.model\n",
      "0s - loss: 0.5589 - acc: 0.7539 - val_loss: 0.5629 - val_acc: 0.7459\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.56285 to 0.56105, saving model to best.model\n",
      "0s - loss: 0.5572 - acc: 0.7539 - val_loss: 0.5610 - val_acc: 0.7459\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5560 - acc: 0.7541 - val_loss: 0.5616 - val_acc: 0.7459\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.56105 to 0.56054, saving model to best.model\n",
      "0s - loss: 0.5564 - acc: 0.7544 - val_loss: 0.5605 - val_acc: 0.7461\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.56054 to 0.55562, saving model to best.model\n",
      "0s - loss: 0.5553 - acc: 0.7545 - val_loss: 0.5556 - val_acc: 0.7464\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.55562 to 0.55412, saving model to best.model\n",
      "0s - loss: 0.5522 - acc: 0.7561 - val_loss: 0.5541 - val_acc: 0.7473\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5630 - acc: 0.7529 - val_loss: 0.5659 - val_acc: 0.7459\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5594 - acc: 0.7541 - val_loss: 0.5660 - val_acc: 0.7459\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5593 - acc: 0.7541 - val_loss: 0.5663 - val_acc: 0.7459\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5596 - acc: 0.7540 - val_loss: 0.5668 - val_acc: 0.7459\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5606 - acc: 0.7542 - val_loss: 0.5660 - val_acc: 0.7461\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5603 - acc: 0.7543 - val_loss: 0.5680 - val_acc: 0.7461\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5590 - acc: 0.7543 - val_loss: 0.5658 - val_acc: 0.7461\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5599 - acc: 0.7547 - val_loss: 0.5669 - val_acc: 0.7461\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5592 - acc: 0.7543 - val_loss: 0.5659 - val_acc: 0.7464\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7547 - val_loss: 0.5655 - val_acc: 0.7466\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5577 - acc: 0.7550 - val_loss: 0.5664 - val_acc: 0.7466\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5585 - acc: 0.7549 - val_loss: 0.5654 - val_acc: 0.7466\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7551 - val_loss: 0.5655 - val_acc: 0.7466\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5578 - acc: 0.7553 - val_loss: 0.5648 - val_acc: 0.7473\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5588 - acc: 0.7555 - val_loss: 0.5649 - val_acc: 0.7473\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5567 - acc: 0.7556 - val_loss: 0.5670 - val_acc: 0.7473\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5564 - acc: 0.7559 - val_loss: 0.5641 - val_acc: 0.7473\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5572 - acc: 0.7563 - val_loss: 0.5650 - val_acc: 0.7473\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5568 - acc: 0.7565 - val_loss: 0.5648 - val_acc: 0.7473\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5565 - acc: 0.7564 - val_loss: 0.5649 - val_acc: 0.7476\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7565 - val_loss: 0.5634 - val_acc: 0.7481\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5567 - acc: 0.7567 - val_loss: 0.5636 - val_acc: 0.7481\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7570 - val_loss: 0.5630 - val_acc: 0.7481\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7573 - val_loss: 0.5635 - val_acc: 0.7483\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5559 - acc: 0.7570 - val_loss: 0.5627 - val_acc: 0.7488\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7573 - val_loss: 0.5626 - val_acc: 0.7488\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55993, saving model to best.model\n",
      "0s - loss: 0.6033 - acc: 0.7311 - val_loss: 0.5599 - val_acc: 0.7515\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55993 to 0.55774, saving model to best.model\n",
      "0s - loss: 0.5668 - acc: 0.7558 - val_loss: 0.5577 - val_acc: 0.7515\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7584 - val_loss: 0.5595 - val_acc: 0.7515\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5601 - acc: 0.7584 - val_loss: 0.5590 - val_acc: 0.7515\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5574 - acc: 0.7584 - val_loss: 0.5598 - val_acc: 0.7515\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5570 - acc: 0.7584 - val_loss: 0.5592 - val_acc: 0.7515\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5554 - acc: 0.7584 - val_loss: 0.5583 - val_acc: 0.7515\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7584 - val_loss: 0.5583 - val_acc: 0.7515\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5535 - acc: 0.7584 - val_loss: 0.5583 - val_acc: 0.7515\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.55774 to 0.55621, saving model to best.model\n",
      "0s - loss: 0.5543 - acc: 0.7586 - val_loss: 0.5562 - val_acc: 0.7515\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7586 - val_loss: 0.5566 - val_acc: 0.7515\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.55621 to 0.55231, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7596 - val_loss: 0.5523 - val_acc: 0.7532\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.55231 to 0.55204, saving model to best.model\n",
      "0s - loss: 0.5492 - acc: 0.7613 - val_loss: 0.5520 - val_acc: 0.7551\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.55204 to 0.55033, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7619 - val_loss: 0.5503 - val_acc: 0.7585\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7631 - val_loss: 0.5552 - val_acc: 0.7578\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.55033 to 0.53978, saving model to best.model\n",
      "0s - loss: 0.5461 - acc: 0.7643 - val_loss: 0.5398 - val_acc: 0.7600\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7638 - val_loss: 0.5541 - val_acc: 0.7585\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7631 - val_loss: 0.5546 - val_acc: 0.7578\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7633 - val_loss: 0.5523 - val_acc: 0.7595\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7644 - val_loss: 0.5515 - val_acc: 0.7595\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7643 - val_loss: 0.5507 - val_acc: 0.7595\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7640 - val_loss: 0.5519 - val_acc: 0.7595\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7645 - val_loss: 0.5506 - val_acc: 0.7595\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7629 - val_loss: 0.5562 - val_acc: 0.7595\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7633 - val_loss: 0.5516 - val_acc: 0.7595\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7643 - val_loss: 0.5514 - val_acc: 0.7595\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7611 - val_loss: 0.5546 - val_acc: 0.7551\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7631 - val_loss: 0.5533 - val_acc: 0.7558\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7636 - val_loss: 0.5532 - val_acc: 0.7558\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7632 - val_loss: 0.5526 - val_acc: 0.7595\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7635 - val_loss: 0.5517 - val_acc: 0.7595\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7640 - val_loss: 0.5534 - val_acc: 0.7595\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7641 - val_loss: 0.5509 - val_acc: 0.7595\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7639 - val_loss: 0.5515 - val_acc: 0.7595\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7646 - val_loss: 0.5512 - val_acc: 0.7595\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7646 - val_loss: 0.5503 - val_acc: 0.7595\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7649 - val_loss: 0.5504 - val_acc: 0.7595\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5451 - acc: 0.7655 - val_loss: 0.5502 - val_acc: 0.7595\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7643 - val_loss: 0.5504 - val_acc: 0.7595\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5463 - acc: 0.7647 - val_loss: 0.5498 - val_acc: 0.7595\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7647 - val_loss: 0.5496 - val_acc: 0.7595\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5451 - acc: 0.7652 - val_loss: 0.5496 - val_acc: 0.7600\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55175, saving model to best.model\n",
      "0s - loss: 0.6114 - acc: 0.7299 - val_loss: 0.5517 - val_acc: 0.7592\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5690 - acc: 0.7573 - val_loss: 0.5543 - val_acc: 0.7592\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5585 - acc: 0.7605 - val_loss: 0.5525 - val_acc: 0.7592\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.55175 to 0.55124, saving model to best.model\n",
      "0s - loss: 0.5559 - acc: 0.7613 - val_loss: 0.5512 - val_acc: 0.7592\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7613 - val_loss: 0.5517 - val_acc: 0.7592\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7613 - val_loss: 0.5529 - val_acc: 0.7592\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7613 - val_loss: 0.5523 - val_acc: 0.7592\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7613 - val_loss: 0.5531 - val_acc: 0.7592\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7613 - val_loss: 0.5525 - val_acc: 0.7592\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7613 - val_loss: 0.5531 - val_acc: 0.7592\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7613 - val_loss: 0.5523 - val_acc: 0.7592\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7613 - val_loss: 0.5517 - val_acc: 0.7592\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.55124 to 0.55108, saving model to best.model\n",
      "0s - loss: 0.5531 - acc: 0.7613 - val_loss: 0.5511 - val_acc: 0.7592\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.55108 to 0.55107, saving model to best.model\n",
      "0s - loss: 0.5516 - acc: 0.7613 - val_loss: 0.5511 - val_acc: 0.7592\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7613 - val_loss: 0.5531 - val_acc: 0.7592\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.55107 to 0.55081, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7613 - val_loss: 0.5508 - val_acc: 0.7592\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.55081 to 0.55065, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7613 - val_loss: 0.5506 - val_acc: 0.7592\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.55065 to 0.55040, saving model to best.model\n",
      "0s - loss: 0.5518 - acc: 0.7613 - val_loss: 0.5504 - val_acc: 0.7592\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7613 - val_loss: 0.5504 - val_acc: 0.7592\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7613 - val_loss: 0.5504 - val_acc: 0.7592\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7613 - val_loss: 0.5506 - val_acc: 0.7592\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.55040 to 0.54960, saving model to best.model\n",
      "0s - loss: 0.5494 - acc: 0.7613 - val_loss: 0.5496 - val_acc: 0.7592\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.54960 to 0.54844, saving model to best.model\n",
      "0s - loss: 0.5497 - acc: 0.7617 - val_loss: 0.5484 - val_acc: 0.7592\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54844 to 0.54491, saving model to best.model\n",
      "0s - loss: 0.5495 - acc: 0.7626 - val_loss: 0.5449 - val_acc: 0.7609\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7621 - val_loss: 0.5488 - val_acc: 0.7597\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7630 - val_loss: 0.5509 - val_acc: 0.7602\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7629 - val_loss: 0.5478 - val_acc: 0.7617\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7641 - val_loss: 0.5473 - val_acc: 0.7638\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7650 - val_loss: 0.5471 - val_acc: 0.7638\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7658 - val_loss: 0.5462 - val_acc: 0.7638\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7658 - val_loss: 0.5463 - val_acc: 0.7643\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5463 - acc: 0.7660 - val_loss: 0.5453 - val_acc: 0.7643\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7664 - val_loss: 0.5450 - val_acc: 0.7643\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54491 to 0.54474, saving model to best.model\n",
      "0s - loss: 0.5441 - acc: 0.7669 - val_loss: 0.5447 - val_acc: 0.7643\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7671 - val_loss: 0.5448 - val_acc: 0.7648\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5445 - acc: 0.7671 - val_loss: 0.5455 - val_acc: 0.7643\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54474 to 0.54457, saving model to best.model\n",
      "0s - loss: 0.5431 - acc: 0.7672 - val_loss: 0.5446 - val_acc: 0.7648\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54457 to 0.54446, saving model to best.model\n",
      "0s - loss: 0.5439 - acc: 0.7675 - val_loss: 0.5445 - val_acc: 0.7648\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7675 - val_loss: 0.5446 - val_acc: 0.7648\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.54446 to 0.54390, saving model to best.model\n",
      "0s - loss: 0.5436 - acc: 0.7674 - val_loss: 0.5439 - val_acc: 0.7660\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.54390 to 0.54383, saving model to best.model\n",
      "0s - loss: 0.5421 - acc: 0.7682 - val_loss: 0.5438 - val_acc: 0.7660\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7677 - val_loss: 0.5460 - val_acc: 0.7648\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7678 - val_loss: 0.5446 - val_acc: 0.7648\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7679 - val_loss: 0.5464 - val_acc: 0.7648\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7681 - val_loss: 0.5449 - val_acc: 0.7648\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7682 - val_loss: 0.5464 - val_acc: 0.7648\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7682 - val_loss: 0.5449 - val_acc: 0.7648\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7680 - val_loss: 0.5446 - val_acc: 0.7648\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7678 - val_loss: 0.5447 - val_acc: 0.7648\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7679 - val_loss: 0.5454 - val_acc: 0.7648\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7679 - val_loss: 0.5447 - val_acc: 0.7648\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7679 - val_loss: 0.5447 - val_acc: 0.7648\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7680 - val_loss: 0.5447 - val_acc: 0.7648\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7681 - val_loss: 0.5446 - val_acc: 0.7648\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7677 - val_loss: 0.5448 - val_acc: 0.7648\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7681 - val_loss: 0.5456 - val_acc: 0.7648\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7684 - val_loss: 0.5454 - val_acc: 0.7648\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7680 - val_loss: 0.5448 - val_acc: 0.7648\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7679 - val_loss: 0.5448 - val_acc: 0.7648\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7683 - val_loss: 0.5453 - val_acc: 0.7648\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7679 - val_loss: 0.5449 - val_acc: 0.7648\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7681 - val_loss: 0.5452 - val_acc: 0.7648\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7680 - val_loss: 0.5449 - val_acc: 0.7648\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7681 - val_loss: 0.5453 - val_acc: 0.7648\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7685 - val_loss: 0.5460 - val_acc: 0.7648\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7682 - val_loss: 0.5449 - val_acc: 0.7648\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7684 - val_loss: 0.5453 - val_acc: 0.7648\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55181, saving model to best.model\n",
      "0s - loss: 0.6125 - acc: 0.7251 - val_loss: 0.5518 - val_acc: 0.7575\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5772 - acc: 0.7490 - val_loss: 0.5529 - val_acc: 0.7575\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5689 - acc: 0.7528 - val_loss: 0.5533 - val_acc: 0.7575\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5659 - acc: 0.7531 - val_loss: 0.5526 - val_acc: 0.7575\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5640 - acc: 0.7531 - val_loss: 0.5546 - val_acc: 0.7575\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5639 - acc: 0.7531 - val_loss: 0.5545 - val_acc: 0.7575\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5613 - acc: 0.7531 - val_loss: 0.5538 - val_acc: 0.7575\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5618 - acc: 0.7531 - val_loss: 0.5538 - val_acc: 0.7575\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5629 - acc: 0.7531 - val_loss: 0.5549 - val_acc: 0.7575\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5615 - acc: 0.7531 - val_loss: 0.5541 - val_acc: 0.7575\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5614 - acc: 0.7531 - val_loss: 0.5554 - val_acc: 0.7575\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7531 - val_loss: 0.5544 - val_acc: 0.7575\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5615 - acc: 0.7531 - val_loss: 0.5537 - val_acc: 0.7575\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5625 - acc: 0.7531 - val_loss: 0.5546 - val_acc: 0.7575\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7531 - val_loss: 0.5560 - val_acc: 0.7575\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7531 - val_loss: 0.5540 - val_acc: 0.7575\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5606 - acc: 0.7531 - val_loss: 0.5536 - val_acc: 0.7575\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5601 - acc: 0.7531 - val_loss: 0.5539 - val_acc: 0.7575\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7531 - val_loss: 0.5538 - val_acc: 0.7575\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5607 - acc: 0.7531 - val_loss: 0.5541 - val_acc: 0.7575\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7531 - val_loss: 0.5549 - val_acc: 0.7575\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5614 - acc: 0.7531 - val_loss: 0.5560 - val_acc: 0.7575\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5608 - acc: 0.7531 - val_loss: 0.5561 - val_acc: 0.7575\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5615 - acc: 0.7531 - val_loss: 0.5542 - val_acc: 0.7575\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5604 - acc: 0.7531 - val_loss: 0.5535 - val_acc: 0.7575\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5605 - acc: 0.7531 - val_loss: 0.5542 - val_acc: 0.7575\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7531 - val_loss: 0.5534 - val_acc: 0.7575\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54335, saving model to best.model\n",
      "0s - loss: 0.6209 - acc: 0.7245 - val_loss: 0.5434 - val_acc: 0.7658\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54335 to 0.54302, saving model to best.model\n",
      "0s - loss: 0.5765 - acc: 0.7526 - val_loss: 0.5430 - val_acc: 0.7658\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5654 - acc: 0.7579 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5589 - acc: 0.7581 - val_loss: 0.5439 - val_acc: 0.7658\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5591 - acc: 0.7581 - val_loss: 0.5438 - val_acc: 0.7658\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5578 - acc: 0.7581 - val_loss: 0.5438 - val_acc: 0.7658\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.54302 to 0.54264, saving model to best.model\n",
      "0s - loss: 0.5577 - acc: 0.7581 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7581 - val_loss: 0.5441 - val_acc: 0.7658\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5574 - acc: 0.7581 - val_loss: 0.5441 - val_acc: 0.7658\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7581 - val_loss: 0.5435 - val_acc: 0.7658\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5554 - acc: 0.7581 - val_loss: 0.5427 - val_acc: 0.7658\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.54264 to 0.54154, saving model to best.model\n",
      "0s - loss: 0.5554 - acc: 0.7581 - val_loss: 0.5415 - val_acc: 0.7658\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7581 - val_loss: 0.5435 - val_acc: 0.7658\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7581 - val_loss: 0.5456 - val_acc: 0.7658\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7581 - val_loss: 0.5462 - val_acc: 0.7658\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7581 - val_loss: 0.5441 - val_acc: 0.7658\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7581 - val_loss: 0.5443 - val_acc: 0.7658\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5557 - acc: 0.7581 - val_loss: 0.5450 - val_acc: 0.7658\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5560 - acc: 0.7581 - val_loss: 0.5447 - val_acc: 0.7658\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5557 - acc: 0.7581 - val_loss: 0.5450 - val_acc: 0.7658\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5549 - acc: 0.7581 - val_loss: 0.5464 - val_acc: 0.7658\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7581 - val_loss: 0.5466 - val_acc: 0.7658\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7581 - val_loss: 0.5441 - val_acc: 0.7658\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7581 - val_loss: 0.5451 - val_acc: 0.7658\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7581 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7581 - val_loss: 0.5452 - val_acc: 0.7658\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7581 - val_loss: 0.5458 - val_acc: 0.7658\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7581 - val_loss: 0.5443 - val_acc: 0.7658\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7581 - val_loss: 0.5446 - val_acc: 0.7658\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5549 - acc: 0.7581 - val_loss: 0.5442 - val_acc: 0.7658\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5541 - acc: 0.7581 - val_loss: 0.5454 - val_acc: 0.7658\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7581 - val_loss: 0.5460 - val_acc: 0.7658\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7581 - val_loss: 0.5440 - val_acc: 0.7658\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7581 - val_loss: 0.5440 - val_acc: 0.7658\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7581 - val_loss: 0.5443 - val_acc: 0.7658\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7581 - val_loss: 0.5450 - val_acc: 0.7658\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5541 - acc: 0.7581 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7581 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54200, saving model to best.model\n",
      "0s - loss: 0.6562 - acc: 0.7091 - val_loss: 0.5420 - val_acc: 0.7670\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54200 to 0.54199, saving model to best.model\n",
      "0s - loss: 0.5878 - acc: 0.7497 - val_loss: 0.5420 - val_acc: 0.7670\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5633 - acc: 0.7646 - val_loss: 0.5425 - val_acc: 0.7670\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7673 - val_loss: 0.5428 - val_acc: 0.7670\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7680 - val_loss: 0.5423 - val_acc: 0.7670\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7680 - val_loss: 0.5424 - val_acc: 0.7670\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7680 - val_loss: 0.5422 - val_acc: 0.7670\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7680 - val_loss: 0.5422 - val_acc: 0.7670\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5450 - acc: 0.7680 - val_loss: 0.5425 - val_acc: 0.7670\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5455 - acc: 0.7680 - val_loss: 0.5421 - val_acc: 0.7670\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.54199 to 0.54164, saving model to best.model\n",
      "0s - loss: 0.5456 - acc: 0.7680 - val_loss: 0.5416 - val_acc: 0.7670\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.54164 to 0.54141, saving model to best.model\n",
      "0s - loss: 0.5451 - acc: 0.7680 - val_loss: 0.5414 - val_acc: 0.7670\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.54141 to 0.54097, saving model to best.model\n",
      "0s - loss: 0.5431 - acc: 0.7680 - val_loss: 0.5410 - val_acc: 0.7670\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.54097 to 0.54052, saving model to best.model\n",
      "0s - loss: 0.5426 - acc: 0.7681 - val_loss: 0.5405 - val_acc: 0.7670\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.54052 to 0.54045, saving model to best.model\n",
      "0s - loss: 0.5427 - acc: 0.7681 - val_loss: 0.5404 - val_acc: 0.7670\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.54045 to 0.53964, saving model to best.model\n",
      "0s - loss: 0.5435 - acc: 0.7686 - val_loss: 0.5396 - val_acc: 0.7672\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.53964 to 0.53848, saving model to best.model\n",
      "0s - loss: 0.5416 - acc: 0.7696 - val_loss: 0.5385 - val_acc: 0.7682\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.53848 to 0.53723, saving model to best.model\n",
      "0s - loss: 0.5401 - acc: 0.7703 - val_loss: 0.5372 - val_acc: 0.7687\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.53723 to 0.53695, saving model to best.model\n",
      "0s - loss: 0.5412 - acc: 0.7715 - val_loss: 0.5369 - val_acc: 0.7724\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.53695 to 0.53520, saving model to best.model\n",
      "0s - loss: 0.5385 - acc: 0.7727 - val_loss: 0.5352 - val_acc: 0.7743\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.53520 to 0.53422, saving model to best.model\n",
      "0s - loss: 0.5373 - acc: 0.7734 - val_loss: 0.5342 - val_acc: 0.7753\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.53422 to 0.53369, saving model to best.model\n",
      "0s - loss: 0.5371 - acc: 0.7739 - val_loss: 0.5337 - val_acc: 0.7753\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5388 - acc: 0.7740 - val_loss: 0.5347 - val_acc: 0.7753\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.53369 to 0.53279, saving model to best.model\n",
      "0s - loss: 0.5365 - acc: 0.7749 - val_loss: 0.5328 - val_acc: 0.7753\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.53279 to 0.53197, saving model to best.model\n",
      "0s - loss: 0.5352 - acc: 0.7751 - val_loss: 0.5320 - val_acc: 0.7753\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5338 - acc: 0.7757 - val_loss: 0.5323 - val_acc: 0.7753\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.53197 to 0.53145, saving model to best.model\n",
      "0s - loss: 0.5352 - acc: 0.7755 - val_loss: 0.5315 - val_acc: 0.7753\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5346 - acc: 0.7757 - val_loss: 0.5319 - val_acc: 0.7753\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5333 - acc: 0.7756 - val_loss: 0.5321 - val_acc: 0.7753\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.53145 to 0.53111, saving model to best.model\n",
      "0s - loss: 0.5334 - acc: 0.7758 - val_loss: 0.5311 - val_acc: 0.7753\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.53111 to 0.53107, saving model to best.model\n",
      "0s - loss: 0.5332 - acc: 0.7757 - val_loss: 0.5311 - val_acc: 0.7753\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5340 - acc: 0.7755 - val_loss: 0.5323 - val_acc: 0.7753\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5341 - acc: 0.7759 - val_loss: 0.5313 - val_acc: 0.7753\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53107 to 0.53106, saving model to best.model\n",
      "0s - loss: 0.5330 - acc: 0.7757 - val_loss: 0.5311 - val_acc: 0.7753\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.53106 to 0.53092, saving model to best.model\n",
      "0s - loss: 0.5327 - acc: 0.7758 - val_loss: 0.5309 - val_acc: 0.7753\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.53092 to 0.53080, saving model to best.model\n",
      "0s - loss: 0.5322 - acc: 0.7756 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5329 - acc: 0.7759 - val_loss: 0.5311 - val_acc: 0.7753\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.53080 to 0.53078, saving model to best.model\n",
      "0s - loss: 0.5327 - acc: 0.7759 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.53078 to 0.53077, saving model to best.model\n",
      "0s - loss: 0.5324 - acc: 0.7758 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5331 - acc: 0.7757 - val_loss: 0.5309 - val_acc: 0.7753\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5318 - acc: 0.7760 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5323 - acc: 0.7760 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.53077 to 0.53074, saving model to best.model\n",
      "0s - loss: 0.5316 - acc: 0.7763 - val_loss: 0.5307 - val_acc: 0.7753\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5323 - acc: 0.7760 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5326 - acc: 0.7761 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5327 - acc: 0.7763 - val_loss: 0.5312 - val_acc: 0.7753\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5321 - acc: 0.7762 - val_loss: 0.5310 - val_acc: 0.7753\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5320 - acc: 0.7759 - val_loss: 0.5312 - val_acc: 0.7753\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5323 - acc: 0.7765 - val_loss: 0.5310 - val_acc: 0.7753\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5315 - acc: 0.7764 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5311 - acc: 0.7765 - val_loss: 0.5309 - val_acc: 0.7753\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5322 - acc: 0.7762 - val_loss: 0.5310 - val_acc: 0.7753\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5312 - acc: 0.7767 - val_loss: 0.5311 - val_acc: 0.7753\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5324 - acc: 0.7763 - val_loss: 0.5308 - val_acc: 0.7753\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5316 - acc: 0.7763 - val_loss: 0.5311 - val_acc: 0.7753\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5314 - acc: 0.7765 - val_loss: 0.5312 - val_acc: 0.7753\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5311 - acc: 0.7765 - val_loss: 0.5309 - val_acc: 0.7753\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5313 - acc: 0.7768 - val_loss: 0.5310 - val_acc: 0.7753\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5313 - acc: 0.7764 - val_loss: 0.5315 - val_acc: 0.7753\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5319 - acc: 0.7762 - val_loss: 0.5309 - val_acc: 0.7753\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5316 - acc: 0.7767 - val_loss: 0.5310 - val_acc: 0.7753\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5312 - acc: 0.7768 - val_loss: 0.5310 - val_acc: 0.7753\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5314 - acc: 0.7763 - val_loss: 0.5310 - val_acc: 0.7753\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5313 - acc: 0.7765 - val_loss: 0.5309 - val_acc: 0.7753\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5309 - acc: 0.7767 - val_loss: 0.5314 - val_acc: 0.7753\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5318 - acc: 0.7762 - val_loss: 0.5321 - val_acc: 0.7753\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5317 - acc: 0.7765 - val_loss: 0.5311 - val_acc: 0.7753\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5309 - acc: 0.7764 - val_loss: 0.5316 - val_acc: 0.7753\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5310 - acc: 0.7766 - val_loss: 0.5312 - val_acc: 0.7753\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55194, saving model to best.model\n",
      "0s - loss: 0.6205 - acc: 0.7206 - val_loss: 0.5519 - val_acc: 0.7570\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5732 - acc: 0.7517 - val_loss: 0.5537 - val_acc: 0.7570\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5648 - acc: 0.7575 - val_loss: 0.5525 - val_acc: 0.7570\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5603 - acc: 0.7579 - val_loss: 0.5528 - val_acc: 0.7570\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7579 - val_loss: 0.5543 - val_acc: 0.7570\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5586 - acc: 0.7579 - val_loss: 0.5545 - val_acc: 0.7570\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5577 - acc: 0.7579 - val_loss: 0.5541 - val_acc: 0.7570\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5576 - acc: 0.7579 - val_loss: 0.5550 - val_acc: 0.7570\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5570 - acc: 0.7579 - val_loss: 0.5546 - val_acc: 0.7570\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7579 - val_loss: 0.5538 - val_acc: 0.7570\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5581 - acc: 0.7579 - val_loss: 0.5546 - val_acc: 0.7570\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7579 - val_loss: 0.5546 - val_acc: 0.7570\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5566 - acc: 0.7579 - val_loss: 0.5543 - val_acc: 0.7570\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5565 - acc: 0.7579 - val_loss: 0.5556 - val_acc: 0.7570\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5560 - acc: 0.7579 - val_loss: 0.5543 - val_acc: 0.7570\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5550 - acc: 0.7579 - val_loss: 0.5546 - val_acc: 0.7570\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7579 - val_loss: 0.5550 - val_acc: 0.7570\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7579 - val_loss: 0.5542 - val_acc: 0.7570\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5552 - acc: 0.7579 - val_loss: 0.5542 - val_acc: 0.7570\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5557 - acc: 0.7579 - val_loss: 0.5542 - val_acc: 0.7570\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7579 - val_loss: 0.5549 - val_acc: 0.7570\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5552 - acc: 0.7579 - val_loss: 0.5545 - val_acc: 0.7570\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5549 - acc: 0.7579 - val_loss: 0.5540 - val_acc: 0.7570\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5550 - acc: 0.7579 - val_loss: 0.5542 - val_acc: 0.7570\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5552 - acc: 0.7579 - val_loss: 0.5549 - val_acc: 0.7570\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7579 - val_loss: 0.5539 - val_acc: 0.7570\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5549 - acc: 0.7579 - val_loss: 0.5540 - val_acc: 0.7570\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55124, saving model to best.model\n",
      "0s - loss: 0.6257 - acc: 0.7157 - val_loss: 0.5512 - val_acc: 0.7587\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5829 - acc: 0.7446 - val_loss: 0.5515 - val_acc: 0.7587\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5680 - acc: 0.7538 - val_loss: 0.5517 - val_acc: 0.7587\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5634 - acc: 0.7542 - val_loss: 0.5515 - val_acc: 0.7587\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5628 - acc: 0.7543 - val_loss: 0.5514 - val_acc: 0.7587\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.55124 to 0.55108, saving model to best.model\n",
      "0s - loss: 0.5608 - acc: 0.7543 - val_loss: 0.5511 - val_acc: 0.7587\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7543 - val_loss: 0.5538 - val_acc: 0.7587\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5601 - acc: 0.7543 - val_loss: 0.5527 - val_acc: 0.7587\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7543 - val_loss: 0.5516 - val_acc: 0.7587\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7543 - val_loss: 0.5522 - val_acc: 0.7587\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5592 - acc: 0.7544 - val_loss: 0.5515 - val_acc: 0.7587\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5588 - acc: 0.7544 - val_loss: 0.5512 - val_acc: 0.7587\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5592 - acc: 0.7544 - val_loss: 0.5520 - val_acc: 0.7587\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5607 - acc: 0.7544 - val_loss: 0.5516 - val_acc: 0.7587\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5599 - acc: 0.7544 - val_loss: 0.5516 - val_acc: 0.7587\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5606 - acc: 0.7543 - val_loss: 0.5529 - val_acc: 0.7587\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5595 - acc: 0.7543 - val_loss: 0.5517 - val_acc: 0.7587\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5586 - acc: 0.7544 - val_loss: 0.5514 - val_acc: 0.7587\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5592 - acc: 0.7545 - val_loss: 0.5513 - val_acc: 0.7587\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.55108 to 0.55093, saving model to best.model\n",
      "0s - loss: 0.5591 - acc: 0.7547 - val_loss: 0.5509 - val_acc: 0.7590\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.55093 to 0.55068, saving model to best.model\n",
      "0s - loss: 0.5586 - acc: 0.7550 - val_loss: 0.5507 - val_acc: 0.7595\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.55068 to 0.55011, saving model to best.model\n",
      "0s - loss: 0.5576 - acc: 0.7553 - val_loss: 0.5501 - val_acc: 0.7602\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.55011 to 0.54947, saving model to best.model\n",
      "0s - loss: 0.5565 - acc: 0.7568 - val_loss: 0.5495 - val_acc: 0.7612\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5566 - acc: 0.7567 - val_loss: 0.5514 - val_acc: 0.7614\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54947 to 0.54861, saving model to best.model\n",
      "0s - loss: 0.5560 - acc: 0.7573 - val_loss: 0.5486 - val_acc: 0.7614\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5560 - acc: 0.7570 - val_loss: 0.5487 - val_acc: 0.7614\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54861 to 0.54833, saving model to best.model\n",
      "0s - loss: 0.5548 - acc: 0.7580 - val_loss: 0.5483 - val_acc: 0.7621\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54833 to 0.54767, saving model to best.model\n",
      "0s - loss: 0.5553 - acc: 0.7578 - val_loss: 0.5477 - val_acc: 0.7621\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7584 - val_loss: 0.5478 - val_acc: 0.7621\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54767 to 0.54744, saving model to best.model\n",
      "0s - loss: 0.5545 - acc: 0.7586 - val_loss: 0.5474 - val_acc: 0.7621\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7589 - val_loss: 0.5476 - val_acc: 0.7634\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5541 - acc: 0.7587 - val_loss: 0.5475 - val_acc: 0.7634\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7584 - val_loss: 0.5490 - val_acc: 0.7634\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54744 to 0.54702, saving model to best.model\n",
      "0s - loss: 0.5535 - acc: 0.7587 - val_loss: 0.5470 - val_acc: 0.7634\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7586 - val_loss: 0.5488 - val_acc: 0.7634\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.54702 to 0.54647, saving model to best.model\n",
      "0s - loss: 0.5527 - acc: 0.7590 - val_loss: 0.5465 - val_acc: 0.7634\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7590 - val_loss: 0.5467 - val_acc: 0.7634\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7587 - val_loss: 0.5465 - val_acc: 0.7634\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.54647 to 0.54629, saving model to best.model\n",
      "0s - loss: 0.5533 - acc: 0.7585 - val_loss: 0.5463 - val_acc: 0.7634\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7589 - val_loss: 0.5469 - val_acc: 0.7634\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7587 - val_loss: 0.5468 - val_acc: 0.7634\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7590 - val_loss: 0.5463 - val_acc: 0.7634\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54629 to 0.54617, saving model to best.model\n",
      "0s - loss: 0.5524 - acc: 0.7590 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7590 - val_loss: 0.5464 - val_acc: 0.7634\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7592 - val_loss: 0.5464 - val_acc: 0.7634\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7592 - val_loss: 0.5463 - val_acc: 0.7634\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7592 - val_loss: 0.5464 - val_acc: 0.7634\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.54617 to 0.54614, saving model to best.model\n",
      "0s - loss: 0.5536 - acc: 0.7591 - val_loss: 0.5461 - val_acc: 0.7634\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7594 - val_loss: 0.5464 - val_acc: 0.7634\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7590 - val_loss: 0.5466 - val_acc: 0.7634\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.54614 to 0.54602, saving model to best.model\n",
      "0s - loss: 0.5515 - acc: 0.7595 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.54602 to 0.54602, saving model to best.model\n",
      "0s - loss: 0.5508 - acc: 0.7594 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7592 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7591 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7594 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.54602 to 0.54600, saving model to best.model\n",
      "0s - loss: 0.5517 - acc: 0.7595 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7595 - val_loss: 0.5461 - val_acc: 0.7634\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7593 - val_loss: 0.5467 - val_acc: 0.7634\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7596 - val_loss: 0.5466 - val_acc: 0.7634\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7593 - val_loss: 0.5461 - val_acc: 0.7634\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7594 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7594 - val_loss: 0.5461 - val_acc: 0.7634\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7593 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7596 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7595 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7593 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7595 - val_loss: 0.5464 - val_acc: 0.7634\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7594 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7595 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7594 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7597 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7595 - val_loss: 0.5461 - val_acc: 0.7634\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7592 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7595 - val_loss: 0.5461 - val_acc: 0.7634\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7595 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7597 - val_loss: 0.5468 - val_acc: 0.7634\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7594 - val_loss: 0.5472 - val_acc: 0.7634\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7593 - val_loss: 0.5461 - val_acc: 0.7634\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7595 - val_loss: 0.5473 - val_acc: 0.7634\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7595 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7596 - val_loss: 0.5460 - val_acc: 0.7634\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54200, saving model to best.model\n",
      "0s - loss: 0.6362 - acc: 0.7146 - val_loss: 0.5420 - val_acc: 0.7658\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54200 to 0.53904, saving model to best.model\n",
      "0s - loss: 0.5805 - acc: 0.7502 - val_loss: 0.5390 - val_acc: 0.7658\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7615 - val_loss: 0.5391 - val_acc: 0.7658\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7621 - val_loss: 0.5398 - val_acc: 0.7658\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7623 - val_loss: 0.5391 - val_acc: 0.7658\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7624 - val_loss: 0.5393 - val_acc: 0.7658\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7623 - val_loss: 0.5404 - val_acc: 0.7658\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7625 - val_loss: 0.5421 - val_acc: 0.7658\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7623 - val_loss: 0.5420 - val_acc: 0.7658\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7626 - val_loss: 0.5419 - val_acc: 0.7658\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7625 - val_loss: 0.5421 - val_acc: 0.7665\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7625 - val_loss: 0.5420 - val_acc: 0.7668\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7632 - val_loss: 0.5404 - val_acc: 0.7670\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7638 - val_loss: 0.5395 - val_acc: 0.7682\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.53904 to 0.53767, saving model to best.model\n",
      "0s - loss: 0.5480 - acc: 0.7647 - val_loss: 0.5377 - val_acc: 0.7697\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7658 - val_loss: 0.5382 - val_acc: 0.7711\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5466 - acc: 0.7655 - val_loss: 0.5381 - val_acc: 0.7711\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.53767 to 0.53732, saving model to best.model\n",
      "0s - loss: 0.5470 - acc: 0.7664 - val_loss: 0.5373 - val_acc: 0.7716\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.53732 to 0.53594, saving model to best.model\n",
      "0s - loss: 0.5444 - acc: 0.7675 - val_loss: 0.5359 - val_acc: 0.7719\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.53594 to 0.53574, saving model to best.model\n",
      "0s - loss: 0.5451 - acc: 0.7676 - val_loss: 0.5357 - val_acc: 0.7721\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.53574 to 0.53573, saving model to best.model\n",
      "0s - loss: 0.5429 - acc: 0.7680 - val_loss: 0.5357 - val_acc: 0.7721\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.53573 to 0.53495, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7681 - val_loss: 0.5349 - val_acc: 0.7721\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.53495 to 0.53426, saving model to best.model\n",
      "0s - loss: 0.5422 - acc: 0.7685 - val_loss: 0.5343 - val_acc: 0.7721\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7686 - val_loss: 0.5344 - val_acc: 0.7721\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.53426 to 0.53403, saving model to best.model\n",
      "0s - loss: 0.5417 - acc: 0.7689 - val_loss: 0.5340 - val_acc: 0.7721\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7690 - val_loss: 0.5342 - val_acc: 0.7721\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.53403 to 0.53343, saving model to best.model\n",
      "0s - loss: 0.5406 - acc: 0.7694 - val_loss: 0.5334 - val_acc: 0.7721\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.53343 to 0.53322, saving model to best.model\n",
      "0s - loss: 0.5411 - acc: 0.7696 - val_loss: 0.5332 - val_acc: 0.7721\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7694 - val_loss: 0.5333 - val_acc: 0.7728\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.53322 to 0.53260, saving model to best.model\n",
      "0s - loss: 0.5401 - acc: 0.7697 - val_loss: 0.5326 - val_acc: 0.7743\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.53260 to 0.53202, saving model to best.model\n",
      "0s - loss: 0.5392 - acc: 0.7701 - val_loss: 0.5320 - val_acc: 0.7743\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.53202 to 0.53190, saving model to best.model\n",
      "0s - loss: 0.5393 - acc: 0.7699 - val_loss: 0.5319 - val_acc: 0.7743\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.53190 to 0.53148, saving model to best.model\n",
      "0s - loss: 0.5402 - acc: 0.7705 - val_loss: 0.5315 - val_acc: 0.7748\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53148 to 0.53123, saving model to best.model\n",
      "0s - loss: 0.5396 - acc: 0.7704 - val_loss: 0.5312 - val_acc: 0.7748\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7703 - val_loss: 0.5319 - val_acc: 0.7743\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.53123 to 0.53060, saving model to best.model\n",
      "0s - loss: 0.5382 - acc: 0.7706 - val_loss: 0.5306 - val_acc: 0.7748\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7707 - val_loss: 0.5308 - val_acc: 0.7748\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7706 - val_loss: 0.5318 - val_acc: 0.7748\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.53060 to 0.53038, saving model to best.model\n",
      "0s - loss: 0.5384 - acc: 0.7706 - val_loss: 0.5304 - val_acc: 0.7748\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.53038 to 0.53027, saving model to best.model\n",
      "0s - loss: 0.5375 - acc: 0.7711 - val_loss: 0.5303 - val_acc: 0.7748\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.53027 to 0.53017, saving model to best.model\n",
      "0s - loss: 0.5377 - acc: 0.7711 - val_loss: 0.5302 - val_acc: 0.7748\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7707 - val_loss: 0.5307 - val_acc: 0.7748\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7705 - val_loss: 0.5304 - val_acc: 0.7748\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7711 - val_loss: 0.5307 - val_acc: 0.7748\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.53017 to 0.53009, saving model to best.model\n",
      "0s - loss: 0.5383 - acc: 0.7708 - val_loss: 0.5301 - val_acc: 0.7748\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.53009 to 0.52981, saving model to best.model\n",
      "0s - loss: 0.5383 - acc: 0.7706 - val_loss: 0.5298 - val_acc: 0.7748\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7713 - val_loss: 0.5303 - val_acc: 0.7748\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7715 - val_loss: 0.5301 - val_acc: 0.7748\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.52981 to 0.52964, saving model to best.model\n",
      "0s - loss: 0.5379 - acc: 0.7708 - val_loss: 0.5296 - val_acc: 0.7748\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7713 - val_loss: 0.5300 - val_acc: 0.7748\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7712 - val_loss: 0.5300 - val_acc: 0.7748\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5384 - acc: 0.7711 - val_loss: 0.5302 - val_acc: 0.7748\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.52964 to 0.52933, saving model to best.model\n",
      "0s - loss: 0.5371 - acc: 0.7712 - val_loss: 0.5293 - val_acc: 0.7748\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7714 - val_loss: 0.5298 - val_acc: 0.7748\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5383 - acc: 0.7705 - val_loss: 0.5302 - val_acc: 0.7748\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7714 - val_loss: 0.5296 - val_acc: 0.7748\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5383 - acc: 0.7709 - val_loss: 0.5300 - val_acc: 0.7748\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7713 - val_loss: 0.5309 - val_acc: 0.7748\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5371 - acc: 0.7714 - val_loss: 0.5301 - val_acc: 0.7748\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.52933 to 0.52922, saving model to best.model\n",
      "0s - loss: 0.5375 - acc: 0.7711 - val_loss: 0.5292 - val_acc: 0.7748\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7713 - val_loss: 0.5296 - val_acc: 0.7748\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5363 - acc: 0.7712 - val_loss: 0.5293 - val_acc: 0.7748\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7717 - val_loss: 0.5298 - val_acc: 0.7748\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7711 - val_loss: 0.5296 - val_acc: 0.7748\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5371 - acc: 0.7712 - val_loss: 0.5299 - val_acc: 0.7748\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7712 - val_loss: 0.5298 - val_acc: 0.7748\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7712 - val_loss: 0.5296 - val_acc: 0.7748\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5358 - acc: 0.7719 - val_loss: 0.5294 - val_acc: 0.7748\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.52922 to 0.52897, saving model to best.model\n",
      "0s - loss: 0.5365 - acc: 0.7711 - val_loss: 0.5290 - val_acc: 0.7748\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5366 - acc: 0.7716 - val_loss: 0.5297 - val_acc: 0.7748\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7712 - val_loss: 0.5292 - val_acc: 0.7748\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7712 - val_loss: 0.5294 - val_acc: 0.7748\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5376 - acc: 0.7710 - val_loss: 0.5298 - val_acc: 0.7748\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7708 - val_loss: 0.5297 - val_acc: 0.7748\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7714 - val_loss: 0.5297 - val_acc: 0.7748\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.52897 to 0.52817, saving model to best.model\n",
      "0s - loss: 0.5365 - acc: 0.7714 - val_loss: 0.5282 - val_acc: 0.7772\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7714 - val_loss: 0.5295 - val_acc: 0.7748\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7712 - val_loss: 0.5301 - val_acc: 0.7748\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7714 - val_loss: 0.5294 - val_acc: 0.7748\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7717 - val_loss: 0.5285 - val_acc: 0.7772\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7716 - val_loss: 0.5290 - val_acc: 0.7748\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5367 - acc: 0.7714 - val_loss: 0.5298 - val_acc: 0.7748\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7714 - val_loss: 0.5293 - val_acc: 0.7748\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7717 - val_loss: 0.5292 - val_acc: 0.7748\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7714 - val_loss: 0.5299 - val_acc: 0.7748\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7714 - val_loss: 0.5292 - val_acc: 0.7748\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5372 - acc: 0.7714 - val_loss: 0.5292 - val_acc: 0.7748\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7711 - val_loss: 0.5291 - val_acc: 0.7748\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5357 - acc: 0.7720 - val_loss: 0.5290 - val_acc: 0.7748\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7714 - val_loss: 0.5282 - val_acc: 0.7772\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5372 - acc: 0.7712 - val_loss: 0.5291 - val_acc: 0.7748\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5361 - acc: 0.7715 - val_loss: 0.5303 - val_acc: 0.7748\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5364 - acc: 0.7716 - val_loss: 0.5290 - val_acc: 0.7748\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5359 - acc: 0.7720 - val_loss: 0.5290 - val_acc: 0.7772\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5355 - acc: 0.7719 - val_loss: 0.5288 - val_acc: 0.7772\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5363 - acc: 0.7714 - val_loss: 0.5286 - val_acc: 0.7772\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5364 - acc: 0.7714 - val_loss: 0.5288 - val_acc: 0.7748\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5364 - acc: 0.7711 - val_loss: 0.5299 - val_acc: 0.7748\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7714 - val_loss: 0.5295 - val_acc: 0.7748\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5361 - acc: 0.7719 - val_loss: 0.5296 - val_acc: 0.7748\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7714 - val_loss: 0.5296 - val_acc: 0.7748\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7715 - val_loss: 0.5293 - val_acc: 0.7748\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.53155, saving model to best.model\n",
      "0s - loss: 0.6197 - acc: 0.7281 - val_loss: 0.5315 - val_acc: 0.7760\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5712 - acc: 0.7567 - val_loss: 0.5347 - val_acc: 0.7760\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5602 - acc: 0.7595 - val_loss: 0.5344 - val_acc: 0.7760\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5568 - acc: 0.7594 - val_loss: 0.5347 - val_acc: 0.7760\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7594 - val_loss: 0.5379 - val_acc: 0.7760\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5571 - acc: 0.7594 - val_loss: 0.5334 - val_acc: 0.7760\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7594 - val_loss: 0.5349 - val_acc: 0.7760\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5547 - acc: 0.7594 - val_loss: 0.5332 - val_acc: 0.7760\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7594 - val_loss: 0.5342 - val_acc: 0.7760\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7594 - val_loss: 0.5358 - val_acc: 0.7760\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.53155 to 0.53052, saving model to best.model\n",
      "0s - loss: 0.5534 - acc: 0.7594 - val_loss: 0.5305 - val_acc: 0.7760\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5546 - acc: 0.7594 - val_loss: 0.5325 - val_acc: 0.7760\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7594 - val_loss: 0.5324 - val_acc: 0.7760\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7594 - val_loss: 0.5339 - val_acc: 0.7760\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7594 - val_loss: 0.5330 - val_acc: 0.7760\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7594 - val_loss: 0.5315 - val_acc: 0.7760\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7594 - val_loss: 0.5332 - val_acc: 0.7760\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7594 - val_loss: 0.5343 - val_acc: 0.7760\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7594 - val_loss: 0.5323 - val_acc: 0.7760\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7594 - val_loss: 0.5332 - val_acc: 0.7760\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7594 - val_loss: 0.5338 - val_acc: 0.7760\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5546 - acc: 0.7594 - val_loss: 0.5334 - val_acc: 0.7760\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7594 - val_loss: 0.5323 - val_acc: 0.7760\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7594 - val_loss: 0.5319 - val_acc: 0.7760\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7594 - val_loss: 0.5342 - val_acc: 0.7760\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7594 - val_loss: 0.5318 - val_acc: 0.7760\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7594 - val_loss: 0.5343 - val_acc: 0.7760\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7594 - val_loss: 0.5312 - val_acc: 0.7760\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7594 - val_loss: 0.5359 - val_acc: 0.7760\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5535 - acc: 0.7594 - val_loss: 0.5312 - val_acc: 0.7760\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7594 - val_loss: 0.5378 - val_acc: 0.7760\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7594 - val_loss: 0.5336 - val_acc: 0.7760\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7594 - val_loss: 0.5311 - val_acc: 0.7760\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7594 - val_loss: 0.5326 - val_acc: 0.7760\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7594 - val_loss: 0.5305 - val_acc: 0.7760\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7594 - val_loss: 0.5305 - val_acc: 0.7760\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7594 - val_loss: 0.5311 - val_acc: 0.7760\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54364, saving model to best.model\n",
      "0s - loss: 0.6032 - acc: 0.7333 - val_loss: 0.5436 - val_acc: 0.7653\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5695 - acc: 0.7559 - val_loss: 0.5469 - val_acc: 0.7653\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5619 - acc: 0.7576 - val_loss: 0.5452 - val_acc: 0.7653\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5596 - acc: 0.7581 - val_loss: 0.5446 - val_acc: 0.7653\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5566 - acc: 0.7581 - val_loss: 0.5447 - val_acc: 0.7653\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5578 - acc: 0.7581 - val_loss: 0.5455 - val_acc: 0.7653\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5563 - acc: 0.7581 - val_loss: 0.5445 - val_acc: 0.7653\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7581 - val_loss: 0.5445 - val_acc: 0.7653\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5563 - acc: 0.7581 - val_loss: 0.5457 - val_acc: 0.7653\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5567 - acc: 0.7581 - val_loss: 0.5453 - val_acc: 0.7653\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.54364 to 0.54355, saving model to best.model\n",
      "0s - loss: 0.5559 - acc: 0.7581 - val_loss: 0.5436 - val_acc: 0.7653\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7581 - val_loss: 0.5456 - val_acc: 0.7653\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7581 - val_loss: 0.5498 - val_acc: 0.7653\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7581 - val_loss: 0.5455 - val_acc: 0.7653\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5565 - acc: 0.7581 - val_loss: 0.5455 - val_acc: 0.7653\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7581 - val_loss: 0.5444 - val_acc: 0.7653\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7581 - val_loss: 0.5443 - val_acc: 0.7653\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7581 - val_loss: 0.5444 - val_acc: 0.7653\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7581 - val_loss: 0.5463 - val_acc: 0.7653\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7581 - val_loss: 0.5447 - val_acc: 0.7653\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7581 - val_loss: 0.5441 - val_acc: 0.7653\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5539 - acc: 0.7581 - val_loss: 0.5445 - val_acc: 0.7653\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5550 - acc: 0.7581 - val_loss: 0.5446 - val_acc: 0.7653\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7581 - val_loss: 0.5440 - val_acc: 0.7653\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5550 - acc: 0.7581 - val_loss: 0.5438 - val_acc: 0.7653\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5541 - acc: 0.7581 - val_loss: 0.5442 - val_acc: 0.7653\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7582 - val_loss: 0.5450 - val_acc: 0.7653\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5546 - acc: 0.7582 - val_loss: 0.5443 - val_acc: 0.7653\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7584 - val_loss: 0.5443 - val_acc: 0.7653\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54355 to 0.54352, saving model to best.model\n",
      "0s - loss: 0.5538 - acc: 0.7584 - val_loss: 0.5435 - val_acc: 0.7653\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7587 - val_loss: 0.5448 - val_acc: 0.7653\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54352 to 0.54313, saving model to best.model\n",
      "0s - loss: 0.5532 - acc: 0.7593 - val_loss: 0.5431 - val_acc: 0.7658\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.54313 to 0.54261, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7599 - val_loss: 0.5426 - val_acc: 0.7663\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7602 - val_loss: 0.5432 - val_acc: 0.7665\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7604 - val_loss: 0.5445 - val_acc: 0.7680\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7610 - val_loss: 0.5430 - val_acc: 0.7682\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7609 - val_loss: 0.5442 - val_acc: 0.7682\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54261 to 0.54110, saving model to best.model\n",
      "0s - loss: 0.5505 - acc: 0.7610 - val_loss: 0.5411 - val_acc: 0.7682\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7612 - val_loss: 0.5415 - val_acc: 0.7682\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.54110 to 0.54078, saving model to best.model\n",
      "0s - loss: 0.5518 - acc: 0.7613 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.54078 to 0.54076, saving model to best.model\n",
      "0s - loss: 0.5511 - acc: 0.7612 - val_loss: 0.5408 - val_acc: 0.7685\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7613 - val_loss: 0.5408 - val_acc: 0.7685\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54076 to 0.54032, saving model to best.model\n",
      "0s - loss: 0.5500 - acc: 0.7619 - val_loss: 0.5403 - val_acc: 0.7685\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7618 - val_loss: 0.5408 - val_acc: 0.7685\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7616 - val_loss: 0.5418 - val_acc: 0.7685\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.54032 to 0.54017, saving model to best.model\n",
      "0s - loss: 0.5502 - acc: 0.7619 - val_loss: 0.5402 - val_acc: 0.7685\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7617 - val_loss: 0.5403 - val_acc: 0.7685\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7619 - val_loss: 0.5402 - val_acc: 0.7685\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.54017 to 0.53971, saving model to best.model\n",
      "0s - loss: 0.5491 - acc: 0.7622 - val_loss: 0.5397 - val_acc: 0.7685\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7620 - val_loss: 0.5402 - val_acc: 0.7685\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7621 - val_loss: 0.5405 - val_acc: 0.7685\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7620 - val_loss: 0.5417 - val_acc: 0.7685\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7621 - val_loss: 0.5414 - val_acc: 0.7685\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7621 - val_loss: 0.5398 - val_acc: 0.7694\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7621 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7620 - val_loss: 0.5403 - val_acc: 0.7685\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7623 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7619 - val_loss: 0.5400 - val_acc: 0.7685\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7623 - val_loss: 0.5398 - val_acc: 0.7685\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7620 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7621 - val_loss: 0.5402 - val_acc: 0.7685\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7621 - val_loss: 0.5403 - val_acc: 0.7685\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.53971 to 0.53964, saving model to best.model\n",
      "0s - loss: 0.5489 - acc: 0.7622 - val_loss: 0.5396 - val_acc: 0.7685\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7622 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7623 - val_loss: 0.5405 - val_acc: 0.7694\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7621 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7622 - val_loss: 0.5406 - val_acc: 0.7685\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7623 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7621 - val_loss: 0.5406 - val_acc: 0.7685\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7623 - val_loss: 0.5399 - val_acc: 0.7694\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.53964 to 0.53942, saving model to best.model\n",
      "0s - loss: 0.5484 - acc: 0.7624 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7622 - val_loss: 0.5404 - val_acc: 0.7694\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7624 - val_loss: 0.5396 - val_acc: 0.7685\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7623 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7622 - val_loss: 0.5404 - val_acc: 0.7694\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.53942 to 0.53935, saving model to best.model\n",
      "0s - loss: 0.5482 - acc: 0.7621 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7621 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7623 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7625 - val_loss: 0.5395 - val_acc: 0.7685\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7623 - val_loss: 0.5400 - val_acc: 0.7685\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7624 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7624 - val_loss: 0.5396 - val_acc: 0.7685\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7620 - val_loss: 0.5395 - val_acc: 0.7685\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7623 - val_loss: 0.5397 - val_acc: 0.7685\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7623 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7623 - val_loss: 0.5400 - val_acc: 0.7694\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.53935 to 0.53932, saving model to best.model\n",
      "0s - loss: 0.5477 - acc: 0.7623 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7626 - val_loss: 0.5400 - val_acc: 0.7694\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7623 - val_loss: 0.5401 - val_acc: 0.7685\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7620 - val_loss: 0.5404 - val_acc: 0.7694\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7621 - val_loss: 0.5399 - val_acc: 0.7694\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7621 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7621 - val_loss: 0.5401 - val_acc: 0.7694\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7621 - val_loss: 0.5398 - val_acc: 0.7694\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.53932 to 0.53927, saving model to best.model\n",
      "0s - loss: 0.5479 - acc: 0.7625 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7623 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7623 - val_loss: 0.5398 - val_acc: 0.7694\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7622 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7623 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7620 - val_loss: 0.5398 - val_acc: 0.7694\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7621 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7624 - val_loss: 0.5401 - val_acc: 0.7694\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7621 - val_loss: 0.5400 - val_acc: 0.7694\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7623 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.53927 to 0.53914, saving model to best.model\n",
      "0s - loss: 0.5474 - acc: 0.7624 - val_loss: 0.5391 - val_acc: 0.7694\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7621 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.53914 to 0.53913, saving model to best.model\n",
      "0s - loss: 0.5479 - acc: 0.7621 - val_loss: 0.5391 - val_acc: 0.7694\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7622 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7624 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7624 - val_loss: 0.5400 - val_acc: 0.7694\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7622 - val_loss: 0.5396 - val_acc: 0.7694\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7623 - val_loss: 0.5392 - val_acc: 0.7694\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.5479 - acc: 0.7620 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.53913 to 0.53900, saving model to best.model\n",
      "0s - loss: 0.5475 - acc: 0.7622 - val_loss: 0.5390 - val_acc: 0.7694\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "49s - loss: 0.5476 - acc: 0.7621 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.5470 - acc: 0.7626 - val_loss: 0.5396 - val_acc: 0.7694\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7624 - val_loss: 0.5390 - val_acc: 0.7694\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7625 - val_loss: 0.5392 - val_acc: 0.7694\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7621 - val_loss: 0.5392 - val_acc: 0.7694\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.5470 - acc: 0.7626 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7624 - val_loss: 0.5390 - val_acc: 0.7694\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7624 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7621 - val_loss: 0.5396 - val_acc: 0.7694\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7624 - val_loss: 0.5392 - val_acc: 0.7694\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7620 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7621 - val_loss: 0.5398 - val_acc: 0.7685\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7621 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7625 - val_loss: 0.5397 - val_acc: 0.7685\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7623 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7620 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7620 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7618 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.53900 to 0.53900, saving model to best.model\n",
      "0s - loss: 0.5471 - acc: 0.7624 - val_loss: 0.5390 - val_acc: 0.7694\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7623 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7622 - val_loss: 0.5392 - val_acc: 0.7694\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7620 - val_loss: 0.5390 - val_acc: 0.7694\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.53900 to 0.53888, saving model to best.model\n",
      "0s - loss: 0.5469 - acc: 0.7624 - val_loss: 0.5389 - val_acc: 0.7694\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7620 - val_loss: 0.5396 - val_acc: 0.7685\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7625 - val_loss: 0.5397 - val_acc: 0.7685\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7622 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7619 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7623 - val_loss: 0.5397 - val_acc: 0.7685\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7621 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7625 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7623 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7626 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7623 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7625 - val_loss: 0.5396 - val_acc: 0.7685\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7623 - val_loss: 0.5396 - val_acc: 0.7694\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7621 - val_loss: 0.5396 - val_acc: 0.7694\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7623 - val_loss: 0.5398 - val_acc: 0.7685\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7624 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7621 - val_loss: 0.5397 - val_acc: 0.7694\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7625 - val_loss: 0.5392 - val_acc: 0.7694\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7620 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7620 - val_loss: 0.5391 - val_acc: 0.7694\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7623 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7618 - val_loss: 0.5397 - val_acc: 0.7685\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7621 - val_loss: 0.5398 - val_acc: 0.7685\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7623 - val_loss: 0.5397 - val_acc: 0.7685\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.5475 - acc: 0.7622 - val_loss: 0.5393 - val_acc: 0.7694\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7626 - val_loss: 0.5395 - val_acc: 0.7694\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7623 - val_loss: 0.5394 - val_acc: 0.7694\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54345, saving model to best.model\n",
      "0s - loss: 0.6566 - acc: 0.7028 - val_loss: 0.5435 - val_acc: 0.7655\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54345 to 0.54303, saving model to best.model\n",
      "0s - loss: 0.5912 - acc: 0.7400 - val_loss: 0.5430 - val_acc: 0.7655\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5699 - acc: 0.7556 - val_loss: 0.5431 - val_acc: 0.7655\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5644 - acc: 0.7582 - val_loss: 0.5442 - val_acc: 0.7655\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5611 - acc: 0.7586 - val_loss: 0.5445 - val_acc: 0.7655\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5585 - acc: 0.7586 - val_loss: 0.5458 - val_acc: 0.7655\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7586 - val_loss: 0.5455 - val_acc: 0.7655\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5577 - acc: 0.7586 - val_loss: 0.5468 - val_acc: 0.7655\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5564 - acc: 0.7586 - val_loss: 0.5459 - val_acc: 0.7655\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7586 - val_loss: 0.5461 - val_acc: 0.7655\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5566 - acc: 0.7586 - val_loss: 0.5444 - val_acc: 0.7655\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5560 - acc: 0.7586 - val_loss: 0.5458 - val_acc: 0.7655\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5557 - acc: 0.7586 - val_loss: 0.5466 - val_acc: 0.7655\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7586 - val_loss: 0.5453 - val_acc: 0.7655\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5554 - acc: 0.7586 - val_loss: 0.5471 - val_acc: 0.7655\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5557 - acc: 0.7586 - val_loss: 0.5443 - val_acc: 0.7655\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5557 - acc: 0.7586 - val_loss: 0.5452 - val_acc: 0.7655\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5546 - acc: 0.7586 - val_loss: 0.5456 - val_acc: 0.7655\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5550 - acc: 0.7586 - val_loss: 0.5463 - val_acc: 0.7655\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7586 - val_loss: 0.5456 - val_acc: 0.7655\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7586 - val_loss: 0.5454 - val_acc: 0.7655\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7586 - val_loss: 0.5443 - val_acc: 0.7655\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7586 - val_loss: 0.5442 - val_acc: 0.7655\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7586 - val_loss: 0.5440 - val_acc: 0.7655\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7586 - val_loss: 0.5460 - val_acc: 0.7655\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7586 - val_loss: 0.5456 - val_acc: 0.7655\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7586 - val_loss: 0.5438 - val_acc: 0.7655\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7586 - val_loss: 0.5437 - val_acc: 0.7655\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55001, saving model to best.model\n",
      "0s - loss: 0.6067 - acc: 0.7325 - val_loss: 0.5500 - val_acc: 0.7609\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5661 - acc: 0.7598 - val_loss: 0.5501 - val_acc: 0.7609\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5592 - acc: 0.7615 - val_loss: 0.5504 - val_acc: 0.7609\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7615 - val_loss: 0.5503 - val_acc: 0.7609\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7615 - val_loss: 0.5532 - val_acc: 0.7609\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7615 - val_loss: 0.5514 - val_acc: 0.7609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.55001 to 0.54979, saving model to best.model\n",
      "0s - loss: 0.5526 - acc: 0.7615 - val_loss: 0.5498 - val_acc: 0.7609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.54979 to 0.54961, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7615 - val_loss: 0.5496 - val_acc: 0.7609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.54961 to 0.54955, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7615 - val_loss: 0.5496 - val_acc: 0.7609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.54955 to 0.54933, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7615 - val_loss: 0.5493 - val_acc: 0.7609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.54933 to 0.54909, saving model to best.model\n",
      "0s - loss: 0.5514 - acc: 0.7615 - val_loss: 0.5491 - val_acc: 0.7609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7615 - val_loss: 0.5494 - val_acc: 0.7609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7615 - val_loss: 0.5492 - val_acc: 0.7609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.54909 to 0.54891, saving model to best.model\n",
      "0s - loss: 0.5524 - acc: 0.7615 - val_loss: 0.5489 - val_acc: 0.7609\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7615 - val_loss: 0.5510 - val_acc: 0.7609\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.54891 to 0.54852, saving model to best.model\n",
      "0s - loss: 0.5507 - acc: 0.7615 - val_loss: 0.5485 - val_acc: 0.7609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7615 - val_loss: 0.5503 - val_acc: 0.7609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.54852 to 0.54794, saving model to best.model\n",
      "0s - loss: 0.5507 - acc: 0.7615 - val_loss: 0.5479 - val_acc: 0.7609\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.54794 to 0.54772, saving model to best.model\n",
      "0s - loss: 0.5509 - acc: 0.7615 - val_loss: 0.5477 - val_acc: 0.7609\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.54772 to 0.54770, saving model to best.model\n",
      "0s - loss: 0.5504 - acc: 0.7615 - val_loss: 0.5477 - val_acc: 0.7609\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.54770 to 0.54724, saving model to best.model\n",
      "0s - loss: 0.5502 - acc: 0.7615 - val_loss: 0.5472 - val_acc: 0.7609\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7615 - val_loss: 0.5475 - val_acc: 0.7609\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.54724 to 0.54677, saving model to best.model\n",
      "0s - loss: 0.5499 - acc: 0.7616 - val_loss: 0.5468 - val_acc: 0.7609\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54677 to 0.54609, saving model to best.model\n",
      "0s - loss: 0.5502 - acc: 0.7618 - val_loss: 0.5461 - val_acc: 0.7609\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7618 - val_loss: 0.5474 - val_acc: 0.7609\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54609 to 0.54554, saving model to best.model\n",
      "0s - loss: 0.5493 - acc: 0.7622 - val_loss: 0.5455 - val_acc: 0.7609\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54554 to 0.54497, saving model to best.model\n",
      "0s - loss: 0.5500 - acc: 0.7625 - val_loss: 0.5450 - val_acc: 0.7609\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54497 to 0.54489, saving model to best.model\n",
      "0s - loss: 0.5496 - acc: 0.7630 - val_loss: 0.5449 - val_acc: 0.7609\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54489 to 0.54456, saving model to best.model\n",
      "0s - loss: 0.5484 - acc: 0.7629 - val_loss: 0.5446 - val_acc: 0.7643\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54456 to 0.54381, saving model to best.model\n",
      "0s - loss: 0.5485 - acc: 0.7637 - val_loss: 0.5438 - val_acc: 0.7651\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54381 to 0.54346, saving model to best.model\n",
      "0s - loss: 0.5485 - acc: 0.7639 - val_loss: 0.5435 - val_acc: 0.7675\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54346 to 0.54265, saving model to best.model\n",
      "0s - loss: 0.5482 - acc: 0.7640 - val_loss: 0.5427 - val_acc: 0.7677\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.54265 to 0.54183, saving model to best.model\n",
      "0s - loss: 0.5464 - acc: 0.7648 - val_loss: 0.5418 - val_acc: 0.7677\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7655 - val_loss: 0.5419 - val_acc: 0.7680\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54183 to 0.54035, saving model to best.model\n",
      "0s - loss: 0.5474 - acc: 0.7649 - val_loss: 0.5403 - val_acc: 0.7677\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7652 - val_loss: 0.5408 - val_acc: 0.7680\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54035 to 0.53905, saving model to best.model\n",
      "0s - loss: 0.5451 - acc: 0.7663 - val_loss: 0.5391 - val_acc: 0.7692\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.53905 to 0.53898, saving model to best.model\n",
      "0s - loss: 0.5448 - acc: 0.7664 - val_loss: 0.5390 - val_acc: 0.7692\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.53898 to 0.53878, saving model to best.model\n",
      "0s - loss: 0.5441 - acc: 0.7663 - val_loss: 0.5388 - val_acc: 0.7692\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7666 - val_loss: 0.5392 - val_acc: 0.7692\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.53878 to 0.53817, saving model to best.model\n",
      "0s - loss: 0.5441 - acc: 0.7661 - val_loss: 0.5382 - val_acc: 0.7692\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.53817 to 0.53771, saving model to best.model\n",
      "0s - loss: 0.5450 - acc: 0.7666 - val_loss: 0.5377 - val_acc: 0.7692\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5447 - acc: 0.7662 - val_loss: 0.5379 - val_acc: 0.7692\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5447 - acc: 0.7664 - val_loss: 0.5382 - val_acc: 0.7692\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7665 - val_loss: 0.5381 - val_acc: 0.7707\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7668 - val_loss: 0.5385 - val_acc: 0.7707\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7664 - val_loss: 0.5380 - val_acc: 0.7707\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.53771 to 0.53679, saving model to best.model\n",
      "0s - loss: 0.5438 - acc: 0.7667 - val_loss: 0.5368 - val_acc: 0.7707\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7669 - val_loss: 0.5372 - val_acc: 0.7707\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.53679 to 0.53648, saving model to best.model\n",
      "0s - loss: 0.5432 - acc: 0.7671 - val_loss: 0.5365 - val_acc: 0.7707\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7670 - val_loss: 0.5368 - val_acc: 0.7707\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.53648 to 0.53621, saving model to best.model\n",
      "0s - loss: 0.5429 - acc: 0.7671 - val_loss: 0.5362 - val_acc: 0.7707\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7672 - val_loss: 0.5363 - val_acc: 0.7707\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7674 - val_loss: 0.5368 - val_acc: 0.7707\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7671 - val_loss: 0.5367 - val_acc: 0.7707\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7672 - val_loss: 0.5363 - val_acc: 0.7707\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7672 - val_loss: 0.5362 - val_acc: 0.7707\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7672 - val_loss: 0.5385 - val_acc: 0.7707\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.53621 to 0.53596, saving model to best.model\n",
      "0s - loss: 0.5426 - acc: 0.7672 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7669 - val_loss: 0.5362 - val_acc: 0.7707\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7671 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7671 - val_loss: 0.5361 - val_acc: 0.7707\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7671 - val_loss: 0.5361 - val_acc: 0.7707\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.53596 to 0.53595, saving model to best.model\n",
      "0s - loss: 0.5422 - acc: 0.7675 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7672 - val_loss: 0.5367 - val_acc: 0.7707\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7673 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7674 - val_loss: 0.5361 - val_acc: 0.7707\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7674 - val_loss: 0.5362 - val_acc: 0.7707\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7673 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.53595 to 0.53593, saving model to best.model\n",
      "0s - loss: 0.5422 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.53593 to 0.53574, saving model to best.model\n",
      "0s - loss: 0.5422 - acc: 0.7671 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7672 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7671 - val_loss: 0.5364 - val_acc: 0.7707\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7674 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7675 - val_loss: 0.5371 - val_acc: 0.7707\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7672 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7675 - val_loss: 0.5363 - val_acc: 0.7707\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7675 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7675 - val_loss: 0.5365 - val_acc: 0.7707\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7671 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.53574 to 0.53564, saving model to best.model\n",
      "0s - loss: 0.5418 - acc: 0.7675 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7671 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7672 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7672 - val_loss: 0.5365 - val_acc: 0.7707\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7674 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7675 - val_loss: 0.5361 - val_acc: 0.7707\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7675 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7674 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7673 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7673 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7673 - val_loss: 0.5364 - val_acc: 0.7707\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7671 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7672 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7672 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7672 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7674 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7677 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7672 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.53564 to 0.53557, saving model to best.model\n",
      "0s - loss: 0.5414 - acc: 0.7675 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7672 - val_loss: 0.5362 - val_acc: 0.7707\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7671 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7674 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7675 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7675 - val_loss: 0.5362 - val_acc: 0.7707\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7672 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7675 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7673 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7672 - val_loss: 0.5364 - val_acc: 0.7707\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7670 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7671 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7673 - val_loss: 0.5361 - val_acc: 0.7707\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7675 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7671 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7674 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.53557 to 0.53556, saving model to best.model\n",
      "0s - loss: 0.5416 - acc: 0.7674 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7674 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7676 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7675 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7674 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.53556 to 0.53554, saving model to best.model\n",
      "0s - loss: 0.5416 - acc: 0.7672 - val_loss: 0.5355 - val_acc: 0.7707\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.53554 to 0.53552, saving model to best.model\n",
      "0s - loss: 0.5417 - acc: 0.7671 - val_loss: 0.5355 - val_acc: 0.7707\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7674 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7675 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7676 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7675 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7674 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7675 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7674 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7673 - val_loss: 0.5355 - val_acc: 0.7707\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.53552 to 0.53550, saving model to best.model\n",
      "0s - loss: 0.5412 - acc: 0.7674 - val_loss: 0.5355 - val_acc: 0.7707\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7673 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7675 - val_loss: 0.5355 - val_acc: 0.7707\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7674 - val_loss: 0.5355 - val_acc: 0.7707\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7674 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7673 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7677 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7671 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7674 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7671 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7673 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7676 - val_loss: 0.5355 - val_acc: 0.7707\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7675 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7675 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7673 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7672 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7672 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7674 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7672 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7671 - val_loss: 0.5356 - val_acc: 0.7707\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7677 - val_loss: 0.5357 - val_acc: 0.7707\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7675 - val_loss: 0.5358 - val_acc: 0.7707\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7673 - val_loss: 0.5360 - val_acc: 0.7707\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7674 - val_loss: 0.5359 - val_acc: 0.7707\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55432, saving model to best.model\n",
      "0s - loss: 0.6192 - acc: 0.7234 - val_loss: 0.5543 - val_acc: 0.7568\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5744 - acc: 0.7520 - val_loss: 0.5544 - val_acc: 0.7568\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5676 - acc: 0.7533 - val_loss: 0.5546 - val_acc: 0.7568\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5649 - acc: 0.7536 - val_loss: 0.5550 - val_acc: 0.7568\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5633 - acc: 0.7536 - val_loss: 0.5589 - val_acc: 0.7568\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5629 - acc: 0.7536 - val_loss: 0.5555 - val_acc: 0.7568\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5619 - acc: 0.7536 - val_loss: 0.5555 - val_acc: 0.7568\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5619 - acc: 0.7536 - val_loss: 0.5557 - val_acc: 0.7568\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5607 - acc: 0.7536 - val_loss: 0.5566 - val_acc: 0.7568\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5612 - acc: 0.7536 - val_loss: 0.5556 - val_acc: 0.7568\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5604 - acc: 0.7536 - val_loss: 0.5559 - val_acc: 0.7568\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.55432 to 0.55407, saving model to best.model\n",
      "0s - loss: 0.5612 - acc: 0.7536 - val_loss: 0.5541 - val_acc: 0.7568\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5608 - acc: 0.7536 - val_loss: 0.5557 - val_acc: 0.7568\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.55407 to 0.55390, saving model to best.model\n",
      "0s - loss: 0.5604 - acc: 0.7536 - val_loss: 0.5539 - val_acc: 0.7568\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5607 - acc: 0.7536 - val_loss: 0.5552 - val_acc: 0.7568\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5612 - acc: 0.7536 - val_loss: 0.5547 - val_acc: 0.7568\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.55390 to 0.55363, saving model to best.model\n",
      "0s - loss: 0.5598 - acc: 0.7536 - val_loss: 0.5536 - val_acc: 0.7568\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5603 - acc: 0.7536 - val_loss: 0.5590 - val_acc: 0.7568\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5607 - acc: 0.7536 - val_loss: 0.5538 - val_acc: 0.7568\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.55363 to 0.55290, saving model to best.model\n",
      "0s - loss: 0.5602 - acc: 0.7536 - val_loss: 0.5529 - val_acc: 0.7568\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.55290 to 0.55245, saving model to best.model\n",
      "0s - loss: 0.5584 - acc: 0.7536 - val_loss: 0.5524 - val_acc: 0.7568\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5596 - acc: 0.7536 - val_loss: 0.5534 - val_acc: 0.7568\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.55245 to 0.55233, saving model to best.model\n",
      "0s - loss: 0.5595 - acc: 0.7536 - val_loss: 0.5523 - val_acc: 0.7568\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5596 - acc: 0.7536 - val_loss: 0.5526 - val_acc: 0.7568\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5595 - acc: 0.7536 - val_loss: 0.5525 - val_acc: 0.7568\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.55233 to 0.55176, saving model to best.model\n",
      "0s - loss: 0.5595 - acc: 0.7536 - val_loss: 0.5518 - val_acc: 0.7568\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7536 - val_loss: 0.5520 - val_acc: 0.7568\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.55176 to 0.55133, saving model to best.model\n",
      "0s - loss: 0.5584 - acc: 0.7536 - val_loss: 0.5513 - val_acc: 0.7568\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.55133 to 0.55104, saving model to best.model\n",
      "0s - loss: 0.5586 - acc: 0.7536 - val_loss: 0.5510 - val_acc: 0.7568\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.55104 to 0.55047, saving model to best.model\n",
      "0s - loss: 0.5590 - acc: 0.7536 - val_loss: 0.5505 - val_acc: 0.7568\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.55047 to 0.55009, saving model to best.model\n",
      "0s - loss: 0.5575 - acc: 0.7538 - val_loss: 0.5501 - val_acc: 0.7568\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.55009 to 0.54922, saving model to best.model\n",
      "0s - loss: 0.5575 - acc: 0.7542 - val_loss: 0.5492 - val_acc: 0.7587\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.54922 to 0.54825, saving model to best.model\n",
      "0s - loss: 0.5577 - acc: 0.7553 - val_loss: 0.5482 - val_acc: 0.7607\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54825 to 0.54701, saving model to best.model\n",
      "0s - loss: 0.5567 - acc: 0.7555 - val_loss: 0.5470 - val_acc: 0.7619\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54701 to 0.54662, saving model to best.model\n",
      "0s - loss: 0.5563 - acc: 0.7567 - val_loss: 0.5466 - val_acc: 0.7624\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.54662 to 0.54472, saving model to best.model\n",
      "0s - loss: 0.5554 - acc: 0.7573 - val_loss: 0.5447 - val_acc: 0.7638\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54472 to 0.54302, saving model to best.model\n",
      "0s - loss: 0.5554 - acc: 0.7578 - val_loss: 0.5430 - val_acc: 0.7672\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54302 to 0.54271, saving model to best.model\n",
      "0s - loss: 0.5548 - acc: 0.7586 - val_loss: 0.5427 - val_acc: 0.7672\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.54271 to 0.54167, saving model to best.model\n",
      "0s - loss: 0.5539 - acc: 0.7584 - val_loss: 0.5417 - val_acc: 0.7672\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5539 - acc: 0.7591 - val_loss: 0.5418 - val_acc: 0.7672\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.54167 to 0.54081, saving model to best.model\n",
      "0s - loss: 0.5525 - acc: 0.7599 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7593 - val_loss: 0.5412 - val_acc: 0.7672\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54081 to 0.53991, saving model to best.model\n",
      "0s - loss: 0.5534 - acc: 0.7599 - val_loss: 0.5399 - val_acc: 0.7672\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7595 - val_loss: 0.5400 - val_acc: 0.7672\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.53991 to 0.53987, saving model to best.model\n",
      "0s - loss: 0.5525 - acc: 0.7603 - val_loss: 0.5399 - val_acc: 0.7672\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.53987 to 0.53968, saving model to best.model\n",
      "0s - loss: 0.5525 - acc: 0.7603 - val_loss: 0.5397 - val_acc: 0.7672\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.53968 to 0.53946, saving model to best.model\n",
      "0s - loss: 0.5524 - acc: 0.7599 - val_loss: 0.5395 - val_acc: 0.7672\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7597 - val_loss: 0.5397 - val_acc: 0.7672\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7598 - val_loss: 0.5398 - val_acc: 0.7672\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7599 - val_loss: 0.5412 - val_acc: 0.7672\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.53946 to 0.53906, saving model to best.model\n",
      "0s - loss: 0.5517 - acc: 0.7596 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7596 - val_loss: 0.5395 - val_acc: 0.7672\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7601 - val_loss: 0.5401 - val_acc: 0.7672\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7599 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7600 - val_loss: 0.5393 - val_acc: 0.7672\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7602 - val_loss: 0.5408 - val_acc: 0.7672\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7603 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.53906 to 0.53895, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7599 - val_loss: 0.5389 - val_acc: 0.7672\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.53895 to 0.53865, saving model to best.model\n",
      "0s - loss: 0.5505 - acc: 0.7600 - val_loss: 0.5386 - val_acc: 0.7672\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7601 - val_loss: 0.5400 - val_acc: 0.7672\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7600 - val_loss: 0.5395 - val_acc: 0.7672\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7594 - val_loss: 0.5393 - val_acc: 0.7672\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7601 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7599 - val_loss: 0.5403 - val_acc: 0.7672\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7598 - val_loss: 0.5390 - val_acc: 0.7672\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7603 - val_loss: 0.5392 - val_acc: 0.7672\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7599 - val_loss: 0.5389 - val_acc: 0.7672\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7599 - val_loss: 0.5387 - val_acc: 0.7672\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7597 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7596 - val_loss: 0.5393 - val_acc: 0.7672\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7598 - val_loss: 0.5388 - val_acc: 0.7672\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7601 - val_loss: 0.5392 - val_acc: 0.7672\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7600 - val_loss: 0.5398 - val_acc: 0.7672\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7598 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7596 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7601 - val_loss: 0.5393 - val_acc: 0.7672\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7604 - val_loss: 0.5390 - val_acc: 0.7672\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7600 - val_loss: 0.5392 - val_acc: 0.7672\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7603 - val_loss: 0.5387 - val_acc: 0.7672\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7601 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7599 - val_loss: 0.5399 - val_acc: 0.7672\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7604 - val_loss: 0.5391 - val_acc: 0.7672\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7598 - val_loss: 0.5390 - val_acc: 0.7672\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7594 - val_loss: 0.5396 - val_acc: 0.7672\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7601 - val_loss: 0.5397 - val_acc: 0.7672\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54342, saving model to best.model\n",
      "0s - loss: 0.6822 - acc: 0.6918 - val_loss: 0.5434 - val_acc: 0.7658\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5964 - acc: 0.7352 - val_loss: 0.5438 - val_acc: 0.7658\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5732 - acc: 0.7532 - val_loss: 0.5449 - val_acc: 0.7658\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5651 - acc: 0.7589 - val_loss: 0.5436 - val_acc: 0.7658\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5622 - acc: 0.7592 - val_loss: 0.5456 - val_acc: 0.7658\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5589 - acc: 0.7591 - val_loss: 0.5442 - val_acc: 0.7658\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5566 - acc: 0.7592 - val_loss: 0.5461 - val_acc: 0.7658\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5568 - acc: 0.7592 - val_loss: 0.5438 - val_acc: 0.7658\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5566 - acc: 0.7592 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7592 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7592 - val_loss: 0.5439 - val_acc: 0.7658\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7592 - val_loss: 0.5450 - val_acc: 0.7658\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7592 - val_loss: 0.5436 - val_acc: 0.7658\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5547 - acc: 0.7592 - val_loss: 0.5453 - val_acc: 0.7658\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.54342 to 0.54336, saving model to best.model\n",
      "0s - loss: 0.5539 - acc: 0.7593 - val_loss: 0.5434 - val_acc: 0.7658\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5535 - acc: 0.7594 - val_loss: 0.5435 - val_acc: 0.7658\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.54336 to 0.54323, saving model to best.model\n",
      "0s - loss: 0.5535 - acc: 0.7593 - val_loss: 0.5432 - val_acc: 0.7658\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.54323 to 0.54245, saving model to best.model\n",
      "0s - loss: 0.5518 - acc: 0.7596 - val_loss: 0.5425 - val_acc: 0.7660\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7602 - val_loss: 0.5428 - val_acc: 0.7665\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7609 - val_loss: 0.5428 - val_acc: 0.7670\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.54245 to 0.54208, saving model to best.model\n",
      "0s - loss: 0.5530 - acc: 0.7606 - val_loss: 0.5421 - val_acc: 0.7670\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.54208 to 0.54100, saving model to best.model\n",
      "0s - loss: 0.5501 - acc: 0.7611 - val_loss: 0.5410 - val_acc: 0.7677\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7619 - val_loss: 0.5419 - val_acc: 0.7687\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54100 to 0.53975, saving model to best.model\n",
      "0s - loss: 0.5505 - acc: 0.7622 - val_loss: 0.5398 - val_acc: 0.7687\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5501 - acc: 0.7627 - val_loss: 0.5409 - val_acc: 0.7692\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.53975 to 0.53887, saving model to best.model\n",
      "0s - loss: 0.5502 - acc: 0.7627 - val_loss: 0.5389 - val_acc: 0.7692\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.53887 to 0.53847, saving model to best.model\n",
      "0s - loss: 0.5490 - acc: 0.7633 - val_loss: 0.5385 - val_acc: 0.7699\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.53847 to 0.53831, saving model to best.model\n",
      "0s - loss: 0.5485 - acc: 0.7634 - val_loss: 0.5383 - val_acc: 0.7699\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.53831 to 0.53800, saving model to best.model\n",
      "0s - loss: 0.5483 - acc: 0.7636 - val_loss: 0.5380 - val_acc: 0.7699\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7641 - val_loss: 0.5383 - val_acc: 0.7707\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.53800 to 0.53787, saving model to best.model\n",
      "0s - loss: 0.5479 - acc: 0.7645 - val_loss: 0.5379 - val_acc: 0.7707\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.53787 to 0.53745, saving model to best.model\n",
      "0s - loss: 0.5483 - acc: 0.7640 - val_loss: 0.5374 - val_acc: 0.7707\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.53745 to 0.53722, saving model to best.model\n",
      "0s - loss: 0.5482 - acc: 0.7644 - val_loss: 0.5372 - val_acc: 0.7707\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53722 to 0.53697, saving model to best.model\n",
      "0s - loss: 0.5467 - acc: 0.7644 - val_loss: 0.5370 - val_acc: 0.7707\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.53697 to 0.53663, saving model to best.model\n",
      "0s - loss: 0.5470 - acc: 0.7643 - val_loss: 0.5366 - val_acc: 0.7707\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.53663 to 0.53643, saving model to best.model\n",
      "0s - loss: 0.5461 - acc: 0.7646 - val_loss: 0.5364 - val_acc: 0.7707\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.53643 to 0.53606, saving model to best.model\n",
      "0s - loss: 0.5462 - acc: 0.7649 - val_loss: 0.5361 - val_acc: 0.7707\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.53606 to 0.53598, saving model to best.model\n",
      "0s - loss: 0.5464 - acc: 0.7650 - val_loss: 0.5360 - val_acc: 0.7726\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5455 - acc: 0.7654 - val_loss: 0.5363 - val_acc: 0.7726\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7655 - val_loss: 0.5373 - val_acc: 0.7726\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5466 - acc: 0.7650 - val_loss: 0.5361 - val_acc: 0.7726\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.53598 to 0.53540, saving model to best.model\n",
      "0s - loss: 0.5455 - acc: 0.7657 - val_loss: 0.5354 - val_acc: 0.7726\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.53540 to 0.53293, saving model to best.model\n",
      "0s - loss: 0.5449 - acc: 0.7661 - val_loss: 0.5329 - val_acc: 0.7726\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5573 - acc: 0.7573 - val_loss: 0.5419 - val_acc: 0.7677\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7617 - val_loss: 0.5408 - val_acc: 0.7680\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7621 - val_loss: 0.5418 - val_acc: 0.7687\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7623 - val_loss: 0.5406 - val_acc: 0.7692\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7628 - val_loss: 0.5393 - val_acc: 0.7692\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7637 - val_loss: 0.5392 - val_acc: 0.7699\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7638 - val_loss: 0.5386 - val_acc: 0.7699\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5467 - acc: 0.7637 - val_loss: 0.5385 - val_acc: 0.7699\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7635 - val_loss: 0.5384 - val_acc: 0.7699\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7636 - val_loss: 0.5382 - val_acc: 0.7699\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5467 - acc: 0.7639 - val_loss: 0.5388 - val_acc: 0.7699\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5466 - acc: 0.7642 - val_loss: 0.5388 - val_acc: 0.7699\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7637 - val_loss: 0.5383 - val_acc: 0.7699\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7637 - val_loss: 0.5382 - val_acc: 0.7699\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7638 - val_loss: 0.5379 - val_acc: 0.7699\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7641 - val_loss: 0.5392 - val_acc: 0.7699\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7637 - val_loss: 0.5381 - val_acc: 0.7699\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7641 - val_loss: 0.5387 - val_acc: 0.7699\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7637 - val_loss: 0.5385 - val_acc: 0.7699\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5467 - acc: 0.7641 - val_loss: 0.5379 - val_acc: 0.7699\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7641 - val_loss: 0.5408 - val_acc: 0.7699\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7642 - val_loss: 0.5388 - val_acc: 0.7699\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7638 - val_loss: 0.5378 - val_acc: 0.7699\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7643 - val_loss: 0.5381 - val_acc: 0.7707\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7637 - val_loss: 0.5376 - val_acc: 0.7707\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7645 - val_loss: 0.5379 - val_acc: 0.7707\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55329, saving model to best.model\n",
      "0s - loss: 0.6198 - acc: 0.7252 - val_loss: 0.5533 - val_acc: 0.7578\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5737 - acc: 0.7525 - val_loss: 0.5546 - val_acc: 0.7578\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55329 to 0.55271, saving model to best.model\n",
      "0s - loss: 0.5656 - acc: 0.7568 - val_loss: 0.5527 - val_acc: 0.7578\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.55271 to 0.55198, saving model to best.model\n",
      "0s - loss: 0.5618 - acc: 0.7575 - val_loss: 0.5520 - val_acc: 0.7578\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5594 - acc: 0.7575 - val_loss: 0.5526 - val_acc: 0.7578\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5591 - acc: 0.7575 - val_loss: 0.5568 - val_acc: 0.7578\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5590 - acc: 0.7575 - val_loss: 0.5532 - val_acc: 0.7578\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7575 - val_loss: 0.5543 - val_acc: 0.7578\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5565 - acc: 0.7575 - val_loss: 0.5528 - val_acc: 0.7578\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7575 - val_loss: 0.5529 - val_acc: 0.7578\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5568 - acc: 0.7575 - val_loss: 0.5526 - val_acc: 0.7578\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5564 - acc: 0.7575 - val_loss: 0.5525 - val_acc: 0.7578\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5567 - acc: 0.7575 - val_loss: 0.5525 - val_acc: 0.7578\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5570 - acc: 0.7575 - val_loss: 0.5525 - val_acc: 0.7578\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5549 - acc: 0.7575 - val_loss: 0.5529 - val_acc: 0.7578\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5552 - acc: 0.7575 - val_loss: 0.5524 - val_acc: 0.7578\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7575 - val_loss: 0.5543 - val_acc: 0.7578\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5564 - acc: 0.7575 - val_loss: 0.5524 - val_acc: 0.7578\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7575 - val_loss: 0.5534 - val_acc: 0.7578\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5546 - acc: 0.7575 - val_loss: 0.5521 - val_acc: 0.7578\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.55198 to 0.55185, saving model to best.model\n",
      "0s - loss: 0.5538 - acc: 0.7575 - val_loss: 0.5518 - val_acc: 0.7578\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.55185 to 0.55176, saving model to best.model\n",
      "0s - loss: 0.5548 - acc: 0.7575 - val_loss: 0.5518 - val_acc: 0.7578\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7575 - val_loss: 0.5519 - val_acc: 0.7578\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.55176 to 0.55128, saving model to best.model\n",
      "0s - loss: 0.5551 - acc: 0.7575 - val_loss: 0.5513 - val_acc: 0.7578\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7575 - val_loss: 0.5515 - val_acc: 0.7578\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.55128 to 0.55119, saving model to best.model\n",
      "0s - loss: 0.5542 - acc: 0.7575 - val_loss: 0.5512 - val_acc: 0.7578\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.55119 to 0.55087, saving model to best.model\n",
      "0s - loss: 0.5539 - acc: 0.7575 - val_loss: 0.5509 - val_acc: 0.7578\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.55087 to 0.55053, saving model to best.model\n",
      "0s - loss: 0.5534 - acc: 0.7578 - val_loss: 0.5505 - val_acc: 0.7578\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7582 - val_loss: 0.5508 - val_acc: 0.7600\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7588 - val_loss: 0.5520 - val_acc: 0.7604\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.55053 to 0.54925, saving model to best.model\n",
      "0s - loss: 0.5531 - acc: 0.7595 - val_loss: 0.5493 - val_acc: 0.7612\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54925 to 0.54884, saving model to best.model\n",
      "0s - loss: 0.5523 - acc: 0.7607 - val_loss: 0.5488 - val_acc: 0.7612\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7609 - val_loss: 0.5496 - val_acc: 0.7619\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54884 to 0.54801, saving model to best.model\n",
      "0s - loss: 0.5514 - acc: 0.7610 - val_loss: 0.5480 - val_acc: 0.7619\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54801 to 0.54737, saving model to best.model\n",
      "0s - loss: 0.5504 - acc: 0.7621 - val_loss: 0.5474 - val_acc: 0.7619\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7627 - val_loss: 0.5475 - val_acc: 0.7624\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54737 to 0.54704, saving model to best.model\n",
      "0s - loss: 0.5492 - acc: 0.7625 - val_loss: 0.5470 - val_acc: 0.7626\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54704 to 0.54652, saving model to best.model\n",
      "0s - loss: 0.5482 - acc: 0.7635 - val_loss: 0.5465 - val_acc: 0.7626\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7628 - val_loss: 0.5473 - val_acc: 0.7626\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.54652 to 0.54611, saving model to best.model\n",
      "0s - loss: 0.5488 - acc: 0.7635 - val_loss: 0.5461 - val_acc: 0.7626\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.54611 to 0.54601, saving model to best.model\n",
      "0s - loss: 0.5472 - acc: 0.7641 - val_loss: 0.5460 - val_acc: 0.7626\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7637 - val_loss: 0.5460 - val_acc: 0.7626\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54601 to 0.54552, saving model to best.model\n",
      "0s - loss: 0.5473 - acc: 0.7638 - val_loss: 0.5455 - val_acc: 0.7626\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7634 - val_loss: 0.5459 - val_acc: 0.7626\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.54552 to 0.54522, saving model to best.model\n",
      "0s - loss: 0.5472 - acc: 0.7640 - val_loss: 0.5452 - val_acc: 0.7626\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7640 - val_loss: 0.5452 - val_acc: 0.7626\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7638 - val_loss: 0.5458 - val_acc: 0.7626\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7641 - val_loss: 0.5455 - val_acc: 0.7626\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7635 - val_loss: 0.5467 - val_acc: 0.7626\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7639 - val_loss: 0.5462 - val_acc: 0.7626\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7644 - val_loss: 0.5459 - val_acc: 0.7626\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.54522 to 0.54512, saving model to best.model\n",
      "0s - loss: 0.5475 - acc: 0.7640 - val_loss: 0.5451 - val_acc: 0.7626\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.54512 to 0.54505, saving model to best.model\n",
      "0s - loss: 0.5470 - acc: 0.7637 - val_loss: 0.5450 - val_acc: 0.7626\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.54505 to 0.54437, saving model to best.model\n",
      "0s - loss: 0.5460 - acc: 0.7643 - val_loss: 0.5444 - val_acc: 0.7626\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7647 - val_loss: 0.5457 - val_acc: 0.7626\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7643 - val_loss: 0.5465 - val_acc: 0.7626\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7637 - val_loss: 0.5456 - val_acc: 0.7626\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5467 - acc: 0.7642 - val_loss: 0.5452 - val_acc: 0.7626\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7636 - val_loss: 0.5455 - val_acc: 0.7626\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7641 - val_loss: 0.5445 - val_acc: 0.7626\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7639 - val_loss: 0.5452 - val_acc: 0.7626\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7647 - val_loss: 0.5454 - val_acc: 0.7626\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7639 - val_loss: 0.5452 - val_acc: 0.7626\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7637 - val_loss: 0.5453 - val_acc: 0.7626\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7639 - val_loss: 0.5457 - val_acc: 0.7626\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7646 - val_loss: 0.5454 - val_acc: 0.7626\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5458 - acc: 0.7641 - val_loss: 0.5455 - val_acc: 0.7626\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7644 - val_loss: 0.5451 - val_acc: 0.7626\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5456 - acc: 0.7644 - val_loss: 0.5453 - val_acc: 0.7626\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7647 - val_loss: 0.5451 - val_acc: 0.7626\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5455 - acc: 0.7643 - val_loss: 0.5454 - val_acc: 0.7626\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7640 - val_loss: 0.5454 - val_acc: 0.7626\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7642 - val_loss: 0.5453 - val_acc: 0.7626\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5468 - acc: 0.7641 - val_loss: 0.5456 - val_acc: 0.7626\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7640 - val_loss: 0.5452 - val_acc: 0.7626\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7639 - val_loss: 0.5452 - val_acc: 0.7626\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5453 - acc: 0.7644 - val_loss: 0.5444 - val_acc: 0.7626\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5463 - acc: 0.7640 - val_loss: 0.5457 - val_acc: 0.7626\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7639 - val_loss: 0.5457 - val_acc: 0.7626\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5450 - acc: 0.7643 - val_loss: 0.5445 - val_acc: 0.7626\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55821, saving model to best.model\n",
      "0s - loss: 0.6096 - acc: 0.7282 - val_loss: 0.5582 - val_acc: 0.7536\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55821 to 0.55723, saving model to best.model\n",
      "0s - loss: 0.5715 - acc: 0.7548 - val_loss: 0.5572 - val_acc: 0.7536\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55723 to 0.55700, saving model to best.model\n",
      "0s - loss: 0.5628 - acc: 0.7562 - val_loss: 0.5570 - val_acc: 0.7536\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "1s - loss: 0.5590 - acc: 0.7565 - val_loss: 0.5570 - val_acc: 0.7536\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5601 - acc: 0.7565 - val_loss: 0.5576 - val_acc: 0.7536\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5591 - acc: 0.7565 - val_loss: 0.5579 - val_acc: 0.7536\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5580 - acc: 0.7565 - val_loss: 0.5573 - val_acc: 0.7536\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5575 - acc: 0.7565 - val_loss: 0.5574 - val_acc: 0.7536\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5578 - acc: 0.7565 - val_loss: 0.5591 - val_acc: 0.7536\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5567 - acc: 0.7565 - val_loss: 0.5578 - val_acc: 0.7536\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7565 - val_loss: 0.5589 - val_acc: 0.7536\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5586 - acc: 0.7565 - val_loss: 0.5573 - val_acc: 0.7536\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5575 - acc: 0.7565 - val_loss: 0.5577 - val_acc: 0.7536\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5571 - acc: 0.7565 - val_loss: 0.5575 - val_acc: 0.7536\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.55700 to 0.55699, saving model to best.model\n",
      "0s - loss: 0.5572 - acc: 0.7565 - val_loss: 0.5570 - val_acc: 0.7536\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.55699 to 0.55676, saving model to best.model\n",
      "0s - loss: 0.5566 - acc: 0.7565 - val_loss: 0.5568 - val_acc: 0.7536\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5571 - acc: 0.7565 - val_loss: 0.5569 - val_acc: 0.7536\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5568 - acc: 0.7565 - val_loss: 0.5568 - val_acc: 0.7536\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7567 - val_loss: 0.5577 - val_acc: 0.7536\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.55676 to 0.55577, saving model to best.model\n",
      "0s - loss: 0.5547 - acc: 0.7568 - val_loss: 0.5558 - val_acc: 0.7536\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.55577 to 0.55531, saving model to best.model\n",
      "0s - loss: 0.5551 - acc: 0.7570 - val_loss: 0.5553 - val_acc: 0.7541\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.55531 to 0.55463, saving model to best.model\n",
      "0s - loss: 0.5543 - acc: 0.7578 - val_loss: 0.5546 - val_acc: 0.7549\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.55463 to 0.55397, saving model to best.model\n",
      "0s - loss: 0.5532 - acc: 0.7590 - val_loss: 0.5540 - val_acc: 0.7563\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.55397 to 0.55345, saving model to best.model\n",
      "0s - loss: 0.5531 - acc: 0.7594 - val_loss: 0.5535 - val_acc: 0.7568\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.55345 to 0.55257, saving model to best.model\n",
      "0s - loss: 0.5522 - acc: 0.7601 - val_loss: 0.5526 - val_acc: 0.7570\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.55257 to 0.55221, saving model to best.model\n",
      "0s - loss: 0.5510 - acc: 0.7608 - val_loss: 0.5522 - val_acc: 0.7570\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.55221 to 0.55165, saving model to best.model\n",
      "0s - loss: 0.5504 - acc: 0.7621 - val_loss: 0.5517 - val_acc: 0.7575\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.55165 to 0.55153, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7612 - val_loss: 0.5515 - val_acc: 0.7570\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.55153 to 0.55053, saving model to best.model\n",
      "0s - loss: 0.5489 - acc: 0.7621 - val_loss: 0.5505 - val_acc: 0.7575\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.55053 to 0.54986, saving model to best.model\n",
      "0s - loss: 0.5497 - acc: 0.7624 - val_loss: 0.5499 - val_acc: 0.7614\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54986 to 0.54923, saving model to best.model\n",
      "0s - loss: 0.5486 - acc: 0.7633 - val_loss: 0.5492 - val_acc: 0.7614\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54923 to 0.54888, saving model to best.model\n",
      "0s - loss: 0.5484 - acc: 0.7635 - val_loss: 0.5489 - val_acc: 0.7614\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7639 - val_loss: 0.5494 - val_acc: 0.7614\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54888 to 0.54886, saving model to best.model\n",
      "0s - loss: 0.5482 - acc: 0.7635 - val_loss: 0.5489 - val_acc: 0.7614\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7638 - val_loss: 0.5489 - val_acc: 0.7614\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.54886 to 0.54863, saving model to best.model\n",
      "0s - loss: 0.5478 - acc: 0.7638 - val_loss: 0.5486 - val_acc: 0.7614\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54863 to 0.54807, saving model to best.model\n",
      "0s - loss: 0.5481 - acc: 0.7635 - val_loss: 0.5481 - val_acc: 0.7614\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54807 to 0.54797, saving model to best.model\n",
      "0s - loss: 0.5465 - acc: 0.7648 - val_loss: 0.5480 - val_acc: 0.7614\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.54797 to 0.54760, saving model to best.model\n",
      "0s - loss: 0.5463 - acc: 0.7641 - val_loss: 0.5476 - val_acc: 0.7614\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7645 - val_loss: 0.5485 - val_acc: 0.7614\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7635 - val_loss: 0.5481 - val_acc: 0.7614\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7635 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7639 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7639 - val_loss: 0.5479 - val_acc: 0.7614\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7641 - val_loss: 0.5489 - val_acc: 0.7614\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.54760 to 0.54757, saving model to best.model\n",
      "0s - loss: 0.5469 - acc: 0.7641 - val_loss: 0.5476 - val_acc: 0.7614\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.54757 to 0.54754, saving model to best.model\n",
      "0s - loss: 0.5467 - acc: 0.7638 - val_loss: 0.5475 - val_acc: 0.7614\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7637 - val_loss: 0.5476 - val_acc: 0.7614\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5458 - acc: 0.7647 - val_loss: 0.5480 - val_acc: 0.7614\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.54754 to 0.54744, saving model to best.model\n",
      "0s - loss: 0.5451 - acc: 0.7648 - val_loss: 0.5474 - val_acc: 0.7614\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5456 - acc: 0.7645 - val_loss: 0.5483 - val_acc: 0.7614\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7638 - val_loss: 0.5483 - val_acc: 0.7614\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5466 - acc: 0.7645 - val_loss: 0.5484 - val_acc: 0.7614\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7641 - val_loss: 0.5479 - val_acc: 0.7614\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7641 - val_loss: 0.5481 - val_acc: 0.7614\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5456 - acc: 0.7643 - val_loss: 0.5479 - val_acc: 0.7614\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7645 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7643 - val_loss: 0.5487 - val_acc: 0.7614\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5474 - acc: 0.7638 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7643 - val_loss: 0.5486 - val_acc: 0.7614\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7642 - val_loss: 0.5487 - val_acc: 0.7614\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7640 - val_loss: 0.5482 - val_acc: 0.7614\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7639 - val_loss: 0.5479 - val_acc: 0.7614\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5452 - acc: 0.7643 - val_loss: 0.5477 - val_acc: 0.7614\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5468 - acc: 0.7637 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7644 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5453 - acc: 0.7647 - val_loss: 0.5489 - val_acc: 0.7614\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7644 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7640 - val_loss: 0.5479 - val_acc: 0.7614\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5458 - acc: 0.7640 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7646 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7647 - val_loss: 0.5481 - val_acc: 0.7614\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5467 - acc: 0.7641 - val_loss: 0.5481 - val_acc: 0.7614\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7638 - val_loss: 0.5481 - val_acc: 0.7614\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7643 - val_loss: 0.5478 - val_acc: 0.7614\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7641 - val_loss: 0.5481 - val_acc: 0.7614\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55630, saving model to best.model\n",
      "0s - loss: 0.6454 - acc: 0.7090 - val_loss: 0.5563 - val_acc: 0.7561\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55630 to 0.55547, saving model to best.model\n",
      "0s - loss: 0.5882 - acc: 0.7425 - val_loss: 0.5555 - val_acc: 0.7561\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55547 to 0.55320, saving model to best.model\n",
      "0s - loss: 0.5723 - acc: 0.7514 - val_loss: 0.5532 - val_acc: 0.7561\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5655 - acc: 0.7536 - val_loss: 0.5535 - val_acc: 0.7561\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.55320 to 0.55265, saving model to best.model\n",
      "0s - loss: 0.5633 - acc: 0.7538 - val_loss: 0.5526 - val_acc: 0.7561\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5616 - acc: 0.7539 - val_loss: 0.5528 - val_acc: 0.7561\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.55265 to 0.55202, saving model to best.model\n",
      "0s - loss: 0.5618 - acc: 0.7538 - val_loss: 0.5520 - val_acc: 0.7561\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.55202 to 0.55083, saving model to best.model\n",
      "0s - loss: 0.5597 - acc: 0.7538 - val_loss: 0.5508 - val_acc: 0.7561\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5585 - acc: 0.7540 - val_loss: 0.5510 - val_acc: 0.7561\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5598 - acc: 0.7545 - val_loss: 0.5516 - val_acc: 0.7561\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5616 - acc: 0.7540 - val_loss: 0.5526 - val_acc: 0.7561\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5595 - acc: 0.7543 - val_loss: 0.5528 - val_acc: 0.7561\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5598 - acc: 0.7544 - val_loss: 0.5543 - val_acc: 0.7566\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5587 - acc: 0.7548 - val_loss: 0.5512 - val_acc: 0.7566\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5581 - acc: 0.7552 - val_loss: 0.5511 - val_acc: 0.7590\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.55083 to 0.54848, saving model to best.model\n",
      "0s - loss: 0.5580 - acc: 0.7555 - val_loss: 0.5485 - val_acc: 0.7614\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7571 - val_loss: 0.5491 - val_acc: 0.7624\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5580 - acc: 0.7569 - val_loss: 0.5497 - val_acc: 0.7614\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5560 - acc: 0.7570 - val_loss: 0.5493 - val_acc: 0.7624\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.54848 to 0.54789, saving model to best.model\n",
      "0s - loss: 0.5556 - acc: 0.7584 - val_loss: 0.5479 - val_acc: 0.7624\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7579 - val_loss: 0.5483 - val_acc: 0.7624\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7577 - val_loss: 0.5481 - val_acc: 0.7624\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5561 - acc: 0.7581 - val_loss: 0.5482 - val_acc: 0.7624\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5554 - acc: 0.7580 - val_loss: 0.5481 - val_acc: 0.7624\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7582 - val_loss: 0.5483 - val_acc: 0.7624\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54789 to 0.54785, saving model to best.model\n",
      "0s - loss: 0.5559 - acc: 0.7579 - val_loss: 0.5479 - val_acc: 0.7624\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54785 to 0.54646, saving model to best.model\n",
      "0s - loss: 0.5539 - acc: 0.7592 - val_loss: 0.5465 - val_acc: 0.7624\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54646 to 0.54619, saving model to best.model\n",
      "0s - loss: 0.5538 - acc: 0.7589 - val_loss: 0.5462 - val_acc: 0.7624\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7586 - val_loss: 0.5469 - val_acc: 0.7634\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54619 to 0.54612, saving model to best.model\n",
      "0s - loss: 0.5534 - acc: 0.7592 - val_loss: 0.5461 - val_acc: 0.7624\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5534 - acc: 0.7587 - val_loss: 0.5465 - val_acc: 0.7624\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7587 - val_loss: 0.5464 - val_acc: 0.7634\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.54612 to 0.54592, saving model to best.model\n",
      "0s - loss: 0.5537 - acc: 0.7592 - val_loss: 0.5459 - val_acc: 0.7624\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54592 to 0.54573, saving model to best.model\n",
      "0s - loss: 0.5527 - acc: 0.7588 - val_loss: 0.5457 - val_acc: 0.7634\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54573 to 0.54557, saving model to best.model\n",
      "0s - loss: 0.5533 - acc: 0.7589 - val_loss: 0.5456 - val_acc: 0.7634\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7591 - val_loss: 0.5458 - val_acc: 0.7634\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54557 to 0.54544, saving model to best.model\n",
      "0s - loss: 0.5534 - acc: 0.7591 - val_loss: 0.5454 - val_acc: 0.7634\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7592 - val_loss: 0.5468 - val_acc: 0.7634\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7593 - val_loss: 0.5455 - val_acc: 0.7634\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7592 - val_loss: 0.5455 - val_acc: 0.7634\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7595 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.54544 to 0.54537, saving model to best.model\n",
      "0s - loss: 0.5522 - acc: 0.7595 - val_loss: 0.5454 - val_acc: 0.7634\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7592 - val_loss: 0.5455 - val_acc: 0.7634\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.54537 to 0.54536, saving model to best.model\n",
      "0s - loss: 0.5517 - acc: 0.7596 - val_loss: 0.5454 - val_acc: 0.7636\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7591 - val_loss: 0.5454 - val_acc: 0.7636\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.54536 to 0.54529, saving model to best.model\n",
      "0s - loss: 0.5510 - acc: 0.7593 - val_loss: 0.5453 - val_acc: 0.7636\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7592 - val_loss: 0.5453 - val_acc: 0.7634\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7597 - val_loss: 0.5453 - val_acc: 0.7634\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7594 - val_loss: 0.5457 - val_acc: 0.7636\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7593 - val_loss: 0.5453 - val_acc: 0.7634\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7596 - val_loss: 0.5453 - val_acc: 0.7634\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7592 - val_loss: 0.5457 - val_acc: 0.7634\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5459 - val_acc: 0.7636\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7595 - val_loss: 0.5454 - val_acc: 0.7636\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7596 - val_loss: 0.5460 - val_acc: 0.7636\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7596 - val_loss: 0.5457 - val_acc: 0.7636\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7594 - val_loss: 0.5455 - val_acc: 0.7636\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7595 - val_loss: 0.5455 - val_acc: 0.7636\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7597 - val_loss: 0.5457 - val_acc: 0.7636\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7595 - val_loss: 0.5454 - val_acc: 0.7636\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7594 - val_loss: 0.5466 - val_acc: 0.7636\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7593 - val_loss: 0.5454 - val_acc: 0.7636\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7594 - val_loss: 0.5456 - val_acc: 0.7636\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7599 - val_loss: 0.5455 - val_acc: 0.7636\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7595 - val_loss: 0.5459 - val_acc: 0.7636\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7597 - val_loss: 0.5459 - val_acc: 0.7636\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7598 - val_loss: 0.5460 - val_acc: 0.7636\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7595 - val_loss: 0.5455 - val_acc: 0.7636\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7597 - val_loss: 0.5460 - val_acc: 0.7636\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7597 - val_loss: 0.5456 - val_acc: 0.7636\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7596 - val_loss: 0.5456 - val_acc: 0.7636\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7598 - val_loss: 0.5466 - val_acc: 0.7636\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55235, saving model to best.model\n",
      "0s - loss: 0.6177 - acc: 0.7240 - val_loss: 0.5524 - val_acc: 0.7595\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55235 to 0.55154, saving model to best.model\n",
      "0s - loss: 0.5774 - acc: 0.7505 - val_loss: 0.5515 - val_acc: 0.7595\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5659 - acc: 0.7544 - val_loss: 0.5519 - val_acc: 0.7595\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.55154 to 0.55071, saving model to best.model\n",
      "0s - loss: 0.5633 - acc: 0.7544 - val_loss: 0.5507 - val_acc: 0.7595\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5601 - acc: 0.7544 - val_loss: 0.5525 - val_acc: 0.7595\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7544 - val_loss: 0.5515 - val_acc: 0.7595\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5606 - acc: 0.7544 - val_loss: 0.5521 - val_acc: 0.7595\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7544 - val_loss: 0.5532 - val_acc: 0.7595\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5608 - acc: 0.7544 - val_loss: 0.5537 - val_acc: 0.7595\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5604 - acc: 0.7544 - val_loss: 0.5515 - val_acc: 0.7595\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5599 - acc: 0.7544 - val_loss: 0.5517 - val_acc: 0.7595\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5598 - acc: 0.7544 - val_loss: 0.5511 - val_acc: 0.7595\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5602 - acc: 0.7544 - val_loss: 0.5540 - val_acc: 0.7595\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7544 - val_loss: 0.5513 - val_acc: 0.7595\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5599 - acc: 0.7544 - val_loss: 0.5521 - val_acc: 0.7595\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5595 - acc: 0.7544 - val_loss: 0.5509 - val_acc: 0.7595\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7544 - val_loss: 0.5512 - val_acc: 0.7595\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5595 - acc: 0.7544 - val_loss: 0.5510 - val_acc: 0.7595\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5597 - acc: 0.7544 - val_loss: 0.5510 - val_acc: 0.7595\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5596 - acc: 0.7544 - val_loss: 0.5531 - val_acc: 0.7595\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5583 - acc: 0.7544 - val_loss: 0.5513 - val_acc: 0.7595\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5579 - acc: 0.7544 - val_loss: 0.5510 - val_acc: 0.7595\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.55071 to 0.54966, saving model to best.model\n",
      "0s - loss: 0.5574 - acc: 0.7545 - val_loss: 0.5497 - val_acc: 0.7595\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54966 to 0.54927, saving model to best.model\n",
      "0s - loss: 0.5596 - acc: 0.7545 - val_loss: 0.5493 - val_acc: 0.7595\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5575 - acc: 0.7548 - val_loss: 0.5517 - val_acc: 0.7612\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54927 to 0.54891, saving model to best.model\n",
      "0s - loss: 0.5582 - acc: 0.7550 - val_loss: 0.5489 - val_acc: 0.7617\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54891 to 0.54850, saving model to best.model\n",
      "0s - loss: 0.5570 - acc: 0.7561 - val_loss: 0.5485 - val_acc: 0.7617\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54850 to 0.54783, saving model to best.model\n",
      "0s - loss: 0.5572 - acc: 0.7558 - val_loss: 0.5478 - val_acc: 0.7617\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54783 to 0.54755, saving model to best.model\n",
      "0s - loss: 0.5561 - acc: 0.7570 - val_loss: 0.5475 - val_acc: 0.7624\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54755 to 0.54721, saving model to best.model\n",
      "0s - loss: 0.5566 - acc: 0.7570 - val_loss: 0.5472 - val_acc: 0.7624\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54721 to 0.54686, saving model to best.model\n",
      "0s - loss: 0.5559 - acc: 0.7575 - val_loss: 0.5469 - val_acc: 0.7634\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54686 to 0.54639, saving model to best.model\n",
      "0s - loss: 0.5544 - acc: 0.7576 - val_loss: 0.5464 - val_acc: 0.7634\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.54639 to 0.54623, saving model to best.model\n",
      "0s - loss: 0.5553 - acc: 0.7584 - val_loss: 0.5462 - val_acc: 0.7634\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7579 - val_loss: 0.5478 - val_acc: 0.7636\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54623 to 0.54573, saving model to best.model\n",
      "0s - loss: 0.5537 - acc: 0.7585 - val_loss: 0.5457 - val_acc: 0.7636\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7584 - val_loss: 0.5463 - val_acc: 0.7636\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5539 - acc: 0.7587 - val_loss: 0.5476 - val_acc: 0.7641\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5539 - acc: 0.7585 - val_loss: 0.5461 - val_acc: 0.7641\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.54573 to 0.54559, saving model to best.model\n",
      "0s - loss: 0.5536 - acc: 0.7587 - val_loss: 0.5456 - val_acc: 0.7641\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7585 - val_loss: 0.5478 - val_acc: 0.7641\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5541 - acc: 0.7588 - val_loss: 0.5456 - val_acc: 0.7641\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.54559 to 0.54517, saving model to best.model\n",
      "0s - loss: 0.5526 - acc: 0.7589 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7589 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7589 - val_loss: 0.5463 - val_acc: 0.7641\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7590 - val_loss: 0.5458 - val_acc: 0.7641\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7589 - val_loss: 0.5465 - val_acc: 0.7641\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7593 - val_loss: 0.5462 - val_acc: 0.7641\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7589 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7592 - val_loss: 0.5458 - val_acc: 0.7641\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5541 - acc: 0.7593 - val_loss: 0.5473 - val_acc: 0.7641\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7593 - val_loss: 0.5457 - val_acc: 0.7641\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7592 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7596 - val_loss: 0.5470 - val_acc: 0.7641\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7591 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7596 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7591 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7593 - val_loss: 0.5457 - val_acc: 0.7641\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7594 - val_loss: 0.5456 - val_acc: 0.7641\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7590 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7592 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.54517 to 0.54516, saving model to best.model\n",
      "0s - loss: 0.5517 - acc: 0.7595 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7595 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7592 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7594 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.54516 to 0.54510, saving model to best.model\n",
      "0s - loss: 0.5526 - acc: 0.7595 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7593 - val_loss: 0.5464 - val_acc: 0.7641\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7591 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.54510 to 0.54508, saving model to best.model\n",
      "0s - loss: 0.5516 - acc: 0.7593 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7595 - val_loss: 0.5457 - val_acc: 0.7641\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7592 - val_loss: 0.5465 - val_acc: 0.7641\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7592 - val_loss: 0.5458 - val_acc: 0.7641\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7592 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7591 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7592 - val_loss: 0.5458 - val_acc: 0.7641\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7596 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7593 - val_loss: 0.5457 - val_acc: 0.7641\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7596 - val_loss: 0.5457 - val_acc: 0.7641\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7593 - val_loss: 0.5459 - val_acc: 0.7641\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7594 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7593 - val_loss: 0.5458 - val_acc: 0.7641\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.54508 to 0.54490, saving model to best.model\n",
      "0s - loss: 0.5515 - acc: 0.7593 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7596 - val_loss: 0.5460 - val_acc: 0.7641\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7595 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7593 - val_loss: 0.5459 - val_acc: 0.7641\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7598 - val_loss: 0.5457 - val_acc: 0.7641\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7593 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7597 - val_loss: 0.5459 - val_acc: 0.7641\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7596 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7595 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7593 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7597 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7591 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7595 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.54490 to 0.54486, saving model to best.model\n",
      "0s - loss: 0.5515 - acc: 0.7593 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7592 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.54486 to 0.54450, saving model to best.model\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7592 - val_loss: 0.5457 - val_acc: 0.7641\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7592 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7595 - val_loss: 0.5450 - val_acc: 0.7658\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7593 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7598 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7597 - val_loss: 0.5458 - val_acc: 0.7641\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7593 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7593 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7592 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7596 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7594 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7595 - val_loss: 0.5447 - val_acc: 0.7658\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5502 - acc: 0.7599 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7589 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7590 - val_loss: 0.5448 - val_acc: 0.7641\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7599 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.54450 to 0.54446, saving model to best.model\n",
      "0s - loss: 0.5508 - acc: 0.7596 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7593 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7593 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7594 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7593 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7594 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7595 - val_loss: 0.5446 - val_acc: 0.7658\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7596 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7595 - val_loss: 0.5448 - val_acc: 0.7641\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7593 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7594 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7595 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7595 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7599 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7593 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7597 - val_loss: 0.5448 - val_acc: 0.7641\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.54446 to 0.54439, saving model to best.model\n",
      "0s - loss: 0.5504 - acc: 0.7596 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7595 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7593 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7592 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7594 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7595 - val_loss: 0.5448 - val_acc: 0.7641\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7590 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.54439 to 0.54435, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7596 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7591 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7595 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7598 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7593 - val_loss: 0.5446 - val_acc: 0.7641\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7596 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7593 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7598 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7596 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7595 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7593 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7592 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7596 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.54435 to 0.54425, saving model to best.model\n",
      "0s - loss: 0.5507 - acc: 0.7596 - val_loss: 0.5442 - val_acc: 0.7658\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7595 - val_loss: 0.5445 - val_acc: 0.7658\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7596 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7596 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7592 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7596 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7594 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7594 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7596 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7595 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7594 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7592 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7598 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7597 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7594 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7592 - val_loss: 0.5444 - val_acc: 0.7658\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7594 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7594 - val_loss: 0.5446 - val_acc: 0.7641\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7592 - val_loss: 0.5448 - val_acc: 0.7641\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7597 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7594 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.54425 to 0.54391, saving model to best.model\n",
      "0s - loss: 0.5509 - acc: 0.7593 - val_loss: 0.5439 - val_acc: 0.7658\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7595 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7596 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7595 - val_loss: 0.5443 - val_acc: 0.7658\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7596 - val_loss: 0.5448 - val_acc: 0.7641\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7597 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7596 - val_loss: 0.5448 - val_acc: 0.7641\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7596 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7592 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7595 - val_loss: 0.5451 - val_acc: 0.7641\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7597 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7595 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7592 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7598 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7596 - val_loss: 0.5454 - val_acc: 0.7641\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7594 - val_loss: 0.5449 - val_acc: 0.7641\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7598 - val_loss: 0.5447 - val_acc: 0.7641\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7596 - val_loss: 0.5448 - val_acc: 0.7641\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7594 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7592 - val_loss: 0.5452 - val_acc: 0.7641\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7593 - val_loss: 0.5446 - val_acc: 0.7641\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7590 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7596 - val_loss: 0.5450 - val_acc: 0.7641\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.54391 to 0.54389, saving model to best.model\n",
      "0s - loss: 0.5508 - acc: 0.7593 - val_loss: 0.5439 - val_acc: 0.7658\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7596 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7595 - val_loss: 0.5453 - val_acc: 0.7641\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55488, saving model to best.model\n",
      "0s - loss: 0.6463 - acc: 0.7099 - val_loss: 0.5549 - val_acc: 0.7575\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55488 to 0.55208, saving model to best.model\n",
      "0s - loss: 0.5845 - acc: 0.7452 - val_loss: 0.5521 - val_acc: 0.7575\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55208 to 0.54829, saving model to best.model\n",
      "0s - loss: 0.5673 - acc: 0.7555 - val_loss: 0.5483 - val_acc: 0.7575\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5586 - acc: 0.7574 - val_loss: 0.5507 - val_acc: 0.7575\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.54829 to 0.54800, saving model to best.model\n",
      "0s - loss: 0.5581 - acc: 0.7575 - val_loss: 0.5480 - val_acc: 0.7575\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.54800 to 0.54633, saving model to best.model\n",
      "0s - loss: 0.5558 - acc: 0.7577 - val_loss: 0.5463 - val_acc: 0.7575\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.54633 to 0.54361, saving model to best.model\n",
      "0s - loss: 0.5536 - acc: 0.7582 - val_loss: 0.5436 - val_acc: 0.7575\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.54361 to 0.53929, saving model to best.model\n",
      "0s - loss: 0.5490 - acc: 0.7593 - val_loss: 0.5393 - val_acc: 0.7590\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7640 - val_loss: 0.5415 - val_acc: 0.7590\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7592 - val_loss: 0.5472 - val_acc: 0.7587\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7596 - val_loss: 0.5462 - val_acc: 0.7590\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7601 - val_loss: 0.5458 - val_acc: 0.7597\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7621 - val_loss: 0.5447 - val_acc: 0.7602\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7619 - val_loss: 0.5393 - val_acc: 0.7604\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.53929 to 0.53651, saving model to best.model\n",
      "0s - loss: 0.5460 - acc: 0.7621 - val_loss: 0.5365 - val_acc: 0.7621\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5456 - acc: 0.7665 - val_loss: 0.5379 - val_acc: 0.7621\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.53651 to 0.53410, saving model to best.model\n",
      "0s - loss: 0.5448 - acc: 0.7676 - val_loss: 0.5341 - val_acc: 0.7629\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7703 - val_loss: 0.5377 - val_acc: 0.7714\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.53410 to 0.52354, saving model to best.model\n",
      "0s - loss: 0.5391 - acc: 0.7719 - val_loss: 0.5235 - val_acc: 0.7794\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5354 - acc: 0.7753 - val_loss: 0.5255 - val_acc: 0.7748\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7710 - val_loss: 0.5270 - val_acc: 0.7745\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5338 - acc: 0.7765 - val_loss: 0.5265 - val_acc: 0.7726\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.52354 to 0.52304, saving model to best.model\n",
      "0s - loss: 0.5355 - acc: 0.7769 - val_loss: 0.5230 - val_acc: 0.7809\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.52304 to 0.51220, saving model to best.model\n",
      "0s - loss: 0.5332 - acc: 0.7765 - val_loss: 0.5122 - val_acc: 0.7940\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5320 - acc: 0.7775 - val_loss: 0.5219 - val_acc: 0.7777\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7681 - val_loss: 0.5502 - val_acc: 0.7617\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7598 - val_loss: 0.5510 - val_acc: 0.7590\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7605 - val_loss: 0.5496 - val_acc: 0.7597\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7621 - val_loss: 0.5481 - val_acc: 0.7602\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7632 - val_loss: 0.5477 - val_acc: 0.7604\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7653 - val_loss: 0.5462 - val_acc: 0.7617\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7646 - val_loss: 0.5464 - val_acc: 0.7617\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5467 - acc: 0.7650 - val_loss: 0.5434 - val_acc: 0.7617\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7681 - val_loss: 0.5409 - val_acc: 0.7617\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7636 - val_loss: 0.5470 - val_acc: 0.7617\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5451 - acc: 0.7666 - val_loss: 0.5434 - val_acc: 0.7617\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7672 - val_loss: 0.5435 - val_acc: 0.7621\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7683 - val_loss: 0.5424 - val_acc: 0.7621\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7681 - val_loss: 0.5404 - val_acc: 0.7621\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7687 - val_loss: 0.5408 - val_acc: 0.7621\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7703 - val_loss: 0.5407 - val_acc: 0.7621\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7683 - val_loss: 0.5424 - val_acc: 0.7621\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7694 - val_loss: 0.5425 - val_acc: 0.7621\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5449 - acc: 0.7662 - val_loss: 0.5430 - val_acc: 0.7621\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5449 - acc: 0.7661 - val_loss: 0.5411 - val_acc: 0.7621\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7685 - val_loss: 0.5456 - val_acc: 0.7617\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7689 - val_loss: 0.5412 - val_acc: 0.7621\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7677 - val_loss: 0.5417 - val_acc: 0.7621\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7664 - val_loss: 0.5419 - val_acc: 0.7621\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7709 - val_loss: 0.5372 - val_acc: 0.7621\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.56271, saving model to best.model\n",
      "0s - loss: 0.6039 - acc: 0.7341 - val_loss: 0.5627 - val_acc: 0.7481\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5665 - acc: 0.7587 - val_loss: 0.5634 - val_acc: 0.7481\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.56271 to 0.56257, saving model to best.model\n",
      "0s - loss: 0.5580 - acc: 0.7598 - val_loss: 0.5626 - val_acc: 0.7481\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5564 - acc: 0.7598 - val_loss: 0.5646 - val_acc: 0.7481\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7598 - val_loss: 0.5626 - val_acc: 0.7481\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5535 - acc: 0.7598 - val_loss: 0.5630 - val_acc: 0.7481\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7598 - val_loss: 0.5627 - val_acc: 0.7481\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7598 - val_loss: 0.5627 - val_acc: 0.7481\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.56257 to 0.56210, saving model to best.model\n",
      "0s - loss: 0.5524 - acc: 0.7598 - val_loss: 0.5621 - val_acc: 0.7481\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.56210 to 0.56158, saving model to best.model\n",
      "0s - loss: 0.5524 - acc: 0.7598 - val_loss: 0.5616 - val_acc: 0.7481\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.56158 to 0.56075, saving model to best.model\n",
      "0s - loss: 0.5523 - acc: 0.7598 - val_loss: 0.5607 - val_acc: 0.7481\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.56075 to 0.55811, saving model to best.model\n",
      "0s - loss: 0.5504 - acc: 0.7598 - val_loss: 0.5581 - val_acc: 0.7481\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.55811 to 0.55491, saving model to best.model\n",
      "0s - loss: 0.5495 - acc: 0.7601 - val_loss: 0.5549 - val_acc: 0.7481\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7610 - val_loss: 0.5568 - val_acc: 0.7493\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.55491 to 0.55025, saving model to best.model\n",
      "0s - loss: 0.5472 - acc: 0.7627 - val_loss: 0.5503 - val_acc: 0.7517\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7648 - val_loss: 0.5549 - val_acc: 0.7549\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7649 - val_loss: 0.5568 - val_acc: 0.7549\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7655 - val_loss: 0.5538 - val_acc: 0.7549\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7658 - val_loss: 0.5526 - val_acc: 0.7580\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7658 - val_loss: 0.5524 - val_acc: 0.7580\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7666 - val_loss: 0.5514 - val_acc: 0.7580\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7669 - val_loss: 0.5508 - val_acc: 0.7580\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.55025 to 0.55011, saving model to best.model\n",
      "0s - loss: 0.5432 - acc: 0.7683 - val_loss: 0.5501 - val_acc: 0.7580\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.55011 to 0.55003, saving model to best.model\n",
      "0s - loss: 0.5423 - acc: 0.7692 - val_loss: 0.5500 - val_acc: 0.7585\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7681 - val_loss: 0.5513 - val_acc: 0.7585\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7691 - val_loss: 0.5501 - val_acc: 0.7585\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5450 - acc: 0.7659 - val_loss: 0.5610 - val_acc: 0.7536\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5469 - acc: 0.7644 - val_loss: 0.5569 - val_acc: 0.7522\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5454 - acc: 0.7650 - val_loss: 0.5550 - val_acc: 0.7549\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5455 - acc: 0.7665 - val_loss: 0.5550 - val_acc: 0.7549\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5450 - acc: 0.7672 - val_loss: 0.5533 - val_acc: 0.7549\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7672 - val_loss: 0.5546 - val_acc: 0.7549\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7675 - val_loss: 0.5523 - val_acc: 0.7551\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7673 - val_loss: 0.5520 - val_acc: 0.7580\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7672 - val_loss: 0.5521 - val_acc: 0.7551\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7671 - val_loss: 0.5515 - val_acc: 0.7580\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7681 - val_loss: 0.5521 - val_acc: 0.7580\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7674 - val_loss: 0.5518 - val_acc: 0.7580\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7680 - val_loss: 0.5524 - val_acc: 0.7580\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7678 - val_loss: 0.5504 - val_acc: 0.7580\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7689 - val_loss: 0.5508 - val_acc: 0.7580\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7684 - val_loss: 0.5506 - val_acc: 0.7580\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.55003 to 0.54962, saving model to best.model\n",
      "0s - loss: 0.5403 - acc: 0.7693 - val_loss: 0.5496 - val_acc: 0.7580\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7693 - val_loss: 0.5517 - val_acc: 0.7580\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7684 - val_loss: 0.5503 - val_acc: 0.7580\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7698 - val_loss: 0.5506 - val_acc: 0.7580\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7689 - val_loss: 0.5497 - val_acc: 0.7580\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7695 - val_loss: 0.5498 - val_acc: 0.7580\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7695 - val_loss: 0.5497 - val_acc: 0.7580\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7697 - val_loss: 0.5504 - val_acc: 0.7580\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.54962 to 0.54841, saving model to best.model\n",
      "0s - loss: 0.5392 - acc: 0.7700 - val_loss: 0.5484 - val_acc: 0.7580\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7700 - val_loss: 0.5501 - val_acc: 0.7580\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7694 - val_loss: 0.5495 - val_acc: 0.7580\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7703 - val_loss: 0.5498 - val_acc: 0.7580\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7697 - val_loss: 0.5491 - val_acc: 0.7580\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7703 - val_loss: 0.5514 - val_acc: 0.7580\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7698 - val_loss: 0.5499 - val_acc: 0.7580\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7702 - val_loss: 0.5493 - val_acc: 0.7580\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7706 - val_loss: 0.5486 - val_acc: 0.7580\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5378 - acc: 0.7710 - val_loss: 0.5500 - val_acc: 0.7580\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7692 - val_loss: 0.5494 - val_acc: 0.7580\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7700 - val_loss: 0.5489 - val_acc: 0.7580\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.54841 to 0.54805, saving model to best.model\n",
      "0s - loss: 0.5386 - acc: 0.7705 - val_loss: 0.5480 - val_acc: 0.7580\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7691 - val_loss: 0.5497 - val_acc: 0.7580\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7700 - val_loss: 0.5485 - val_acc: 0.7580\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7694 - val_loss: 0.5495 - val_acc: 0.7580\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7698 - val_loss: 0.5486 - val_acc: 0.7580\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7703 - val_loss: 0.5491 - val_acc: 0.7580\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7706 - val_loss: 0.5495 - val_acc: 0.7580\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7695 - val_loss: 0.5495 - val_acc: 0.7580\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7700 - val_loss: 0.5497 - val_acc: 0.7580\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.54805 to 0.54760, saving model to best.model\n",
      "0s - loss: 0.5380 - acc: 0.7708 - val_loss: 0.5476 - val_acc: 0.7580\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7703 - val_loss: 0.5481 - val_acc: 0.7580\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7698 - val_loss: 0.5498 - val_acc: 0.7580\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5378 - acc: 0.7708 - val_loss: 0.5479 - val_acc: 0.7580\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7705 - val_loss: 0.5486 - val_acc: 0.7580\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7702 - val_loss: 0.5490 - val_acc: 0.7580\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7701 - val_loss: 0.5483 - val_acc: 0.7580\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7698 - val_loss: 0.5499 - val_acc: 0.7580\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7708 - val_loss: 0.5494 - val_acc: 0.7580\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5384 - acc: 0.7700 - val_loss: 0.5491 - val_acc: 0.7580\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5388 - acc: 0.7698 - val_loss: 0.5496 - val_acc: 0.7580\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5384 - acc: 0.7703 - val_loss: 0.5489 - val_acc: 0.7580\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7700 - val_loss: 0.5504 - val_acc: 0.7580\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7707 - val_loss: 0.5481 - val_acc: 0.7580\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5384 - acc: 0.7702 - val_loss: 0.5495 - val_acc: 0.7580\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7701 - val_loss: 0.5493 - val_acc: 0.7580\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7695 - val_loss: 0.5484 - val_acc: 0.7580\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7697 - val_loss: 0.5502 - val_acc: 0.7580\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7702 - val_loss: 0.5489 - val_acc: 0.7580\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7702 - val_loss: 0.5489 - val_acc: 0.7580\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.54760 to 0.54709, saving model to best.model\n",
      "0s - loss: 0.5375 - acc: 0.7706 - val_loss: 0.5471 - val_acc: 0.7580\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7702 - val_loss: 0.5503 - val_acc: 0.7580\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7695 - val_loss: 0.5510 - val_acc: 0.7580\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7700 - val_loss: 0.5485 - val_acc: 0.7580\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5372 - acc: 0.7711 - val_loss: 0.5486 - val_acc: 0.7580\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7701 - val_loss: 0.5482 - val_acc: 0.7580\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7705 - val_loss: 0.5496 - val_acc: 0.7580\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7703 - val_loss: 0.5495 - val_acc: 0.7580\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5374 - acc: 0.7706 - val_loss: 0.5483 - val_acc: 0.7580\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5374 - acc: 0.7706 - val_loss: 0.5485 - val_acc: 0.7580\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7693 - val_loss: 0.5482 - val_acc: 0.7580\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7688 - val_loss: 0.5489 - val_acc: 0.7580\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5384 - acc: 0.7700 - val_loss: 0.5487 - val_acc: 0.7580\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.5388 - acc: 0.7698 - val_loss: 0.5487 - val_acc: 0.7580\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7696 - val_loss: 0.5481 - val_acc: 0.7580\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7702 - val_loss: 0.5482 - val_acc: 0.7580\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7699 - val_loss: 0.5478 - val_acc: 0.7580\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7700 - val_loss: 0.5480 - val_acc: 0.7580\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7705 - val_loss: 0.5492 - val_acc: 0.7580\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7706 - val_loss: 0.5488 - val_acc: 0.7580\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7710 - val_loss: 0.5491 - val_acc: 0.7580\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7706 - val_loss: 0.5494 - val_acc: 0.7580\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7707 - val_loss: 0.5487 - val_acc: 0.7580\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7698 - val_loss: 0.5497 - val_acc: 0.7580\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7700 - val_loss: 0.5483 - val_acc: 0.7580\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5376 - acc: 0.7705 - val_loss: 0.5475 - val_acc: 0.7580\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7693 - val_loss: 0.5483 - val_acc: 0.7580\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55397, saving model to best.model\n",
      "0s - loss: 0.6058 - acc: 0.7339 - val_loss: 0.5540 - val_acc: 0.7570\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55397 to 0.55273, saving model to best.model\n",
      "0s - loss: 0.5706 - acc: 0.7568 - val_loss: 0.5527 - val_acc: 0.7570\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55273 to 0.55256, saving model to best.model\n",
      "0s - loss: 0.5585 - acc: 0.7588 - val_loss: 0.5526 - val_acc: 0.7570\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5554 - acc: 0.7591 - val_loss: 0.5532 - val_acc: 0.7570\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7590 - val_loss: 0.5528 - val_acc: 0.7570\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.55256 to 0.55218, saving model to best.model\n",
      "0s - loss: 0.5547 - acc: 0.7590 - val_loss: 0.5522 - val_acc: 0.7570\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5553 - acc: 0.7590 - val_loss: 0.5555 - val_acc: 0.7570\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5547 - acc: 0.7590 - val_loss: 0.5556 - val_acc: 0.7570\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.55218 to 0.55136, saving model to best.model\n",
      "0s - loss: 0.5534 - acc: 0.7590 - val_loss: 0.5514 - val_acc: 0.7570\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7590 - val_loss: 0.5520 - val_acc: 0.7570\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7591 - val_loss: 0.5520 - val_acc: 0.7570\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.55136 to 0.55106, saving model to best.model\n",
      "0s - loss: 0.5526 - acc: 0.7594 - val_loss: 0.5511 - val_acc: 0.7570\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7596 - val_loss: 0.5513 - val_acc: 0.7573\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.55106 to 0.54966, saving model to best.model\n",
      "0s - loss: 0.5511 - acc: 0.7605 - val_loss: 0.5497 - val_acc: 0.7597\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.54966 to 0.54917, saving model to best.model\n",
      "0s - loss: 0.5505 - acc: 0.7618 - val_loss: 0.5492 - val_acc: 0.7602\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.54917 to 0.54765, saving model to best.model\n",
      "0s - loss: 0.5503 - acc: 0.7624 - val_loss: 0.5477 - val_acc: 0.7609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.54765 to 0.54576, saving model to best.model\n",
      "0s - loss: 0.5478 - acc: 0.7629 - val_loss: 0.5458 - val_acc: 0.7612\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.54576 to 0.54519, saving model to best.model\n",
      "0s - loss: 0.5473 - acc: 0.7641 - val_loss: 0.5452 - val_acc: 0.7617\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7647 - val_loss: 0.5463 - val_acc: 0.7617\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7657 - val_loss: 0.5453 - val_acc: 0.7617\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.54519 to 0.54412, saving model to best.model\n",
      "0s - loss: 0.5459 - acc: 0.7661 - val_loss: 0.5441 - val_acc: 0.7617\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7667 - val_loss: 0.5455 - val_acc: 0.7617\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5471 - acc: 0.7669 - val_loss: 0.5461 - val_acc: 0.7617\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5453 - acc: 0.7667 - val_loss: 0.5454 - val_acc: 0.7619\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54412 to 0.54288, saving model to best.model\n",
      "0s - loss: 0.5438 - acc: 0.7681 - val_loss: 0.5429 - val_acc: 0.7653\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7676 - val_loss: 0.5456 - val_acc: 0.7617\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7681 - val_loss: 0.5444 - val_acc: 0.7617\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7683 - val_loss: 0.5443 - val_acc: 0.7617\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7681 - val_loss: 0.5444 - val_acc: 0.7617\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7685 - val_loss: 0.5451 - val_acc: 0.7617\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5435 - acc: 0.7680 - val_loss: 0.5453 - val_acc: 0.7617\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7687 - val_loss: 0.5430 - val_acc: 0.7646\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7686 - val_loss: 0.5431 - val_acc: 0.7646\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54288 to 0.54229, saving model to best.model\n",
      "0s - loss: 0.5427 - acc: 0.7677 - val_loss: 0.5423 - val_acc: 0.7646\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54229 to 0.54211, saving model to best.model\n",
      "0s - loss: 0.5422 - acc: 0.7682 - val_loss: 0.5421 - val_acc: 0.7646\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7686 - val_loss: 0.5435 - val_acc: 0.7646\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7691 - val_loss: 0.5429 - val_acc: 0.7646\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7699 - val_loss: 0.5433 - val_acc: 0.7646\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7688 - val_loss: 0.5449 - val_acc: 0.7646\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7679 - val_loss: 0.5426 - val_acc: 0.7646\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.54211 to 0.54193, saving model to best.model\n",
      "0s - loss: 0.5409 - acc: 0.7698 - val_loss: 0.5419 - val_acc: 0.7646\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7696 - val_loss: 0.5421 - val_acc: 0.7646\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7688 - val_loss: 0.5424 - val_acc: 0.7646\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.54193 to 0.54182, saving model to best.model\n",
      "0s - loss: 0.5420 - acc: 0.7688 - val_loss: 0.5418 - val_acc: 0.7646\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7696 - val_loss: 0.5426 - val_acc: 0.7646\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.54182 to 0.54058, saving model to best.model\n",
      "0s - loss: 0.5413 - acc: 0.7692 - val_loss: 0.5406 - val_acc: 0.7668\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7698 - val_loss: 0.5410 - val_acc: 0.7646\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7696 - val_loss: 0.5414 - val_acc: 0.7646\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7704 - val_loss: 0.5417 - val_acc: 0.7646\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7685 - val_loss: 0.5419 - val_acc: 0.7646\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7693 - val_loss: 0.5415 - val_acc: 0.7646\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7693 - val_loss: 0.5412 - val_acc: 0.7646\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7700 - val_loss: 0.5416 - val_acc: 0.7646\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7699 - val_loss: 0.5423 - val_acc: 0.7646\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7698 - val_loss: 0.5425 - val_acc: 0.7646\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7698 - val_loss: 0.5422 - val_acc: 0.7646\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7703 - val_loss: 0.5422 - val_acc: 0.7646\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7699 - val_loss: 0.5416 - val_acc: 0.7646\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7699 - val_loss: 0.5419 - val_acc: 0.7646\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7698 - val_loss: 0.5425 - val_acc: 0.7646\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7695 - val_loss: 0.5426 - val_acc: 0.7646\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7708 - val_loss: 0.5421 - val_acc: 0.7646\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7700 - val_loss: 0.5417 - val_acc: 0.7646\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7695 - val_loss: 0.5427 - val_acc: 0.7646\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7702 - val_loss: 0.5416 - val_acc: 0.7646\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7699 - val_loss: 0.5411 - val_acc: 0.7646\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7691 - val_loss: 0.5420 - val_acc: 0.7646\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7697 - val_loss: 0.5416 - val_acc: 0.7646\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7692 - val_loss: 0.5427 - val_acc: 0.7646\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7696 - val_loss: 0.5416 - val_acc: 0.7646\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7699 - val_loss: 0.5423 - val_acc: 0.7646\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7697 - val_loss: 0.5420 - val_acc: 0.7646\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.53939, saving model to best.model\n",
      "0s - loss: 0.6286 - acc: 0.7168 - val_loss: 0.5394 - val_acc: 0.7697\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.53939 to 0.53909, saving model to best.model\n",
      "0s - loss: 0.5788 - acc: 0.7519 - val_loss: 0.5391 - val_acc: 0.7697\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5634 - acc: 0.7606 - val_loss: 0.5392 - val_acc: 0.7697\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5582 - acc: 0.7613 - val_loss: 0.5392 - val_acc: 0.7697\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.53909 to 0.53902, saving model to best.model\n",
      "0s - loss: 0.5539 - acc: 0.7613 - val_loss: 0.5390 - val_acc: 0.7697\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7613 - val_loss: 0.5435 - val_acc: 0.7697\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5547 - acc: 0.7613 - val_loss: 0.5405 - val_acc: 0.7697\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.53902 to 0.53871, saving model to best.model\n",
      "0s - loss: 0.5522 - acc: 0.7613 - val_loss: 0.5387 - val_acc: 0.7697\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.53871 to 0.53817, saving model to best.model\n",
      "0s - loss: 0.5523 - acc: 0.7613 - val_loss: 0.5382 - val_acc: 0.7697\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7613 - val_loss: 0.5392 - val_acc: 0.7697\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7613 - val_loss: 0.5387 - val_acc: 0.7697\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7613 - val_loss: 0.5388 - val_acc: 0.7697\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7613 - val_loss: 0.5388 - val_acc: 0.7697\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7613 - val_loss: 0.5427 - val_acc: 0.7697\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7613 - val_loss: 0.5442 - val_acc: 0.7697\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7613 - val_loss: 0.5393 - val_acc: 0.7697\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7614 - val_loss: 0.5384 - val_acc: 0.7697\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7614 - val_loss: 0.5391 - val_acc: 0.7697\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7613 - val_loss: 0.5387 - val_acc: 0.7697\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7617 - val_loss: 0.5386 - val_acc: 0.7699\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.53817 to 0.53762, saving model to best.model\n",
      "0s - loss: 0.5497 - acc: 0.7621 - val_loss: 0.5376 - val_acc: 0.7699\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.53762 to 0.53682, saving model to best.model\n",
      "0s - loss: 0.5489 - acc: 0.7626 - val_loss: 0.5368 - val_acc: 0.7702\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7632 - val_loss: 0.5383 - val_acc: 0.7719\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.53682 to 0.53583, saving model to best.model\n",
      "0s - loss: 0.5484 - acc: 0.7643 - val_loss: 0.5358 - val_acc: 0.7719\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.53583 to 0.53545, saving model to best.model\n",
      "0s - loss: 0.5482 - acc: 0.7646 - val_loss: 0.5354 - val_acc: 0.7726\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.53545 to 0.53495, saving model to best.model\n",
      "0s - loss: 0.5469 - acc: 0.7648 - val_loss: 0.5350 - val_acc: 0.7726\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.53495 to 0.53421, saving model to best.model\n",
      "0s - loss: 0.5466 - acc: 0.7655 - val_loss: 0.5342 - val_acc: 0.7738\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.53421 to 0.53401, saving model to best.model\n",
      "0s - loss: 0.5450 - acc: 0.7660 - val_loss: 0.5340 - val_acc: 0.7738\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.53401 to 0.53390, saving model to best.model\n",
      "0s - loss: 0.5452 - acc: 0.7663 - val_loss: 0.5339 - val_acc: 0.7741\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.53390 to 0.53361, saving model to best.model\n",
      "0s - loss: 0.5456 - acc: 0.7662 - val_loss: 0.5336 - val_acc: 0.7741\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.53361 to 0.53285, saving model to best.model\n",
      "0s - loss: 0.5450 - acc: 0.7666 - val_loss: 0.5329 - val_acc: 0.7741\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5445 - acc: 0.7668 - val_loss: 0.5334 - val_acc: 0.7741\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7672 - val_loss: 0.5332 - val_acc: 0.7741\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53285 to 0.53270, saving model to best.model\n",
      "0s - loss: 0.5437 - acc: 0.7673 - val_loss: 0.5327 - val_acc: 0.7750\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.53270 to 0.53247, saving model to best.model\n",
      "0s - loss: 0.5427 - acc: 0.7671 - val_loss: 0.5325 - val_acc: 0.7750\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7674 - val_loss: 0.5327 - val_acc: 0.7741\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7675 - val_loss: 0.5326 - val_acc: 0.7758\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.53247 to 0.53242, saving model to best.model\n",
      "0s - loss: 0.5424 - acc: 0.7676 - val_loss: 0.5324 - val_acc: 0.7758\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.53242 to 0.53224, saving model to best.model\n",
      "0s - loss: 0.5417 - acc: 0.7680 - val_loss: 0.5322 - val_acc: 0.7760\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7680 - val_loss: 0.5324 - val_acc: 0.7760\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7678 - val_loss: 0.5335 - val_acc: 0.7760\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7678 - val_loss: 0.5331 - val_acc: 0.7760\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7680 - val_loss: 0.5326 - val_acc: 0.7760\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7685 - val_loss: 0.5326 - val_acc: 0.7760\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7678 - val_loss: 0.5332 - val_acc: 0.7760\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7683 - val_loss: 0.5327 - val_acc: 0.7760\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7680 - val_loss: 0.5324 - val_acc: 0.7760\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7681 - val_loss: 0.5331 - val_acc: 0.7760\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7681 - val_loss: 0.5331 - val_acc: 0.7760\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7681 - val_loss: 0.5325 - val_acc: 0.7760\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7685 - val_loss: 0.5329 - val_acc: 0.7760\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7684 - val_loss: 0.5357 - val_acc: 0.7741\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7673 - val_loss: 0.5339 - val_acc: 0.7741\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7674 - val_loss: 0.5338 - val_acc: 0.7741\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7670 - val_loss: 0.5346 - val_acc: 0.7741\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7677 - val_loss: 0.5344 - val_acc: 0.7741\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7676 - val_loss: 0.5341 - val_acc: 0.7741\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7680 - val_loss: 0.5359 - val_acc: 0.7741\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7678 - val_loss: 0.5346 - val_acc: 0.7741\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7677 - val_loss: 0.5341 - val_acc: 0.7741\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7674 - val_loss: 0.5342 - val_acc: 0.7741\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7677 - val_loss: 0.5342 - val_acc: 0.7741\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7678 - val_loss: 0.5343 - val_acc: 0.7741\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7680 - val_loss: 0.5341 - val_acc: 0.7741\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7675 - val_loss: 0.5342 - val_acc: 0.7741\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54915, saving model to best.model\n",
      "0s - loss: 0.6704 - acc: 0.6974 - val_loss: 0.5492 - val_acc: 0.7621\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54915 to 0.54744, saving model to best.model\n",
      "0s - loss: 0.5923 - acc: 0.7383 - val_loss: 0.5474 - val_acc: 0.7621\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.54744 to 0.54676, saving model to best.model\n",
      "0s - loss: 0.5738 - acc: 0.7526 - val_loss: 0.5468 - val_acc: 0.7621\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5653 - acc: 0.7555 - val_loss: 0.5480 - val_acc: 0.7621\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5654 - acc: 0.7556 - val_loss: 0.5496 - val_acc: 0.7621\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5624 - acc: 0.7558 - val_loss: 0.5482 - val_acc: 0.7621\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5605 - acc: 0.7557 - val_loss: 0.5484 - val_acc: 0.7621\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5597 - acc: 0.7557 - val_loss: 0.5489 - val_acc: 0.7621\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5599 - acc: 0.7557 - val_loss: 0.5490 - val_acc: 0.7621\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5601 - acc: 0.7557 - val_loss: 0.5526 - val_acc: 0.7621\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7557 - val_loss: 0.5491 - val_acc: 0.7621\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5597 - acc: 0.7557 - val_loss: 0.5482 - val_acc: 0.7621\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5586 - acc: 0.7557 - val_loss: 0.5498 - val_acc: 0.7621\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5576 - acc: 0.7557 - val_loss: 0.5483 - val_acc: 0.7621\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5574 - acc: 0.7557 - val_loss: 0.5520 - val_acc: 0.7621\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5595 - acc: 0.7557 - val_loss: 0.5484 - val_acc: 0.7621\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5578 - acc: 0.7558 - val_loss: 0.5483 - val_acc: 0.7621\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5572 - acc: 0.7557 - val_loss: 0.5476 - val_acc: 0.7621\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5580 - acc: 0.7558 - val_loss: 0.5484 - val_acc: 0.7621\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5577 - acc: 0.7558 - val_loss: 0.5490 - val_acc: 0.7621\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.54676 to 0.54663, saving model to best.model\n",
      "0s - loss: 0.5557 - acc: 0.7559 - val_loss: 0.5466 - val_acc: 0.7621\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.54663 to 0.54595, saving model to best.model\n",
      "0s - loss: 0.5561 - acc: 0.7567 - val_loss: 0.5459 - val_acc: 0.7626\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.54595 to 0.54534, saving model to best.model\n",
      "0s - loss: 0.5560 - acc: 0.7566 - val_loss: 0.5453 - val_acc: 0.7629\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54534 to 0.54469, saving model to best.model\n",
      "0s - loss: 0.5550 - acc: 0.7573 - val_loss: 0.5447 - val_acc: 0.7643\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7583 - val_loss: 0.5448 - val_acc: 0.7651\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54469 to 0.54353, saving model to best.model\n",
      "0s - loss: 0.5537 - acc: 0.7590 - val_loss: 0.5435 - val_acc: 0.7655\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54353 to 0.54301, saving model to best.model\n",
      "0s - loss: 0.5526 - acc: 0.7596 - val_loss: 0.5430 - val_acc: 0.7668\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7607 - val_loss: 0.5431 - val_acc: 0.7668\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7611 - val_loss: 0.5436 - val_acc: 0.7668\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54301 to 0.54156, saving model to best.model\n",
      "0s - loss: 0.5517 - acc: 0.7610 - val_loss: 0.5416 - val_acc: 0.7668\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54156 to 0.54139, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7612 - val_loss: 0.5414 - val_acc: 0.7668\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7613 - val_loss: 0.5414 - val_acc: 0.7668\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.54139 to 0.54120, saving model to best.model\n",
      "0s - loss: 0.5501 - acc: 0.7616 - val_loss: 0.5412 - val_acc: 0.7668\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7616 - val_loss: 0.5418 - val_acc: 0.7685\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54120 to 0.54071, saving model to best.model\n",
      "0s - loss: 0.5497 - acc: 0.7622 - val_loss: 0.5407 - val_acc: 0.7685\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7621 - val_loss: 0.5408 - val_acc: 0.7685\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5501 - acc: 0.7620 - val_loss: 0.5410 - val_acc: 0.7685\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54071 to 0.54069, saving model to best.model\n",
      "0s - loss: 0.5497 - acc: 0.7623 - val_loss: 0.5407 - val_acc: 0.7685\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7625 - val_loss: 0.5411 - val_acc: 0.7685\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7618 - val_loss: 0.5413 - val_acc: 0.7680\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7615 - val_loss: 0.5419 - val_acc: 0.7668\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7620 - val_loss: 0.5419 - val_acc: 0.7668\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7621 - val_loss: 0.5419 - val_acc: 0.7668\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7623 - val_loss: 0.5419 - val_acc: 0.7668\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7625 - val_loss: 0.5426 - val_acc: 0.7668\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7618 - val_loss: 0.5418 - val_acc: 0.7668\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7623 - val_loss: 0.5420 - val_acc: 0.7668\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7621 - val_loss: 0.5422 - val_acc: 0.7668\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7611 - val_loss: 0.5440 - val_acc: 0.7668\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7619 - val_loss: 0.5421 - val_acc: 0.7668\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7621 - val_loss: 0.5422 - val_acc: 0.7668\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7620 - val_loss: 0.5420 - val_acc: 0.7668\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7626 - val_loss: 0.5427 - val_acc: 0.7668\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7623 - val_loss: 0.5420 - val_acc: 0.7668\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7618 - val_loss: 0.5427 - val_acc: 0.7668\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7620 - val_loss: 0.5428 - val_acc: 0.7668\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7623 - val_loss: 0.5428 - val_acc: 0.7668\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7620 - val_loss: 0.5428 - val_acc: 0.7668\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7619 - val_loss: 0.5431 - val_acc: 0.7668\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7622 - val_loss: 0.5433 - val_acc: 0.7668\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7626 - val_loss: 0.5422 - val_acc: 0.7668\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5486 - acc: 0.7623 - val_loss: 0.5468 - val_acc: 0.7668\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7621 - val_loss: 0.5426 - val_acc: 0.7668\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7623 - val_loss: 0.5443 - val_acc: 0.7668\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54791, saving model to best.model\n",
      "0s - loss: 0.6142 - acc: 0.7277 - val_loss: 0.5479 - val_acc: 0.7609\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54791 to 0.54747, saving model to best.model\n",
      "0s - loss: 0.5719 - acc: 0.7537 - val_loss: 0.5475 - val_acc: 0.7609\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.54747 to 0.54542, saving model to best.model\n",
      "0s - loss: 0.5625 - acc: 0.7590 - val_loss: 0.5454 - val_acc: 0.7609\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5580 - acc: 0.7596 - val_loss: 0.5465 - val_acc: 0.7609\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7595 - val_loss: 0.5492 - val_acc: 0.7609\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5574 - acc: 0.7595 - val_loss: 0.5490 - val_acc: 0.7609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7595 - val_loss: 0.5498 - val_acc: 0.7609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7595 - val_loss: 0.5498 - val_acc: 0.7609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7595 - val_loss: 0.5497 - val_acc: 0.7609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7595 - val_loss: 0.5495 - val_acc: 0.7609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7595 - val_loss: 0.5499 - val_acc: 0.7609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7595 - val_loss: 0.5493 - val_acc: 0.7609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5535 - acc: 0.7595 - val_loss: 0.5490 - val_acc: 0.7609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5539 - acc: 0.7595 - val_loss: 0.5493 - val_acc: 0.7609\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7595 - val_loss: 0.5488 - val_acc: 0.7609\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7595 - val_loss: 0.5487 - val_acc: 0.7609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7595 - val_loss: 0.5488 - val_acc: 0.7609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7595 - val_loss: 0.5488 - val_acc: 0.7609\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7595 - val_loss: 0.5475 - val_acc: 0.7609\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7595 - val_loss: 0.5488 - val_acc: 0.7609\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7595 - val_loss: 0.5477 - val_acc: 0.7609\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7596 - val_loss: 0.5474 - val_acc: 0.7609\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7597 - val_loss: 0.5471 - val_acc: 0.7609\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7598 - val_loss: 0.5463 - val_acc: 0.7609\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7601 - val_loss: 0.5456 - val_acc: 0.7619\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54542 to 0.54493, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7612 - val_loss: 0.5449 - val_acc: 0.7631\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54493 to 0.54383, saving model to best.model\n",
      "0s - loss: 0.5503 - acc: 0.7616 - val_loss: 0.5438 - val_acc: 0.7648\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54383 to 0.54249, saving model to best.model\n",
      "0s - loss: 0.5505 - acc: 0.7620 - val_loss: 0.5425 - val_acc: 0.7660\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54249 to 0.54098, saving model to best.model\n",
      "0s - loss: 0.5492 - acc: 0.7633 - val_loss: 0.5410 - val_acc: 0.7685\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7639 - val_loss: 0.5412 - val_acc: 0.7692\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5473 - acc: 0.7640 - val_loss: 0.5416 - val_acc: 0.7692\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54098 to 0.53903, saving model to best.model\n",
      "0s - loss: 0.5469 - acc: 0.7647 - val_loss: 0.5390 - val_acc: 0.7692\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.53903 to 0.53767, saving model to best.model\n",
      "0s - loss: 0.5474 - acc: 0.7650 - val_loss: 0.5377 - val_acc: 0.7692\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53767 to 0.53765, saving model to best.model\n",
      "0s - loss: 0.5454 - acc: 0.7657 - val_loss: 0.5376 - val_acc: 0.7692\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7652 - val_loss: 0.5378 - val_acc: 0.7692\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.53765 to 0.53673, saving model to best.model\n",
      "0s - loss: 0.5455 - acc: 0.7658 - val_loss: 0.5367 - val_acc: 0.7709\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7661 - val_loss: 0.5369 - val_acc: 0.7709\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.53673 to 0.53620, saving model to best.model\n",
      "0s - loss: 0.5450 - acc: 0.7657 - val_loss: 0.5362 - val_acc: 0.7709\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5457 - acc: 0.7663 - val_loss: 0.5362 - val_acc: 0.7709\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5449 - acc: 0.7659 - val_loss: 0.5364 - val_acc: 0.7709\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.53620 to 0.53602, saving model to best.model\n",
      "0s - loss: 0.5452 - acc: 0.7663 - val_loss: 0.5360 - val_acc: 0.7709\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.53602 to 0.53597, saving model to best.model\n",
      "0s - loss: 0.5448 - acc: 0.7663 - val_loss: 0.5360 - val_acc: 0.7709\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.53597 to 0.53583, saving model to best.model\n",
      "0s - loss: 0.5442 - acc: 0.7666 - val_loss: 0.5358 - val_acc: 0.7709\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5443 - acc: 0.7664 - val_loss: 0.5365 - val_acc: 0.7709\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5437 - acc: 0.7672 - val_loss: 0.5361 - val_acc: 0.7709\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7660 - val_loss: 0.5359 - val_acc: 0.7709\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.53583 to 0.53568, saving model to best.model\n",
      "0s - loss: 0.5439 - acc: 0.7663 - val_loss: 0.5357 - val_acc: 0.7709\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.53568 to 0.53553, saving model to best.model\n",
      "0s - loss: 0.5434 - acc: 0.7669 - val_loss: 0.5355 - val_acc: 0.7709\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7667 - val_loss: 0.5357 - val_acc: 0.7709\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7669 - val_loss: 0.5359 - val_acc: 0.7709\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7665 - val_loss: 0.5362 - val_acc: 0.7709\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5437 - acc: 0.7665 - val_loss: 0.5365 - val_acc: 0.7709\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7666 - val_loss: 0.5359 - val_acc: 0.7709\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7661 - val_loss: 0.5358 - val_acc: 0.7709\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7666 - val_loss: 0.5366 - val_acc: 0.7709\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.53553 to 0.53553, saving model to best.model\n",
      "0s - loss: 0.5430 - acc: 0.7669 - val_loss: 0.5355 - val_acc: 0.7709\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.53553 to 0.53553, saving model to best.model\n",
      "0s - loss: 0.5445 - acc: 0.7661 - val_loss: 0.5355 - val_acc: 0.7709\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7667 - val_loss: 0.5360 - val_acc: 0.7709\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5437 - acc: 0.7664 - val_loss: 0.5371 - val_acc: 0.7709\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7667 - val_loss: 0.5358 - val_acc: 0.7709\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7669 - val_loss: 0.5369 - val_acc: 0.7709\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7667 - val_loss: 0.5363 - val_acc: 0.7709\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7667 - val_loss: 0.5360 - val_acc: 0.7709\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7667 - val_loss: 0.5359 - val_acc: 0.7709\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5441 - acc: 0.7666 - val_loss: 0.5357 - val_acc: 0.7709\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7669 - val_loss: 0.5357 - val_acc: 0.7709\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7663 - val_loss: 0.5356 - val_acc: 0.7709\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7665 - val_loss: 0.5359 - val_acc: 0.7709\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5448 - acc: 0.7665 - val_loss: 0.5359 - val_acc: 0.7709\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7666 - val_loss: 0.5356 - val_acc: 0.7709\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5437 - acc: 0.7660 - val_loss: 0.5367 - val_acc: 0.7709\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7676 - val_loss: 0.5356 - val_acc: 0.7709\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7667 - val_loss: 0.5358 - val_acc: 0.7709\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7670 - val_loss: 0.5360 - val_acc: 0.7709\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7672 - val_loss: 0.5358 - val_acc: 0.7709\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7670 - val_loss: 0.5358 - val_acc: 0.7709\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7670 - val_loss: 0.5357 - val_acc: 0.7709\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7669 - val_loss: 0.5363 - val_acc: 0.7709\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7668 - val_loss: 0.5359 - val_acc: 0.7709\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7672 - val_loss: 0.5356 - val_acc: 0.7709\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7666 - val_loss: 0.5363 - val_acc: 0.7709\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7666 - val_loss: 0.5358 - val_acc: 0.7709\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7664 - val_loss: 0.5355 - val_acc: 0.7709\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54822, saving model to best.model\n",
      "0s - loss: 0.6455 - acc: 0.7120 - val_loss: 0.5482 - val_acc: 0.7692\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54822 to 0.53896, saving model to best.model\n",
      "0s - loss: 0.5842 - acc: 0.7483 - val_loss: 0.5390 - val_acc: 0.7692\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5657 - acc: 0.7594 - val_loss: 0.5398 - val_acc: 0.7692\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5593 - acc: 0.7615 - val_loss: 0.5399 - val_acc: 0.7692\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5559 - acc: 0.7615 - val_loss: 0.5422 - val_acc: 0.7692\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7615 - val_loss: 0.5406 - val_acc: 0.7692\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7615 - val_loss: 0.5414 - val_acc: 0.7692\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7615 - val_loss: 0.5406 - val_acc: 0.7692\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7615 - val_loss: 0.5404 - val_acc: 0.7692\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7615 - val_loss: 0.5407 - val_acc: 0.7692\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7615 - val_loss: 0.5401 - val_acc: 0.7692\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7615 - val_loss: 0.5409 - val_acc: 0.7692\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7615 - val_loss: 0.5400 - val_acc: 0.7692\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7615 - val_loss: 0.5409 - val_acc: 0.7692\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7615 - val_loss: 0.5407 - val_acc: 0.7692\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7615 - val_loss: 0.5426 - val_acc: 0.7692\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7615 - val_loss: 0.5407 - val_acc: 0.7692\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7615 - val_loss: 0.5411 - val_acc: 0.7692\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7615 - val_loss: 0.5397 - val_acc: 0.7692\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7615 - val_loss: 0.5396 - val_acc: 0.7692\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7615 - val_loss: 0.5392 - val_acc: 0.7692\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7615 - val_loss: 0.5391 - val_acc: 0.7692\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.53896 to 0.53885, saving model to best.model\n",
      "0s - loss: 0.5514 - acc: 0.7615 - val_loss: 0.5388 - val_acc: 0.7692\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.53885 to 0.53872, saving model to best.model\n",
      "0s - loss: 0.5502 - acc: 0.7615 - val_loss: 0.5387 - val_acc: 0.7692\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.53872 to 0.53856, saving model to best.model\n",
      "0s - loss: 0.5502 - acc: 0.7615 - val_loss: 0.5386 - val_acc: 0.7692\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.53856 to 0.53839, saving model to best.model\n",
      "0s - loss: 0.5500 - acc: 0.7615 - val_loss: 0.5384 - val_acc: 0.7692\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.53839 to 0.53812, saving model to best.model\n",
      "0s - loss: 0.5512 - acc: 0.7615 - val_loss: 0.5381 - val_acc: 0.7692\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.53812 to 0.53803, saving model to best.model\n",
      "0s - loss: 0.5495 - acc: 0.7616 - val_loss: 0.5380 - val_acc: 0.7692\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.53803 to 0.53742, saving model to best.model\n",
      "0s - loss: 0.5505 - acc: 0.7616 - val_loss: 0.5374 - val_acc: 0.7692\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7621 - val_loss: 0.5415 - val_acc: 0.7694\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7622 - val_loss: 0.5414 - val_acc: 0.7719\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.53742 to 0.53661, saving model to best.model\n",
      "0s - loss: 0.5481 - acc: 0.7631 - val_loss: 0.5366 - val_acc: 0.7719\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.53661 to 0.53657, saving model to best.model\n",
      "0s - loss: 0.5483 - acc: 0.7637 - val_loss: 0.5366 - val_acc: 0.7728\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53657 to 0.53639, saving model to best.model\n",
      "0s - loss: 0.5477 - acc: 0.7642 - val_loss: 0.5364 - val_acc: 0.7736\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.53639 to 0.53463, saving model to best.model\n",
      "0s - loss: 0.5477 - acc: 0.7646 - val_loss: 0.5346 - val_acc: 0.7736\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.53463 to 0.53342, saving model to best.model\n",
      "0s - loss: 0.5472 - acc: 0.7654 - val_loss: 0.5334 - val_acc: 0.7736\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5468 - acc: 0.7659 - val_loss: 0.5356 - val_acc: 0.7738\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.53342 to 0.53262, saving model to best.model\n",
      "0s - loss: 0.5452 - acc: 0.7663 - val_loss: 0.5326 - val_acc: 0.7745\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7667 - val_loss: 0.5327 - val_acc: 0.7750\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.53262 to 0.53200, saving model to best.model\n",
      "0s - loss: 0.5445 - acc: 0.7667 - val_loss: 0.5320 - val_acc: 0.7750\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.53200 to 0.53199, saving model to best.model\n",
      "0s - loss: 0.5448 - acc: 0.7666 - val_loss: 0.5320 - val_acc: 0.7750\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.53199 to 0.53192, saving model to best.model\n",
      "0s - loss: 0.5434 - acc: 0.7671 - val_loss: 0.5319 - val_acc: 0.7750\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.53192 to 0.53167, saving model to best.model\n",
      "0s - loss: 0.5437 - acc: 0.7674 - val_loss: 0.5317 - val_acc: 0.7750\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7676 - val_loss: 0.5327 - val_acc: 0.7750\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.53167 to 0.53102, saving model to best.model\n",
      "0s - loss: 0.5432 - acc: 0.7673 - val_loss: 0.5310 - val_acc: 0.7750\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7679 - val_loss: 0.5316 - val_acc: 0.7750\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7676 - val_loss: 0.5311 - val_acc: 0.7750\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7674 - val_loss: 0.5315 - val_acc: 0.7750\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7677 - val_loss: 0.5342 - val_acc: 0.7750\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7678 - val_loss: 0.5312 - val_acc: 0.7750\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7678 - val_loss: 0.5323 - val_acc: 0.7750\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7674 - val_loss: 0.5316 - val_acc: 0.7758\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7677 - val_loss: 0.5311 - val_acc: 0.7758\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7678 - val_loss: 0.5318 - val_acc: 0.7758\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7679 - val_loss: 0.5316 - val_acc: 0.7750\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7678 - val_loss: 0.5319 - val_acc: 0.7758\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7674 - val_loss: 0.5315 - val_acc: 0.7750\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7683 - val_loss: 0.5317 - val_acc: 0.7758\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7676 - val_loss: 0.5330 - val_acc: 0.7758\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7680 - val_loss: 0.5317 - val_acc: 0.7758\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7675 - val_loss: 0.5313 - val_acc: 0.7758\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7680 - val_loss: 0.5313 - val_acc: 0.7758\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7677 - val_loss: 0.5323 - val_acc: 0.7758\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7678 - val_loss: 0.5316 - val_acc: 0.7750\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7675 - val_loss: 0.5325 - val_acc: 0.7758\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7678 - val_loss: 0.5315 - val_acc: 0.7758\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7680 - val_loss: 0.5323 - val_acc: 0.7758\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7675 - val_loss: 0.5316 - val_acc: 0.7750\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7676 - val_loss: 0.5311 - val_acc: 0.7758\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.53102 to 0.53071, saving model to best.model\n",
      "0s - loss: 0.5409 - acc: 0.7678 - val_loss: 0.5307 - val_acc: 0.7758\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7681 - val_loss: 0.5327 - val_acc: 0.7758\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7676 - val_loss: 0.5339 - val_acc: 0.7758\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7677 - val_loss: 0.5319 - val_acc: 0.7758\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7681 - val_loss: 0.5314 - val_acc: 0.7758\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7681 - val_loss: 0.5319 - val_acc: 0.7758\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7680 - val_loss: 0.5317 - val_acc: 0.7758\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7678 - val_loss: 0.5315 - val_acc: 0.7758\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7681 - val_loss: 0.5315 - val_acc: 0.7758\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7676 - val_loss: 0.5312 - val_acc: 0.7758\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7681 - val_loss: 0.5323 - val_acc: 0.7758\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7678 - val_loss: 0.5330 - val_acc: 0.7758\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7683 - val_loss: 0.5315 - val_acc: 0.7758\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7678 - val_loss: 0.5313 - val_acc: 0.7758\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7679 - val_loss: 0.5312 - val_acc: 0.7758\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7673 - val_loss: 0.5321 - val_acc: 0.7758\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7678 - val_loss: 0.5321 - val_acc: 0.7758\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7680 - val_loss: 0.5312 - val_acc: 0.7758\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7682 - val_loss: 0.5318 - val_acc: 0.7758\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7677 - val_loss: 0.5318 - val_acc: 0.7758\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7675 - val_loss: 0.5313 - val_acc: 0.7758\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7677 - val_loss: 0.5316 - val_acc: 0.7758\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7678 - val_loss: 0.5324 - val_acc: 0.7758\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7675 - val_loss: 0.5315 - val_acc: 0.7758\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7678 - val_loss: 0.5319 - val_acc: 0.7758\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7679 - val_loss: 0.5316 - val_acc: 0.7758\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7678 - val_loss: 0.5315 - val_acc: 0.7758\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55116, saving model to best.model\n",
      "0s - loss: 0.6204 - acc: 0.7225 - val_loss: 0.5512 - val_acc: 0.7590\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55116 to 0.55053, saving model to best.model\n",
      "0s - loss: 0.5787 - acc: 0.7488 - val_loss: 0.5505 - val_acc: 0.7590\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5660 - acc: 0.7534 - val_loss: 0.5526 - val_acc: 0.7590\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5644 - acc: 0.7532 - val_loss: 0.5511 - val_acc: 0.7590\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.55053 to 0.55038, saving model to best.model\n",
      "0s - loss: 0.5617 - acc: 0.7532 - val_loss: 0.5504 - val_acc: 0.7590\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5623 - acc: 0.7532 - val_loss: 0.5512 - val_acc: 0.7590\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5611 - acc: 0.7532 - val_loss: 0.5520 - val_acc: 0.7590\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5616 - acc: 0.7532 - val_loss: 0.5506 - val_acc: 0.7590\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5606 - acc: 0.7532 - val_loss: 0.5521 - val_acc: 0.7590\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5621 - acc: 0.7532 - val_loss: 0.5523 - val_acc: 0.7590\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5621 - acc: 0.7532 - val_loss: 0.5521 - val_acc: 0.7590\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5623 - acc: 0.7532 - val_loss: 0.5518 - val_acc: 0.7590\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5613 - acc: 0.7532 - val_loss: 0.5516 - val_acc: 0.7590\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7532 - val_loss: 0.5522 - val_acc: 0.7590\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5609 - acc: 0.7532 - val_loss: 0.5518 - val_acc: 0.7590\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5599 - acc: 0.7532 - val_loss: 0.5520 - val_acc: 0.7590\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5596 - acc: 0.7532 - val_loss: 0.5515 - val_acc: 0.7590\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5602 - acc: 0.7532 - val_loss: 0.5528 - val_acc: 0.7590\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5598 - acc: 0.7532 - val_loss: 0.5528 - val_acc: 0.7590\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5599 - acc: 0.7532 - val_loss: 0.5510 - val_acc: 0.7590\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7532 - val_loss: 0.5505 - val_acc: 0.7590\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5598 - acc: 0.7532 - val_loss: 0.5506 - val_acc: 0.7590\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5592 - acc: 0.7532 - val_loss: 0.5505 - val_acc: 0.7590\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.55038 to 0.55036, saving model to best.model\n",
      "0s - loss: 0.5590 - acc: 0.7533 - val_loss: 0.5504 - val_acc: 0.7590\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.55036 to 0.55029, saving model to best.model\n",
      "0s - loss: 0.5603 - acc: 0.7533 - val_loss: 0.5503 - val_acc: 0.7590\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.55029 to 0.54970, saving model to best.model\n",
      "0s - loss: 0.5590 - acc: 0.7534 - val_loss: 0.5497 - val_acc: 0.7590\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5587 - acc: 0.7540 - val_loss: 0.5513 - val_acc: 0.7602\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54970 to 0.54924, saving model to best.model\n",
      "0s - loss: 0.5580 - acc: 0.7547 - val_loss: 0.5492 - val_acc: 0.7617\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54924 to 0.54795, saving model to best.model\n",
      "0s - loss: 0.5574 - acc: 0.7560 - val_loss: 0.5479 - val_acc: 0.7617\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54795 to 0.54715, saving model to best.model\n",
      "0s - loss: 0.5571 - acc: 0.7563 - val_loss: 0.5472 - val_acc: 0.7638\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7571 - val_loss: 0.5488 - val_acc: 0.7638\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54715 to 0.54605, saving model to best.model\n",
      "0s - loss: 0.5547 - acc: 0.7579 - val_loss: 0.5460 - val_acc: 0.7638\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.54605 to 0.54604, saving model to best.model\n",
      "0s - loss: 0.5566 - acc: 0.7572 - val_loss: 0.5460 - val_acc: 0.7638\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54604 to 0.54499, saving model to best.model\n",
      "0s - loss: 0.5538 - acc: 0.7584 - val_loss: 0.5450 - val_acc: 0.7643\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5559 - acc: 0.7579 - val_loss: 0.5451 - val_acc: 0.7643\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7584 - val_loss: 0.5451 - val_acc: 0.7643\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7586 - val_loss: 0.5488 - val_acc: 0.7643\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54499 to 0.54476, saving model to best.model\n",
      "0s - loss: 0.5544 - acc: 0.7587 - val_loss: 0.5448 - val_acc: 0.7643\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7596 - val_loss: 0.5451 - val_acc: 0.7643\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.54476 to 0.54425, saving model to best.model\n",
      "0s - loss: 0.5535 - acc: 0.7597 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7591 - val_loss: 0.5450 - val_acc: 0.7643\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7595 - val_loss: 0.5443 - val_acc: 0.7643\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54425 to 0.54400, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7596 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7592 - val_loss: 0.5443 - val_acc: 0.7643\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.54400 to 0.54383, saving model to best.model\n",
      "0s - loss: 0.5530 - acc: 0.7596 - val_loss: 0.5438 - val_acc: 0.7643\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5534 - acc: 0.7587 - val_loss: 0.5445 - val_acc: 0.7643\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5528 - acc: 0.7592 - val_loss: 0.5464 - val_acc: 0.7643\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7596 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7593 - val_loss: 0.5448 - val_acc: 0.7643\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7591 - val_loss: 0.5449 - val_acc: 0.7643\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7591 - val_loss: 0.5454 - val_acc: 0.7643\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7596 - val_loss: 0.5444 - val_acc: 0.7643\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7595 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7594 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7591 - val_loss: 0.5449 - val_acc: 0.7643\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7596 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7593 - val_loss: 0.5454 - val_acc: 0.7643\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7599 - val_loss: 0.5439 - val_acc: 0.7643\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7594 - val_loss: 0.5441 - val_acc: 0.7643\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7595 - val_loss: 0.5444 - val_acc: 0.7643\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7596 - val_loss: 0.5448 - val_acc: 0.7643\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7601 - val_loss: 0.5439 - val_acc: 0.7643\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7596 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.54383 to 0.54367, saving model to best.model\n",
      "0s - loss: 0.5514 - acc: 0.7593 - val_loss: 0.5437 - val_acc: 0.7643\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7595 - val_loss: 0.5444 - val_acc: 0.7643\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7601 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7595 - val_loss: 0.5444 - val_acc: 0.7643\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7592 - val_loss: 0.5451 - val_acc: 0.7643\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5439 - val_acc: 0.7643\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7592 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.54367 to 0.54351, saving model to best.model\n",
      "0s - loss: 0.5503 - acc: 0.7603 - val_loss: 0.5435 - val_acc: 0.7643\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7597 - val_loss: 0.5444 - val_acc: 0.7643\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7589 - val_loss: 0.5445 - val_acc: 0.7643\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7599 - val_loss: 0.5441 - val_acc: 0.7643\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7592 - val_loss: 0.5439 - val_acc: 0.7643\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7595 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7592 - val_loss: 0.5445 - val_acc: 0.7643\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7597 - val_loss: 0.5443 - val_acc: 0.7643\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7596 - val_loss: 0.5444 - val_acc: 0.7643\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7603 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7595 - val_loss: 0.5435 - val_acc: 0.7643\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7596 - val_loss: 0.5441 - val_acc: 0.7643\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7595 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7601 - val_loss: 0.5437 - val_acc: 0.7643\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7596 - val_loss: 0.5441 - val_acc: 0.7643\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7594 - val_loss: 0.5445 - val_acc: 0.7643\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7596 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7597 - val_loss: 0.5446 - val_acc: 0.7643\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7596 - val_loss: 0.5441 - val_acc: 0.7643\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7594 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7599 - val_loss: 0.5441 - val_acc: 0.7643\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7596 - val_loss: 0.5443 - val_acc: 0.7643\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7596 - val_loss: 0.5446 - val_acc: 0.7643\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7600 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7599 - val_loss: 0.5442 - val_acc: 0.7643\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.5441 - val_acc: 0.7643\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7600 - val_loss: 0.5443 - val_acc: 0.7643\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55126, saving model to best.model\n",
      "0s - loss: 0.5984 - acc: 0.7372 - val_loss: 0.5513 - val_acc: 0.7583\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5657 - acc: 0.7592 - val_loss: 0.5553 - val_acc: 0.7583\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5592 - acc: 0.7604 - val_loss: 0.5539 - val_acc: 0.7583\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5563 - acc: 0.7606 - val_loss: 0.5528 - val_acc: 0.7583\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7605 - val_loss: 0.5536 - val_acc: 0.7583\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5539 - acc: 0.7605 - val_loss: 0.5547 - val_acc: 0.7583\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7605 - val_loss: 0.5529 - val_acc: 0.7583\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7605 - val_loss: 0.5529 - val_acc: 0.7583\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7605 - val_loss: 0.5528 - val_acc: 0.7583\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7605 - val_loss: 0.5529 - val_acc: 0.7583\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7605 - val_loss: 0.5532 - val_acc: 0.7583\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7605 - val_loss: 0.5526 - val_acc: 0.7583\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5522 - acc: 0.7605 - val_loss: 0.5531 - val_acc: 0.7583\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7605 - val_loss: 0.5527 - val_acc: 0.7583\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7605 - val_loss: 0.5531 - val_acc: 0.7583\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5531 - acc: 0.7605 - val_loss: 0.5522 - val_acc: 0.7583\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7605 - val_loss: 0.5525 - val_acc: 0.7583\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7605 - val_loss: 0.5516 - val_acc: 0.7583\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7605 - val_loss: 0.5514 - val_acc: 0.7583\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7605 - val_loss: 0.5515 - val_acc: 0.7583\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.55126 to 0.55120, saving model to best.model\n",
      "0s - loss: 0.5517 - acc: 0.7605 - val_loss: 0.5512 - val_acc: 0.7583\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.55120 to 0.55098, saving model to best.model\n",
      "0s - loss: 0.5517 - acc: 0.7606 - val_loss: 0.5510 - val_acc: 0.7583\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.55098 to 0.55088, saving model to best.model\n",
      "0s - loss: 0.5515 - acc: 0.7605 - val_loss: 0.5509 - val_acc: 0.7583\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.55088 to 0.54744, saving model to best.model\n",
      "0s - loss: 0.5503 - acc: 0.7605 - val_loss: 0.5474 - val_acc: 0.7583\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7607 - val_loss: 0.5548 - val_acc: 0.7587\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7603 - val_loss: 0.5523 - val_acc: 0.7583\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7605 - val_loss: 0.5521 - val_acc: 0.7583\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7605 - val_loss: 0.5521 - val_acc: 0.7583\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7605 - val_loss: 0.5525 - val_acc: 0.7583\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7605 - val_loss: 0.5518 - val_acc: 0.7583\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7607 - val_loss: 0.5522 - val_acc: 0.7583\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7608 - val_loss: 0.5514 - val_acc: 0.7587\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7610 - val_loss: 0.5514 - val_acc: 0.7587\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7612 - val_loss: 0.5509 - val_acc: 0.7590\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7621 - val_loss: 0.5506 - val_acc: 0.7600\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7624 - val_loss: 0.5503 - val_acc: 0.7600\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7624 - val_loss: 0.5525 - val_acc: 0.7609\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5484 - acc: 0.7631 - val_loss: 0.5497 - val_acc: 0.7609\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7631 - val_loss: 0.5504 - val_acc: 0.7609\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7637 - val_loss: 0.5490 - val_acc: 0.7612\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7637 - val_loss: 0.5488 - val_acc: 0.7614\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5483 - acc: 0.7637 - val_loss: 0.5484 - val_acc: 0.7621\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5478 - acc: 0.7641 - val_loss: 0.5485 - val_acc: 0.7621\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5467 - acc: 0.7646 - val_loss: 0.5482 - val_acc: 0.7621\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5468 - acc: 0.7640 - val_loss: 0.5481 - val_acc: 0.7621\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5482 - acc: 0.7638 - val_loss: 0.5492 - val_acc: 0.7621\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7642 - val_loss: 0.5479 - val_acc: 0.7621\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5470 - acc: 0.7643 - val_loss: 0.5478 - val_acc: 0.7621\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7647 - val_loss: 0.5478 - val_acc: 0.7621\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7644 - val_loss: 0.5476 - val_acc: 0.7621\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.56091, saving model to best.model\n",
      "0s - loss: 0.6135 - acc: 0.7285 - val_loss: 0.5609 - val_acc: 0.7502\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5667 - acc: 0.7568 - val_loss: 0.5610 - val_acc: 0.7502\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.56091 to 0.55966, saving model to best.model\n",
      "0s - loss: 0.5615 - acc: 0.7595 - val_loss: 0.5597 - val_acc: 0.7502\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5567 - acc: 0.7599 - val_loss: 0.5604 - val_acc: 0.7502\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5562 - acc: 0.7599 - val_loss: 0.5613 - val_acc: 0.7502\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5548 - acc: 0.7599 - val_loss: 0.5622 - val_acc: 0.7502\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7599 - val_loss: 0.5623 - val_acc: 0.7502\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7599 - val_loss: 0.5615 - val_acc: 0.7502\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5541 - acc: 0.7599 - val_loss: 0.5616 - val_acc: 0.7502\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7599 - val_loss: 0.5619 - val_acc: 0.7502\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5545 - acc: 0.7599 - val_loss: 0.5615 - val_acc: 0.7502\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7599 - val_loss: 0.5617 - val_acc: 0.7502\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7599 - val_loss: 0.5610 - val_acc: 0.7502\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7599 - val_loss: 0.5614 - val_acc: 0.7502\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5535 - acc: 0.7599 - val_loss: 0.5621 - val_acc: 0.7502\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7599 - val_loss: 0.5616 - val_acc: 0.7502\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7599 - val_loss: 0.5622 - val_acc: 0.7502\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5536 - acc: 0.7599 - val_loss: 0.5612 - val_acc: 0.7502\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7599 - val_loss: 0.5612 - val_acc: 0.7502\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5537 - acc: 0.7599 - val_loss: 0.5613 - val_acc: 0.7502\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7599 - val_loss: 0.5608 - val_acc: 0.7502\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5538 - acc: 0.7599 - val_loss: 0.5608 - val_acc: 0.7502\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5519 - acc: 0.7599 - val_loss: 0.5607 - val_acc: 0.7502\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7599 - val_loss: 0.5604 - val_acc: 0.7502\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7599 - val_loss: 0.5604 - val_acc: 0.7502\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7599 - val_loss: 0.5603 - val_acc: 0.7502\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7599 - val_loss: 0.5603 - val_acc: 0.7502\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7599 - val_loss: 0.5601 - val_acc: 0.7502\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7599 - val_loss: 0.5605 - val_acc: 0.7502\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55467, saving model to best.model\n",
      "0s - loss: 0.6051 - acc: 0.7329 - val_loss: 0.5547 - val_acc: 0.7578\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55467 to 0.55295, saving model to best.model\n",
      "0s - loss: 0.5670 - acc: 0.7572 - val_loss: 0.5530 - val_acc: 0.7578\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5618 - acc: 0.7601 - val_loss: 0.5532 - val_acc: 0.7578\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5560 - acc: 0.7603 - val_loss: 0.5537 - val_acc: 0.7578\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5549 - acc: 0.7603 - val_loss: 0.5530 - val_acc: 0.7578\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.55295 to 0.55293, saving model to best.model\n",
      "0s - loss: 0.5557 - acc: 0.7603 - val_loss: 0.5529 - val_acc: 0.7578\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7603 - val_loss: 0.5529 - val_acc: 0.7578\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7603 - val_loss: 0.5539 - val_acc: 0.7578\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.55293 to 0.55280, saving model to best.model\n",
      "0s - loss: 0.5533 - acc: 0.7603 - val_loss: 0.5528 - val_acc: 0.7578\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.55280 to 0.54912, saving model to best.model\n",
      "0s - loss: 0.5521 - acc: 0.7603 - val_loss: 0.5491 - val_acc: 0.7578\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.54912 to 0.54465, saving model to best.model\n",
      "0s - loss: 0.5491 - acc: 0.7603 - val_loss: 0.5447 - val_acc: 0.7578\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5463 - acc: 0.7604 - val_loss: 0.5456 - val_acc: 0.7578\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5460 - acc: 0.7609 - val_loss: 0.5453 - val_acc: 0.7585\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.54465 to 0.54170, saving model to best.model\n",
      "0s - loss: 0.5499 - acc: 0.7613 - val_loss: 0.5417 - val_acc: 0.7583\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5461 - acc: 0.7627 - val_loss: 0.5444 - val_acc: 0.7600\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7614 - val_loss: 0.5499 - val_acc: 0.7578\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7606 - val_loss: 0.5505 - val_acc: 0.7580\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7610 - val_loss: 0.5498 - val_acc: 0.7585\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7618 - val_loss: 0.5495 - val_acc: 0.7592\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7624 - val_loss: 0.5476 - val_acc: 0.7600\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7645 - val_loss: 0.5482 - val_acc: 0.7602\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7640 - val_loss: 0.5463 - val_acc: 0.7621\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7652 - val_loss: 0.5450 - val_acc: 0.7621\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5459 - acc: 0.7653 - val_loss: 0.5466 - val_acc: 0.7621\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7679 - val_loss: 0.5428 - val_acc: 0.7658\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7671 - val_loss: 0.5429 - val_acc: 0.7658\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54170 to 0.54131, saving model to best.model\n",
      "0s - loss: 0.5431 - acc: 0.7685 - val_loss: 0.5413 - val_acc: 0.7663\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7694 - val_loss: 0.5426 - val_acc: 0.7663\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54131 to 0.53948, saving model to best.model\n",
      "0s - loss: 0.5417 - acc: 0.7691 - val_loss: 0.5395 - val_acc: 0.7663\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7694 - val_loss: 0.5413 - val_acc: 0.7663\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7698 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7702 - val_loss: 0.5421 - val_acc: 0.7663\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7703 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53948 to 0.53888, saving model to best.model\n",
      "0s - loss: 0.5404 - acc: 0.7695 - val_loss: 0.5389 - val_acc: 0.7663\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7701 - val_loss: 0.5391 - val_acc: 0.7663\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7701 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7705 - val_loss: 0.5389 - val_acc: 0.7663\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7696 - val_loss: 0.5400 - val_acc: 0.7663\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7707 - val_loss: 0.5412 - val_acc: 0.7663\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7699 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7700 - val_loss: 0.5397 - val_acc: 0.7663\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7706 - val_loss: 0.5398 - val_acc: 0.7663\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7708 - val_loss: 0.5397 - val_acc: 0.7663\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7703 - val_loss: 0.5396 - val_acc: 0.7663\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7704 - val_loss: 0.5399 - val_acc: 0.7663\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7705 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7700 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7699 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7706 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7715 - val_loss: 0.5394 - val_acc: 0.7663\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7698 - val_loss: 0.5412 - val_acc: 0.7663\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7707 - val_loss: 0.5398 - val_acc: 0.7663\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7709 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7700 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5378 - acc: 0.7711 - val_loss: 0.5397 - val_acc: 0.7663\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.53888 to 0.53858, saving model to best.model\n",
      "0s - loss: 0.5359 - acc: 0.7725 - val_loss: 0.5386 - val_acc: 0.7663\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7705 - val_loss: 0.5399 - val_acc: 0.7663\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7695 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7710 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7710 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7700 - val_loss: 0.5393 - val_acc: 0.7663\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7705 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7710 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7700 - val_loss: 0.5409 - val_acc: 0.7663\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7706 - val_loss: 0.5408 - val_acc: 0.7663\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7708 - val_loss: 0.5395 - val_acc: 0.7663\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7700 - val_loss: 0.5398 - val_acc: 0.7663\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7710 - val_loss: 0.5386 - val_acc: 0.7663\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7708 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5366 - acc: 0.7712 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7699 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5374 - acc: 0.7706 - val_loss: 0.5410 - val_acc: 0.7663\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7702 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7702 - val_loss: 0.5399 - val_acc: 0.7663\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.53858 to 0.53837, saving model to best.model\n",
      "0s - loss: 0.5380 - acc: 0.7711 - val_loss: 0.5384 - val_acc: 0.7663\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7709 - val_loss: 0.5391 - val_acc: 0.7663\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7707 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5378 - acc: 0.7709 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5383 - acc: 0.7709 - val_loss: 0.5395 - val_acc: 0.7663\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5372 - acc: 0.7711 - val_loss: 0.5396 - val_acc: 0.7663\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7708 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7705 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7705 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5376 - acc: 0.7706 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7704 - val_loss: 0.5399 - val_acc: 0.7663\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5371 - acc: 0.7711 - val_loss: 0.5399 - val_acc: 0.7663\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7704 - val_loss: 0.5402 - val_acc: 0.7663\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5383 - acc: 0.7703 - val_loss: 0.5397 - val_acc: 0.7663\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7711 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7714 - val_loss: 0.5396 - val_acc: 0.7663\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7705 - val_loss: 0.5407 - val_acc: 0.7663\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7701 - val_loss: 0.5400 - val_acc: 0.7663\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7704 - val_loss: 0.5397 - val_acc: 0.7663\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7706 - val_loss: 0.5393 - val_acc: 0.7663\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7712 - val_loss: 0.5392 - val_acc: 0.7663\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5383 - acc: 0.7703 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5383 - acc: 0.7703 - val_loss: 0.5392 - val_acc: 0.7663\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7707 - val_loss: 0.5396 - val_acc: 0.7663\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.53837 to 0.53826, saving model to best.model\n",
      "0s - loss: 0.5359 - acc: 0.7715 - val_loss: 0.5383 - val_acc: 0.7663\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7691 - val_loss: 0.5412 - val_acc: 0.7663\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7711 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7704 - val_loss: 0.5411 - val_acc: 0.7663\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7713 - val_loss: 0.5396 - val_acc: 0.7663\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5376 - acc: 0.7708 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7709 - val_loss: 0.5391 - val_acc: 0.7663\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7704 - val_loss: 0.5389 - val_acc: 0.7663\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5367 - acc: 0.7712 - val_loss: 0.5389 - val_acc: 0.7663\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7711 - val_loss: 0.5398 - val_acc: 0.7663\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5364 - acc: 0.7711 - val_loss: 0.5391 - val_acc: 0.7663\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5367 - acc: 0.7712 - val_loss: 0.5396 - val_acc: 0.7663\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7702 - val_loss: 0.5409 - val_acc: 0.7663\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7708 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.53826 to 0.53758, saving model to best.model\n",
      "0s - loss: 0.5346 - acc: 0.7724 - val_loss: 0.5376 - val_acc: 0.7663\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.5366 - acc: 0.7714 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7703 - val_loss: 0.5402 - val_acc: 0.7663\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.5376 - acc: 0.7706 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5363 - acc: 0.7713 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7705 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.5359 - acc: 0.7715 - val_loss: 0.5396 - val_acc: 0.7663\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7715 - val_loss: 0.5383 - val_acc: 0.7663\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7711 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7702 - val_loss: 0.5409 - val_acc: 0.7663\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7709 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.5374 - acc: 0.7709 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7710 - val_loss: 0.5402 - val_acc: 0.7663\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.5371 - acc: 0.7708 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7704 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.5388 - acc: 0.7696 - val_loss: 0.5407 - val_acc: 0.7663\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.5374 - acc: 0.7707 - val_loss: 0.5395 - val_acc: 0.7663\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7707 - val_loss: 0.5387 - val_acc: 0.7663\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7702 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.5378 - acc: 0.7703 - val_loss: 0.5389 - val_acc: 0.7663\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7711 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.5361 - acc: 0.7714 - val_loss: 0.5414 - val_acc: 0.7663\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.5361 - acc: 0.7716 - val_loss: 0.5396 - val_acc: 0.7663\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.53758 to 0.53716, saving model to best.model\n",
      "0s - loss: 0.5366 - acc: 0.7709 - val_loss: 0.5372 - val_acc: 0.7728\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7705 - val_loss: 0.5394 - val_acc: 0.7663\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7713 - val_loss: 0.5411 - val_acc: 0.7663\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.5371 - acc: 0.7711 - val_loss: 0.5394 - val_acc: 0.7663\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7700 - val_loss: 0.5376 - val_acc: 0.7663\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7703 - val_loss: 0.5390 - val_acc: 0.7663\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.5374 - acc: 0.7705 - val_loss: 0.5393 - val_acc: 0.7663\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.5367 - acc: 0.7708 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7706 - val_loss: 0.5391 - val_acc: 0.7663\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.5364 - acc: 0.7712 - val_loss: 0.5410 - val_acc: 0.7663\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.5366 - acc: 0.7709 - val_loss: 0.5407 - val_acc: 0.7663\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7709 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.5383 - acc: 0.7700 - val_loss: 0.5414 - val_acc: 0.7663\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7705 - val_loss: 0.5399 - val_acc: 0.7663\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.5378 - acc: 0.7703 - val_loss: 0.5393 - val_acc: 0.7663\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.53716 to 0.53715, saving model to best.model\n",
      "0s - loss: 0.5375 - acc: 0.7702 - val_loss: 0.5371 - val_acc: 0.7663\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.5366 - acc: 0.7714 - val_loss: 0.5401 - val_acc: 0.7663\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.5372 - acc: 0.7706 - val_loss: 0.5402 - val_acc: 0.7663\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.5372 - acc: 0.7708 - val_loss: 0.5400 - val_acc: 0.7663\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.5362 - acc: 0.7712 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7709 - val_loss: 0.5397 - val_acc: 0.7663\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.5363 - acc: 0.7712 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7706 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.5380 - acc: 0.7700 - val_loss: 0.5398 - val_acc: 0.7663\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.5374 - acc: 0.7703 - val_loss: 0.5381 - val_acc: 0.7663\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7708 - val_loss: 0.5397 - val_acc: 0.7663\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.5359 - acc: 0.7715 - val_loss: 0.5375 - val_acc: 0.7663\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.5362 - acc: 0.7714 - val_loss: 0.5377 - val_acc: 0.7663\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7700 - val_loss: 0.5403 - val_acc: 0.7663\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7703 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7708 - val_loss: 0.5389 - val_acc: 0.7663\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7705 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.5357 - acc: 0.7716 - val_loss: 0.5377 - val_acc: 0.7663\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.5379 - acc: 0.7704 - val_loss: 0.5402 - val_acc: 0.7663\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.5372 - acc: 0.7705 - val_loss: 0.5373 - val_acc: 0.7663\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7703 - val_loss: 0.5389 - val_acc: 0.7663\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7706 - val_loss: 0.5404 - val_acc: 0.7663\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.53715 to 0.53637, saving model to best.model\n",
      "0s - loss: 0.5363 - acc: 0.7712 - val_loss: 0.5364 - val_acc: 0.7728\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7708 - val_loss: 0.5402 - val_acc: 0.7663\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.5378 - acc: 0.7708 - val_loss: 0.5374 - val_acc: 0.7663\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.5361 - acc: 0.7714 - val_loss: 0.5370 - val_acc: 0.7728\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7712 - val_loss: 0.5389 - val_acc: 0.7663\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7710 - val_loss: 0.5391 - val_acc: 0.7663\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.5375 - acc: 0.7703 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.5359 - acc: 0.7715 - val_loss: 0.5388 - val_acc: 0.7663\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7710 - val_loss: 0.5386 - val_acc: 0.7663\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.5362 - acc: 0.7711 - val_loss: 0.5387 - val_acc: 0.7663\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7700 - val_loss: 0.5409 - val_acc: 0.7663\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7706 - val_loss: 0.5397 - val_acc: 0.7663\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.5371 - acc: 0.7705 - val_loss: 0.5407 - val_acc: 0.7663\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.5363 - acc: 0.7711 - val_loss: 0.5402 - val_acc: 0.7663\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.53637 to 0.53580, saving model to best.model\n",
      "331s - loss: 0.5364 - acc: 0.7711 - val_loss: 0.5358 - val_acc: 0.7728\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7704 - val_loss: 0.5379 - val_acc: 0.7663\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.5376 - acc: 0.7703 - val_loss: 0.5399 - val_acc: 0.7663\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.5372 - acc: 0.7705 - val_loss: 0.5393 - val_acc: 0.7663\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7712 - val_loss: 0.5376 - val_acc: 0.7663\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.5374 - acc: 0.7705 - val_loss: 0.5387 - val_acc: 0.7663\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.5368 - acc: 0.7710 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.5370 - acc: 0.7709 - val_loss: 0.5377 - val_acc: 0.7663\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7711 - val_loss: 0.5393 - val_acc: 0.7663\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7709 - val_loss: 0.5383 - val_acc: 0.7663\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7701 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7712 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.5353 - acc: 0.7719 - val_loss: 0.5405 - val_acc: 0.7663\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7715 - val_loss: 0.5406 - val_acc: 0.7663\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54166, saving model to best.model\n",
      "0s - loss: 0.6189 - acc: 0.7257 - val_loss: 0.5417 - val_acc: 0.7675\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5781 - acc: 0.7494 - val_loss: 0.5445 - val_acc: 0.7675\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5668 - acc: 0.7548 - val_loss: 0.5422 - val_acc: 0.7675\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5630 - acc: 0.7553 - val_loss: 0.5423 - val_acc: 0.7675\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.54166 to 0.54123, saving model to best.model\n",
      "0s - loss: 0.5609 - acc: 0.7553 - val_loss: 0.5412 - val_acc: 0.7675\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.54123 to 0.54085, saving model to best.model\n",
      "0s - loss: 0.5594 - acc: 0.7553 - val_loss: 0.5408 - val_acc: 0.7675\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.54085 to 0.54035, saving model to best.model\n",
      "0s - loss: 0.5591 - acc: 0.7553 - val_loss: 0.5404 - val_acc: 0.7675\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.54035 to 0.53868, saving model to best.model\n",
      "0s - loss: 0.5584 - acc: 0.7553 - val_loss: 0.5387 - val_acc: 0.7675\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5571 - acc: 0.7556 - val_loss: 0.5396 - val_acc: 0.7675\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.53868 to 0.53825, saving model to best.model\n",
      "0s - loss: 0.5557 - acc: 0.7558 - val_loss: 0.5382 - val_acc: 0.7677\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.53825 to 0.53506, saving model to best.model\n",
      "0s - loss: 0.5524 - acc: 0.7578 - val_loss: 0.5351 - val_acc: 0.7687\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7584 - val_loss: 0.5376 - val_acc: 0.7682\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5559 - acc: 0.7585 - val_loss: 0.5373 - val_acc: 0.7690\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.53506 to 0.53249, saving model to best.model\n",
      "0s - loss: 0.5522 - acc: 0.7602 - val_loss: 0.5325 - val_acc: 0.7728\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7613 - val_loss: 0.5365 - val_acc: 0.7687\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7599 - val_loss: 0.5357 - val_acc: 0.7690\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7606 - val_loss: 0.5342 - val_acc: 0.7731\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5547 - acc: 0.7572 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5572 - acc: 0.7563 - val_loss: 0.5433 - val_acc: 0.7682\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5560 - acc: 0.7569 - val_loss: 0.5408 - val_acc: 0.7682\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5565 - acc: 0.7567 - val_loss: 0.5407 - val_acc: 0.7682\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5552 - acc: 0.7570 - val_loss: 0.5399 - val_acc: 0.7682\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7572 - val_loss: 0.5399 - val_acc: 0.7687\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5549 - acc: 0.7581 - val_loss: 0.5391 - val_acc: 0.7690\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5543 - acc: 0.7587 - val_loss: 0.5399 - val_acc: 0.7690\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7589 - val_loss: 0.5383 - val_acc: 0.7690\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7596 - val_loss: 0.5378 - val_acc: 0.7690\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5533 - acc: 0.7593 - val_loss: 0.5376 - val_acc: 0.7690\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5532 - acc: 0.7597 - val_loss: 0.5395 - val_acc: 0.7716\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7601 - val_loss: 0.5363 - val_acc: 0.7731\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7603 - val_loss: 0.5375 - val_acc: 0.7731\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5500 - acc: 0.7610 - val_loss: 0.5352 - val_acc: 0.7728\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5514 - acc: 0.7608 - val_loss: 0.5349 - val_acc: 0.7728\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7613 - val_loss: 0.5353 - val_acc: 0.7728\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7613 - val_loss: 0.5354 - val_acc: 0.7728\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5491 - acc: 0.7618 - val_loss: 0.5344 - val_acc: 0.7728\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5502 - acc: 0.7613 - val_loss: 0.5345 - val_acc: 0.7728\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7624 - val_loss: 0.5336 - val_acc: 0.7728\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7617 - val_loss: 0.5339 - val_acc: 0.7728\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7626 - val_loss: 0.5335 - val_acc: 0.7728\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.57095, saving model to best.model\n",
      "0s - loss: 0.6241 - acc: 0.7247 - val_loss: 0.5709 - val_acc: 0.7415\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5759 - acc: 0.7531 - val_loss: 0.5729 - val_acc: 0.7415\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5600 - acc: 0.7615 - val_loss: 0.5722 - val_acc: 0.7415\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5550 - acc: 0.7632 - val_loss: 0.5716 - val_acc: 0.7415\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7630 - val_loss: 0.5732 - val_acc: 0.7415\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5515 - acc: 0.7632 - val_loss: 0.5718 - val_acc: 0.7415\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7632 - val_loss: 0.5714 - val_acc: 0.7415\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7632 - val_loss: 0.5715 - val_acc: 0.7415\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5502 - acc: 0.7632 - val_loss: 0.5713 - val_acc: 0.7415\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5518 - acc: 0.7632 - val_loss: 0.5715 - val_acc: 0.7415\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5509 - acc: 0.7632 - val_loss: 0.5716 - val_acc: 0.7415\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7632 - val_loss: 0.5718 - val_acc: 0.7415\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7632 - val_loss: 0.5728 - val_acc: 0.7415\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7632 - val_loss: 0.5725 - val_acc: 0.7415\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7632 - val_loss: 0.5737 - val_acc: 0.7415\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7632 - val_loss: 0.5723 - val_acc: 0.7415\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7632 - val_loss: 0.5716 - val_acc: 0.7415\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5480 - acc: 0.7632 - val_loss: 0.5730 - val_acc: 0.7415\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5493 - acc: 0.7632 - val_loss: 0.5716 - val_acc: 0.7415\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7632 - val_loss: 0.5717 - val_acc: 0.7415\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "3597s - loss: 0.5486 - acc: 0.7632 - val_loss: 0.5724 - val_acc: 0.7415\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7633 - val_loss: 0.5716 - val_acc: 0.7415\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7633 - val_loss: 0.5711 - val_acc: 0.7415\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.57095 to 0.57035, saving model to best.model\n",
      "0s - loss: 0.5482 - acc: 0.7634 - val_loss: 0.5704 - val_acc: 0.7420\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7639 - val_loss: 0.5708 - val_acc: 0.7420\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5472 - acc: 0.7644 - val_loss: 0.5715 - val_acc: 0.7427\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.57035 to 0.56685, saving model to best.model\n",
      "0s - loss: 0.5456 - acc: 0.7657 - val_loss: 0.5669 - val_acc: 0.7434\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7666 - val_loss: 0.5693 - val_acc: 0.7444\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7671 - val_loss: 0.5723 - val_acc: 0.7444\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7676 - val_loss: 0.5703 - val_acc: 0.7444\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7669 - val_loss: 0.5676 - val_acc: 0.7444\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7683 - val_loss: 0.5687 - val_acc: 0.7454\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7680 - val_loss: 0.5681 - val_acc: 0.7451\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7686 - val_loss: 0.5683 - val_acc: 0.7456\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7687 - val_loss: 0.5674 - val_acc: 0.7464\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7695 - val_loss: 0.5674 - val_acc: 0.7464\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.56685 to 0.56549, saving model to best.model\n",
      "0s - loss: 0.5412 - acc: 0.7692 - val_loss: 0.5655 - val_acc: 0.7464\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7691 - val_loss: 0.5685 - val_acc: 0.7464\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7695 - val_loss: 0.5662 - val_acc: 0.7464\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.56549 to 0.56523, saving model to best.model\n",
      "0s - loss: 0.5424 - acc: 0.7691 - val_loss: 0.5652 - val_acc: 0.7464\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.56523 to 0.56481, saving model to best.model\n",
      "0s - loss: 0.5405 - acc: 0.7701 - val_loss: 0.5648 - val_acc: 0.7464\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7693 - val_loss: 0.5659 - val_acc: 0.7464\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7699 - val_loss: 0.5652 - val_acc: 0.7464\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.56481 to 0.56437, saving model to best.model\n",
      "0s - loss: 0.5401 - acc: 0.7699 - val_loss: 0.5644 - val_acc: 0.7464\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7701 - val_loss: 0.5646 - val_acc: 0.7464\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7694 - val_loss: 0.5660 - val_acc: 0.7464\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7699 - val_loss: 0.5649 - val_acc: 0.7464\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7699 - val_loss: 0.5646 - val_acc: 0.7464\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7702 - val_loss: 0.5652 - val_acc: 0.7464\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7696 - val_loss: 0.5658 - val_acc: 0.7464\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7701 - val_loss: 0.5650 - val_acc: 0.7464\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7698 - val_loss: 0.5668 - val_acc: 0.7464\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7694 - val_loss: 0.5651 - val_acc: 0.7464\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7699 - val_loss: 0.5651 - val_acc: 0.7464\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7698 - val_loss: 0.5671 - val_acc: 0.7464\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.56437 to 0.56418, saving model to best.model\n",
      "0s - loss: 0.5397 - acc: 0.7701 - val_loss: 0.5642 - val_acc: 0.7464\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7697 - val_loss: 0.5663 - val_acc: 0.7464\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5385 - acc: 0.7703 - val_loss: 0.5645 - val_acc: 0.7464\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7700 - val_loss: 0.5655 - val_acc: 0.7464\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7703 - val_loss: 0.5658 - val_acc: 0.7464\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7691 - val_loss: 0.5684 - val_acc: 0.7464\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7701 - val_loss: 0.5648 - val_acc: 0.7464\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7696 - val_loss: 0.5655 - val_acc: 0.7464\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7697 - val_loss: 0.5643 - val_acc: 0.7464\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7697 - val_loss: 0.5669 - val_acc: 0.7464\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7704 - val_loss: 0.5655 - val_acc: 0.7464\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7702 - val_loss: 0.5650 - val_acc: 0.7464\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7694 - val_loss: 0.5660 - val_acc: 0.7464\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7696 - val_loss: 0.5653 - val_acc: 0.7464\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7704 - val_loss: 0.5656 - val_acc: 0.7464\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5401 - acc: 0.7695 - val_loss: 0.5652 - val_acc: 0.7464\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7697 - val_loss: 0.5653 - val_acc: 0.7464\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7700 - val_loss: 0.5651 - val_acc: 0.7464\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7702 - val_loss: 0.5669 - val_acc: 0.7464\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7701 - val_loss: 0.5671 - val_acc: 0.7464\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7698 - val_loss: 0.5650 - val_acc: 0.7464\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5382 - acc: 0.7701 - val_loss: 0.5666 - val_acc: 0.7464\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7699 - val_loss: 0.5645 - val_acc: 0.7464\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7699 - val_loss: 0.5664 - val_acc: 0.7464\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.56418 to 0.56402, saving model to best.model\n",
      "0s - loss: 0.5399 - acc: 0.7702 - val_loss: 0.5640 - val_acc: 0.7464\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7703 - val_loss: 0.5643 - val_acc: 0.7464\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7697 - val_loss: 0.5655 - val_acc: 0.7464\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7701 - val_loss: 0.5662 - val_acc: 0.7464\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7702 - val_loss: 0.5656 - val_acc: 0.7464\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7699 - val_loss: 0.5646 - val_acc: 0.7464\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7702 - val_loss: 0.5646 - val_acc: 0.7464\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7698 - val_loss: 0.5648 - val_acc: 0.7464\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7694 - val_loss: 0.5648 - val_acc: 0.7464\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7697 - val_loss: 0.5656 - val_acc: 0.7464\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5384 - acc: 0.7700 - val_loss: 0.5656 - val_acc: 0.7464\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5384 - acc: 0.7704 - val_loss: 0.5648 - val_acc: 0.7464\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7698 - val_loss: 0.5648 - val_acc: 0.7464\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7698 - val_loss: 0.5648 - val_acc: 0.7464\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5377 - acc: 0.7704 - val_loss: 0.5651 - val_acc: 0.7464\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7702 - val_loss: 0.5676 - val_acc: 0.7464\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7695 - val_loss: 0.5646 - val_acc: 0.7464\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7697 - val_loss: 0.5656 - val_acc: 0.7464\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7701 - val_loss: 0.5653 - val_acc: 0.7464\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7700 - val_loss: 0.5667 - val_acc: 0.7464\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7699 - val_loss: 0.5660 - val_acc: 0.7464\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5381 - acc: 0.7703 - val_loss: 0.5659 - val_acc: 0.7464\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5387 - acc: 0.7702 - val_loss: 0.5657 - val_acc: 0.7464\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7694 - val_loss: 0.5653 - val_acc: 0.7464\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5383 - acc: 0.7702 - val_loss: 0.5658 - val_acc: 0.7464\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7694 - val_loss: 0.5654 - val_acc: 0.7464\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7700 - val_loss: 0.5654 - val_acc: 0.7464\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54366, saving model to best.model\n",
      "0s - loss: 0.6637 - acc: 0.7045 - val_loss: 0.5437 - val_acc: 0.7646\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5823 - acc: 0.7466 - val_loss: 0.5441 - val_acc: 0.7646\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5659 - acc: 0.7595 - val_loss: 0.5441 - val_acc: 0.7646\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5589 - acc: 0.7626 - val_loss: 0.5441 - val_acc: 0.7646\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5555 - acc: 0.7629 - val_loss: 0.5445 - val_acc: 0.7646\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5540 - acc: 0.7629 - val_loss: 0.5443 - val_acc: 0.7646\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7629 - val_loss: 0.5446 - val_acc: 0.7646\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7629 - val_loss: 0.5451 - val_acc: 0.7646\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7630 - val_loss: 0.5443 - val_acc: 0.7646\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5498 - acc: 0.7629 - val_loss: 0.5459 - val_acc: 0.7646\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5501 - acc: 0.7629 - val_loss: 0.5439 - val_acc: 0.7646\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.54366 to 0.54356, saving model to best.model\n",
      "0s - loss: 0.5494 - acc: 0.7630 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7631 - val_loss: 0.5446 - val_acc: 0.7646\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7631 - val_loss: 0.5447 - val_acc: 0.7646\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.54356 to 0.54355, saving model to best.model\n",
      "0s - loss: 0.5490 - acc: 0.7630 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5492 - acc: 0.7634 - val_loss: 0.5438 - val_acc: 0.7655\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.54355 to 0.54293, saving model to best.model\n",
      "0s - loss: 0.5490 - acc: 0.7636 - val_loss: 0.5429 - val_acc: 0.7655\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.54293 to 0.54218, saving model to best.model\n",
      "0s - loss: 0.5487 - acc: 0.7639 - val_loss: 0.5422 - val_acc: 0.7660\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.54218 to 0.54169, saving model to best.model\n",
      "0s - loss: 0.5486 - acc: 0.7643 - val_loss: 0.5417 - val_acc: 0.7665\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.54169 to 0.54016, saving model to best.model\n",
      "0s - loss: 0.5469 - acc: 0.7647 - val_loss: 0.5402 - val_acc: 0.7680\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.54016 to 0.53968, saving model to best.model\n",
      "0s - loss: 0.5468 - acc: 0.7658 - val_loss: 0.5397 - val_acc: 0.7680\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5468 - acc: 0.7654 - val_loss: 0.5404 - val_acc: 0.7680\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5463 - acc: 0.7657 - val_loss: 0.5408 - val_acc: 0.7680\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5455 - acc: 0.7666 - val_loss: 0.5399 - val_acc: 0.7682\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.53968 to 0.53897, saving model to best.model\n",
      "0s - loss: 0.5462 - acc: 0.7667 - val_loss: 0.5390 - val_acc: 0.7685\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.53897 to 0.53894, saving model to best.model\n",
      "0s - loss: 0.5452 - acc: 0.7672 - val_loss: 0.5389 - val_acc: 0.7685\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.53894 to 0.53820, saving model to best.model\n",
      "0s - loss: 0.5443 - acc: 0.7676 - val_loss: 0.5382 - val_acc: 0.7690\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.53820 to 0.53784, saving model to best.model\n",
      "0s - loss: 0.5432 - acc: 0.7681 - val_loss: 0.5378 - val_acc: 0.7690\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.53784 to 0.53772, saving model to best.model\n",
      "0s - loss: 0.5429 - acc: 0.7682 - val_loss: 0.5377 - val_acc: 0.7690\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.53772 to 0.53663, saving model to best.model\n",
      "0s - loss: 0.5431 - acc: 0.7683 - val_loss: 0.5366 - val_acc: 0.7690\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.53663 to 0.53579, saving model to best.model\n",
      "0s - loss: 0.5413 - acc: 0.7692 - val_loss: 0.5358 - val_acc: 0.7692\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.53579 to 0.53535, saving model to best.model\n",
      "0s - loss: 0.5416 - acc: 0.7696 - val_loss: 0.5354 - val_acc: 0.7726\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7695 - val_loss: 0.5356 - val_acc: 0.7692\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.53535 to 0.53493, saving model to best.model\n",
      "0s - loss: 0.5397 - acc: 0.7703 - val_loss: 0.5349 - val_acc: 0.7726\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7702 - val_loss: 0.5352 - val_acc: 0.7726\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.53493 to 0.53396, saving model to best.model\n",
      "0s - loss: 0.5409 - acc: 0.7693 - val_loss: 0.5340 - val_acc: 0.7726\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.53396 to 0.53333, saving model to best.model\n",
      "0s - loss: 0.5399 - acc: 0.7705 - val_loss: 0.5333 - val_acc: 0.7726\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7706 - val_loss: 0.5334 - val_acc: 0.7726\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5479 - acc: 0.7636 - val_loss: 0.5412 - val_acc: 0.7680\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7667 - val_loss: 0.5404 - val_acc: 0.7682\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5434 - acc: 0.7673 - val_loss: 0.5396 - val_acc: 0.7685\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7682 - val_loss: 0.5399 - val_acc: 0.7685\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7688 - val_loss: 0.5395 - val_acc: 0.7685\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7680 - val_loss: 0.5400 - val_acc: 0.7690\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7686 - val_loss: 0.5393 - val_acc: 0.7685\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7681 - val_loss: 0.5386 - val_acc: 0.7690\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7686 - val_loss: 0.5389 - val_acc: 0.7690\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7686 - val_loss: 0.5393 - val_acc: 0.7690\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7686 - val_loss: 0.5382 - val_acc: 0.7690\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7698 - val_loss: 0.5381 - val_acc: 0.7690\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7685 - val_loss: 0.5372 - val_acc: 0.7690\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7695 - val_loss: 0.5380 - val_acc: 0.7690\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7692 - val_loss: 0.5380 - val_acc: 0.7690\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7692 - val_loss: 0.5381 - val_acc: 0.7690\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7695 - val_loss: 0.5379 - val_acc: 0.7690\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7698 - val_loss: 0.5373 - val_acc: 0.7690\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7694 - val_loss: 0.5378 - val_acc: 0.7690\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7702 - val_loss: 0.5368 - val_acc: 0.7690\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7692 - val_loss: 0.5377 - val_acc: 0.7690\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7696 - val_loss: 0.5369 - val_acc: 0.7690\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7697 - val_loss: 0.5355 - val_acc: 0.7690\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7706 - val_loss: 0.5368 - val_acc: 0.7690\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7697 - val_loss: 0.5370 - val_acc: 0.7690\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55059, saving model to best.model\n",
      "0s - loss: 0.5981 - acc: 0.7382 - val_loss: 0.5506 - val_acc: 0.7597\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55059 to 0.55054, saving model to best.model\n",
      "0s - loss: 0.5625 - acc: 0.7601 - val_loss: 0.5505 - val_acc: 0.7597\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5569 - acc: 0.7610 - val_loss: 0.5507 - val_acc: 0.7597\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5556 - acc: 0.7611 - val_loss: 0.5510 - val_acc: 0.7597\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7611 - val_loss: 0.5506 - val_acc: 0.7597\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.55054 to 0.54981, saving model to best.model\n",
      "0s - loss: 0.5511 - acc: 0.7611 - val_loss: 0.5498 - val_acc: 0.7597\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.54981 to 0.54977, saving model to best.model\n",
      "0s - loss: 0.5517 - acc: 0.7611 - val_loss: 0.5498 - val_acc: 0.7597\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7611 - val_loss: 0.5503 - val_acc: 0.7597\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.54977 to 0.54914, saving model to best.model\n",
      "0s - loss: 0.5520 - acc: 0.7611 - val_loss: 0.5491 - val_acc: 0.7597\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.54914 to 0.54876, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7611 - val_loss: 0.5488 - val_acc: 0.7597\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.54876 to 0.54788, saving model to best.model\n",
      "0s - loss: 0.5499 - acc: 0.7612 - val_loss: 0.5479 - val_acc: 0.7597\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.54788 to 0.54552, saving model to best.model\n",
      "0s - loss: 0.5495 - acc: 0.7616 - val_loss: 0.5455 - val_acc: 0.7597\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5489 - acc: 0.7620 - val_loss: 0.5473 - val_acc: 0.7602\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.54552 to 0.54527, saving model to best.model\n",
      "0s - loss: 0.5495 - acc: 0.7626 - val_loss: 0.5453 - val_acc: 0.7621\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.54527 to 0.54479, saving model to best.model\n",
      "0s - loss: 0.5478 - acc: 0.7645 - val_loss: 0.5448 - val_acc: 0.7646\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5476 - acc: 0.7648 - val_loss: 0.5467 - val_acc: 0.7646\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5485 - acc: 0.7643 - val_loss: 0.5456 - val_acc: 0.7646\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7643 - val_loss: 0.5455 - val_acc: 0.7646\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.54479 to 0.54423, saving model to best.model\n",
      "0s - loss: 0.5469 - acc: 0.7654 - val_loss: 0.5442 - val_acc: 0.7648\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.54423 to 0.54367, saving model to best.model\n",
      "0s - loss: 0.5450 - acc: 0.7660 - val_loss: 0.5437 - val_acc: 0.7648\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5465 - acc: 0.7660 - val_loss: 0.5441 - val_acc: 0.7648\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.54367 to 0.54265, saving model to best.model\n",
      "0s - loss: 0.5456 - acc: 0.7663 - val_loss: 0.5427 - val_acc: 0.7675\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5455 - acc: 0.7662 - val_loss: 0.5435 - val_acc: 0.7675\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5453 - acc: 0.7669 - val_loss: 0.5445 - val_acc: 0.7675\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54265 to 0.54179, saving model to best.model\n",
      "0s - loss: 0.5459 - acc: 0.7668 - val_loss: 0.5418 - val_acc: 0.7675\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54179 to 0.54131, saving model to best.model\n",
      "0s - loss: 0.5442 - acc: 0.7667 - val_loss: 0.5413 - val_acc: 0.7675\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54131 to 0.54127, saving model to best.model\n",
      "0s - loss: 0.5443 - acc: 0.7671 - val_loss: 0.5413 - val_acc: 0.7675\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54127 to 0.54125, saving model to best.model\n",
      "0s - loss: 0.5441 - acc: 0.7669 - val_loss: 0.5413 - val_acc: 0.7675\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54125 to 0.54098, saving model to best.model\n",
      "0s - loss: 0.5434 - acc: 0.7670 - val_loss: 0.5410 - val_acc: 0.7675\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54098 to 0.54063, saving model to best.model\n",
      "0s - loss: 0.5422 - acc: 0.7673 - val_loss: 0.5406 - val_acc: 0.7675\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7674 - val_loss: 0.5419 - val_acc: 0.7675\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.5440 - acc: 0.7674 - val_loss: 0.5412 - val_acc: 0.7675\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7677 - val_loss: 0.5408 - val_acc: 0.7675\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.5443 - acc: 0.7672 - val_loss: 0.5409 - val_acc: 0.7675\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54063 to 0.53926, saving model to best.model\n",
      "0s - loss: 0.5422 - acc: 0.7676 - val_loss: 0.5393 - val_acc: 0.7675\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7656 - val_loss: 0.5430 - val_acc: 0.7675\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.5433 - acc: 0.7671 - val_loss: 0.5413 - val_acc: 0.7675\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7669 - val_loss: 0.5412 - val_acc: 0.7675\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7671 - val_loss: 0.5410 - val_acc: 0.7675\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7672 - val_loss: 0.5409 - val_acc: 0.7675\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7675 - val_loss: 0.5409 - val_acc: 0.7675\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7672 - val_loss: 0.5405 - val_acc: 0.7675\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7678 - val_loss: 0.5406 - val_acc: 0.7675\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7673 - val_loss: 0.5408 - val_acc: 0.7675\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7671 - val_loss: 0.5407 - val_acc: 0.7675\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7672 - val_loss: 0.5403 - val_acc: 0.7675\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7676 - val_loss: 0.5405 - val_acc: 0.7675\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7675 - val_loss: 0.5404 - val_acc: 0.7675\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7675 - val_loss: 0.5405 - val_acc: 0.7675\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7671 - val_loss: 0.5403 - val_acc: 0.7675\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7678 - val_loss: 0.5403 - val_acc: 0.7675\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7675 - val_loss: 0.5403 - val_acc: 0.7675\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7676 - val_loss: 0.5403 - val_acc: 0.7675\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7674 - val_loss: 0.5406 - val_acc: 0.7675\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7678 - val_loss: 0.5405 - val_acc: 0.7675\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7678 - val_loss: 0.5402 - val_acc: 0.7675\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7677 - val_loss: 0.5406 - val_acc: 0.7675\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7678 - val_loss: 0.5403 - val_acc: 0.7675\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7678 - val_loss: 0.5402 - val_acc: 0.7675\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7680 - val_loss: 0.5403 - val_acc: 0.7675\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7677 - val_loss: 0.5403 - val_acc: 0.7675\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55189, saving model to best.model\n",
      "0s - loss: 0.6583 - acc: 0.7025 - val_loss: 0.5519 - val_acc: 0.7590\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5894 - acc: 0.7452 - val_loss: 0.5533 - val_acc: 0.7590\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.55189 to 0.55148, saving model to best.model\n",
      "0s - loss: 0.5693 - acc: 0.7574 - val_loss: 0.5515 - val_acc: 0.7590\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.55148 to 0.55060, saving model to best.model\n",
      "0s - loss: 0.5617 - acc: 0.7609 - val_loss: 0.5506 - val_acc: 0.7590\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5552 - acc: 0.7609 - val_loss: 0.5506 - val_acc: 0.7590\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5550 - acc: 0.7610 - val_loss: 0.5519 - val_acc: 0.7590\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5551 - acc: 0.7610 - val_loss: 0.5521 - val_acc: 0.7590\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5547 - acc: 0.7610 - val_loss: 0.5529 - val_acc: 0.7590\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5520 - acc: 0.7610 - val_loss: 0.5515 - val_acc: 0.7590\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7610 - val_loss: 0.5515 - val_acc: 0.7590\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7610 - val_loss: 0.5513 - val_acc: 0.7590\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5527 - acc: 0.7610 - val_loss: 0.5507 - val_acc: 0.7590\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7610 - val_loss: 0.5512 - val_acc: 0.7590\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7610 - val_loss: 0.5511 - val_acc: 0.7590\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7610 - val_loss: 0.5514 - val_acc: 0.7590\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7610 - val_loss: 0.5512 - val_acc: 0.7590\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5526 - acc: 0.7610 - val_loss: 0.5511 - val_acc: 0.7590\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7610 - val_loss: 0.5510 - val_acc: 0.7590\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7611 - val_loss: 0.5508 - val_acc: 0.7590\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7611 - val_loss: 0.5516 - val_acc: 0.7590\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7612 - val_loss: 0.5514 - val_acc: 0.7590\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.55060 to 0.55001, saving model to best.model\n",
      "0s - loss: 0.5508 - acc: 0.7615 - val_loss: 0.5500 - val_acc: 0.7590\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.55001 to 0.54985, saving model to best.model\n",
      "0s - loss: 0.5501 - acc: 0.7618 - val_loss: 0.5498 - val_acc: 0.7592\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54985 to 0.54875, saving model to best.model\n",
      "0s - loss: 0.5492 - acc: 0.7623 - val_loss: 0.5487 - val_acc: 0.7600\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54875 to 0.54821, saving model to best.model\n",
      "0s - loss: 0.5498 - acc: 0.7633 - val_loss: 0.5482 - val_acc: 0.7614\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54821 to 0.54817, saving model to best.model\n",
      "0s - loss: 0.5488 - acc: 0.7644 - val_loss: 0.5482 - val_acc: 0.7619\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.54817 to 0.54725, saving model to best.model\n",
      "0s - loss: 0.5490 - acc: 0.7639 - val_loss: 0.5472 - val_acc: 0.7619\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54725 to 0.54674, saving model to best.model\n",
      "0s - loss: 0.5474 - acc: 0.7645 - val_loss: 0.5467 - val_acc: 0.7638\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54674 to 0.54638, saving model to best.model\n",
      "0s - loss: 0.5471 - acc: 0.7647 - val_loss: 0.5464 - val_acc: 0.7646\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.54638 to 0.54567, saving model to best.model\n",
      "0s - loss: 0.5472 - acc: 0.7650 - val_loss: 0.5457 - val_acc: 0.7646\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54567 to 0.54560, saving model to best.model\n",
      "0s - loss: 0.5467 - acc: 0.7655 - val_loss: 0.5456 - val_acc: 0.7646\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54560 to 0.54502, saving model to best.model\n",
      "0s - loss: 0.5458 - acc: 0.7657 - val_loss: 0.5450 - val_acc: 0.7646\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.54502 to 0.54497, saving model to best.model\n",
      "0s - loss: 0.5462 - acc: 0.7660 - val_loss: 0.5450 - val_acc: 0.7646\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54497 to 0.54462, saving model to best.model\n",
      "0s - loss: 0.5457 - acc: 0.7661 - val_loss: 0.5446 - val_acc: 0.7646\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.54462 to 0.54450, saving model to best.model\n",
      "0s - loss: 0.5448 - acc: 0.7660 - val_loss: 0.5445 - val_acc: 0.7646\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.54450 to 0.54443, saving model to best.model\n",
      "0s - loss: 0.5442 - acc: 0.7661 - val_loss: 0.5444 - val_acc: 0.7646\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54443 to 0.54421, saving model to best.model\n",
      "0s - loss: 0.5436 - acc: 0.7671 - val_loss: 0.5442 - val_acc: 0.7646\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54421 to 0.54412, saving model to best.model\n",
      "0s - loss: 0.5454 - acc: 0.7663 - val_loss: 0.5441 - val_acc: 0.7646\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7673 - val_loss: 0.5444 - val_acc: 0.7646\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7671 - val_loss: 0.5441 - val_acc: 0.7646\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7671 - val_loss: 0.5442 - val_acc: 0.7646\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.5437 - acc: 0.7670 - val_loss: 0.5441 - val_acc: 0.7646\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.54412 to 0.54399, saving model to best.model\n",
      "0s - loss: 0.5437 - acc: 0.7671 - val_loss: 0.5440 - val_acc: 0.7646\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7670 - val_loss: 0.5442 - val_acc: 0.7646\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7666 - val_loss: 0.5444 - val_acc: 0.7646\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7668 - val_loss: 0.5441 - val_acc: 0.7646\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.54399 to 0.54389, saving model to best.model\n",
      "0s - loss: 0.5437 - acc: 0.7674 - val_loss: 0.5439 - val_acc: 0.7646\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.54389 to 0.54389, saving model to best.model\n",
      "0s - loss: 0.5429 - acc: 0.7674 - val_loss: 0.5439 - val_acc: 0.7646\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.54389 to 0.54379, saving model to best.model\n",
      "0s - loss: 0.5426 - acc: 0.7677 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7674 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7678 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7669 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7677 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7677 - val_loss: 0.5439 - val_acc: 0.7646\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7672 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.54379 to 0.54376, saving model to best.model\n",
      "0s - loss: 0.5417 - acc: 0.7677 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.54376 to 0.54360, saving model to best.model\n",
      "0s - loss: 0.5423 - acc: 0.7672 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7671 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7674 - val_loss: 0.5445 - val_acc: 0.7646\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7673 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7673 - val_loss: 0.5440 - val_acc: 0.7646\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7674 - val_loss: 0.5445 - val_acc: 0.7646\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.54360 to 0.54358, saving model to best.model\n",
      "0s - loss: 0.5421 - acc: 0.7674 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7673 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7677 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7677 - val_loss: 0.5439 - val_acc: 0.7646\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7674 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7678 - val_loss: 0.5447 - val_acc: 0.7646\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7675 - val_loss: 0.5437 - val_acc: 0.7646\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7671 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7673 - val_loss: 0.5437 - val_acc: 0.7646\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7675 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7677 - val_loss: 0.5437 - val_acc: 0.7655\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.54358 to 0.54346, saving model to best.model\n",
      "0s - loss: 0.5414 - acc: 0.7676 - val_loss: 0.5435 - val_acc: 0.7655\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7677 - val_loss: 0.5438 - val_acc: 0.7655\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5425 - acc: 0.7673 - val_loss: 0.5439 - val_acc: 0.7646\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7675 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7677 - val_loss: 0.5435 - val_acc: 0.7646\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.54346 to 0.54334, saving model to best.model\n",
      "0s - loss: 0.5420 - acc: 0.7672 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7674 - val_loss: 0.5439 - val_acc: 0.7646\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7674 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7679 - val_loss: 0.5437 - val_acc: 0.7646\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.54334 to 0.54330, saving model to best.model\n",
      "0s - loss: 0.5415 - acc: 0.7674 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.54330 to 0.54315, saving model to best.model\n",
      "0s - loss: 0.5418 - acc: 0.7673 - val_loss: 0.5431 - val_acc: 0.7658\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7674 - val_loss: 0.5435 - val_acc: 0.7646\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7677 - val_loss: 0.5434 - val_acc: 0.7646\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7672 - val_loss: 0.5435 - val_acc: 0.7646\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.5426 - acc: 0.7668 - val_loss: 0.5434 - val_acc: 0.7646\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7675 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7677 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7674 - val_loss: 0.5434 - val_acc: 0.7646\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7675 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7672 - val_loss: 0.5435 - val_acc: 0.7646\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.5415 - acc: 0.7672 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7675 - val_loss: 0.5439 - val_acc: 0.7646\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7675 - val_loss: 0.5436 - val_acc: 0.7646\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.5417 - acc: 0.7673 - val_loss: 0.5435 - val_acc: 0.7655\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7678 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7675 - val_loss: 0.5435 - val_acc: 0.7655\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.54315 to 0.54313, saving model to best.model\n",
      "0s - loss: 0.5418 - acc: 0.7673 - val_loss: 0.5431 - val_acc: 0.7655\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.5419 - acc: 0.7672 - val_loss: 0.5432 - val_acc: 0.7655\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7677 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7675 - val_loss: 0.5435 - val_acc: 0.7646\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.5423 - acc: 0.7671 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.54313 to 0.54304, saving model to best.model\n",
      "0s - loss: 0.5408 - acc: 0.7676 - val_loss: 0.5430 - val_acc: 0.7658\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7677 - val_loss: 0.5432 - val_acc: 0.7655\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7673 - val_loss: 0.5431 - val_acc: 0.7658\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7671 - val_loss: 0.5437 - val_acc: 0.7646\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7675 - val_loss: 0.5434 - val_acc: 0.7646\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7671 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7674 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7676 - val_loss: 0.5431 - val_acc: 0.7655\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7675 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7678 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.54304 to 0.54288, saving model to best.model\n",
      "0s - loss: 0.5400 - acc: 0.7681 - val_loss: 0.5429 - val_acc: 0.7658\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.54288 to 0.54288, saving model to best.model\n",
      "0s - loss: 0.5402 - acc: 0.7678 - val_loss: 0.5429 - val_acc: 0.7658\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7675 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7675 - val_loss: 0.5429 - val_acc: 0.7658\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7673 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7675 - val_loss: 0.5438 - val_acc: 0.7646\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7678 - val_loss: 0.5437 - val_acc: 0.7646\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7673 - val_loss: 0.5434 - val_acc: 0.7655\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7671 - val_loss: 0.5435 - val_acc: 0.7646\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7672 - val_loss: 0.5435 - val_acc: 0.7646\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7677 - val_loss: 0.5432 - val_acc: 0.7655\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7677 - val_loss: 0.5432 - val_acc: 0.7655\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7676 - val_loss: 0.5431 - val_acc: 0.7655\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.54288 to 0.54283, saving model to best.model\n",
      "0s - loss: 0.5410 - acc: 0.7674 - val_loss: 0.5428 - val_acc: 0.7658\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7674 - val_loss: 0.5429 - val_acc: 0.7658\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7674 - val_loss: 0.5429 - val_acc: 0.7658\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7675 - val_loss: 0.5430 - val_acc: 0.7658\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7673 - val_loss: 0.5432 - val_acc: 0.7655\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7673 - val_loss: 0.5431 - val_acc: 0.7655\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7672 - val_loss: 0.5429 - val_acc: 0.7658\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.54283 to 0.54260, saving model to best.model\n",
      "0s - loss: 0.5407 - acc: 0.7677 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7675 - val_loss: 0.5431 - val_acc: 0.7655\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.5410 - acc: 0.7675 - val_loss: 0.5428 - val_acc: 0.7658\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7671 - val_loss: 0.5427 - val_acc: 0.7658\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.5409 - acc: 0.7673 - val_loss: 0.5429 - val_acc: 0.7658\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.54260 to 0.54259, saving model to best.model\n",
      "0s - loss: 0.5409 - acc: 0.7675 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.5414 - acc: 0.7671 - val_loss: 0.5430 - val_acc: 0.7658\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7680 - val_loss: 0.5428 - val_acc: 0.7658\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.5408 - acc: 0.7676 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.5412 - acc: 0.7673 - val_loss: 0.5430 - val_acc: 0.7658\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.54259 to 0.54255, saving model to best.model\n",
      "0s - loss: 0.5417 - acc: 0.7670 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7674 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.54255 to 0.54250, saving model to best.model\n",
      "0s - loss: 0.5407 - acc: 0.7675 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7675 - val_loss: 0.5427 - val_acc: 0.7658\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7677 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7678 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7678 - val_loss: 0.5427 - val_acc: 0.7658\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.5411 - acc: 0.7674 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7678 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7677 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7683 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.5405 - acc: 0.7678 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7681 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.5406 - acc: 0.7678 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7684 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7681 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.54250 to 0.54250, saving model to best.model\n",
      "0s - loss: 0.5399 - acc: 0.7683 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.54250 to 0.54245, saving model to best.model\n",
      "0s - loss: 0.5395 - acc: 0.7684 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7686 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7685 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7689 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7684 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.54245 to 0.54238, saving model to best.model\n",
      "0s - loss: 0.5398 - acc: 0.7685 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7683 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.54238 to 0.54234, saving model to best.model\n",
      "0s - loss: 0.5396 - acc: 0.7685 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7690 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7684 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7683 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7685 - val_loss: 0.5426 - val_acc: 0.7658\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7686 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7685 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.5397 - acc: 0.7684 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.5400 - acc: 0.7683 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7691 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.54234 to 0.54233, saving model to best.model\n",
      "0s - loss: 0.5394 - acc: 0.7685 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.54233 to 0.54231, saving model to best.model\n",
      "0s - loss: 0.5398 - acc: 0.7683 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7688 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7682 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7688 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.5390 - acc: 0.7688 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.54231 to 0.54231, saving model to best.model\n",
      "0s - loss: 0.5396 - acc: 0.7685 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.54231 to 0.54222, saving model to best.model\n",
      "0s - loss: 0.5386 - acc: 0.7690 - val_loss: 0.5422 - val_acc: 0.7658\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.5386 - acc: 0.7692 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7688 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.5396 - acc: 0.7685 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.5394 - acc: 0.7685 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7688 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7687 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7688 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.5398 - acc: 0.7683 - val_loss: 0.5425 - val_acc: 0.7658\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.5393 - acc: 0.7687 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.5389 - acc: 0.7689 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7688 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.5392 - acc: 0.7688 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.5399 - acc: 0.7683 - val_loss: 0.5424 - val_acc: 0.7658\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.5395 - acc: 0.7688 - val_loss: 0.5423 - val_acc: 0.7658\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54577, saving model to best.model\n",
      "0s - loss: 0.5962 - acc: 0.7375 - val_loss: 0.5458 - val_acc: 0.7631\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.5617 - acc: 0.7603 - val_loss: 0.5469 - val_acc: 0.7631\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5590 - acc: 0.7618 - val_loss: 0.5468 - val_acc: 0.7631\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5542 - acc: 0.7619 - val_loss: 0.5499 - val_acc: 0.7631\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5535 - acc: 0.7619 - val_loss: 0.5467 - val_acc: 0.7631\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5524 - acc: 0.7619 - val_loss: 0.5481 - val_acc: 0.7631\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7619 - val_loss: 0.5497 - val_acc: 0.7631\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5530 - acc: 0.7619 - val_loss: 0.5474 - val_acc: 0.7631\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7619 - val_loss: 0.5486 - val_acc: 0.7631\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5525 - acc: 0.7619 - val_loss: 0.5475 - val_acc: 0.7631\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5516 - acc: 0.7619 - val_loss: 0.5474 - val_acc: 0.7631\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5504 - acc: 0.7619 - val_loss: 0.5472 - val_acc: 0.7631\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7619 - val_loss: 0.5461 - val_acc: 0.7631\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7619 - val_loss: 0.5476 - val_acc: 0.7631\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5508 - acc: 0.7619 - val_loss: 0.5473 - val_acc: 0.7631\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5513 - acc: 0.7619 - val_loss: 0.5467 - val_acc: 0.7631\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7619 - val_loss: 0.5467 - val_acc: 0.7631\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7619 - val_loss: 0.5479 - val_acc: 0.7631\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7619 - val_loss: 0.5473 - val_acc: 0.7631\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5506 - acc: 0.7619 - val_loss: 0.5465 - val_acc: 0.7631\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5502 - acc: 0.7619 - val_loss: 0.5464 - val_acc: 0.7631\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5499 - acc: 0.7619 - val_loss: 0.5464 - val_acc: 0.7631\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7619 - val_loss: 0.5463 - val_acc: 0.7631\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7619 - val_loss: 0.5462 - val_acc: 0.7631\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5487 - acc: 0.7619 - val_loss: 0.5471 - val_acc: 0.7631\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7619 - val_loss: 0.5467 - val_acc: 0.7631\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7619 - val_loss: 0.5459 - val_acc: 0.7631\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.54986, saving model to best.model\n",
      "0s - loss: 0.6044 - acc: 0.7377 - val_loss: 0.5499 - val_acc: 0.7617\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.54986 to 0.54871, saving model to best.model\n",
      "0s - loss: 0.5636 - acc: 0.7635 - val_loss: 0.5487 - val_acc: 0.7617\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.54871 to 0.54859, saving model to best.model\n",
      "0s - loss: 0.5541 - acc: 0.7668 - val_loss: 0.5486 - val_acc: 0.7617\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.54859 to 0.54848, saving model to best.model\n",
      "0s - loss: 0.5507 - acc: 0.7671 - val_loss: 0.5485 - val_acc: 0.7617\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.54848 to 0.54787, saving model to best.model\n",
      "0s - loss: 0.5480 - acc: 0.7671 - val_loss: 0.5479 - val_acc: 0.7617\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.54787 to 0.54751, saving model to best.model\n",
      "0s - loss: 0.5464 - acc: 0.7671 - val_loss: 0.5475 - val_acc: 0.7617\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5463 - acc: 0.7671 - val_loss: 0.5496 - val_acc: 0.7617\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5466 - acc: 0.7671 - val_loss: 0.5488 - val_acc: 0.7617\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5458 - acc: 0.7671 - val_loss: 0.5487 - val_acc: 0.7617\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5462 - acc: 0.7671 - val_loss: 0.5488 - val_acc: 0.7617\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5467 - acc: 0.7671 - val_loss: 0.5483 - val_acc: 0.7617\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5464 - acc: 0.7671 - val_loss: 0.5483 - val_acc: 0.7617\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5453 - acc: 0.7671 - val_loss: 0.5482 - val_acc: 0.7617\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5450 - acc: 0.7671 - val_loss: 0.5481 - val_acc: 0.7617\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5445 - acc: 0.7671 - val_loss: 0.5481 - val_acc: 0.7617\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5443 - acc: 0.7671 - val_loss: 0.5480 - val_acc: 0.7617\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5444 - acc: 0.7671 - val_loss: 0.5479 - val_acc: 0.7617\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7671 - val_loss: 0.5479 - val_acc: 0.7617\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7671 - val_loss: 0.5477 - val_acc: 0.7617\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5446 - acc: 0.7671 - val_loss: 0.5476 - val_acc: 0.7617\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5439 - acc: 0.7671 - val_loss: 0.5479 - val_acc: 0.7617\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.54751 to 0.54750, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7671 - val_loss: 0.5475 - val_acc: 0.7617\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.54750 to 0.54698, saving model to best.model\n",
      "0s - loss: 0.5445 - acc: 0.7671 - val_loss: 0.5470 - val_acc: 0.7617\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.54698 to 0.54695, saving model to best.model\n",
      "0s - loss: 0.5438 - acc: 0.7671 - val_loss: 0.5470 - val_acc: 0.7617\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.54695 to 0.54669, saving model to best.model\n",
      "0s - loss: 0.5434 - acc: 0.7671 - val_loss: 0.5467 - val_acc: 0.7617\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.54669 to 0.54657, saving model to best.model\n",
      "0s - loss: 0.5438 - acc: 0.7671 - val_loss: 0.5466 - val_acc: 0.7617\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7671 - val_loss: 0.5470 - val_acc: 0.7617\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.54657 to 0.54619, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7671 - val_loss: 0.5462 - val_acc: 0.7617\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.54619 to 0.54597, saving model to best.model\n",
      "0s - loss: 0.5420 - acc: 0.7671 - val_loss: 0.5460 - val_acc: 0.7617\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7671 - val_loss: 0.5461 - val_acc: 0.7617\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7671 - val_loss: 0.5469 - val_acc: 0.7617\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.54597 to 0.54549, saving model to best.model\n",
      "0s - loss: 0.5421 - acc: 0.7675 - val_loss: 0.5455 - val_acc: 0.7621\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.5418 - acc: 0.7677 - val_loss: 0.5460 - val_acc: 0.7624\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.54549 to 0.54405, saving model to best.model\n",
      "0s - loss: 0.5414 - acc: 0.7684 - val_loss: 0.5440 - val_acc: 0.7643\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.5421 - acc: 0.7690 - val_loss: 0.5443 - val_acc: 0.7653\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.54405 to 0.54282, saving model to best.model\n",
      "0s - loss: 0.5405 - acc: 0.7703 - val_loss: 0.5428 - val_acc: 0.7660\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.54282 to 0.54180, saving model to best.model\n",
      "0s - loss: 0.5404 - acc: 0.7712 - val_loss: 0.5418 - val_acc: 0.7687\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.54180 to 0.54151, saving model to best.model\n",
      "0s - loss: 0.5387 - acc: 0.7719 - val_loss: 0.5415 - val_acc: 0.7687\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.54151 to 0.54094, saving model to best.model\n",
      "0s - loss: 0.5394 - acc: 0.7715 - val_loss: 0.5409 - val_acc: 0.7687\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.54094 to 0.54077, saving model to best.model\n",
      "0s - loss: 0.5378 - acc: 0.7721 - val_loss: 0.5408 - val_acc: 0.7687\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.54077 to 0.54063, saving model to best.model\n",
      "0s - loss: 0.5384 - acc: 0.7722 - val_loss: 0.5406 - val_acc: 0.7687\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.54063 to 0.53951, saving model to best.model\n",
      "0s - loss: 0.5379 - acc: 0.7726 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.53951 to 0.53944, saving model to best.model\n",
      "0s - loss: 0.5382 - acc: 0.7726 - val_loss: 0.5394 - val_acc: 0.7687\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.53944 to 0.53912, saving model to best.model\n",
      "0s - loss: 0.5370 - acc: 0.7732 - val_loss: 0.5391 - val_acc: 0.7687\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.5373 - acc: 0.7728 - val_loss: 0.5397 - val_acc: 0.7687\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.53912 to 0.53899, saving model to best.model\n",
      "0s - loss: 0.5376 - acc: 0.7728 - val_loss: 0.5390 - val_acc: 0.7687\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.5365 - acc: 0.7736 - val_loss: 0.5398 - val_acc: 0.7687\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.53899 to 0.53862, saving model to best.model\n",
      "0s - loss: 0.5365 - acc: 0.7730 - val_loss: 0.5386 - val_acc: 0.7687\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.5369 - acc: 0.7731 - val_loss: 0.5388 - val_acc: 0.7687\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.5362 - acc: 0.7727 - val_loss: 0.5395 - val_acc: 0.7687\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.53862 to 0.53854, saving model to best.model\n",
      "0s - loss: 0.5362 - acc: 0.7733 - val_loss: 0.5385 - val_acc: 0.7687\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.5360 - acc: 0.7729 - val_loss: 0.5390 - val_acc: 0.7687\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.53854 to 0.52965, saving model to best.model\n",
      "0s - loss: 0.5300 - acc: 0.7784 - val_loss: 0.5296 - val_acc: 0.7692\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.5749 - acc: 0.7548 - val_loss: 0.5488 - val_acc: 0.7617\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.5442 - acc: 0.7672 - val_loss: 0.5488 - val_acc: 0.7617\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7675 - val_loss: 0.5486 - val_acc: 0.7617\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.5438 - acc: 0.7672 - val_loss: 0.5487 - val_acc: 0.7617\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.5427 - acc: 0.7672 - val_loss: 0.5482 - val_acc: 0.7617\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.5430 - acc: 0.7672 - val_loss: 0.5482 - val_acc: 0.7621\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.5436 - acc: 0.7674 - val_loss: 0.5482 - val_acc: 0.7621\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7674 - val_loss: 0.5479 - val_acc: 0.7621\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.5424 - acc: 0.7674 - val_loss: 0.5480 - val_acc: 0.7621\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7677 - val_loss: 0.5479 - val_acc: 0.7621\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.5431 - acc: 0.7677 - val_loss: 0.5481 - val_acc: 0.7624\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.5429 - acc: 0.7675 - val_loss: 0.5477 - val_acc: 0.7624\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.5432 - acc: 0.7677 - val_loss: 0.5476 - val_acc: 0.7624\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.5428 - acc: 0.7676 - val_loss: 0.5478 - val_acc: 0.7624\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.5422 - acc: 0.7679 - val_loss: 0.5479 - val_acc: 0.7624\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.5420 - acc: 0.7681 - val_loss: 0.5476 - val_acc: 0.7624\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7681 - val_loss: 0.5465 - val_acc: 0.7624\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.5416 - acc: 0.7687 - val_loss: 0.5462 - val_acc: 0.7641\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7691 - val_loss: 0.5457 - val_acc: 0.7641\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7689 - val_loss: 0.5455 - val_acc: 0.7641\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.5413 - acc: 0.7689 - val_loss: 0.5449 - val_acc: 0.7653\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.5402 - acc: 0.7693 - val_loss: 0.5445 - val_acc: 0.7653\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.5403 - acc: 0.7698 - val_loss: 0.5443 - val_acc: 0.7653\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.5404 - acc: 0.7695 - val_loss: 0.5441 - val_acc: 0.7653\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.5407 - acc: 0.7695 - val_loss: 0.5446 - val_acc: 0.7653\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.5391 - acc: 0.7699 - val_loss: 0.5437 - val_acc: 0.7653\n",
      "Train on 16463 samples, validate on 4116 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55154, saving model to best.model\n",
      "0s - loss: 0.6550 - acc: 0.7076 - val_loss: 0.5515 - val_acc: 0.7600\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55154 to 0.54866, saving model to best.model\n",
      "0s - loss: 0.5792 - acc: 0.7516 - val_loss: 0.5487 - val_acc: 0.7600\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.5637 - acc: 0.7614 - val_loss: 0.5520 - val_acc: 0.7600\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.5573 - acc: 0.7629 - val_loss: 0.5509 - val_acc: 0.7600\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.5558 - acc: 0.7629 - val_loss: 0.5511 - val_acc: 0.7600\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.5544 - acc: 0.7627 - val_loss: 0.5510 - val_acc: 0.7600\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.5521 - acc: 0.7627 - val_loss: 0.5511 - val_acc: 0.7600\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.5517 - acc: 0.7627 - val_loss: 0.5510 - val_acc: 0.7600\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.5529 - acc: 0.7627 - val_loss: 0.5515 - val_acc: 0.7600\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.5523 - acc: 0.7627 - val_loss: 0.5510 - val_acc: 0.7600\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.5512 - acc: 0.7627 - val_loss: 0.5529 - val_acc: 0.7600\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.5510 - acc: 0.7627 - val_loss: 0.5512 - val_acc: 0.7600\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7627 - val_loss: 0.5508 - val_acc: 0.7600\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.5507 - acc: 0.7627 - val_loss: 0.5517 - val_acc: 0.7600\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.5501 - acc: 0.7627 - val_loss: 0.5512 - val_acc: 0.7600\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.5511 - acc: 0.7627 - val_loss: 0.5507 - val_acc: 0.7600\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.5503 - acc: 0.7627 - val_loss: 0.5513 - val_acc: 0.7600\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7627 - val_loss: 0.5507 - val_acc: 0.7600\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7627 - val_loss: 0.5511 - val_acc: 0.7600\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7627 - val_loss: 0.5507 - val_acc: 0.7600\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.5505 - acc: 0.7627 - val_loss: 0.5509 - val_acc: 0.7600\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.5490 - acc: 0.7627 - val_loss: 0.5506 - val_acc: 0.7600\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.5496 - acc: 0.7628 - val_loss: 0.5510 - val_acc: 0.7600\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.5497 - acc: 0.7627 - val_loss: 0.5506 - val_acc: 0.7600\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.5488 - acc: 0.7630 - val_loss: 0.5507 - val_acc: 0.7600\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.5495 - acc: 0.7630 - val_loss: 0.5507 - val_acc: 0.7600\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.5481 - acc: 0.7630 - val_loss: 0.5502 - val_acc: 0.7600\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.5477 - acc: 0.7635 - val_loss: 0.5502 - val_acc: 0.7602\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for i in range(50):\n",
    "    y_pred=train_nn_simple(train,X_val,y_val)\n",
    "    result.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_new=np.array(result)\n",
    "result_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_new1=result_new.sum(axis=0)\n",
    "result_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 38,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 38,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 40,\n",
       " 0,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 39,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 38,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 40,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 37,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 38,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 40,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re=result_new1.tolist()\n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for each in re:\n",
    "    if each>=1:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6114   91]\n",
      " [1562  374]]\n",
      "79.6953691193\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.99      0.88      6205\n",
      "          1       0.80      0.19      0.31      1936\n",
      "\n",
      "avg / total       0.80      0.80      0.75      8141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(y_val, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_val, y_pred) * 100) \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
