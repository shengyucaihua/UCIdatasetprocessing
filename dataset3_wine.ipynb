{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/Users/bruce/Desktop/datasets/wine/wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>178.0</td>\n",
       "      <td>1.938202</td>\n",
       "      <td>0.775035</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alcohol</th>\n",
       "      <td>178.0</td>\n",
       "      <td>13.000618</td>\n",
       "      <td>0.811827</td>\n",
       "      <td>11.03</td>\n",
       "      <td>12.3625</td>\n",
       "      <td>13.050</td>\n",
       "      <td>13.6775</td>\n",
       "      <td>14.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Malic acid</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.6025</td>\n",
       "      <td>1.865</td>\n",
       "      <td>3.0825</td>\n",
       "      <td>5.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ash</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.2100</td>\n",
       "      <td>2.360</td>\n",
       "      <td>2.5575</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <td>178.0</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>10.60</td>\n",
       "      <td>17.2000</td>\n",
       "      <td>19.500</td>\n",
       "      <td>21.5000</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Magnesium</th>\n",
       "      <td>178.0</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>70.00</td>\n",
       "      <td>88.0000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>107.0000</td>\n",
       "      <td>162.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total phenols</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.7425</td>\n",
       "      <td>2.355</td>\n",
       "      <td>2.8000</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flavanoids</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.2050</td>\n",
       "      <td>2.135</td>\n",
       "      <td>2.8750</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <td>178.0</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <td>178.0</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>1.555</td>\n",
       "      <td>1.9500</td>\n",
       "      <td>3.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Color intensity</th>\n",
       "      <td>178.0</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.2200</td>\n",
       "      <td>4.690</td>\n",
       "      <td>6.2000</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hue</th>\n",
       "      <td>178.0</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.1200</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.9375</td>\n",
       "      <td>2.780</td>\n",
       "      <td>3.1700</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Proline</th>\n",
       "      <td>178.0</td>\n",
       "      <td>746.893258</td>\n",
       "      <td>314.907474</td>\n",
       "      <td>278.00</td>\n",
       "      <td>500.5000</td>\n",
       "      <td>673.500</td>\n",
       "      <td>985.0000</td>\n",
       "      <td>1680.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              count        mean         std     min       25%  \\\n",
       "class                         178.0    1.938202    0.775035    1.00    1.0000   \n",
       "Alcohol                       178.0   13.000618    0.811827   11.03   12.3625   \n",
       "Malic acid                    178.0    2.336348    1.117146    0.74    1.6025   \n",
       "Ash                           178.0    2.366517    0.274344    1.36    2.2100   \n",
       "Alcalinity of ash             178.0   19.494944    3.339564   10.60   17.2000   \n",
       "Magnesium                     178.0   99.741573   14.282484   70.00   88.0000   \n",
       "Total phenols                 178.0    2.295112    0.625851    0.98    1.7425   \n",
       "Flavanoids                    178.0    2.029270    0.998859    0.34    1.2050   \n",
       "Nonflavanoid phenols          178.0    0.361854    0.124453    0.13    0.2700   \n",
       "Proanthocyanins               178.0    1.590899    0.572359    0.41    1.2500   \n",
       "Color intensity               178.0    5.058090    2.318286    1.28    3.2200   \n",
       "Hue                           178.0    0.957449    0.228572    0.48    0.7825   \n",
       "OD280/OD315 of diluted wines  178.0    2.611685    0.709990    1.27    1.9375   \n",
       "Proline                       178.0  746.893258  314.907474  278.00  500.5000   \n",
       "\n",
       "                                  50%       75%      max  \n",
       "class                           2.000    3.0000     3.00  \n",
       "Alcohol                        13.050   13.6775    14.83  \n",
       "Malic acid                      1.865    3.0825     5.80  \n",
       "Ash                             2.360    2.5575     3.23  \n",
       "Alcalinity of ash              19.500   21.5000    30.00  \n",
       "Magnesium                      98.000  107.0000   162.00  \n",
       "Total phenols                   2.355    2.8000     3.88  \n",
       "Flavanoids                      2.135    2.8750     5.08  \n",
       "Nonflavanoid phenols            0.340    0.4375     0.66  \n",
       "Proanthocyanins                 1.555    1.9500     3.58  \n",
       "Color intensity                 4.690    6.2000    13.00  \n",
       "Hue                             0.965    1.1200     1.71  \n",
       "OD280/OD315 of diluted wines    2.780    3.1700     4.00  \n",
       "Proline                       673.500  985.0000  1680.00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAHfCAYAAACf7ObVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAFpFAAQAAWEQCBQAAYNH/A3PHUABvdKGZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118168450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.hist(figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  Total phenols  \\\n",
       "0    14.23        1.71  2.43               15.6        127           2.80   \n",
       "1    13.20        1.78  2.14               11.2        100           2.65   \n",
       "2    13.16        2.36  2.67               18.6        101           2.80   \n",
       "3    14.37        1.95  2.50               16.8        113           3.85   \n",
       "4    13.24        2.59  2.87               21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid phenols  Proanthocyanins  Color intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280/OD315 of diluted wines  Proline  \n",
       "0                          3.92     1065  \n",
       "1                          3.40     1050  \n",
       "2                          3.17     1185  \n",
       "3                          3.45     1480  \n",
       "4                          2.93      735  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=train.drop(\"class\",axis=1)\n",
    "outcomes=train[\"class\"].values\n",
    "features.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "for i in features.columns: \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    train[i] = scaler.fit_transform(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.481555</td>\n",
       "      <td>-0.517367</td>\n",
       "      <td>0.305159</td>\n",
       "      <td>-1.289707</td>\n",
       "      <td>0.860705</td>\n",
       "      <td>1.562093</td>\n",
       "      <td>1.366128</td>\n",
       "      <td>-0.176095</td>\n",
       "      <td>0.664217</td>\n",
       "      <td>0.731870</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>0.336606</td>\n",
       "      <td>2.239039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1.716255</td>\n",
       "      <td>-0.418624</td>\n",
       "      <td>0.305159</td>\n",
       "      <td>-1.469878</td>\n",
       "      <td>-0.262708</td>\n",
       "      <td>0.328298</td>\n",
       "      <td>0.492677</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>0.681738</td>\n",
       "      <td>0.083015</td>\n",
       "      <td>0.274431</td>\n",
       "      <td>1.367689</td>\n",
       "      <td>1.729520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.308617</td>\n",
       "      <td>-0.167278</td>\n",
       "      <td>0.890014</td>\n",
       "      <td>-0.569023</td>\n",
       "      <td>1.492625</td>\n",
       "      <td>0.488531</td>\n",
       "      <td>0.482637</td>\n",
       "      <td>-0.417829</td>\n",
       "      <td>-0.597284</td>\n",
       "      <td>-0.003499</td>\n",
       "      <td>0.449924</td>\n",
       "      <td>1.367689</td>\n",
       "      <td>1.745442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2.259772</td>\n",
       "      <td>-0.625086</td>\n",
       "      <td>-0.718336</td>\n",
       "      <td>-1.650049</td>\n",
       "      <td>-0.192495</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.954502</td>\n",
       "      <td>-0.578985</td>\n",
       "      <td>0.681738</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>0.537671</td>\n",
       "      <td>0.336606</td>\n",
       "      <td>0.949319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1.061565</td>\n",
       "      <td>-0.885409</td>\n",
       "      <td>-0.352802</td>\n",
       "      <td>-1.049479</td>\n",
       "      <td>-0.122282</td>\n",
       "      <td>1.097417</td>\n",
       "      <td>1.125176</td>\n",
       "      <td>-1.143031</td>\n",
       "      <td>0.453967</td>\n",
       "      <td>0.935177</td>\n",
       "      <td>0.230557</td>\n",
       "      <td>1.325316</td>\n",
       "      <td>0.949319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class   Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
       "1      1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "2      1  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
       "3      1  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "4      1  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "5      1  1.481555   -0.517367  0.305159          -1.289707   0.860705   \n",
       "6      1  1.716255   -0.418624  0.305159          -1.469878  -0.262708   \n",
       "7      1  1.308617   -0.167278  0.890014          -0.569023   1.492625   \n",
       "8      1  2.259772   -0.625086 -0.718336          -1.650049  -0.192495   \n",
       "9      1  1.061565   -0.885409 -0.352802          -1.049479  -0.122282   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0       0.808997    1.034819             -0.659563         1.224884   \n",
       "1       0.568648    0.733629             -0.820719        -0.544721   \n",
       "2       0.808997    1.215533             -0.498407         2.135968   \n",
       "3       2.491446    1.466525             -0.981875         1.032155   \n",
       "4       0.808997    0.663351              0.226796         0.401404   \n",
       "5       1.562093    1.366128             -0.176095         0.664217   \n",
       "6       0.328298    0.492677             -0.498407         0.681738   \n",
       "7       0.488531    0.482637             -0.417829        -0.597284   \n",
       "8       0.808997    0.954502             -0.578985         0.681738   \n",
       "9       1.097417    1.125176             -1.143031         0.453967   \n",
       "\n",
       "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \n",
       "0         0.251717  0.362177                      1.847920  1.013009  \n",
       "1        -0.293321  0.406051                      1.113449  0.965242  \n",
       "2         0.269020  0.318304                      0.788587  1.395148  \n",
       "3         1.186068 -0.427544                      1.184071  2.334574  \n",
       "4        -0.319276  0.362177                      0.449601 -0.037874  \n",
       "5         0.731870  0.406051                      0.336606  2.239039  \n",
       "6         0.083015  0.274431                      1.367689  1.729520  \n",
       "7        -0.003499  0.449924                      1.367689  1.745442  \n",
       "8         0.061386  0.537671                      0.336606  0.949319  \n",
       "9         0.935177  0.230557                      1.325316  0.949319  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, outcomes, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Activation, Dropout,Input\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "# m.add(Dropout(0.5))\n",
    "m.add(Dense(12, activation='sigmoid'))\n",
    "# m.add(Dropout(0.5))\n",
    "# m.add(Dense(128, activation='sigmoid'))\n",
    "# m.add(Dropout(0.5))\n",
    "m.add(Dense(len(np.unique(Y_train)), activation='softmax'))\n",
    "    \n",
    "m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119 samples, validate on 14 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.18073, saving model to best.model\n",
      "0s - loss: 1.2128 - acc: 0.3445 - val_loss: 1.1807 - val_acc: 0.3571\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.18073 to 1.16522, saving model to best.model\n",
      "0s - loss: 1.1955 - acc: 0.3445 - val_loss: 1.1652 - val_acc: 0.3571\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.16522 to 1.15093, saving model to best.model\n",
      "0s - loss: 1.1794 - acc: 0.3445 - val_loss: 1.1509 - val_acc: 0.3571\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.15093 to 1.13774, saving model to best.model\n",
      "0s - loss: 1.1646 - acc: 0.3445 - val_loss: 1.1377 - val_acc: 0.3571\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.13774 to 1.12564, saving model to best.model\n",
      "0s - loss: 1.1509 - acc: 0.3445 - val_loss: 1.1256 - val_acc: 0.3571\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.12564 to 1.11468, saving model to best.model\n",
      "0s - loss: 1.1383 - acc: 0.3445 - val_loss: 1.1147 - val_acc: 0.3571\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.11468 to 1.10486, saving model to best.model\n",
      "0s - loss: 1.1265 - acc: 0.3445 - val_loss: 1.1049 - val_acc: 0.4286\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.10486 to 1.09620, saving model to best.model\n",
      "0s - loss: 1.1157 - acc: 0.4202 - val_loss: 1.0962 - val_acc: 0.5714\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.09620 to 1.08862, saving model to best.model\n",
      "0s - loss: 1.1057 - acc: 0.5042 - val_loss: 1.0886 - val_acc: 0.6429\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.08862 to 1.08207, saving model to best.model\n",
      "0s - loss: 1.0965 - acc: 0.5378 - val_loss: 1.0821 - val_acc: 0.6429\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.08207 to 1.07668, saving model to best.model\n",
      "0s - loss: 1.0882 - acc: 0.5882 - val_loss: 1.0767 - val_acc: 0.6429\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.07668 to 1.07302, saving model to best.model\n",
      "0s - loss: 1.0810 - acc: 0.6723 - val_loss: 1.0730 - val_acc: 0.3571\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.07302 to 1.07171, saving model to best.model\n",
      "0s - loss: 1.0753 - acc: 0.3613 - val_loss: 1.0717 - val_acc: 0.3571\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.0716 - acc: 0.3613 - val_loss: 1.0722 - val_acc: 0.3571\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.0698 - acc: 0.3613 - val_loss: 1.0726 - val_acc: 0.3571\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.0685 - acc: 0.3613 - val_loss: 1.0720 - val_acc: 0.3571\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.07171 to 1.07059, saving model to best.model\n",
      "0s - loss: 1.0670 - acc: 0.3613 - val_loss: 1.0706 - val_acc: 0.3571\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.07059 to 1.06898, saving model to best.model\n",
      "0s - loss: 1.0657 - acc: 0.3613 - val_loss: 1.0690 - val_acc: 0.3571\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.06898 to 1.06732, saving model to best.model\n",
      "0s - loss: 1.0646 - acc: 0.3613 - val_loss: 1.0673 - val_acc: 0.3571\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.06732 to 1.06543, saving model to best.model\n",
      "0s - loss: 1.0632 - acc: 0.3613 - val_loss: 1.0654 - val_acc: 0.3571\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.06543 to 1.06307, saving model to best.model\n",
      "0s - loss: 1.0616 - acc: 0.3613 - val_loss: 1.0631 - val_acc: 0.3571\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.06307 to 1.06022, saving model to best.model\n",
      "0s - loss: 1.0598 - acc: 0.3613 - val_loss: 1.0602 - val_acc: 0.3571\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.06022 to 1.05704, saving model to best.model\n",
      "0s - loss: 1.0577 - acc: 0.3613 - val_loss: 1.0570 - val_acc: 0.3571\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.05704 to 1.05370, saving model to best.model\n",
      "0s - loss: 1.0553 - acc: 0.3613 - val_loss: 1.0537 - val_acc: 0.3571\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.05370 to 1.05002, saving model to best.model\n",
      "0s - loss: 1.0524 - acc: 0.3613 - val_loss: 1.0500 - val_acc: 0.3571\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.05002 to 1.04590, saving model to best.model\n",
      "0s - loss: 1.0489 - acc: 0.3613 - val_loss: 1.0459 - val_acc: 0.3571\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.04590 to 1.04157, saving model to best.model\n",
      "0s - loss: 1.0449 - acc: 0.3613 - val_loss: 1.0416 - val_acc: 0.3571\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.04157 to 1.03772, saving model to best.model\n",
      "0s - loss: 1.0407 - acc: 0.3613 - val_loss: 1.0377 - val_acc: 0.3571\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.03772 to 1.03502, saving model to best.model\n",
      "0s - loss: 1.0366 - acc: 0.3613 - val_loss: 1.0350 - val_acc: 0.3571\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.03502 to 1.03356, saving model to best.model\n",
      "0s - loss: 1.0333 - acc: 0.3613 - val_loss: 1.0336 - val_acc: 0.3571\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.03356 to 1.03186, saving model to best.model\n",
      "0s - loss: 1.0313 - acc: 0.3613 - val_loss: 1.0319 - val_acc: 0.4286\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.03186 to 1.02900, saving model to best.model\n",
      "0s - loss: 1.0292 - acc: 0.5210 - val_loss: 1.0290 - val_acc: 0.5714\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.02900 to 1.02511, saving model to best.model\n",
      "0s - loss: 1.0263 - acc: 0.5882 - val_loss: 1.0251 - val_acc: 0.5714\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.02511 to 1.02078, saving model to best.model\n",
      "0s - loss: 1.0227 - acc: 0.6303 - val_loss: 1.0208 - val_acc: 0.5714\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.02078 to 1.01652, saving model to best.model\n",
      "0s - loss: 1.0188 - acc: 0.6471 - val_loss: 1.0165 - val_acc: 0.6429\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.01652 to 1.01261, saving model to best.model\n",
      "0s - loss: 1.0149 - acc: 0.6471 - val_loss: 1.0126 - val_acc: 0.6429\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.01261 to 1.00917, saving model to best.model\n",
      "0s - loss: 1.0114 - acc: 0.6471 - val_loss: 1.0092 - val_acc: 0.6429\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.00917 to 1.00647, saving model to best.model\n",
      "0s - loss: 1.0082 - acc: 0.6555 - val_loss: 1.0065 - val_acc: 0.6429\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.00647 to 1.00423, saving model to best.model\n",
      "0s - loss: 1.0053 - acc: 0.6639 - val_loss: 1.0042 - val_acc: 0.6429\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.00423 to 1.00196, saving model to best.model\n",
      "0s - loss: 1.0029 - acc: 0.6639 - val_loss: 1.0020 - val_acc: 0.6429\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.00196 to 0.99967, saving model to best.model\n",
      "0s - loss: 1.0005 - acc: 0.6723 - val_loss: 0.9997 - val_acc: 0.6429\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.99967 to 0.99730, saving model to best.model\n",
      "0s - loss: 0.9978 - acc: 0.6723 - val_loss: 0.9973 - val_acc: 0.6429\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.99730 to 0.99488, saving model to best.model\n",
      "0s - loss: 0.9949 - acc: 0.6639 - val_loss: 0.9949 - val_acc: 0.6429\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.99488 to 0.99264, saving model to best.model\n",
      "0s - loss: 0.9916 - acc: 0.6639 - val_loss: 0.9926 - val_acc: 0.6429\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.99264 to 0.99052, saving model to best.model\n",
      "0s - loss: 0.9882 - acc: 0.6723 - val_loss: 0.9905 - val_acc: 0.6429\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.99052 to 0.98865, saving model to best.model\n",
      "0s - loss: 0.9849 - acc: 0.6723 - val_loss: 0.9887 - val_acc: 0.6429\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.98865 to 0.98679, saving model to best.model\n",
      "0s - loss: 0.9816 - acc: 0.6723 - val_loss: 0.9868 - val_acc: 0.6429\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.98679 to 0.98450, saving model to best.model\n",
      "0s - loss: 0.9785 - acc: 0.6639 - val_loss: 0.9845 - val_acc: 0.6429\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.98450 to 0.98166, saving model to best.model\n",
      "0s - loss: 0.9755 - acc: 0.6639 - val_loss: 0.9817 - val_acc: 0.6429\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.98166 to 0.97819, saving model to best.model\n",
      "0s - loss: 0.9722 - acc: 0.6639 - val_loss: 0.9782 - val_acc: 0.6429\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.97819 to 0.97441, saving model to best.model\n",
      "0s - loss: 0.9687 - acc: 0.6723 - val_loss: 0.9744 - val_acc: 0.6429\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.97441 to 0.97076, saving model to best.model\n",
      "0s - loss: 0.9650 - acc: 0.6723 - val_loss: 0.9708 - val_acc: 0.6429\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.97076 to 0.96720, saving model to best.model\n",
      "0s - loss: 0.9612 - acc: 0.6723 - val_loss: 0.9672 - val_acc: 0.6429\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.96720 to 0.96362, saving model to best.model\n",
      "0s - loss: 0.9573 - acc: 0.6639 - val_loss: 0.9636 - val_acc: 0.6429\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.96362 to 0.95995, saving model to best.model\n",
      "0s - loss: 0.9532 - acc: 0.6639 - val_loss: 0.9599 - val_acc: 0.6429\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.95995 to 0.95691, saving model to best.model\n",
      "0s - loss: 0.9494 - acc: 0.6723 - val_loss: 0.9569 - val_acc: 0.6429\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.95691 to 0.95415, saving model to best.model\n",
      "0s - loss: 0.9458 - acc: 0.6723 - val_loss: 0.9542 - val_acc: 0.6429\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.95415 to 0.95149, saving model to best.model\n",
      "0s - loss: 0.9416 - acc: 0.6723 - val_loss: 0.9515 - val_acc: 0.6429\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.95149 to 0.94850, saving model to best.model\n",
      "0s - loss: 0.9375 - acc: 0.6723 - val_loss: 0.9485 - val_acc: 0.6429\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.94850 to 0.94516, saving model to best.model\n",
      "0s - loss: 0.9336 - acc: 0.6723 - val_loss: 0.9452 - val_acc: 0.6429\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.94516 to 0.94132, saving model to best.model\n",
      "0s - loss: 0.9296 - acc: 0.6723 - val_loss: 0.9413 - val_acc: 0.6429\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.94132 to 0.93695, saving model to best.model\n",
      "0s - loss: 0.9255 - acc: 0.6723 - val_loss: 0.9369 - val_acc: 0.6429\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.93695 to 0.93273, saving model to best.model\n",
      "0s - loss: 0.9214 - acc: 0.6723 - val_loss: 0.9327 - val_acc: 0.6429\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.93273 to 0.92895, saving model to best.model\n",
      "0s - loss: 0.9174 - acc: 0.6723 - val_loss: 0.9289 - val_acc: 0.6429\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.92895 to 0.92528, saving model to best.model\n",
      "0s - loss: 0.9134 - acc: 0.6723 - val_loss: 0.9253 - val_acc: 0.6429\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.92528 to 0.92199, saving model to best.model\n",
      "0s - loss: 0.9092 - acc: 0.6723 - val_loss: 0.9220 - val_acc: 0.6429\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.92199 to 0.91891, saving model to best.model\n",
      "0s - loss: 0.9050 - acc: 0.6723 - val_loss: 0.9189 - val_acc: 0.6429\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.91891 to 0.91532, saving model to best.model\n",
      "0s - loss: 0.9011 - acc: 0.6723 - val_loss: 0.9153 - val_acc: 0.6429\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.91532 to 0.91117, saving model to best.model\n",
      "0s - loss: 0.8971 - acc: 0.6723 - val_loss: 0.9112 - val_acc: 0.6429\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.91117 to 0.90700, saving model to best.model\n",
      "0s - loss: 0.8931 - acc: 0.6723 - val_loss: 0.9070 - val_acc: 0.6429\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.90700 to 0.90304, saving model to best.model\n",
      "0s - loss: 0.8890 - acc: 0.6723 - val_loss: 0.9030 - val_acc: 0.6429\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.90304 to 0.89917, saving model to best.model\n",
      "0s - loss: 0.8849 - acc: 0.6723 - val_loss: 0.8992 - val_acc: 0.6429\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.89917 to 0.89566, saving model to best.model\n",
      "0s - loss: 0.8806 - acc: 0.6723 - val_loss: 0.8957 - val_acc: 0.6429\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.89566 to 0.89255, saving model to best.model\n",
      "0s - loss: 0.8764 - acc: 0.6723 - val_loss: 0.8926 - val_acc: 0.6429\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.89255 to 0.88946, saving model to best.model\n",
      "0s - loss: 0.8724 - acc: 0.6723 - val_loss: 0.8895 - val_acc: 0.6429\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.88946 to 0.88632, saving model to best.model\n",
      "0s - loss: 0.8686 - acc: 0.6723 - val_loss: 0.8863 - val_acc: 0.6429\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.88632 to 0.88331, saving model to best.model\n",
      "0s - loss: 0.8648 - acc: 0.6723 - val_loss: 0.8833 - val_acc: 0.6429\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.88331 to 0.88036, saving model to best.model\n",
      "0s - loss: 0.8610 - acc: 0.6723 - val_loss: 0.8804 - val_acc: 0.6429\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.88036 to 0.87739, saving model to best.model\n",
      "0s - loss: 0.8571 - acc: 0.6723 - val_loss: 0.8774 - val_acc: 0.6429\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.87739 to 0.87441, saving model to best.model\n",
      "0s - loss: 0.8532 - acc: 0.6723 - val_loss: 0.8744 - val_acc: 0.6429\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.87441 to 0.87134, saving model to best.model\n",
      "0s - loss: 0.8492 - acc: 0.6723 - val_loss: 0.8713 - val_acc: 0.6429\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.87134 to 0.86852, saving model to best.model\n",
      "0s - loss: 0.8452 - acc: 0.6723 - val_loss: 0.8685 - val_acc: 0.6429\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.86852 to 0.86636, saving model to best.model\n",
      "0s - loss: 0.8414 - acc: 0.6723 - val_loss: 0.8664 - val_acc: 0.6429\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.86636 to 0.86431, saving model to best.model\n",
      "0s - loss: 0.8377 - acc: 0.6723 - val_loss: 0.8643 - val_acc: 0.6429\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.86431 to 0.86169, saving model to best.model\n",
      "0s - loss: 0.8342 - acc: 0.6723 - val_loss: 0.8617 - val_acc: 0.6429\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.86169 to 0.85858, saving model to best.model\n",
      "0s - loss: 0.8306 - acc: 0.6723 - val_loss: 0.8586 - val_acc: 0.6429\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.85858 to 0.85532, saving model to best.model\n",
      "0s - loss: 0.8270 - acc: 0.6723 - val_loss: 0.8553 - val_acc: 0.6429\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.85532 to 0.85215, saving model to best.model\n",
      "0s - loss: 0.8233 - acc: 0.6723 - val_loss: 0.8521 - val_acc: 0.6429\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.85215 to 0.84916, saving model to best.model\n",
      "0s - loss: 0.8196 - acc: 0.6723 - val_loss: 0.8492 - val_acc: 0.6429\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.84916 to 0.84642, saving model to best.model\n",
      "0s - loss: 0.8159 - acc: 0.6723 - val_loss: 0.8464 - val_acc: 0.6429\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.84642 to 0.84373, saving model to best.model\n",
      "0s - loss: 0.8120 - acc: 0.6723 - val_loss: 0.8437 - val_acc: 0.6429\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.84373 to 0.84086, saving model to best.model\n",
      "0s - loss: 0.8082 - acc: 0.6723 - val_loss: 0.8409 - val_acc: 0.6429\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.84086 to 0.83830, saving model to best.model\n",
      "0s - loss: 0.8047 - acc: 0.6723 - val_loss: 0.8383 - val_acc: 0.6429\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.83830 to 0.83624, saving model to best.model\n",
      "0s - loss: 0.8015 - acc: 0.6723 - val_loss: 0.8362 - val_acc: 0.6429\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.83624 to 0.83416, saving model to best.model\n",
      "0s - loss: 0.7983 - acc: 0.6723 - val_loss: 0.8342 - val_acc: 0.6429\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.83416 to 0.83193, saving model to best.model\n",
      "0s - loss: 0.7953 - acc: 0.6723 - val_loss: 0.8319 - val_acc: 0.6429\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.83193 to 0.82959, saving model to best.model\n",
      "0s - loss: 0.7922 - acc: 0.6723 - val_loss: 0.8296 - val_acc: 0.6429\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.82959 to 0.82706, saving model to best.model\n",
      "0s - loss: 0.7891 - acc: 0.6723 - val_loss: 0.8271 - val_acc: 0.6429\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.82706 to 0.82423, saving model to best.model\n",
      "0s - loss: 0.7859 - acc: 0.6723 - val_loss: 0.8242 - val_acc: 0.6429\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.82423 to 0.82117, saving model to best.model\n",
      "0s - loss: 0.7828 - acc: 0.6723 - val_loss: 0.8212 - val_acc: 0.6429\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.82117 to 0.81825, saving model to best.model\n",
      "0s - loss: 0.7797 - acc: 0.6723 - val_loss: 0.8182 - val_acc: 0.6429\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.81825 to 0.81583, saving model to best.model\n",
      "0s - loss: 0.7766 - acc: 0.6723 - val_loss: 0.8158 - val_acc: 0.6429\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.81583 to 0.81381, saving model to best.model\n",
      "0s - loss: 0.7735 - acc: 0.6723 - val_loss: 0.8138 - val_acc: 0.6429\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.81381 to 0.81183, saving model to best.model\n",
      "0s - loss: 0.7707 - acc: 0.6723 - val_loss: 0.8118 - val_acc: 0.6429\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.81183 to 0.80974, saving model to best.model\n",
      "0s - loss: 0.7680 - acc: 0.6723 - val_loss: 0.8097 - val_acc: 0.6429\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.80974 to 0.80754, saving model to best.model\n",
      "0s - loss: 0.7653 - acc: 0.6723 - val_loss: 0.8075 - val_acc: 0.6429\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.80754 to 0.80521, saving model to best.model\n",
      "0s - loss: 0.7625 - acc: 0.6723 - val_loss: 0.8052 - val_acc: 0.6429\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.80521 to 0.80275, saving model to best.model\n",
      "0s - loss: 0.7597 - acc: 0.6723 - val_loss: 0.8028 - val_acc: 0.6429\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.80275 to 0.80027, saving model to best.model\n",
      "0s - loss: 0.7569 - acc: 0.6723 - val_loss: 0.8003 - val_acc: 0.6429\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.80027 to 0.79793, saving model to best.model\n",
      "0s - loss: 0.7542 - acc: 0.6723 - val_loss: 0.7979 - val_acc: 0.6429\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.79793 to 0.79575, saving model to best.model\n",
      "0s - loss: 0.7517 - acc: 0.6723 - val_loss: 0.7957 - val_acc: 0.6429\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.79575 to 0.79365, saving model to best.model\n",
      "0s - loss: 0.7491 - acc: 0.6723 - val_loss: 0.7937 - val_acc: 0.6429\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.79365 to 0.79162, saving model to best.model\n",
      "0s - loss: 0.7466 - acc: 0.6723 - val_loss: 0.7916 - val_acc: 0.6429\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.79162 to 0.78966, saving model to best.model\n",
      "0s - loss: 0.7441 - acc: 0.6723 - val_loss: 0.7897 - val_acc: 0.6429\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.78966 to 0.78782, saving model to best.model\n",
      "0s - loss: 0.7416 - acc: 0.6723 - val_loss: 0.7878 - val_acc: 0.6429\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.78782 to 0.78609, saving model to best.model\n",
      "0s - loss: 0.7392 - acc: 0.6723 - val_loss: 0.7861 - val_acc: 0.6429\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.78609 to 0.78439, saving model to best.model\n",
      "0s - loss: 0.7368 - acc: 0.6723 - val_loss: 0.7844 - val_acc: 0.6429\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.78439 to 0.78263, saving model to best.model\n",
      "0s - loss: 0.7344 - acc: 0.6723 - val_loss: 0.7826 - val_acc: 0.6429\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.78263 to 0.78074, saving model to best.model\n",
      "0s - loss: 0.7321 - acc: 0.6723 - val_loss: 0.7807 - val_acc: 0.6429\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.78074 to 0.77871, saving model to best.model\n",
      "0s - loss: 0.7298 - acc: 0.6723 - val_loss: 0.7787 - val_acc: 0.6429\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.77871 to 0.77657, saving model to best.model\n",
      "0s - loss: 0.7276 - acc: 0.6723 - val_loss: 0.7766 - val_acc: 0.6429\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.77657 to 0.77442, saving model to best.model\n",
      "0s - loss: 0.7253 - acc: 0.6723 - val_loss: 0.7744 - val_acc: 0.6429\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.77442 to 0.77235, saving model to best.model\n",
      "0s - loss: 0.7231 - acc: 0.6723 - val_loss: 0.7724 - val_acc: 0.6429\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.77235 to 0.77043, saving model to best.model\n",
      "0s - loss: 0.7210 - acc: 0.6723 - val_loss: 0.7704 - val_acc: 0.6429\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.77043 to 0.76866, saving model to best.model\n",
      "0s - loss: 0.7189 - acc: 0.6723 - val_loss: 0.7687 - val_acc: 0.6429\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.76866 to 0.76704, saving model to best.model\n",
      "0s - loss: 0.7168 - acc: 0.6723 - val_loss: 0.7670 - val_acc: 0.6429\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.76704 to 0.76552, saving model to best.model\n",
      "0s - loss: 0.7148 - acc: 0.6723 - val_loss: 0.7655 - val_acc: 0.6429\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.76552 to 0.76403, saving model to best.model\n",
      "0s - loss: 0.7127 - acc: 0.6723 - val_loss: 0.7640 - val_acc: 0.6429\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.76403 to 0.76251, saving model to best.model\n",
      "0s - loss: 0.7107 - acc: 0.6723 - val_loss: 0.7625 - val_acc: 0.6429\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.76251 to 0.76091, saving model to best.model\n",
      "0s - loss: 0.7088 - acc: 0.6723 - val_loss: 0.7609 - val_acc: 0.6429\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.76091 to 0.75922, saving model to best.model\n",
      "0s - loss: 0.7068 - acc: 0.6723 - val_loss: 0.7592 - val_acc: 0.6429\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.75922 to 0.75743, saving model to best.model\n",
      "0s - loss: 0.7049 - acc: 0.6723 - val_loss: 0.7574 - val_acc: 0.6429\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.75743 to 0.75560, saving model to best.model\n",
      "0s - loss: 0.7031 - acc: 0.6723 - val_loss: 0.7556 - val_acc: 0.6429\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.75560 to 0.75377, saving model to best.model\n",
      "0s - loss: 0.7012 - acc: 0.6723 - val_loss: 0.7538 - val_acc: 0.6429\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.75377 to 0.75202, saving model to best.model\n",
      "0s - loss: 0.6994 - acc: 0.6723 - val_loss: 0.7520 - val_acc: 0.6429\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.75202 to 0.75039, saving model to best.model\n",
      "0s - loss: 0.6976 - acc: 0.6723 - val_loss: 0.7504 - val_acc: 0.6429\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.75039 to 0.74888, saving model to best.model\n",
      "0s - loss: 0.6958 - acc: 0.6723 - val_loss: 0.7489 - val_acc: 0.6429\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.74888 to 0.74746, saving model to best.model\n",
      "0s - loss: 0.6941 - acc: 0.6723 - val_loss: 0.7475 - val_acc: 0.6429\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.74746 to 0.74605, saving model to best.model\n",
      "0s - loss: 0.6923 - acc: 0.6723 - val_loss: 0.7460 - val_acc: 0.6429\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.74605 to 0.74457, saving model to best.model\n",
      "0s - loss: 0.6906 - acc: 0.6723 - val_loss: 0.7446 - val_acc: 0.6429\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.74457 to 0.74300, saving model to best.model\n",
      "0s - loss: 0.6889 - acc: 0.6723 - val_loss: 0.7430 - val_acc: 0.6429\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.74300 to 0.74137, saving model to best.model\n",
      "0s - loss: 0.6872 - acc: 0.6723 - val_loss: 0.7414 - val_acc: 0.6429\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.74137 to 0.73974, saving model to best.model\n",
      "0s - loss: 0.6856 - acc: 0.6723 - val_loss: 0.7397 - val_acc: 0.6429\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.73974 to 0.73819, saving model to best.model\n",
      "0s - loss: 0.6839 - acc: 0.6723 - val_loss: 0.7382 - val_acc: 0.6429\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.73819 to 0.73683, saving model to best.model\n",
      "0s - loss: 0.6824 - acc: 0.6723 - val_loss: 0.7368 - val_acc: 0.6429\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.73683 to 0.73544, saving model to best.model\n",
      "0s - loss: 0.6809 - acc: 0.6723 - val_loss: 0.7354 - val_acc: 0.6429\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.73544 to 0.73403, saving model to best.model\n",
      "0s - loss: 0.6794 - acc: 0.6723 - val_loss: 0.7340 - val_acc: 0.6429\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.73403 to 0.73264, saving model to best.model\n",
      "0s - loss: 0.6779 - acc: 0.6723 - val_loss: 0.7326 - val_acc: 0.6429\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.73264 to 0.73126, saving model to best.model\n",
      "0s - loss: 0.6764 - acc: 0.6723 - val_loss: 0.7313 - val_acc: 0.6429\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.73126 to 0.72982, saving model to best.model\n",
      "0s - loss: 0.6749 - acc: 0.6723 - val_loss: 0.7298 - val_acc: 0.6429\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.72982 to 0.72824, saving model to best.model\n",
      "0s - loss: 0.6735 - acc: 0.6723 - val_loss: 0.7282 - val_acc: 0.6429\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.72824 to 0.72661, saving model to best.model\n",
      "0s - loss: 0.6720 - acc: 0.6723 - val_loss: 0.7266 - val_acc: 0.6429\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.72661 to 0.72505, saving model to best.model\n",
      "0s - loss: 0.6706 - acc: 0.6723 - val_loss: 0.7251 - val_acc: 0.6429\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.72505 to 0.72354, saving model to best.model\n",
      "0s - loss: 0.6692 - acc: 0.6723 - val_loss: 0.7235 - val_acc: 0.6429\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.72354 to 0.72206, saving model to best.model\n",
      "0s - loss: 0.6679 - acc: 0.6723 - val_loss: 0.7221 - val_acc: 0.6429\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.72206 to 0.72060, saving model to best.model\n",
      "0s - loss: 0.6665 - acc: 0.6723 - val_loss: 0.7206 - val_acc: 0.6429\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.72060 to 0.71921, saving model to best.model\n",
      "0s - loss: 0.6652 - acc: 0.6723 - val_loss: 0.7192 - val_acc: 0.6429\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.71921 to 0.71790, saving model to best.model\n",
      "0s - loss: 0.6639 - acc: 0.6723 - val_loss: 0.7179 - val_acc: 0.6429\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.71790 to 0.71665, saving model to best.model\n",
      "0s - loss: 0.6626 - acc: 0.6723 - val_loss: 0.7166 - val_acc: 0.6429\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.71665 to 0.71544, saving model to best.model\n",
      "0s - loss: 0.6614 - acc: 0.6723 - val_loss: 0.7154 - val_acc: 0.6429\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.71544 to 0.71426, saving model to best.model\n",
      "0s - loss: 0.6601 - acc: 0.6723 - val_loss: 0.7143 - val_acc: 0.6429\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.71426 to 0.71310, saving model to best.model\n",
      "0s - loss: 0.6589 - acc: 0.6723 - val_loss: 0.7131 - val_acc: 0.6429\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.71310 to 0.71187, saving model to best.model\n",
      "0s - loss: 0.6577 - acc: 0.6723 - val_loss: 0.7119 - val_acc: 0.6429\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.71187 to 0.71053, saving model to best.model\n",
      "0s - loss: 0.6565 - acc: 0.6723 - val_loss: 0.7105 - val_acc: 0.6429\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.71053 to 0.70913, saving model to best.model\n",
      "0s - loss: 0.6553 - acc: 0.6723 - val_loss: 0.7091 - val_acc: 0.6429\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.70913 to 0.70774, saving model to best.model\n",
      "0s - loss: 0.6542 - acc: 0.6723 - val_loss: 0.7077 - val_acc: 0.6429\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.70774 to 0.70639, saving model to best.model\n",
      "0s - loss: 0.6530 - acc: 0.6723 - val_loss: 0.7064 - val_acc: 0.6429\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.70639 to 0.70507, saving model to best.model\n",
      "0s - loss: 0.6519 - acc: 0.6723 - val_loss: 0.7051 - val_acc: 0.6429\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.70507 to 0.70379, saving model to best.model\n",
      "0s - loss: 0.6508 - acc: 0.6723 - val_loss: 0.7038 - val_acc: 0.6429\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.70379 to 0.70255, saving model to best.model\n",
      "0s - loss: 0.6497 - acc: 0.6723 - val_loss: 0.7025 - val_acc: 0.6429\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.70255 to 0.70133, saving model to best.model\n",
      "0s - loss: 0.6486 - acc: 0.6723 - val_loss: 0.7013 - val_acc: 0.6429\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.70133 to 0.70011, saving model to best.model\n",
      "0s - loss: 0.6475 - acc: 0.6723 - val_loss: 0.7001 - val_acc: 0.6429\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.70011 to 0.69889, saving model to best.model\n",
      "0s - loss: 0.6464 - acc: 0.6723 - val_loss: 0.6989 - val_acc: 0.6429\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.69889 to 0.69765, saving model to best.model\n",
      "0s - loss: 0.6454 - acc: 0.6723 - val_loss: 0.6976 - val_acc: 0.6429\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.69765 to 0.69636, saving model to best.model\n",
      "0s - loss: 0.6444 - acc: 0.6723 - val_loss: 0.6964 - val_acc: 0.6429\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.69636 to 0.69503, saving model to best.model\n",
      "0s - loss: 0.6433 - acc: 0.6723 - val_loss: 0.6950 - val_acc: 0.6429\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.69503 to 0.69369, saving model to best.model\n",
      "0s - loss: 0.6423 - acc: 0.6723 - val_loss: 0.6937 - val_acc: 0.6429\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.69369 to 0.69234, saving model to best.model\n",
      "0s - loss: 0.6413 - acc: 0.6723 - val_loss: 0.6923 - val_acc: 0.6429\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.69234 to 0.69100, saving model to best.model\n",
      "0s - loss: 0.6403 - acc: 0.6723 - val_loss: 0.6910 - val_acc: 0.6429\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.69100 to 0.68967, saving model to best.model\n",
      "0s - loss: 0.6392 - acc: 0.6723 - val_loss: 0.6897 - val_acc: 0.6429\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.68967 to 0.68836, saving model to best.model\n",
      "0s - loss: 0.6382 - acc: 0.6723 - val_loss: 0.6884 - val_acc: 0.6429\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.68836 to 0.68710, saving model to best.model\n",
      "0s - loss: 0.6371 - acc: 0.6723 - val_loss: 0.6871 - val_acc: 0.6429\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.68710 to 0.68594, saving model to best.model\n",
      "0s - loss: 0.6361 - acc: 0.6723 - val_loss: 0.6859 - val_acc: 0.6429\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.68594 to 0.68486, saving model to best.model\n",
      "0s - loss: 0.6351 - acc: 0.6723 - val_loss: 0.6849 - val_acc: 0.6429\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.68486 to 0.68370, saving model to best.model\n",
      "0s - loss: 0.6342 - acc: 0.6723 - val_loss: 0.6837 - val_acc: 0.6429\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.68370 to 0.68235, saving model to best.model\n",
      "0s - loss: 0.6333 - acc: 0.6723 - val_loss: 0.6824 - val_acc: 0.6429\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.68235 to 0.68082, saving model to best.model\n",
      "0s - loss: 0.6324 - acc: 0.6723 - val_loss: 0.6808 - val_acc: 0.6429\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.68082 to 0.67913, saving model to best.model\n",
      "0s - loss: 0.6314 - acc: 0.6723 - val_loss: 0.6791 - val_acc: 0.6429\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.67913 to 0.67741, saving model to best.model\n",
      "0s - loss: 0.6304 - acc: 0.6723 - val_loss: 0.6774 - val_acc: 0.6429\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.67741 to 0.67577, saving model to best.model\n",
      "0s - loss: 0.6294 - acc: 0.6723 - val_loss: 0.6758 - val_acc: 0.6429\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.67577 to 0.67416, saving model to best.model\n",
      "0s - loss: 0.6285 - acc: 0.6723 - val_loss: 0.6742 - val_acc: 0.6429\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.67416 to 0.67256, saving model to best.model\n",
      "0s - loss: 0.6277 - acc: 0.6723 - val_loss: 0.6726 - val_acc: 0.6429\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.67256 to 0.67098, saving model to best.model\n",
      "0s - loss: 0.6267 - acc: 0.6723 - val_loss: 0.6710 - val_acc: 0.6429\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.67098 to 0.66955, saving model to best.model\n",
      "0s - loss: 0.6258 - acc: 0.6723 - val_loss: 0.6695 - val_acc: 0.6429\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.66955 to 0.66854, saving model to best.model\n",
      "0s - loss: 0.6247 - acc: 0.6723 - val_loss: 0.6685 - val_acc: 0.6429\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.66854 to 0.66796, saving model to best.model\n",
      "0s - loss: 0.6237 - acc: 0.6723 - val_loss: 0.6680 - val_acc: 0.6429\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.66796 to 0.66721, saving model to best.model\n",
      "0s - loss: 0.6228 - acc: 0.6723 - val_loss: 0.6672 - val_acc: 0.6429\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.66721 to 0.66608, saving model to best.model\n",
      "0s - loss: 0.6220 - acc: 0.6723 - val_loss: 0.6661 - val_acc: 0.6429\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.66608 to 0.66470, saving model to best.model\n",
      "0s - loss: 0.6210 - acc: 0.6723 - val_loss: 0.6647 - val_acc: 0.6429\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.66470 to 0.66318, saving model to best.model\n",
      "0s - loss: 0.6200 - acc: 0.6723 - val_loss: 0.6632 - val_acc: 0.6429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b25acd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(\n",
    "    # Feature matrix\n",
    "    X_train.values, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    "    batch_size=256, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.load_weights(\"best.model\")\n",
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in m.predict(X_test.values).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9  4  0]\n",
      " [ 1 22  0]\n",
      " [ 0  9  0]]\n",
      "68.8888888889\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.90      0.69      0.78        13\n",
      "          2       0.63      0.96      0.76        23\n",
      "          3       0.00      0.00      0.00         9\n",
      "\n",
      "avg / total       0.58      0.69      0.61        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119 samples, validate on 14 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.09928, saving model to best.model\n",
      "0s - loss: 1.2678 - acc: 0.3193 - val_loss: 1.0993 - val_acc: 0.3571\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.1168 - acc: 0.3361 - val_loss: 1.1354 - val_acc: 0.2857\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.09928 to 1.07441, saving model to best.model\n",
      "0s - loss: 1.1241 - acc: 0.2941 - val_loss: 1.0744 - val_acc: 0.2857\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.07441 to 1.05565, saving model to best.model\n",
      "0s - loss: 1.0655 - acc: 0.3950 - val_loss: 1.0557 - val_acc: 0.3571\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.05565 to 1.05312, saving model to best.model\n",
      "0s - loss: 1.0671 - acc: 0.3613 - val_loss: 1.0531 - val_acc: 0.4286\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.05312 to 1.02897, saving model to best.model\n",
      "0s - loss: 1.0648 - acc: 0.5042 - val_loss: 1.0290 - val_acc: 0.6429\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.02897 to 1.02183, saving model to best.model\n",
      "0s - loss: 1.0572 - acc: 0.4790 - val_loss: 1.0218 - val_acc: 0.7143\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.02183 to 1.00245, saving model to best.model\n",
      "0s - loss: 1.0309 - acc: 0.7143 - val_loss: 1.0024 - val_acc: 1.0000\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.00245 to 0.98657, saving model to best.model\n",
      "0s - loss: 1.0124 - acc: 0.6975 - val_loss: 0.9866 - val_acc: 0.3571\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.98657 to 0.97079, saving model to best.model\n",
      "0s - loss: 1.0042 - acc: 0.5462 - val_loss: 0.9708 - val_acc: 0.6429\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.97079 to 0.95280, saving model to best.model\n",
      "0s - loss: 0.9882 - acc: 0.6218 - val_loss: 0.9528 - val_acc: 1.0000\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.95280 to 0.94004, saving model to best.model\n",
      "0s - loss: 0.9755 - acc: 0.7647 - val_loss: 0.9400 - val_acc: 1.0000\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.94004 to 0.91830, saving model to best.model\n",
      "0s - loss: 0.9595 - acc: 0.8908 - val_loss: 0.9183 - val_acc: 0.9286\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.91830 to 0.89845, saving model to best.model\n",
      "0s - loss: 0.9415 - acc: 0.8739 - val_loss: 0.8985 - val_acc: 0.9286\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.89845 to 0.87786, saving model to best.model\n",
      "0s - loss: 0.9246 - acc: 0.8235 - val_loss: 0.8779 - val_acc: 0.9286\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.87786 to 0.85652, saving model to best.model\n",
      "0s - loss: 0.9099 - acc: 0.8319 - val_loss: 0.8565 - val_acc: 0.9286\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.85652 to 0.83871, saving model to best.model\n",
      "0s - loss: 0.8901 - acc: 0.9076 - val_loss: 0.8387 - val_acc: 1.0000\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.83871 to 0.81149, saving model to best.model\n",
      "0s - loss: 0.8692 - acc: 0.9076 - val_loss: 0.8115 - val_acc: 0.9286\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81149 to 0.79049, saving model to best.model\n",
      "0s - loss: 0.8576 - acc: 0.8824 - val_loss: 0.7905 - val_acc: 0.9286\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.79049 to 0.76671, saving model to best.model\n",
      "0s - loss: 0.8337 - acc: 0.8824 - val_loss: 0.7667 - val_acc: 1.0000\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.76671 to 0.74238, saving model to best.model\n",
      "0s - loss: 0.8110 - acc: 0.9664 - val_loss: 0.7424 - val_acc: 0.9286\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.74238 to 0.71882, saving model to best.model\n",
      "0s - loss: 0.7941 - acc: 0.9328 - val_loss: 0.7188 - val_acc: 0.9286\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.71882 to 0.69409, saving model to best.model\n",
      "0s - loss: 0.7774 - acc: 0.8655 - val_loss: 0.6941 - val_acc: 1.0000\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.69409 to 0.67099, saving model to best.model\n",
      "0s - loss: 0.7465 - acc: 0.9496 - val_loss: 0.6710 - val_acc: 0.9286\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.67099 to 0.64647, saving model to best.model\n",
      "0s - loss: 0.7329 - acc: 0.9496 - val_loss: 0.6465 - val_acc: 0.9286\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.64647 to 0.62427, saving model to best.model\n",
      "0s - loss: 0.7061 - acc: 0.9664 - val_loss: 0.6243 - val_acc: 1.0000\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.62427 to 0.60183, saving model to best.model\n",
      "0s - loss: 0.6996 - acc: 0.9580 - val_loss: 0.6018 - val_acc: 0.9286\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.60183 to 0.57991, saving model to best.model\n",
      "0s - loss: 0.6701 - acc: 0.9160 - val_loss: 0.5799 - val_acc: 1.0000\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.57991 to 0.56541, saving model to best.model\n",
      "0s - loss: 0.6462 - acc: 0.9076 - val_loss: 0.5654 - val_acc: 0.9286\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.56541 to 0.54392, saving model to best.model\n",
      "0s - loss: 0.6350 - acc: 0.9412 - val_loss: 0.5439 - val_acc: 0.9286\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.54392 to 0.50896, saving model to best.model\n",
      "0s - loss: 0.6115 - acc: 0.9580 - val_loss: 0.5090 - val_acc: 1.0000\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.50896 to 0.49381, saving model to best.model\n",
      "0s - loss: 0.5937 - acc: 0.9748 - val_loss: 0.4938 - val_acc: 0.9286\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.49381 to 0.47101, saving model to best.model\n",
      "0s - loss: 0.5817 - acc: 0.9328 - val_loss: 0.4710 - val_acc: 1.0000\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.47101 to 0.45831, saving model to best.model\n",
      "0s - loss: 0.5587 - acc: 0.9664 - val_loss: 0.4583 - val_acc: 0.9286\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.45831 to 0.42908, saving model to best.model\n",
      "0s - loss: 0.5320 - acc: 0.9664 - val_loss: 0.4291 - val_acc: 1.0000\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.42908 to 0.41716, saving model to best.model\n",
      "0s - loss: 0.5208 - acc: 0.9580 - val_loss: 0.4172 - val_acc: 0.9286\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.41716 to 0.38964, saving model to best.model\n",
      "0s - loss: 0.5032 - acc: 0.9664 - val_loss: 0.3896 - val_acc: 1.0000\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.38964 to 0.37952, saving model to best.model\n",
      "0s - loss: 0.4834 - acc: 0.9748 - val_loss: 0.3795 - val_acc: 1.0000\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.37952 to 0.35686, saving model to best.model\n",
      "0s - loss: 0.4690 - acc: 0.9664 - val_loss: 0.3569 - val_acc: 1.0000\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.35686 to 0.35139, saving model to best.model\n",
      "0s - loss: 0.4545 - acc: 0.9664 - val_loss: 0.3514 - val_acc: 1.0000\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.35139 to 0.32497, saving model to best.model\n",
      "0s - loss: 0.4417 - acc: 0.9664 - val_loss: 0.3250 - val_acc: 1.0000\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.32497 to 0.31917, saving model to best.model\n",
      "0s - loss: 0.4265 - acc: 0.9664 - val_loss: 0.3192 - val_acc: 0.9286\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.31917 to 0.29044, saving model to best.model\n",
      "0s - loss: 0.4065 - acc: 0.9664 - val_loss: 0.2904 - val_acc: 1.0000\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.3999 - acc: 0.9412 - val_loss: 0.2955 - val_acc: 0.9286\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.29044 to 0.28843, saving model to best.model\n",
      "0s - loss: 0.3837 - acc: 0.9748 - val_loss: 0.2884 - val_acc: 0.9286\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.28843 to 0.25292, saving model to best.model\n",
      "0s - loss: 0.3710 - acc: 0.9748 - val_loss: 0.2529 - val_acc: 1.0000\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.25292 to 0.24993, saving model to best.model\n",
      "0s - loss: 0.3599 - acc: 0.9748 - val_loss: 0.2499 - val_acc: 1.0000\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.24993 to 0.24317, saving model to best.model\n",
      "0s - loss: 0.3512 - acc: 0.9748 - val_loss: 0.2432 - val_acc: 1.0000\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.24317 to 0.22068, saving model to best.model\n",
      "0s - loss: 0.3314 - acc: 0.9748 - val_loss: 0.2207 - val_acc: 1.0000\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.22068 to 0.22017, saving model to best.model\n",
      "0s - loss: 0.3314 - acc: 0.9748 - val_loss: 0.2202 - val_acc: 1.0000\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.22017 to 0.20045, saving model to best.model\n",
      "0s - loss: 0.3153 - acc: 0.9748 - val_loss: 0.2005 - val_acc: 1.0000\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.3071 - acc: 0.9748 - val_loss: 0.2041 - val_acc: 1.0000\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.20045 to 0.18719, saving model to best.model\n",
      "0s - loss: 0.2987 - acc: 0.9664 - val_loss: 0.1872 - val_acc: 1.0000\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.2890 - acc: 0.9748 - val_loss: 0.1999 - val_acc: 1.0000\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.18719 to 0.17864, saving model to best.model\n",
      "0s - loss: 0.2845 - acc: 0.9664 - val_loss: 0.1786 - val_acc: 1.0000\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.17864 to 0.16708, saving model to best.model\n",
      "0s - loss: 0.2818 - acc: 0.9664 - val_loss: 0.1671 - val_acc: 1.0000\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.2694 - acc: 0.9748 - val_loss: 0.1839 - val_acc: 1.0000\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.16708 to 0.14579, saving model to best.model\n",
      "0s - loss: 0.2583 - acc: 0.9748 - val_loss: 0.1458 - val_acc: 1.0000\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.2626 - acc: 0.9328 - val_loss: 0.1748 - val_acc: 0.9286\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.2660 - acc: 0.9580 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.14579 to 0.13102, saving model to best.model\n",
      "0s - loss: 0.2430 - acc: 0.9580 - val_loss: 0.1310 - val_acc: 1.0000\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.2684 - acc: 0.9496 - val_loss: 0.1682 - val_acc: 0.9286\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.13102 to 0.12043, saving model to best.model\n",
      "0s - loss: 0.2442 - acc: 0.9580 - val_loss: 0.1204 - val_acc: 1.0000\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.2279 - acc: 0.9580 - val_loss: 0.1480 - val_acc: 1.0000\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.2221 - acc: 0.9748 - val_loss: 0.1238 - val_acc: 1.0000\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.2150 - acc: 0.9664 - val_loss: 0.1246 - val_acc: 1.0000\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.2115 - acc: 0.9832 - val_loss: 0.1207 - val_acc: 1.0000\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.12043 to 0.11115, saving model to best.model\n",
      "0s - loss: 0.2026 - acc: 0.9748 - val_loss: 0.1112 - val_acc: 1.0000\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.2069 - acc: 0.9748 - val_loss: 0.1205 - val_acc: 1.0000\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.2096 - acc: 0.9748 - val_loss: 0.1167 - val_acc: 1.0000\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.11115 to 0.09677, saving model to best.model\n",
      "0s - loss: 0.1958 - acc: 0.9748 - val_loss: 0.0968 - val_acc: 1.0000\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.1946 - acc: 0.9748 - val_loss: 0.1066 - val_acc: 1.0000\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.09677 to 0.09655, saving model to best.model\n",
      "0s - loss: 0.1832 - acc: 0.9748 - val_loss: 0.0965 - val_acc: 1.0000\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.1840 - acc: 0.9748 - val_loss: 0.0998 - val_acc: 1.0000\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.1849 - acc: 0.9664 - val_loss: 0.0998 - val_acc: 1.0000\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.1760 - acc: 0.9664 - val_loss: 0.1006 - val_acc: 1.0000\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.09655 to 0.08449, saving model to best.model\n",
      "0s - loss: 0.1715 - acc: 0.9748 - val_loss: 0.0845 - val_acc: 1.0000\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.1712 - acc: 0.9748 - val_loss: 0.0995 - val_acc: 1.0000\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.1723 - acc: 0.9664 - val_loss: 0.0846 - val_acc: 1.0000\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.08449 to 0.08243, saving model to best.model\n",
      "0s - loss: 0.1693 - acc: 0.9664 - val_loss: 0.0824 - val_acc: 1.0000\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.1601 - acc: 0.9832 - val_loss: 0.1066 - val_acc: 1.0000\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.08243 to 0.07705, saving model to best.model\n",
      "0s - loss: 0.1631 - acc: 0.9748 - val_loss: 0.0771 - val_acc: 1.0000\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.1560 - acc: 0.9748 - val_loss: 0.0884 - val_acc: 1.0000\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.07705 to 0.07464, saving model to best.model\n",
      "0s - loss: 0.1513 - acc: 0.9748 - val_loss: 0.0746 - val_acc: 1.0000\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.1524 - acc: 0.9748 - val_loss: 0.0784 - val_acc: 1.0000\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.1483 - acc: 0.9748 - val_loss: 0.0828 - val_acc: 1.0000\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.1576 - acc: 0.9748 - val_loss: 0.0775 - val_acc: 1.0000\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.07464 to 0.06676, saving model to best.model\n",
      "0s - loss: 0.1470 - acc: 0.9832 - val_loss: 0.0668 - val_acc: 1.0000\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.1441 - acc: 0.9748 - val_loss: 0.0787 - val_acc: 1.0000\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.1475 - acc: 0.9748 - val_loss: 0.0724 - val_acc: 1.0000\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.1400 - acc: 0.9832 - val_loss: 0.0874 - val_acc: 1.0000\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.06676 to 0.06099, saving model to best.model\n",
      "0s - loss: 0.1388 - acc: 0.9832 - val_loss: 0.0610 - val_acc: 1.0000\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.1382 - acc: 0.9832 - val_loss: 0.0842 - val_acc: 1.0000\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.1451 - acc: 0.9748 - val_loss: 0.0702 - val_acc: 1.0000\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.06099 to 0.05630, saving model to best.model\n",
      "0s - loss: 0.1362 - acc: 0.9832 - val_loss: 0.0563 - val_acc: 1.0000\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.1457 - acc: 0.9664 - val_loss: 0.0769 - val_acc: 1.0000\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.05630 to 0.05345, saving model to best.model\n",
      "0s - loss: 0.1327 - acc: 0.9832 - val_loss: 0.0534 - val_acc: 1.0000\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.1296 - acc: 0.9748 - val_loss: 0.0722 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.1288 - acc: 0.9748 - val_loss: 0.0669 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.1250 - acc: 0.9748 - val_loss: 0.0627 - val_acc: 1.0000\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.1222 - acc: 0.9748 - val_loss: 0.0546 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.1211 - acc: 0.9748 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.1186 - acc: 0.9748 - val_loss: 0.0640 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.1168 - acc: 0.9748 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.1152 - acc: 0.9748 - val_loss: 0.0594 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.1172 - acc: 0.9748 - val_loss: 0.0620 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.1134 - acc: 0.9748 - val_loss: 0.0541 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.1156 - acc: 0.9748 - val_loss: 0.0607 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.1135 - acc: 0.9832 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.05345 to 0.04812, saving model to best.model\n",
      "0s - loss: 0.1100 - acc: 0.9748 - val_loss: 0.0481 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.1088 - acc: 0.9748 - val_loss: 0.0631 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.1090 - acc: 0.9748 - val_loss: 0.0524 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.1138 - acc: 0.9748 - val_loss: 0.0574 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.1229 - acc: 0.9832 - val_loss: 0.0786 - val_acc: 0.9286\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.04812 to 0.03449, saving model to best.model\n",
      "0s - loss: 0.1144 - acc: 0.9664 - val_loss: 0.0345 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.1163 - acc: 0.9664 - val_loss: 0.0767 - val_acc: 0.9286\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.1186 - acc: 0.9748 - val_loss: 0.0523 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.1062 - acc: 0.9832 - val_loss: 0.0366 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.1018 - acc: 0.9832 - val_loss: 0.0789 - val_acc: 0.9286\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.1144 - acc: 0.9748 - val_loss: 0.0492 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.1024 - acc: 0.9832 - val_loss: 0.0488 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.1218 - acc: 0.9664 - val_loss: 0.0692 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.03449 to 0.03436, saving model to best.model\n",
      "0s - loss: 0.0930 - acc: 0.9832 - val_loss: 0.0344 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.1100 - acc: 0.9832 - val_loss: 0.0677 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.1167 - acc: 0.9748 - val_loss: 0.0691 - val_acc: 0.9286\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.03436 to 0.03054, saving model to best.model\n",
      "0s - loss: 0.0961 - acc: 0.9832 - val_loss: 0.0305 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.1099 - acc: 0.9664 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.1100 - acc: 0.9748 - val_loss: 0.0651 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.0955 - acc: 0.9832 - val_loss: 0.0318 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.1030 - acc: 0.9832 - val_loss: 0.0744 - val_acc: 0.9286\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.1008 - acc: 0.9748 - val_loss: 0.0468 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.0990 - acc: 0.9832 - val_loss: 0.0385 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.0915 - acc: 0.9832 - val_loss: 0.0666 - val_acc: 0.9286\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.0956 - acc: 0.9832 - val_loss: 0.0393 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.0879 - acc: 0.9748 - val_loss: 0.0528 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.0917 - acc: 0.9748 - val_loss: 0.0521 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.0917 - acc: 0.9748 - val_loss: 0.0536 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.0856 - acc: 0.9748 - val_loss: 0.0379 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.0840 - acc: 0.9832 - val_loss: 0.0644 - val_acc: 0.9286\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.0867 - acc: 0.9748 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.0907 - acc: 0.9748 - val_loss: 0.0519 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.0851 - acc: 0.9748 - val_loss: 0.0530 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.0853 - acc: 0.9748 - val_loss: 0.0459 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.0825 - acc: 0.9748 - val_loss: 0.0428 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.0872 - acc: 0.9664 - val_loss: 0.0534 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.0973 - acc: 0.9748 - val_loss: 0.0395 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.0815 - acc: 0.9748 - val_loss: 0.0816 - val_acc: 0.9286\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.0861 - acc: 0.9748 - val_loss: 0.0325 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.0879 - acc: 0.9916 - val_loss: 0.0441 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.0955 - acc: 0.9664 - val_loss: 0.0615 - val_acc: 0.9286\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.03054 to 0.02544, saving model to best.model\n",
      "0s - loss: 0.0772 - acc: 0.9832 - val_loss: 0.0254 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.0836 - acc: 0.9832 - val_loss: 0.0673 - val_acc: 0.9286\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.0911 - acc: 0.9748 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.0787 - acc: 0.9832 - val_loss: 0.0363 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.0761 - acc: 0.9748 - val_loss: 0.0448 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.0763 - acc: 0.9832 - val_loss: 0.0473 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.0748 - acc: 0.9748 - val_loss: 0.0347 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.0785 - acc: 0.9748 - val_loss: 0.0650 - val_acc: 0.9286\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.0841 - acc: 0.9748 - val_loss: 0.0394 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.0738 - acc: 0.9832 - val_loss: 0.0352 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.0747 - acc: 0.9748 - val_loss: 0.0528 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.0834 - acc: 0.9748 - val_loss: 0.0476 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.0726 - acc: 0.9832 - val_loss: 0.0670 - val_acc: 0.9286\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.0779 - acc: 0.9832 - val_loss: 0.0334 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.0737 - acc: 0.9748 - val_loss: 0.0611 - val_acc: 0.9286\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.0755 - acc: 0.9748 - val_loss: 0.0371 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.0693 - acc: 0.9748 - val_loss: 0.0483 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.0696 - acc: 0.9748 - val_loss: 0.0402 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.0783 - acc: 0.9832 - val_loss: 0.0583 - val_acc: 0.9286\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.0691 - acc: 0.9748 - val_loss: 0.0466 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.0697 - acc: 0.9748 - val_loss: 0.0390 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.0708 - acc: 0.9748 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.0717 - acc: 0.9832 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.0818 - acc: 0.9664 - val_loss: 0.0498 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.0828 - acc: 0.9664 - val_loss: 0.0381 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.0816 - acc: 0.9832 - val_loss: 0.0962 - val_acc: 0.9286\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.02544 to 0.02361, saving model to best.model\n",
      "0s - loss: 0.0785 - acc: 0.9748 - val_loss: 0.0236 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.0833 - acc: 0.9664 - val_loss: 0.0519 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.0671 - acc: 0.9748 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.0650 - acc: 0.9748 - val_loss: 0.0436 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.0635 - acc: 0.9832 - val_loss: 0.0389 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.0701 - acc: 0.9748 - val_loss: 0.0506 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.0617 - acc: 0.9832 - val_loss: 0.0549 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.0623 - acc: 0.9832 - val_loss: 0.0489 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.0620 - acc: 0.9748 - val_loss: 0.0505 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.0673 - acc: 0.9832 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.0667 - acc: 0.9832 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.0574 - acc: 0.9748 - val_loss: 0.0660 - val_acc: 0.9286\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.0690 - acc: 0.9748 - val_loss: 0.0343 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.0681 - acc: 0.9748 - val_loss: 0.0472 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.0631 - acc: 0.9832 - val_loss: 0.0375 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.0649 - acc: 0.9916 - val_loss: 0.0404 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.0585 - acc: 0.9832 - val_loss: 0.0653 - val_acc: 0.9286\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.0614 - acc: 0.9832 - val_loss: 0.0383 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.0589 - acc: 0.9832 - val_loss: 0.0545 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.0568 - acc: 0.9832 - val_loss: 0.0333 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.0611 - acc: 0.9916 - val_loss: 0.0778 - val_acc: 0.9286\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.0758 - acc: 0.9832 - val_loss: 0.0428 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.0609 - acc: 0.9916 - val_loss: 0.0339 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.0607 - acc: 0.9832 - val_loss: 0.0713 - val_acc: 0.9286\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,History\n",
    "from keras.models import Model\n",
    "import keras\n",
    "history = History()\n",
    "\n",
    "\n",
    "input_1 = Input(shape=(1,))\n",
    "input_2 = Input(shape=(1,))\n",
    "input_3 = Input(shape=(1,))\n",
    "input_4 = Input(shape=(1,))\n",
    "input_5 = Input(shape=(1,))\n",
    "input_6 = Input(shape=(1,))\n",
    "input_7 = Input(shape=(1,))\n",
    "input_8 = Input(shape=(1,))\n",
    "input_9 = Input(shape=(1,))\n",
    "input_10 = Input(shape=(1,))\n",
    "input_11 = Input(shape=(1,))\n",
    "input_12= Input(shape=(1,))\n",
    "input_13= Input(shape=(1,))\n",
    "\n",
    "\n",
    "\n",
    "hidden_1= Dense(32, activation='sigmoid')(input_1)\n",
    "hidden_2= Dense(32, activation='sigmoid')(input_2)\n",
    "hidden_3 = Dense(32, activation='sigmoid')(input_3)\n",
    "hidden_4 = Dense(32, activation='sigmoid')(input_4)\n",
    "hidden_5 = Dense(32, activation='sigmoid')(input_5)\n",
    "hidden_6 = Dense(32, activation='sigmoid')(input_6)\n",
    "hidden_7 = Dense(32, activation='sigmoid')(input_7)\n",
    "hidden_8 = Dense(32, activation='sigmoid')(input_8)\n",
    "hidden_9= Dense(32, activation='sigmoid')(input_9)\n",
    "hidden_10= Dense(32, activation='sigmoid')(input_10)\n",
    "hidden_11 = Dense(32, activation='sigmoid')(input_11)\n",
    "hidden_12 = Dense(32, activation='sigmoid')(input_12)\n",
    "hidden_13 = Dense(32, activation='sigmoid')(input_13)\n",
    "\n",
    "\n",
    "value_list=[X_train[['Alcohol']].values,\n",
    "            X_train[['Malic acid']].values,\n",
    "            X_train[['Ash']].values,\n",
    "            X_train[['Alcalinity of ash']].values,\n",
    "            X_train[['Magnesium']].values,\n",
    "            X_train[['Total phenols']].values,\n",
    "            X_train[['Flavanoids']].values,\n",
    "            X_train[['Nonflavanoid phenols']].values,\n",
    "            X_train[['Proanthocyanins']].values,\n",
    "            X_train[['Color intensity']].values,\n",
    "            X_train[['Hue']].values,\n",
    "            X_train[['OD280/OD315 of diluted wines']].values,\n",
    "            X_train[['Proline']].values    \n",
    "           ]\n",
    "\n",
    "value_list_test=[X_test[['Alcohol']].values,\n",
    "                X_test[['Malic acid']].values,\n",
    "                X_test[['Ash']].values,\n",
    "                X_test[['Alcalinity of ash']].values,\n",
    "                X_test[['Magnesium']].values,\n",
    "                X_test[['Total phenols']].values,\n",
    "                X_test[['Flavanoids']].values,\n",
    "                X_test[['Nonflavanoid phenols']].values,\n",
    "                X_test[['Proanthocyanins']].values,\n",
    "                X_test[['Color intensity']].values,\n",
    "                X_test[['Hue']].values,\n",
    "                X_test[['OD280/OD315 of diluted wines']].values,\n",
    "                X_test[['Proline']].values\n",
    "           ]\n",
    "\n",
    "\n",
    "x = keras.layers.concatenate([hidden_1,hidden_2,hidden_3,hidden_4,hidden_5,hidden_6,hidden_7,hidden_8,\n",
    "                             hidden_9,hidden_10,hidden_11,hidden_12,hidden_13])\n",
    "\n",
    "x = Dense(96, activation='sigmoid')(x)\n",
    "output = Dense(len(np.unique(Y_train)), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[input_1,input_2,input_3,input_4,input_5,input_6,input_7,input_8,input_9,\n",
    "                      input_10,input_11,input_12,input_13], outputs=[output])\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "hist=model.fit(\n",
    "    # Feature matrix\n",
    "    value_list, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    "    batch_size=32, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.model\")\n",
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in model.predict(value_list_test).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  1  0]\n",
      " [ 2 19  2]\n",
      " [ 0  0  9]]\n",
      "88.8888888889\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.86      0.92      0.89        13\n",
      "          2       0.95      0.83      0.88        23\n",
      "          3       0.82      1.00      0.90         9\n",
      "\n",
      "avg / total       0.90      0.89      0.89        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranom Forest         96.21 (+/-) 5.12 \n"
     ]
    }
   ],
   "source": [
    "model=RandomForestClassifier(n_estimators=5)\n",
    "kfold = KFold(n_splits=10, random_state=0)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "results=[\"Ranom Forest\",cv_result.mean(),cv_result.std()]\n",
    "\n",
    "print('{:20s} {:2.2f} (+/-) {:2.2f} '.format(results[0] , results[1] * 100, results[2] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best accuracy : ', 0.99248120300751874)\n",
      "('Best parameters :', {'max_features': 'log2', 'n_estimators': 100, 'max_depth': 5})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "paramaters = [\n",
    "             {'n_estimators' : [100, 200, 300, 500, 1000], \n",
    "              'max_depth':[3,4,5,6],\n",
    "              'max_features':['auto','log2']\n",
    "             }                                       \n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train,Y_train)\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 21  2]\n",
      " [ 0  0  9]]\n",
      "95.5555555556\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       1.00      0.91      0.95        23\n",
      "          3       0.82      1.00      0.90         9\n",
      "\n",
      "avg / total       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=100,max_features='log2',bootstrap=True,oob_score=True,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 21  2]\n",
      " [ 0  0  9]]\n",
      "95.5555555556\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       1.00      0.91      0.95        23\n",
      "          3       0.82      1.00      0.90         9\n",
      "\n",
      "avg / total       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=100,max_features=None,bootstrap=True,oob_score=True,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  1  0]\n",
      " [ 0 21  2]\n",
      " [ 0  1  8]]\n",
      "91.1111111111\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       0.91      0.91      0.91        23\n",
      "          3       0.80      0.89      0.84         9\n",
      "\n",
      "avg / total       0.92      0.91      0.91        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=1,max_features=None,bootstrap=False,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ranodm forest by xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# model=XGBClassifier(num_boost_round=1,subsample=0.632)\n",
    "\n",
    "# paramaters = {\n",
    "#     'max_depth': [2, 3,4,5],\n",
    "#     'n_estimators':  [100,300,500,1000],\n",
    "#     'colsample_bytree':[np.log2(len(X_train.columns))/len(X_train.columns),\n",
    "#                         np.sqrt(len(X_train.columns))/len(X_train.columns)]\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator = model, \n",
    "#                            param_grid = paramaters,\n",
    "#                            scoring = 'accuracy',\n",
    "#                            cv = 10,\n",
    "#                            n_jobs = -1)\n",
    "# grid_search = grid_search.fit(X_train, Y_train)\n",
    "# best_accuracy = grid_search.best_score_ \n",
    "# best_parameters = grid_search.best_params_  \n",
    "\n",
    "# print('Best accuracy : ', grid_search.best_score_)\n",
    "# print('Best parameters :', grid_search.best_params_  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 1 20  2]\n",
      " [ 0  0  9]]\n",
      "93.3333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.93      1.00      0.96        13\n",
      "          2       1.00      0.87      0.93        23\n",
      "          3       0.82      1.00      0.90         9\n",
      "\n",
      "avg / total       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=1000,num_boost_round=1,max_depth=2,subsample=0.632,colsample_bytree=0.2846492090877763)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 1 20  2]\n",
      " [ 0  0  9]]\n",
      "93.3333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.93      1.00      0.96        13\n",
      "          2       1.00      0.87      0.93        23\n",
      "          3       0.82      1.00      0.90         9\n",
      "\n",
      "avg / total       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=100,num_boost_round=1,max_depth=2,subsample=0.632,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  1  0]\n",
      " [ 3 18  2]\n",
      " [ 0  1  8]]\n",
      "84.4444444444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.80      0.92      0.86        13\n",
      "          2       0.90      0.78      0.84        23\n",
      "          3       0.80      0.89      0.84         9\n",
      "\n",
      "avg / total       0.85      0.84      0.84        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=1,num_boost_round=1,max_depth=3,subsample=1,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'class', u'Alcohol', u'Malic acid', u'Ash', u'Alcalinity of ash',\n",
       "       u'Magnesium', u'Total phenols', u'Flavanoids', u'Nonflavanoid phenols',\n",
       "       u'Proanthocyanins', u'Color intensity', u'Hue',\n",
       "       u'OD280/OD315 of diluted wines', u'Proline'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Neural network ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "train['Alcohol'] = scaler.fit_transform(train['Alcohol'])\n",
    "train['Malic acid'] = scaler.fit_transform(train['Malic acid'])\n",
    "train['Ash'] = scaler.fit_transform(train['Ash'])\n",
    "train['Alcalinity of ash'] = scaler.fit_transform(train['Alcalinity of ash'])\n",
    "train['Magnesium'] = scaler.fit_transform(train['Magnesium'])\n",
    "train['Total phenols'] = scaler.fit_transform(train['Total phenols'])\n",
    "train['Flavanoids'] = scaler.fit_transform(train['Flavanoids'])\n",
    "train['Nonflavanoid phenols'] = scaler.fit_transform(train['Nonflavanoid phenols'])\n",
    "train['Hue'] = scaler.fit_transform(train['Hue'])\n",
    "train['Color intensity'] = scaler.fit_transform(train['Color intensity'])\n",
    "train['Proline'] = scaler.fit_transform(train['Proline'])\n",
    "train['Nonflavanoid phenols'] = scaler.fit_transform(train['Nonflavanoid phenols'])\n",
    "train['OD280/OD315 of diluted wines'] = scaler.fit_transform(train['OD280/OD315 of diluted wines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train,data_val=train_test_split(train,test_size=0.25, random_state=10)\n",
    "X_val=data_val.drop(['class'], axis=1).values\n",
    "y_val=data_val['class'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_nn_simple(data_train,X_val,y_val):\n",
    "    \n",
    "\n",
    "    data_train_new=data_train.sample(frac=0.632,replace=True)\n",
    "    X_train=data_train_new.drop(['class'], axis=1).values\n",
    "    y_train=data_train_new['class'].ravel()\n",
    "    \n",
    "    m = Sequential()\n",
    "    m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "    \n",
    "    m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    m.fit(\n",
    "    # Feature matrix\n",
    "    X_train, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=256, \n",
    "    )\n",
    "    m.load_weights(\"best.model\")\n",
    "    mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    "    )\n",
    "    y_pred = [mapping[pred] for pred in m.predict(X_val).argmax(axis=1)]\n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.42163, saving model to best.model\n",
      "0s - loss: 1.6039 - acc: 0.3146 - val_loss: 1.4216 - val_acc: 0.1739\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.42163 to 1.24904, saving model to best.model\n",
      "0s - loss: 1.3856 - acc: 0.3146 - val_loss: 1.2490 - val_acc: 0.1739\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.24904 to 1.12265, saving model to best.model\n",
      "0s - loss: 1.3300 - acc: 0.3146 - val_loss: 1.1227 - val_acc: 0.1739\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.12265 to 1.04137, saving model to best.model\n",
      "0s - loss: 1.3690 - acc: 0.2697 - val_loss: 1.0414 - val_acc: 0.6957\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.04137 to 0.98904, saving model to best.model\n",
      "0s - loss: 1.1712 - acc: 0.4382 - val_loss: 0.9890 - val_acc: 0.6957\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.98904 to 0.95584, saving model to best.model\n",
      "0s - loss: 1.3069 - acc: 0.3483 - val_loss: 0.9558 - val_acc: 0.6957\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.95584 to 0.93783, saving model to best.model\n",
      "0s - loss: 1.2291 - acc: 0.4382 - val_loss: 0.9378 - val_acc: 0.6957\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.93783 to 0.92781, saving model to best.model\n",
      "0s - loss: 1.3252 - acc: 0.3258 - val_loss: 0.9278 - val_acc: 0.6957\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.92781 to 0.91848, saving model to best.model\n",
      "0s - loss: 1.2680 - acc: 0.4157 - val_loss: 0.9185 - val_acc: 0.6957\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.91848 to 0.91295, saving model to best.model\n",
      "0s - loss: 1.2739 - acc: 0.3596 - val_loss: 0.9129 - val_acc: 0.6957\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.91295 to 0.90817, saving model to best.model\n",
      "0s - loss: 1.3133 - acc: 0.3708 - val_loss: 0.9082 - val_acc: 0.6957\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2248 - acc: 0.4382 - val_loss: 0.9112 - val_acc: 0.6957\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2700 - acc: 0.3371 - val_loss: 0.9181 - val_acc: 0.6957\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.3052 - acc: 0.2921 - val_loss: 0.9265 - val_acc: 0.6957\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2633 - acc: 0.3596 - val_loss: 0.9384 - val_acc: 0.6957\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2860 - acc: 0.3034 - val_loss: 0.9527 - val_acc: 0.6957\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.3224 - acc: 0.3371 - val_loss: 0.9683 - val_acc: 0.6957\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2822 - acc: 0.2921 - val_loss: 0.9880 - val_acc: 0.6957\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2098 - acc: 0.4157 - val_loss: 1.0081 - val_acc: 0.6957\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.3477 - acc: 0.2809 - val_loss: 1.0257 - val_acc: 0.8261\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1360 - acc: 0.3933 - val_loss: 1.0402 - val_acc: 0.2174\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1624 - acc: 0.3933 - val_loss: 1.0499 - val_acc: 0.1739\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2695 - acc: 0.3933 - val_loss: 1.0544 - val_acc: 0.1739\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1734 - acc: 0.4157 - val_loss: 1.0569 - val_acc: 0.1739\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.2645 - acc: 0.3820 - val_loss: 1.0566 - val_acc: 0.1739\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.2789 - acc: 0.3483 - val_loss: 1.0536 - val_acc: 0.1739\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.2860 - acc: 0.2247 - val_loss: 1.0485 - val_acc: 0.1739\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1467 - acc: 0.4157 - val_loss: 1.0427 - val_acc: 0.3043\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1739 - acc: 0.4045 - val_loss: 1.0354 - val_acc: 0.7826\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1808 - acc: 0.3820 - val_loss: 1.0256 - val_acc: 0.8261\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1375 - acc: 0.4494 - val_loss: 1.0140 - val_acc: 0.8696\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.0934 - acc: 0.4270 - val_loss: 0.9978 - val_acc: 0.6957\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.0952 - acc: 0.4719 - val_loss: 0.9811 - val_acc: 0.6957\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1897 - acc: 0.3371 - val_loss: 0.9634 - val_acc: 0.6957\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1556 - acc: 0.3933 - val_loss: 0.9476 - val_acc: 0.6957\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.1627 - acc: 0.3933 - val_loss: 0.9352 - val_acc: 0.6957\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 1.1951 - acc: 0.3820 - val_loss: 0.9232 - val_acc: 0.6957\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.14499, saving model to best.model\n",
      "0s - loss: 1.4232 - acc: 0.2697 - val_loss: 1.1450 - val_acc: 0.2174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.14499 to 1.08717, saving model to best.model\n",
      "0s - loss: 1.3339 - acc: 0.3146 - val_loss: 1.0872 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.08717 to 1.05316, saving model to best.model\n",
      "0s - loss: 1.3373 - acc: 0.2697 - val_loss: 1.0532 - val_acc: 0.5217\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.05316 to 1.03358, saving model to best.model\n",
      "0s - loss: 1.2302 - acc: 0.3708 - val_loss: 1.0336 - val_acc: 0.5217\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.03358 to 1.02522, saving model to best.model\n",
      "0s - loss: 1.0742 - acc: 0.4494 - val_loss: 1.0252 - val_acc: 0.5217\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.02522 to 1.02434, saving model to best.model\n",
      "0s - loss: 1.1884 - acc: 0.4157 - val_loss: 1.0243 - val_acc: 0.5217\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.02434 to 1.02428, saving model to best.model\n",
      "0s - loss: 1.3297 - acc: 0.2809 - val_loss: 1.0243 - val_acc: 0.5217\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.02428 to 1.02420, saving model to best.model\n",
      "0s - loss: 1.2111 - acc: 0.3933 - val_loss: 1.0242 - val_acc: 0.5217\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.02420 to 1.02293, saving model to best.model\n",
      "0s - loss: 1.1301 - acc: 0.4382 - val_loss: 1.0229 - val_acc: 0.5217\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.02293 to 1.02000, saving model to best.model\n",
      "0s - loss: 1.2864 - acc: 0.3371 - val_loss: 1.0200 - val_acc: 0.5217\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.02000 to 1.01659, saving model to best.model\n",
      "0s - loss: 1.2060 - acc: 0.4494 - val_loss: 1.0166 - val_acc: 0.5217\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.01659 to 1.01288, saving model to best.model\n",
      "0s - loss: 1.2106 - acc: 0.4045 - val_loss: 1.0129 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.01288 to 1.00963, saving model to best.model\n",
      "0s - loss: 1.2474 - acc: 0.3596 - val_loss: 1.0096 - val_acc: 0.5217\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.00963 to 1.00786, saving model to best.model\n",
      "0s - loss: 1.2099 - acc: 0.3708 - val_loss: 1.0079 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.00786 to 1.00771, saving model to best.model\n",
      "0s - loss: 1.1466 - acc: 0.3820 - val_loss: 1.0077 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1935 - acc: 0.3371 - val_loss: 1.0090 - val_acc: 0.5217\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1641 - acc: 0.3483 - val_loss: 1.0112 - val_acc: 0.5217\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.3142 - acc: 0.2809 - val_loss: 1.0141 - val_acc: 0.5217\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2429 - acc: 0.3146 - val_loss: 1.0162 - val_acc: 0.5217\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1210 - acc: 0.3483 - val_loss: 1.0177 - val_acc: 0.5217\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.2290 - acc: 0.3933 - val_loss: 1.0189 - val_acc: 0.5217\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.2078 - acc: 0.4045 - val_loss: 1.0194 - val_acc: 0.5217\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1412 - acc: 0.4157 - val_loss: 1.0183 - val_acc: 0.5217\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1594 - acc: 0.3708 - val_loss: 1.0163 - val_acc: 0.5217\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1737 - acc: 0.4045 - val_loss: 1.0133 - val_acc: 0.5217\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.0937 - acc: 0.4270 - val_loss: 1.0110 - val_acc: 0.5217\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.0540 - acc: 0.4270 - val_loss: 1.0077 - val_acc: 0.5217\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.00771 to 1.00451, saving model to best.model\n",
      "0s - loss: 1.1362 - acc: 0.3933 - val_loss: 1.0045 - val_acc: 0.5217\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.00451 to 1.00042, saving model to best.model\n",
      "0s - loss: 1.1753 - acc: 0.3820 - val_loss: 1.0004 - val_acc: 0.5217\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.00042 to 0.99646, saving model to best.model\n",
      "0s - loss: 1.1467 - acc: 0.3708 - val_loss: 0.9965 - val_acc: 0.5217\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.99646 to 0.99208, saving model to best.model\n",
      "0s - loss: 1.2273 - acc: 0.2921 - val_loss: 0.9921 - val_acc: 0.5217\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.99208 to 0.98805, saving model to best.model\n",
      "0s - loss: 1.1470 - acc: 0.3933 - val_loss: 0.9881 - val_acc: 0.5217\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.98805 to 0.98432, saving model to best.model\n",
      "0s - loss: 1.1229 - acc: 0.4719 - val_loss: 0.9843 - val_acc: 0.5217\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.98432 to 0.98096, saving model to best.model\n",
      "0s - loss: 1.1527 - acc: 0.4045 - val_loss: 0.9810 - val_acc: 0.5217\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.98096 to 0.97778, saving model to best.model\n",
      "0s - loss: 1.1913 - acc: 0.3034 - val_loss: 0.9778 - val_acc: 0.5217\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.97778 to 0.97476, saving model to best.model\n",
      "0s - loss: 1.0945 - acc: 0.4157 - val_loss: 0.9748 - val_acc: 0.5217\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.97476 to 0.97211, saving model to best.model\n",
      "0s - loss: 1.2291 - acc: 0.2247 - val_loss: 0.9721 - val_acc: 0.5217\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.97211 to 0.96996, saving model to best.model\n",
      "0s - loss: 1.1197 - acc: 0.4270 - val_loss: 0.9700 - val_acc: 0.5217\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.96996 to 0.96788, saving model to best.model\n",
      "0s - loss: 1.1382 - acc: 0.4045 - val_loss: 0.9679 - val_acc: 0.5217\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.96788 to 0.96581, saving model to best.model\n",
      "0s - loss: 1.0239 - acc: 0.4719 - val_loss: 0.9658 - val_acc: 0.5217\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.96581 to 0.96354, saving model to best.model\n",
      "0s - loss: 1.1080 - acc: 0.4607 - val_loss: 0.9635 - val_acc: 0.5217\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.96354 to 0.96138, saving model to best.model\n",
      "0s - loss: 1.1173 - acc: 0.3933 - val_loss: 0.9614 - val_acc: 0.5217\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.96138 to 0.95934, saving model to best.model\n",
      "0s - loss: 1.0922 - acc: 0.4607 - val_loss: 0.9593 - val_acc: 0.5217\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.95934 to 0.95727, saving model to best.model\n",
      "0s - loss: 1.0535 - acc: 0.4157 - val_loss: 0.9573 - val_acc: 0.5217\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.95727 to 0.95536, saving model to best.model\n",
      "0s - loss: 1.0664 - acc: 0.4157 - val_loss: 0.9554 - val_acc: 0.5217\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.95536 to 0.95269, saving model to best.model\n",
      "0s - loss: 1.0646 - acc: 0.4270 - val_loss: 0.9527 - val_acc: 0.5217\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.95269 to 0.94950, saving model to best.model\n",
      "0s - loss: 1.0961 - acc: 0.4382 - val_loss: 0.9495 - val_acc: 0.5217\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.94950 to 0.94596, saving model to best.model\n",
      "0s - loss: 1.1363 - acc: 0.3820 - val_loss: 0.9460 - val_acc: 0.5217\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.94596 to 0.94248, saving model to best.model\n",
      "0s - loss: 1.0079 - acc: 0.4607 - val_loss: 0.9425 - val_acc: 0.5217\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.94248 to 0.93875, saving model to best.model\n",
      "0s - loss: 1.0535 - acc: 0.4831 - val_loss: 0.9387 - val_acc: 0.5217\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.93875 to 0.93526, saving model to best.model\n",
      "0s - loss: 1.0331 - acc: 0.4719 - val_loss: 0.9353 - val_acc: 0.5217\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.93526 to 0.93229, saving model to best.model\n",
      "0s - loss: 1.0628 - acc: 0.4382 - val_loss: 0.9323 - val_acc: 0.5217\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.93229 to 0.92938, saving model to best.model\n",
      "0s - loss: 1.0754 - acc: 0.4270 - val_loss: 0.9294 - val_acc: 0.5217\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.92938 to 0.92644, saving model to best.model\n",
      "0s - loss: 1.0977 - acc: 0.4157 - val_loss: 0.9264 - val_acc: 0.5217\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.92644 to 0.92354, saving model to best.model\n",
      "0s - loss: 1.0947 - acc: 0.4382 - val_loss: 0.9235 - val_acc: 0.5217\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.92354 to 0.92063, saving model to best.model\n",
      "0s - loss: 1.0489 - acc: 0.4382 - val_loss: 0.9206 - val_acc: 0.5217\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.92063 to 0.91709, saving model to best.model\n",
      "0s - loss: 1.0065 - acc: 0.4944 - val_loss: 0.9171 - val_acc: 0.5217\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.91709 to 0.91299, saving model to best.model\n",
      "0s - loss: 0.9783 - acc: 0.4831 - val_loss: 0.9130 - val_acc: 0.5217\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.91299 to 0.90850, saving model to best.model\n",
      "0s - loss: 1.0570 - acc: 0.5056 - val_loss: 0.9085 - val_acc: 0.5217\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.90850 to 0.90344, saving model to best.model\n",
      "0s - loss: 1.0114 - acc: 0.4831 - val_loss: 0.9034 - val_acc: 0.5217\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.90344 to 0.89782, saving model to best.model\n",
      "0s - loss: 1.0282 - acc: 0.4831 - val_loss: 0.8978 - val_acc: 0.5217\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.89782 to 0.89229, saving model to best.model\n",
      "0s - loss: 1.0270 - acc: 0.5281 - val_loss: 0.8923 - val_acc: 0.5652\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.89229 to 0.88591, saving model to best.model\n",
      "0s - loss: 1.0500 - acc: 0.4045 - val_loss: 0.8859 - val_acc: 0.5652\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.88591 to 0.87946, saving model to best.model\n",
      "0s - loss: 0.9472 - acc: 0.5730 - val_loss: 0.8795 - val_acc: 0.5652\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.87946 to 0.87262, saving model to best.model\n",
      "0s - loss: 1.0438 - acc: 0.4944 - val_loss: 0.8726 - val_acc: 0.5652\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.87262 to 0.86606, saving model to best.model\n",
      "0s - loss: 0.9596 - acc: 0.4831 - val_loss: 0.8661 - val_acc: 0.6087\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.86606 to 0.85986, saving model to best.model\n",
      "0s - loss: 1.0813 - acc: 0.4157 - val_loss: 0.8599 - val_acc: 0.6087\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.85986 to 0.85346, saving model to best.model\n",
      "0s - loss: 1.0653 - acc: 0.4719 - val_loss: 0.8535 - val_acc: 0.6087\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.85346 to 0.84654, saving model to best.model\n",
      "0s - loss: 0.9619 - acc: 0.5281 - val_loss: 0.8465 - val_acc: 0.6522\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.84654 to 0.83964, saving model to best.model\n",
      "0s - loss: 0.9531 - acc: 0.6067 - val_loss: 0.8396 - val_acc: 0.6522\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.83964 to 0.83273, saving model to best.model\n",
      "0s - loss: 1.0064 - acc: 0.4494 - val_loss: 0.8327 - val_acc: 0.6957\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.83273 to 0.82521, saving model to best.model\n",
      "0s - loss: 0.9456 - acc: 0.5730 - val_loss: 0.8252 - val_acc: 0.6957\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.82521 to 0.81791, saving model to best.model\n",
      "0s - loss: 0.9632 - acc: 0.4719 - val_loss: 0.8179 - val_acc: 0.6957\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.81791 to 0.81073, saving model to best.model\n",
      "0s - loss: 0.9433 - acc: 0.5843 - val_loss: 0.8107 - val_acc: 0.6957\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.81073 to 0.80343, saving model to best.model\n",
      "0s - loss: 0.9358 - acc: 0.5618 - val_loss: 0.8034 - val_acc: 0.7391\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.80343 to 0.79596, saving model to best.model\n",
      "0s - loss: 0.9168 - acc: 0.5955 - val_loss: 0.7960 - val_acc: 0.7391\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.79596 to 0.78847, saving model to best.model\n",
      "0s - loss: 1.0040 - acc: 0.4719 - val_loss: 0.7885 - val_acc: 0.7391\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.78847 to 0.78060, saving model to best.model\n",
      "0s - loss: 0.8771 - acc: 0.6067 - val_loss: 0.7806 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.78060 to 0.77270, saving model to best.model\n",
      "0s - loss: 0.9365 - acc: 0.5393 - val_loss: 0.7727 - val_acc: 0.8696\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.77270 to 0.76420, saving model to best.model\n",
      "0s - loss: 0.8376 - acc: 0.6180 - val_loss: 0.7642 - val_acc: 0.8696\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.76420 to 0.75533, saving model to best.model\n",
      "0s - loss: 0.8843 - acc: 0.5618 - val_loss: 0.7553 - val_acc: 0.8696\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.75533 to 0.74635, saving model to best.model\n",
      "0s - loss: 0.8335 - acc: 0.6629 - val_loss: 0.7463 - val_acc: 0.9130\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.74635 to 0.73686, saving model to best.model\n",
      "0s - loss: 0.8801 - acc: 0.6292 - val_loss: 0.7369 - val_acc: 0.9130\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.73686 to 0.72713, saving model to best.model\n",
      "0s - loss: 0.8345 - acc: 0.6742 - val_loss: 0.7271 - val_acc: 0.9130\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.72713 to 0.71787, saving model to best.model\n",
      "0s - loss: 0.8980 - acc: 0.5618 - val_loss: 0.7179 - val_acc: 0.9130\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.71787 to 0.70896, saving model to best.model\n",
      "0s - loss: 0.8917 - acc: 0.5618 - val_loss: 0.7090 - val_acc: 0.9565\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.70896 to 0.69979, saving model to best.model\n",
      "0s - loss: 0.8059 - acc: 0.6404 - val_loss: 0.6998 - val_acc: 1.0000\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.69979 to 0.69044, saving model to best.model\n",
      "0s - loss: 0.8363 - acc: 0.6292 - val_loss: 0.6904 - val_acc: 1.0000\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.69044 to 0.68055, saving model to best.model\n",
      "0s - loss: 0.7951 - acc: 0.6404 - val_loss: 0.6806 - val_acc: 1.0000\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68055 to 0.67025, saving model to best.model\n",
      "0s - loss: 0.8018 - acc: 0.6292 - val_loss: 0.6703 - val_acc: 1.0000\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67025 to 0.65945, saving model to best.model\n",
      "0s - loss: 0.8290 - acc: 0.6180 - val_loss: 0.6594 - val_acc: 1.0000\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.65945 to 0.64791, saving model to best.model\n",
      "0s - loss: 0.7875 - acc: 0.6404 - val_loss: 0.6479 - val_acc: 1.0000\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.64791 to 0.63684, saving model to best.model\n",
      "0s - loss: 0.7616 - acc: 0.6404 - val_loss: 0.6368 - val_acc: 1.0000\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.63684 to 0.62568, saving model to best.model\n",
      "0s - loss: 0.7596 - acc: 0.6517 - val_loss: 0.6257 - val_acc: 1.0000\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.62568 to 0.61425, saving model to best.model\n",
      "0s - loss: 0.7385 - acc: 0.6517 - val_loss: 0.6142 - val_acc: 1.0000\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.61425 to 0.60318, saving model to best.model\n",
      "0s - loss: 0.7387 - acc: 0.6742 - val_loss: 0.6032 - val_acc: 1.0000\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.60318 to 0.59188, saving model to best.model\n",
      "0s - loss: 0.7736 - acc: 0.6742 - val_loss: 0.5919 - val_acc: 1.0000\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.59188 to 0.58103, saving model to best.model\n",
      "0s - loss: 0.7041 - acc: 0.7191 - val_loss: 0.5810 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.58103 to 0.57026, saving model to best.model\n",
      "0s - loss: 0.7707 - acc: 0.6404 - val_loss: 0.5703 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.57026 to 0.55949, saving model to best.model\n",
      "0s - loss: 0.8257 - acc: 0.6404 - val_loss: 0.5595 - val_acc: 1.0000\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.55949 to 0.54858, saving model to best.model\n",
      "0s - loss: 0.6771 - acc: 0.7640 - val_loss: 0.5486 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.54858 to 0.53765, saving model to best.model\n",
      "0s - loss: 0.7087 - acc: 0.6854 - val_loss: 0.5377 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.53765 to 0.52580, saving model to best.model\n",
      "0s - loss: 0.6602 - acc: 0.7191 - val_loss: 0.5258 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.52580 to 0.51437, saving model to best.model\n",
      "0s - loss: 0.6936 - acc: 0.6854 - val_loss: 0.5144 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.51437 to 0.50313, saving model to best.model\n",
      "0s - loss: 0.6458 - acc: 0.7303 - val_loss: 0.5031 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.50313 to 0.49195, saving model to best.model\n",
      "0s - loss: 0.6647 - acc: 0.6966 - val_loss: 0.4919 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.49195 to 0.48043, saving model to best.model\n",
      "0s - loss: 0.6384 - acc: 0.7303 - val_loss: 0.4804 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.48043 to 0.46994, saving model to best.model\n",
      "0s - loss: 0.6805 - acc: 0.6517 - val_loss: 0.4699 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.46994 to 0.46012, saving model to best.model\n",
      "0s - loss: 0.5887 - acc: 0.7753 - val_loss: 0.4601 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.46012 to 0.45008, saving model to best.model\n",
      "0s - loss: 0.6187 - acc: 0.7528 - val_loss: 0.4501 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.45008 to 0.44017, saving model to best.model\n",
      "0s - loss: 0.5915 - acc: 0.7865 - val_loss: 0.4402 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.44017 to 0.43063, saving model to best.model\n",
      "0s - loss: 0.6613 - acc: 0.6854 - val_loss: 0.4306 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.43063 to 0.42044, saving model to best.model\n",
      "0s - loss: 0.5764 - acc: 0.8202 - val_loss: 0.4204 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.42044 to 0.41080, saving model to best.model\n",
      "0s - loss: 0.6055 - acc: 0.7528 - val_loss: 0.4108 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.41080 to 0.40140, saving model to best.model\n",
      "0s - loss: 0.5443 - acc: 0.8090 - val_loss: 0.4014 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.40140 to 0.39210, saving model to best.model\n",
      "0s - loss: 0.6087 - acc: 0.7528 - val_loss: 0.3921 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.39210 to 0.38395, saving model to best.model\n",
      "0s - loss: 0.6022 - acc: 0.7079 - val_loss: 0.3839 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.38395 to 0.37568, saving model to best.model\n",
      "0s - loss: 0.5161 - acc: 0.8315 - val_loss: 0.3757 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.37568 to 0.36694, saving model to best.model\n",
      "0s - loss: 0.5639 - acc: 0.7753 - val_loss: 0.3669 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.36694 to 0.35811, saving model to best.model\n",
      "0s - loss: 0.4939 - acc: 0.8090 - val_loss: 0.3581 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.35811 to 0.34883, saving model to best.model\n",
      "0s - loss: 0.5476 - acc: 0.7528 - val_loss: 0.3488 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.34883 to 0.34011, saving model to best.model\n",
      "0s - loss: 0.4865 - acc: 0.8652 - val_loss: 0.3401 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.34011 to 0.33050, saving model to best.model\n",
      "0s - loss: 0.5073 - acc: 0.7978 - val_loss: 0.3305 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.33050 to 0.32192, saving model to best.model\n",
      "0s - loss: 0.5330 - acc: 0.7978 - val_loss: 0.3219 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.32192 to 0.31325, saving model to best.model\n",
      "0s - loss: 0.5055 - acc: 0.7865 - val_loss: 0.3132 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.31325 to 0.30531, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7865 - val_loss: 0.3053 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.30531 to 0.29726, saving model to best.model\n",
      "0s - loss: 0.4636 - acc: 0.8202 - val_loss: 0.2973 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.29726 to 0.29003, saving model to best.model\n",
      "0s - loss: 0.4882 - acc: 0.8090 - val_loss: 0.2900 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.29003 to 0.28298, saving model to best.model\n",
      "0s - loss: 0.4298 - acc: 0.8652 - val_loss: 0.2830 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.28298 to 0.27594, saving model to best.model\n",
      "0s - loss: 0.4639 - acc: 0.8202 - val_loss: 0.2759 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.27594 to 0.27038, saving model to best.model\n",
      "0s - loss: 0.4500 - acc: 0.8315 - val_loss: 0.2704 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.27038 to 0.26417, saving model to best.model\n",
      "0s - loss: 0.4264 - acc: 0.8315 - val_loss: 0.2642 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.26417 to 0.25742, saving model to best.model\n",
      "0s - loss: 0.4551 - acc: 0.8090 - val_loss: 0.2574 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.25742 to 0.25043, saving model to best.model\n",
      "0s - loss: 0.4698 - acc: 0.8090 - val_loss: 0.2504 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.25043 to 0.24338, saving model to best.model\n",
      "0s - loss: 0.3982 - acc: 0.8202 - val_loss: 0.2434 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.24338 to 0.23578, saving model to best.model\n",
      "0s - loss: 0.4091 - acc: 0.8652 - val_loss: 0.2358 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.23578 to 0.22752, saving model to best.model\n",
      "0s - loss: 0.4102 - acc: 0.8652 - val_loss: 0.2275 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.22752 to 0.21955, saving model to best.model\n",
      "0s - loss: 0.3915 - acc: 0.8764 - val_loss: 0.2195 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.21955 to 0.21230, saving model to best.model\n",
      "0s - loss: 0.3924 - acc: 0.8652 - val_loss: 0.2123 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.21230 to 0.20554, saving model to best.model\n",
      "0s - loss: 0.3840 - acc: 0.8764 - val_loss: 0.2055 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.20554 to 0.19951, saving model to best.model\n",
      "0s - loss: 0.4326 - acc: 0.8427 - val_loss: 0.1995 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.19951 to 0.19372, saving model to best.model\n",
      "0s - loss: 0.2954 - acc: 0.8989 - val_loss: 0.1937 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.19372 to 0.18813, saving model to best.model\n",
      "0s - loss: 0.3600 - acc: 0.8652 - val_loss: 0.1881 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.18813 to 0.18363, saving model to best.model\n",
      "0s - loss: 0.4063 - acc: 0.8764 - val_loss: 0.1836 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.18363 to 0.17974, saving model to best.model\n",
      "0s - loss: 0.3315 - acc: 0.8764 - val_loss: 0.1797 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.17974 to 0.17582, saving model to best.model\n",
      "0s - loss: 0.3886 - acc: 0.8539 - val_loss: 0.1758 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.17582 to 0.17118, saving model to best.model\n",
      "0s - loss: 0.3297 - acc: 0.9213 - val_loss: 0.1712 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.17118 to 0.16656, saving model to best.model\n",
      "0s - loss: 0.4199 - acc: 0.8315 - val_loss: 0.1666 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.16656 to 0.16216, saving model to best.model\n",
      "0s - loss: 0.3059 - acc: 0.8989 - val_loss: 0.1622 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.16216 to 0.15867, saving model to best.model\n",
      "0s - loss: 0.4045 - acc: 0.8539 - val_loss: 0.1587 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.15867 to 0.15547, saving model to best.model\n",
      "0s - loss: 0.3682 - acc: 0.8652 - val_loss: 0.1555 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.15547 to 0.15175, saving model to best.model\n",
      "0s - loss: 0.2808 - acc: 0.8876 - val_loss: 0.1517 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.15175 to 0.14794, saving model to best.model\n",
      "0s - loss: 0.2280 - acc: 0.9551 - val_loss: 0.1479 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.14794 to 0.14368, saving model to best.model\n",
      "0s - loss: 0.3228 - acc: 0.8989 - val_loss: 0.1437 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.14368 to 0.13888, saving model to best.model\n",
      "0s - loss: 0.2928 - acc: 0.9213 - val_loss: 0.1389 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.13888 to 0.13426, saving model to best.model\n",
      "0s - loss: 0.2894 - acc: 0.8989 - val_loss: 0.1343 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.13426 to 0.12957, saving model to best.model\n",
      "0s - loss: 0.3347 - acc: 0.9213 - val_loss: 0.1296 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.12957 to 0.12529, saving model to best.model\n",
      "0s - loss: 0.3472 - acc: 0.8764 - val_loss: 0.1253 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.12529 to 0.12147, saving model to best.model\n",
      "0s - loss: 0.2708 - acc: 0.9213 - val_loss: 0.1215 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.12147 to 0.11737, saving model to best.model\n",
      "0s - loss: 0.3516 - acc: 0.8090 - val_loss: 0.1174 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.11737 to 0.11350, saving model to best.model\n",
      "0s - loss: 0.3064 - acc: 0.8876 - val_loss: 0.1135 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.11350 to 0.11007, saving model to best.model\n",
      "0s - loss: 0.2745 - acc: 0.9101 - val_loss: 0.1101 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.11007 to 0.10742, saving model to best.model\n",
      "0s - loss: 0.2532 - acc: 0.9213 - val_loss: 0.1074 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.10742 to 0.10497, saving model to best.model\n",
      "0s - loss: 0.3310 - acc: 0.8764 - val_loss: 0.1050 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.10497 to 0.10232, saving model to best.model\n",
      "0s - loss: 0.2943 - acc: 0.8876 - val_loss: 0.1023 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.10232 to 0.10008, saving model to best.model\n",
      "0s - loss: 0.2467 - acc: 0.9101 - val_loss: 0.1001 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.10008 to 0.09739, saving model to best.model\n",
      "0s - loss: 0.2594 - acc: 0.8989 - val_loss: 0.0974 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.09739 to 0.09449, saving model to best.model\n",
      "0s - loss: 0.2753 - acc: 0.8989 - val_loss: 0.0945 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.09449 to 0.09092, saving model to best.model\n",
      "0s - loss: 0.2244 - acc: 0.9213 - val_loss: 0.0909 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.09092 to 0.08789, saving model to best.model\n",
      "0s - loss: 0.2631 - acc: 0.8876 - val_loss: 0.0879 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.08789 to 0.08496, saving model to best.model\n",
      "0s - loss: 0.2879 - acc: 0.8539 - val_loss: 0.0850 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.08496 to 0.08231, saving model to best.model\n",
      "0s - loss: 0.2507 - acc: 0.9213 - val_loss: 0.0823 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.08231 to 0.07983, saving model to best.model\n",
      "0s - loss: 0.3059 - acc: 0.8764 - val_loss: 0.0798 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.07983 to 0.07728, saving model to best.model\n",
      "0s - loss: 0.2539 - acc: 0.9438 - val_loss: 0.0773 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.07728 to 0.07470, saving model to best.model\n",
      "0s - loss: 0.2592 - acc: 0.8876 - val_loss: 0.0747 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.07470 to 0.07217, saving model to best.model\n",
      "0s - loss: 0.2683 - acc: 0.8876 - val_loss: 0.0722 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.07217 to 0.06989, saving model to best.model\n",
      "0s - loss: 0.2743 - acc: 0.9101 - val_loss: 0.0699 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.06989 to 0.06785, saving model to best.model\n",
      "0s - loss: 0.2152 - acc: 0.9438 - val_loss: 0.0678 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.06785 to 0.06629, saving model to best.model\n",
      "0s - loss: 0.2628 - acc: 0.9101 - val_loss: 0.0663 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.06629 to 0.06483, saving model to best.model\n",
      "0s - loss: 0.2643 - acc: 0.9213 - val_loss: 0.0648 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.06483 to 0.06349, saving model to best.model\n",
      "0s - loss: 0.1971 - acc: 0.9438 - val_loss: 0.0635 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.06349 to 0.06278, saving model to best.model\n",
      "0s - loss: 0.3095 - acc: 0.8876 - val_loss: 0.0628 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.06278 to 0.06210, saving model to best.model\n",
      "0s - loss: 0.2167 - acc: 0.9551 - val_loss: 0.0621 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.06210 to 0.06155, saving model to best.model\n",
      "0s - loss: 0.2651 - acc: 0.9101 - val_loss: 0.0615 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.06155 to 0.06093, saving model to best.model\n",
      "0s - loss: 0.2368 - acc: 0.9213 - val_loss: 0.0609 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.06093 to 0.06010, saving model to best.model\n",
      "0s - loss: 0.1691 - acc: 0.9551 - val_loss: 0.0601 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.06010 to 0.05873, saving model to best.model\n",
      "0s - loss: 0.2536 - acc: 0.8764 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.05873 to 0.05737, saving model to best.model\n",
      "0s - loss: 0.1757 - acc: 0.9326 - val_loss: 0.0574 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.05737 to 0.05562, saving model to best.model\n",
      "0s - loss: 0.1718 - acc: 0.9438 - val_loss: 0.0556 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.05562 to 0.05383, saving model to best.model\n",
      "0s - loss: 0.1479 - acc: 0.9663 - val_loss: 0.0538 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.05383 to 0.05240, saving model to best.model\n",
      "0s - loss: 0.1561 - acc: 0.9663 - val_loss: 0.0524 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.05240 to 0.05113, saving model to best.model\n",
      "0s - loss: 0.1883 - acc: 0.9663 - val_loss: 0.0511 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.05113 to 0.05007, saving model to best.model\n",
      "0s - loss: 0.1740 - acc: 0.9326 - val_loss: 0.0501 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.05007 to 0.04875, saving model to best.model\n",
      "0s - loss: 0.2118 - acc: 0.9551 - val_loss: 0.0488 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.04875 to 0.04721, saving model to best.model\n",
      "0s - loss: 0.2157 - acc: 0.9213 - val_loss: 0.0472 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.04721 to 0.04554, saving model to best.model\n",
      "0s - loss: 0.1990 - acc: 0.9213 - val_loss: 0.0455 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.04554 to 0.04398, saving model to best.model\n",
      "0s - loss: 0.1943 - acc: 0.8989 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.04398 to 0.04251, saving model to best.model\n",
      "0s - loss: 0.1440 - acc: 0.9551 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.04251 to 0.04127, saving model to best.model\n",
      "0s - loss: 0.1976 - acc: 0.9551 - val_loss: 0.0413 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.04127 to 0.04015, saving model to best.model\n",
      "0s - loss: 0.1894 - acc: 0.9326 - val_loss: 0.0401 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.08947, saving model to best.model\n",
      "0s - loss: 1.2637 - acc: 0.3258 - val_loss: 1.0895 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.3315 - acc: 0.3034 - val_loss: 1.0900 - val_acc: 0.3478\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.3735 - acc: 0.2360 - val_loss: 1.0896 - val_acc: 0.3478\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.08947 to 1.08915, saving model to best.model\n",
      "0s - loss: 1.2102 - acc: 0.3371 - val_loss: 1.0892 - val_acc: 0.3478\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.08915 to 1.08848, saving model to best.model\n",
      "0s - loss: 1.2414 - acc: 0.4045 - val_loss: 1.0885 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.08848 to 1.08638, saving model to best.model\n",
      "0s - loss: 1.2723 - acc: 0.3596 - val_loss: 1.0864 - val_acc: 0.6087\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.08638 to 1.08309, saving model to best.model\n",
      "0s - loss: 1.3430 - acc: 0.3371 - val_loss: 1.0831 - val_acc: 0.6087\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.08309 to 1.07920, saving model to best.model\n",
      "0s - loss: 1.2927 - acc: 0.3034 - val_loss: 1.0792 - val_acc: 0.6087\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.07920 to 1.07518, saving model to best.model\n",
      "0s - loss: 1.2304 - acc: 0.3820 - val_loss: 1.0752 - val_acc: 0.6087\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.07518 to 1.07157, saving model to best.model\n",
      "0s - loss: 1.2578 - acc: 0.3596 - val_loss: 1.0716 - val_acc: 0.5652\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.07157 to 1.06861, saving model to best.model\n",
      "0s - loss: 1.1909 - acc: 0.3820 - val_loss: 1.0686 - val_acc: 0.6522\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.06861 to 1.06631, saving model to best.model\n",
      "0s - loss: 1.2167 - acc: 0.4157 - val_loss: 1.0663 - val_acc: 0.6522\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.06631 to 1.06431, saving model to best.model\n",
      "0s - loss: 1.3581 - acc: 0.2921 - val_loss: 1.0643 - val_acc: 0.4348\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.06431 to 1.06244, saving model to best.model\n",
      "0s - loss: 1.1500 - acc: 0.4045 - val_loss: 1.0624 - val_acc: 0.3913\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.06244 to 1.06039, saving model to best.model\n",
      "0s - loss: 1.2190 - acc: 0.3596 - val_loss: 1.0604 - val_acc: 0.3913\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.06039 to 1.05842, saving model to best.model\n",
      "0s - loss: 1.1964 - acc: 0.3483 - val_loss: 1.0584 - val_acc: 0.5217\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.05842 to 1.05633, saving model to best.model\n",
      "0s - loss: 1.2150 - acc: 0.3708 - val_loss: 1.0563 - val_acc: 0.4348\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.05633 to 1.05399, saving model to best.model\n",
      "0s - loss: 1.4187 - acc: 0.2584 - val_loss: 1.0540 - val_acc: 0.5217\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.05399 to 1.05170, saving model to best.model\n",
      "0s - loss: 1.0846 - acc: 0.4719 - val_loss: 1.0517 - val_acc: 0.5217\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.05170 to 1.04939, saving model to best.model\n",
      "0s - loss: 1.1697 - acc: 0.3708 - val_loss: 1.0494 - val_acc: 0.6087\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.04939 to 1.04695, saving model to best.model\n",
      "0s - loss: 1.2055 - acc: 0.3708 - val_loss: 1.0470 - val_acc: 0.6522\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.04695 to 1.04453, saving model to best.model\n",
      "0s - loss: 1.2223 - acc: 0.3371 - val_loss: 1.0445 - val_acc: 0.9565\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.04453 to 1.04232, saving model to best.model\n",
      "0s - loss: 1.1825 - acc: 0.3483 - val_loss: 1.0423 - val_acc: 1.0000\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.04232 to 1.04041, saving model to best.model\n",
      "0s - loss: 1.2595 - acc: 0.2921 - val_loss: 1.0404 - val_acc: 0.8261\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.04041 to 1.03897, saving model to best.model\n",
      "0s - loss: 1.2131 - acc: 0.4157 - val_loss: 1.0390 - val_acc: 0.7391\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.03897 to 1.03769, saving model to best.model\n",
      "0s - loss: 1.1663 - acc: 0.3708 - val_loss: 1.0377 - val_acc: 0.7391\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.03769 to 1.03573, saving model to best.model\n",
      "0s - loss: 1.0920 - acc: 0.4607 - val_loss: 1.0357 - val_acc: 0.7391\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.03573 to 1.03328, saving model to best.model\n",
      "0s - loss: 1.1674 - acc: 0.3371 - val_loss: 1.0333 - val_acc: 0.6957\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.03328 to 1.03032, saving model to best.model\n",
      "0s - loss: 1.1600 - acc: 0.3708 - val_loss: 1.0303 - val_acc: 0.6957\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.03032 to 1.02704, saving model to best.model\n",
      "0s - loss: 1.0961 - acc: 0.3933 - val_loss: 1.0270 - val_acc: 0.7826\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.02704 to 1.02316, saving model to best.model\n",
      "0s - loss: 1.1120 - acc: 0.3933 - val_loss: 1.0232 - val_acc: 0.9130\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.02316 to 1.01894, saving model to best.model\n",
      "0s - loss: 1.1316 - acc: 0.3596 - val_loss: 1.0189 - val_acc: 0.9130\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.01894 to 1.01473, saving model to best.model\n",
      "0s - loss: 1.0334 - acc: 0.3820 - val_loss: 1.0147 - val_acc: 0.9130\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.01473 to 1.01038, saving model to best.model\n",
      "0s - loss: 1.2086 - acc: 0.3258 - val_loss: 1.0104 - val_acc: 0.8261\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.01038 to 1.00632, saving model to best.model\n",
      "0s - loss: 1.1787 - acc: 0.3371 - val_loss: 1.0063 - val_acc: 0.7826\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.00632 to 1.00254, saving model to best.model\n",
      "0s - loss: 1.2286 - acc: 0.3708 - val_loss: 1.0025 - val_acc: 0.7826\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.00254 to 0.99883, saving model to best.model\n",
      "0s - loss: 1.1630 - acc: 0.4045 - val_loss: 0.9988 - val_acc: 0.6957\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.99883 to 0.99504, saving model to best.model\n",
      "0s - loss: 1.0651 - acc: 0.4382 - val_loss: 0.9950 - val_acc: 0.6957\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.99504 to 0.99106, saving model to best.model\n",
      "0s - loss: 1.1366 - acc: 0.3483 - val_loss: 0.9911 - val_acc: 0.7826\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.99106 to 0.98698, saving model to best.model\n",
      "0s - loss: 1.0762 - acc: 0.4157 - val_loss: 0.9870 - val_acc: 0.8261\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.98698 to 0.98280, saving model to best.model\n",
      "0s - loss: 1.0862 - acc: 0.4270 - val_loss: 0.9828 - val_acc: 0.9565\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.98280 to 0.97851, saving model to best.model\n",
      "0s - loss: 1.0776 - acc: 0.4494 - val_loss: 0.9785 - val_acc: 0.9565\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.97851 to 0.97415, saving model to best.model\n",
      "0s - loss: 1.0173 - acc: 0.5056 - val_loss: 0.9741 - val_acc: 0.9565\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.97415 to 0.96973, saving model to best.model\n",
      "0s - loss: 1.0512 - acc: 0.4045 - val_loss: 0.9697 - val_acc: 0.9130\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.96973 to 0.96519, saving model to best.model\n",
      "0s - loss: 1.1285 - acc: 0.3483 - val_loss: 0.9652 - val_acc: 0.9130\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.96519 to 0.96060, saving model to best.model\n",
      "0s - loss: 1.1042 - acc: 0.4270 - val_loss: 0.9606 - val_acc: 0.9130\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.96060 to 0.95590, saving model to best.model\n",
      "0s - loss: 1.0777 - acc: 0.4270 - val_loss: 0.9559 - val_acc: 0.9565\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.95590 to 0.95095, saving model to best.model\n",
      "0s - loss: 1.1014 - acc: 0.3596 - val_loss: 0.9509 - val_acc: 0.9565\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.95095 to 0.94581, saving model to best.model\n",
      "0s - loss: 1.0451 - acc: 0.4831 - val_loss: 0.9458 - val_acc: 0.9130\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.94581 to 0.94001, saving model to best.model\n",
      "0s - loss: 1.0476 - acc: 0.4719 - val_loss: 0.9400 - val_acc: 0.9130\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.94001 to 0.93422, saving model to best.model\n",
      "0s - loss: 1.0338 - acc: 0.4382 - val_loss: 0.9342 - val_acc: 0.9130\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.93422 to 0.92815, saving model to best.model\n",
      "0s - loss: 0.9632 - acc: 0.5281 - val_loss: 0.9282 - val_acc: 0.9130\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.92815 to 0.92157, saving model to best.model\n",
      "0s - loss: 1.0450 - acc: 0.4382 - val_loss: 0.9216 - val_acc: 0.9130\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.92157 to 0.91458, saving model to best.model\n",
      "0s - loss: 0.9955 - acc: 0.5056 - val_loss: 0.9146 - val_acc: 0.9130\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.91458 to 0.90743, saving model to best.model\n",
      "0s - loss: 0.9733 - acc: 0.5281 - val_loss: 0.9074 - val_acc: 0.9130\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.90743 to 0.89989, saving model to best.model\n",
      "0s - loss: 0.9961 - acc: 0.5281 - val_loss: 0.8999 - val_acc: 0.7826\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.89989 to 0.89198, saving model to best.model\n",
      "0s - loss: 0.9906 - acc: 0.5393 - val_loss: 0.8920 - val_acc: 0.7826\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.89198 to 0.88421, saving model to best.model\n",
      "0s - loss: 0.9762 - acc: 0.4831 - val_loss: 0.8842 - val_acc: 0.7391\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.88421 to 0.87615, saving model to best.model\n",
      "0s - loss: 1.0363 - acc: 0.4719 - val_loss: 0.8762 - val_acc: 0.7391\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.87615 to 0.86778, saving model to best.model\n",
      "0s - loss: 1.0259 - acc: 0.5056 - val_loss: 0.8678 - val_acc: 0.7391\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.86778 to 0.85907, saving model to best.model\n",
      "0s - loss: 0.9332 - acc: 0.5618 - val_loss: 0.8591 - val_acc: 0.7391\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.85907 to 0.85010, saving model to best.model\n",
      "0s - loss: 0.9811 - acc: 0.5056 - val_loss: 0.8501 - val_acc: 0.7391\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.85010 to 0.84094, saving model to best.model\n",
      "0s - loss: 1.0352 - acc: 0.4382 - val_loss: 0.8409 - val_acc: 0.7391\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.84094 to 0.83159, saving model to best.model\n",
      "0s - loss: 0.9159 - acc: 0.5506 - val_loss: 0.8316 - val_acc: 0.8696\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.83159 to 0.82211, saving model to best.model\n",
      "0s - loss: 0.9019 - acc: 0.5618 - val_loss: 0.8221 - val_acc: 0.8696\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.82211 to 0.81256, saving model to best.model\n",
      "0s - loss: 0.9002 - acc: 0.5843 - val_loss: 0.8126 - val_acc: 0.9130\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.81256 to 0.80300, saving model to best.model\n",
      "0s - loss: 0.8925 - acc: 0.5955 - val_loss: 0.8030 - val_acc: 0.9130\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.80300 to 0.79333, saving model to best.model\n",
      "0s - loss: 0.9659 - acc: 0.5169 - val_loss: 0.7933 - val_acc: 0.9130\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.79333 to 0.78363, saving model to best.model\n",
      "0s - loss: 0.9590 - acc: 0.5169 - val_loss: 0.7836 - val_acc: 0.9130\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.78363 to 0.77375, saving model to best.model\n",
      "0s - loss: 0.8437 - acc: 0.6404 - val_loss: 0.7738 - val_acc: 0.9130\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.77375 to 0.76364, saving model to best.model\n",
      "0s - loss: 0.8120 - acc: 0.6629 - val_loss: 0.7636 - val_acc: 0.9130\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.76364 to 0.75312, saving model to best.model\n",
      "0s - loss: 0.8773 - acc: 0.5506 - val_loss: 0.7531 - val_acc: 0.9130\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.75312 to 0.74230, saving model to best.model\n",
      "0s - loss: 0.8616 - acc: 0.6292 - val_loss: 0.7423 - val_acc: 0.9130\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.74230 to 0.73135, saving model to best.model\n",
      "0s - loss: 0.8414 - acc: 0.6067 - val_loss: 0.7314 - val_acc: 0.9130\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.73135 to 0.72046, saving model to best.model\n",
      "0s - loss: 0.8824 - acc: 0.5506 - val_loss: 0.7205 - val_acc: 0.9130\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.72046 to 0.70977, saving model to best.model\n",
      "0s - loss: 0.8927 - acc: 0.5955 - val_loss: 0.7098 - val_acc: 0.9130\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70977 to 0.69917, saving model to best.model\n",
      "0s - loss: 0.8902 - acc: 0.5056 - val_loss: 0.6992 - val_acc: 0.8696\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69917 to 0.68873, saving model to best.model\n",
      "0s - loss: 0.8999 - acc: 0.6180 - val_loss: 0.6887 - val_acc: 0.8696\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.68873 to 0.67864, saving model to best.model\n",
      "0s - loss: 0.7900 - acc: 0.6629 - val_loss: 0.6786 - val_acc: 0.8696\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.67864 to 0.66846, saving model to best.model\n",
      "0s - loss: 0.7247 - acc: 0.7079 - val_loss: 0.6685 - val_acc: 0.8696\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.66846 to 0.65841, saving model to best.model\n",
      "0s - loss: 0.7877 - acc: 0.6404 - val_loss: 0.6584 - val_acc: 0.8696\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.65841 to 0.64807, saving model to best.model\n",
      "0s - loss: 0.7226 - acc: 0.6742 - val_loss: 0.6481 - val_acc: 0.8696\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.64807 to 0.63750, saving model to best.model\n",
      "0s - loss: 0.8065 - acc: 0.6629 - val_loss: 0.6375 - val_acc: 0.8696\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.63750 to 0.62691, saving model to best.model\n",
      "0s - loss: 0.7283 - acc: 0.6292 - val_loss: 0.6269 - val_acc: 0.8696\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.62691 to 0.61618, saving model to best.model\n",
      "0s - loss: 0.7397 - acc: 0.6966 - val_loss: 0.6162 - val_acc: 0.8696\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.61618 to 0.60571, saving model to best.model\n",
      "0s - loss: 0.7655 - acc: 0.6180 - val_loss: 0.6057 - val_acc: 0.8696\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.60571 to 0.59537, saving model to best.model\n",
      "0s - loss: 0.7300 - acc: 0.6629 - val_loss: 0.5954 - val_acc: 0.8696\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.59537 to 0.58524, saving model to best.model\n",
      "0s - loss: 0.7351 - acc: 0.6404 - val_loss: 0.5852 - val_acc: 0.8696\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.58524 to 0.57542, saving model to best.model\n",
      "0s - loss: 0.7109 - acc: 0.7303 - val_loss: 0.5754 - val_acc: 0.8696\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.57542 to 0.56594, saving model to best.model\n",
      "0s - loss: 0.6877 - acc: 0.6966 - val_loss: 0.5659 - val_acc: 0.8696\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.56594 to 0.55666, saving model to best.model\n",
      "0s - loss: 0.6450 - acc: 0.7640 - val_loss: 0.5567 - val_acc: 0.8696\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.55666 to 0.54761, saving model to best.model\n",
      "0s - loss: 0.7266 - acc: 0.6966 - val_loss: 0.5476 - val_acc: 0.8696\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.54761 to 0.53874, saving model to best.model\n",
      "0s - loss: 0.6508 - acc: 0.6742 - val_loss: 0.5387 - val_acc: 0.9130\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.53874 to 0.53003, saving model to best.model\n",
      "0s - loss: 0.6714 - acc: 0.7303 - val_loss: 0.5300 - val_acc: 0.9130\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.53003 to 0.52152, saving model to best.model\n",
      "0s - loss: 0.6396 - acc: 0.7978 - val_loss: 0.5215 - val_acc: 0.9130\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.52152 to 0.51329, saving model to best.model\n",
      "0s - loss: 0.6691 - acc: 0.7528 - val_loss: 0.5133 - val_acc: 0.9130\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.51329 to 0.50521, saving model to best.model\n",
      "0s - loss: 0.6307 - acc: 0.7191 - val_loss: 0.5052 - val_acc: 0.9130\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.50521 to 0.49730, saving model to best.model\n",
      "0s - loss: 0.6425 - acc: 0.6629 - val_loss: 0.4973 - val_acc: 0.9130\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.49730 to 0.48954, saving model to best.model\n",
      "0s - loss: 0.6275 - acc: 0.7191 - val_loss: 0.4895 - val_acc: 0.9130\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.48954 to 0.48212, saving model to best.model\n",
      "0s - loss: 0.5975 - acc: 0.7865 - val_loss: 0.4821 - val_acc: 0.9130\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.48212 to 0.47510, saving model to best.model\n",
      "0s - loss: 0.6452 - acc: 0.7303 - val_loss: 0.4751 - val_acc: 0.9130\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.47510 to 0.46841, saving model to best.model\n",
      "0s - loss: 0.5780 - acc: 0.7416 - val_loss: 0.4684 - val_acc: 0.8696\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.46841 to 0.46211, saving model to best.model\n",
      "0s - loss: 0.5833 - acc: 0.7640 - val_loss: 0.4621 - val_acc: 0.8696\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.46211 to 0.45600, saving model to best.model\n",
      "0s - loss: 0.5750 - acc: 0.8202 - val_loss: 0.4560 - val_acc: 0.8696\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.45600 to 0.45004, saving model to best.model\n",
      "0s - loss: 0.5793 - acc: 0.7753 - val_loss: 0.4500 - val_acc: 0.8696\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.45004 to 0.44371, saving model to best.model\n",
      "0s - loss: 0.5585 - acc: 0.7753 - val_loss: 0.4437 - val_acc: 0.8696\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.44371 to 0.43726, saving model to best.model\n",
      "0s - loss: 0.5364 - acc: 0.7528 - val_loss: 0.4373 - val_acc: 0.8696\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.43726 to 0.43106, saving model to best.model\n",
      "0s - loss: 0.6249 - acc: 0.7528 - val_loss: 0.4311 - val_acc: 0.8696\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.43106 to 0.42491, saving model to best.model\n",
      "0s - loss: 0.5396 - acc: 0.7753 - val_loss: 0.4249 - val_acc: 0.8696\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.42491 to 0.41885, saving model to best.model\n",
      "0s - loss: 0.5114 - acc: 0.7416 - val_loss: 0.4189 - val_acc: 0.8696\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.41885 to 0.41290, saving model to best.model\n",
      "0s - loss: 0.5426 - acc: 0.7416 - val_loss: 0.4129 - val_acc: 0.8696\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.41290 to 0.40704, saving model to best.model\n",
      "0s - loss: 0.5420 - acc: 0.7753 - val_loss: 0.4070 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.40704 to 0.40140, saving model to best.model\n",
      "0s - loss: 0.5396 - acc: 0.7753 - val_loss: 0.4014 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.40140 to 0.39605, saving model to best.model\n",
      "0s - loss: 0.5260 - acc: 0.7416 - val_loss: 0.3961 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.39605 to 0.39079, saving model to best.model\n",
      "0s - loss: 0.4636 - acc: 0.8427 - val_loss: 0.3908 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.39079 to 0.38570, saving model to best.model\n",
      "0s - loss: 0.4767 - acc: 0.7978 - val_loss: 0.3857 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.38570 to 0.38073, saving model to best.model\n",
      "0s - loss: 0.4944 - acc: 0.7640 - val_loss: 0.3807 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.38073 to 0.37596, saving model to best.model\n",
      "0s - loss: 0.4479 - acc: 0.7978 - val_loss: 0.3760 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.37596 to 0.37104, saving model to best.model\n",
      "0s - loss: 0.4577 - acc: 0.7978 - val_loss: 0.3710 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.37104 to 0.36615, saving model to best.model\n",
      "0s - loss: 0.4683 - acc: 0.8315 - val_loss: 0.3661 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.36615 to 0.36136, saving model to best.model\n",
      "0s - loss: 0.4867 - acc: 0.8090 - val_loss: 0.3614 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.36136 to 0.35685, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7416 - val_loss: 0.3568 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.35685 to 0.35246, saving model to best.model\n",
      "0s - loss: 0.5152 - acc: 0.7528 - val_loss: 0.3525 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.35246 to 0.34805, saving model to best.model\n",
      "0s - loss: 0.4106 - acc: 0.9213 - val_loss: 0.3481 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.34805 to 0.34359, saving model to best.model\n",
      "0s - loss: 0.5013 - acc: 0.7753 - val_loss: 0.3436 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.34359 to 0.33913, saving model to best.model\n",
      "0s - loss: 0.4803 - acc: 0.8202 - val_loss: 0.3391 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.33913 to 0.33476, saving model to best.model\n",
      "0s - loss: 0.4603 - acc: 0.7978 - val_loss: 0.3348 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.33476 to 0.33059, saving model to best.model\n",
      "0s - loss: 0.4625 - acc: 0.8427 - val_loss: 0.3306 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.33059 to 0.32643, saving model to best.model\n",
      "0s - loss: 0.4227 - acc: 0.8202 - val_loss: 0.3264 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.32643 to 0.32229, saving model to best.model\n",
      "0s - loss: 0.4874 - acc: 0.7528 - val_loss: 0.3223 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.32229 to 0.31804, saving model to best.model\n",
      "0s - loss: 0.4191 - acc: 0.8427 - val_loss: 0.3180 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.31804 to 0.31386, saving model to best.model\n",
      "0s - loss: 0.4741 - acc: 0.8090 - val_loss: 0.3139 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.31386 to 0.30970, saving model to best.model\n",
      "0s - loss: 0.3701 - acc: 0.8652 - val_loss: 0.3097 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.30970 to 0.30541, saving model to best.model\n",
      "0s - loss: 0.4271 - acc: 0.8315 - val_loss: 0.3054 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.30541 to 0.30119, saving model to best.model\n",
      "0s - loss: 0.3864 - acc: 0.8652 - val_loss: 0.3012 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.30119 to 0.29724, saving model to best.model\n",
      "0s - loss: 0.4495 - acc: 0.8427 - val_loss: 0.2972 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.29724 to 0.29344, saving model to best.model\n",
      "0s - loss: 0.4392 - acc: 0.8090 - val_loss: 0.2934 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.29344 to 0.28946, saving model to best.model\n",
      "0s - loss: 0.4586 - acc: 0.8090 - val_loss: 0.2895 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.28946 to 0.28560, saving model to best.model\n",
      "0s - loss: 0.3743 - acc: 0.8315 - val_loss: 0.2856 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.28560 to 0.28193, saving model to best.model\n",
      "0s - loss: 0.4421 - acc: 0.8202 - val_loss: 0.2819 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.28193 to 0.27808, saving model to best.model\n",
      "0s - loss: 0.3625 - acc: 0.8427 - val_loss: 0.2781 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.27808 to 0.27419, saving model to best.model\n",
      "0s - loss: 0.3904 - acc: 0.8539 - val_loss: 0.2742 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.27419 to 0.27010, saving model to best.model\n",
      "0s - loss: 0.3986 - acc: 0.8427 - val_loss: 0.2701 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.27010 to 0.26608, saving model to best.model\n",
      "0s - loss: 0.4089 - acc: 0.8090 - val_loss: 0.2661 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.26608 to 0.26211, saving model to best.model\n",
      "0s - loss: 0.4136 - acc: 0.8427 - val_loss: 0.2621 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.26211 to 0.25826, saving model to best.model\n",
      "0s - loss: 0.4314 - acc: 0.8202 - val_loss: 0.2583 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.25826 to 0.25453, saving model to best.model\n",
      "0s - loss: 0.3612 - acc: 0.8764 - val_loss: 0.2545 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.25453 to 0.25071, saving model to best.model\n",
      "0s - loss: 0.3929 - acc: 0.8090 - val_loss: 0.2507 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.25071 to 0.24686, saving model to best.model\n",
      "0s - loss: 0.4055 - acc: 0.8315 - val_loss: 0.2469 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.24686 to 0.24305, saving model to best.model\n",
      "0s - loss: 0.3393 - acc: 0.8876 - val_loss: 0.2431 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.24305 to 0.23936, saving model to best.model\n",
      "0s - loss: 0.3760 - acc: 0.8427 - val_loss: 0.2394 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.23936 to 0.23594, saving model to best.model\n",
      "0s - loss: 0.3710 - acc: 0.8652 - val_loss: 0.2359 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.23594 to 0.23277, saving model to best.model\n",
      "0s - loss: 0.3776 - acc: 0.8315 - val_loss: 0.2328 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.23277 to 0.22954, saving model to best.model\n",
      "0s - loss: 0.3324 - acc: 0.8652 - val_loss: 0.2295 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.22954 to 0.22625, saving model to best.model\n",
      "0s - loss: 0.3793 - acc: 0.8652 - val_loss: 0.2263 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.22625 to 0.22290, saving model to best.model\n",
      "0s - loss: 0.3301 - acc: 0.8989 - val_loss: 0.2229 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.22290 to 0.21969, saving model to best.model\n",
      "0s - loss: 0.3230 - acc: 0.9326 - val_loss: 0.2197 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.21969 to 0.21656, saving model to best.model\n",
      "0s - loss: 0.3558 - acc: 0.8539 - val_loss: 0.2166 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.21656 to 0.21305, saving model to best.model\n",
      "0s - loss: 0.2904 - acc: 0.9326 - val_loss: 0.2130 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.21305 to 0.20948, saving model to best.model\n",
      "0s - loss: 0.3500 - acc: 0.8315 - val_loss: 0.2095 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.20948 to 0.20577, saving model to best.model\n",
      "0s - loss: 0.2990 - acc: 0.9213 - val_loss: 0.2058 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.20577 to 0.20220, saving model to best.model\n",
      "0s - loss: 0.3370 - acc: 0.8427 - val_loss: 0.2022 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.20220 to 0.19888, saving model to best.model\n",
      "0s - loss: 0.3226 - acc: 0.8427 - val_loss: 0.1989 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.19888 to 0.19570, saving model to best.model\n",
      "0s - loss: 0.2828 - acc: 0.9213 - val_loss: 0.1957 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.19570 to 0.19264, saving model to best.model\n",
      "0s - loss: 0.3359 - acc: 0.8764 - val_loss: 0.1926 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.19264 to 0.18956, saving model to best.model\n",
      "0s - loss: 0.2958 - acc: 0.9101 - val_loss: 0.1896 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.18956 to 0.18640, saving model to best.model\n",
      "0s - loss: 0.3718 - acc: 0.8202 - val_loss: 0.1864 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.18640 to 0.18312, saving model to best.model\n",
      "0s - loss: 0.2622 - acc: 0.9438 - val_loss: 0.1831 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.18312 to 0.18005, saving model to best.model\n",
      "0s - loss: 0.3028 - acc: 0.9101 - val_loss: 0.1801 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.18005 to 0.17710, saving model to best.model\n",
      "0s - loss: 0.2946 - acc: 0.8989 - val_loss: 0.1771 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.17710 to 0.17430, saving model to best.model\n",
      "0s - loss: 0.3808 - acc: 0.8652 - val_loss: 0.1743 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.17430 to 0.17172, saving model to best.model\n",
      "0s - loss: 0.3530 - acc: 0.8764 - val_loss: 0.1717 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.17172 to 0.16917, saving model to best.model\n",
      "0s - loss: 0.2669 - acc: 0.8876 - val_loss: 0.1692 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.16917 to 0.16683, saving model to best.model\n",
      "0s - loss: 0.2766 - acc: 0.8989 - val_loss: 0.1668 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.16683 to 0.16471, saving model to best.model\n",
      "0s - loss: 0.2572 - acc: 0.9101 - val_loss: 0.1647 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.16471 to 0.16232, saving model to best.model\n",
      "0s - loss: 0.2291 - acc: 0.9438 - val_loss: 0.1623 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.16232 to 0.15953, saving model to best.model\n",
      "0s - loss: 0.2814 - acc: 0.8764 - val_loss: 0.1595 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.15953 to 0.15703, saving model to best.model\n",
      "0s - loss: 0.3271 - acc: 0.8539 - val_loss: 0.1570 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.15703 to 0.15458, saving model to best.model\n",
      "0s - loss: 0.3052 - acc: 0.9326 - val_loss: 0.1546 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.15458 to 0.15162, saving model to best.model\n",
      "0s - loss: 0.2461 - acc: 0.9101 - val_loss: 0.1516 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.15162 to 0.14850, saving model to best.model\n",
      "0s - loss: 0.2719 - acc: 0.9101 - val_loss: 0.1485 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.14850 to 0.14533, saving model to best.model\n",
      "0s - loss: 0.2267 - acc: 0.9326 - val_loss: 0.1453 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.14533 to 0.14232, saving model to best.model\n",
      "0s - loss: 0.2768 - acc: 0.9101 - val_loss: 0.1423 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.14232 to 0.13943, saving model to best.model\n",
      "0s - loss: 0.2691 - acc: 0.8876 - val_loss: 0.1394 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.13943 to 0.13677, saving model to best.model\n",
      "0s - loss: 0.2242 - acc: 0.9213 - val_loss: 0.1368 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.13677 to 0.13437, saving model to best.model\n",
      "0s - loss: 0.2346 - acc: 0.8989 - val_loss: 0.1344 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.13437 to 0.13218, saving model to best.model\n",
      "0s - loss: 0.2130 - acc: 0.9213 - val_loss: 0.1322 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.13218 to 0.13008, saving model to best.model\n",
      "0s - loss: 0.2177 - acc: 0.8989 - val_loss: 0.1301 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.13008 to 0.12823, saving model to best.model\n",
      "0s - loss: 0.2200 - acc: 0.9438 - val_loss: 0.1282 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.12823 to 0.12655, saving model to best.model\n",
      "0s - loss: 0.2143 - acc: 0.9213 - val_loss: 0.1265 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.12655 to 0.12478, saving model to best.model\n",
      "0s - loss: 0.2433 - acc: 0.8989 - val_loss: 0.1248 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.12478 to 0.12301, saving model to best.model\n",
      "0s - loss: 0.2375 - acc: 0.8764 - val_loss: 0.1230 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.12301 to 0.12122, saving model to best.model\n",
      "0s - loss: 0.2079 - acc: 0.9326 - val_loss: 0.1212 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.12122 to 0.11949, saving model to best.model\n",
      "0s - loss: 0.2117 - acc: 0.9438 - val_loss: 0.1195 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.11949 to 0.11788, saving model to best.model\n",
      "0s - loss: 0.2412 - acc: 0.8989 - val_loss: 0.1179 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.11788 to 0.11716, saving model to best.model\n",
      "0s - loss: 0.2364 - acc: 0.8989 - val_loss: 0.1172 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.11716 to 0.11694, saving model to best.model\n",
      "0s - loss: 0.2032 - acc: 0.9438 - val_loss: 0.1169 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.11694 to 0.11692, saving model to best.model\n",
      "0s - loss: 0.1895 - acc: 0.9326 - val_loss: 0.1169 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.11692 to 0.11655, saving model to best.model\n",
      "0s - loss: 0.2224 - acc: 0.9213 - val_loss: 0.1166 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.11655 to 0.11551, saving model to best.model\n",
      "0s - loss: 0.1905 - acc: 0.9101 - val_loss: 0.1155 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.19491, saving model to best.model\n",
      "0s - loss: 1.2261 - acc: 0.4270 - val_loss: 1.1949 - val_acc: 0.3043\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.2022 - acc: 0.3596 - val_loss: 1.2413 - val_acc: 0.3043\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.2137 - acc: 0.3371 - val_loss: 1.2859 - val_acc: 0.3043\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1144 - acc: 0.4494 - val_loss: 1.3294 - val_acc: 0.2609\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2822 - acc: 0.3933 - val_loss: 1.3616 - val_acc: 0.2609\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1665 - acc: 0.4382 - val_loss: 1.3802 - val_acc: 0.2609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2991 - acc: 0.3371 - val_loss: 1.3919 - val_acc: 0.2609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2552 - acc: 0.2472 - val_loss: 1.3946 - val_acc: 0.2609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2482 - acc: 0.3371 - val_loss: 1.3889 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1110 - acc: 0.4382 - val_loss: 1.3795 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2505 - acc: 0.3708 - val_loss: 1.3629 - val_acc: 0.2609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2073 - acc: 0.3483 - val_loss: 1.3413 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1419 - acc: 0.4270 - val_loss: 1.3202 - val_acc: 0.3043\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.0726 - acc: 0.4382 - val_loss: 1.3043 - val_acc: 0.3043\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1113 - acc: 0.4494 - val_loss: 1.2888 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1941 - acc: 0.3371 - val_loss: 1.2735 - val_acc: 0.3043\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2286 - acc: 0.3933 - val_loss: 1.2620 - val_acc: 0.3043\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.9952 - acc: 0.4607 - val_loss: 1.2523 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2272 - acc: 0.3596 - val_loss: 1.2444 - val_acc: 0.3043\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1064 - acc: 0.4382 - val_loss: 1.2408 - val_acc: 0.3043\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.0803 - acc: 0.4831 - val_loss: 1.2399 - val_acc: 0.3043\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.0955 - acc: 0.4270 - val_loss: 1.2395 - val_acc: 0.3043\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1196 - acc: 0.4944 - val_loss: 1.2373 - val_acc: 0.3043\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.2007 - acc: 0.3933 - val_loss: 1.2366 - val_acc: 0.3043\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.0641 - acc: 0.4607 - val_loss: 1.2357 - val_acc: 0.3478\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.0959 - acc: 0.4494 - val_loss: 1.2332 - val_acc: 0.3478\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1014 - acc: 0.5056 - val_loss: 1.2318 - val_acc: 0.3913\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.13337, saving model to best.model\n",
      "0s - loss: 1.3326 - acc: 0.2809 - val_loss: 1.1334 - val_acc: 0.3913\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.2989 - acc: 0.3258 - val_loss: 1.1342 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.13337 to 1.13084, saving model to best.model\n",
      "0s - loss: 1.2016 - acc: 0.3933 - val_loss: 1.1308 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.13084 to 1.12519, saving model to best.model\n",
      "0s - loss: 1.2087 - acc: 0.3708 - val_loss: 1.1252 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.12519 to 1.11906, saving model to best.model\n",
      "0s - loss: 1.2345 - acc: 0.4045 - val_loss: 1.1191 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.11906 to 1.11120, saving model to best.model\n",
      "0s - loss: 1.1461 - acc: 0.4607 - val_loss: 1.1112 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.11120 to 1.10554, saving model to best.model\n",
      "0s - loss: 1.1667 - acc: 0.4382 - val_loss: 1.1055 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.10554 to 1.10087, saving model to best.model\n",
      "0s - loss: 1.2018 - acc: 0.4270 - val_loss: 1.1009 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.10087 to 1.09690, saving model to best.model\n",
      "0s - loss: 1.3034 - acc: 0.2809 - val_loss: 1.0969 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.09690 to 1.09309, saving model to best.model\n",
      "0s - loss: 1.1990 - acc: 0.3820 - val_loss: 1.0931 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.09309 to 1.09031, saving model to best.model\n",
      "0s - loss: 1.1586 - acc: 0.4494 - val_loss: 1.0903 - val_acc: 0.3913\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.09031 to 1.08795, saving model to best.model\n",
      "0s - loss: 1.2897 - acc: 0.3596 - val_loss: 1.0880 - val_acc: 0.3913\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.08795 to 1.08635, saving model to best.model\n",
      "0s - loss: 1.3186 - acc: 0.3258 - val_loss: 1.0863 - val_acc: 0.3913\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.08635 to 1.08493, saving model to best.model\n",
      "0s - loss: 1.1890 - acc: 0.3708 - val_loss: 1.0849 - val_acc: 0.3913\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.08493 to 1.08376, saving model to best.model\n",
      "0s - loss: 1.0742 - acc: 0.4382 - val_loss: 1.0838 - val_acc: 0.3913\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.08376 to 1.08212, saving model to best.model\n",
      "0s - loss: 1.2692 - acc: 0.3933 - val_loss: 1.0821 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.08212 to 1.08034, saving model to best.model\n",
      "0s - loss: 1.2272 - acc: 0.4157 - val_loss: 1.0803 - val_acc: 0.3913\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1540 - acc: 0.4607 - val_loss: 1.0804 - val_acc: 0.3913\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2112 - acc: 0.3708 - val_loss: 1.0811 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1583 - acc: 0.3820 - val_loss: 1.0815 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1711 - acc: 0.4270 - val_loss: 1.0805 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.08034 to 1.07873, saving model to best.model\n",
      "0s - loss: 1.1177 - acc: 0.4157 - val_loss: 1.0787 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.07873 to 1.07637, saving model to best.model\n",
      "0s - loss: 1.1073 - acc: 0.4382 - val_loss: 1.0764 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.07637 to 1.07248, saving model to best.model\n",
      "0s - loss: 1.1591 - acc: 0.4157 - val_loss: 1.0725 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.07248 to 1.06866, saving model to best.model\n",
      "0s - loss: 1.1480 - acc: 0.4045 - val_loss: 1.0687 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.06866 to 1.06619, saving model to best.model\n",
      "0s - loss: 1.1552 - acc: 0.3820 - val_loss: 1.0662 - val_acc: 0.3913\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.06619 to 1.06283, saving model to best.model\n",
      "0s - loss: 1.1661 - acc: 0.3820 - val_loss: 1.0628 - val_acc: 0.3913\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.06283 to 1.06014, saving model to best.model\n",
      "0s - loss: 1.2231 - acc: 0.3596 - val_loss: 1.0601 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.06014 to 1.05738, saving model to best.model\n",
      "0s - loss: 1.1143 - acc: 0.4270 - val_loss: 1.0574 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.05738 to 1.05499, saving model to best.model\n",
      "0s - loss: 1.1212 - acc: 0.3820 - val_loss: 1.0550 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.05499 to 1.05240, saving model to best.model\n",
      "0s - loss: 1.0634 - acc: 0.4494 - val_loss: 1.0524 - val_acc: 0.3913\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.05240 to 1.04900, saving model to best.model\n",
      "0s - loss: 1.1156 - acc: 0.3933 - val_loss: 1.0490 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.04900 to 1.04593, saving model to best.model\n",
      "0s - loss: 1.1784 - acc: 0.3933 - val_loss: 1.0459 - val_acc: 0.3913\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.04593 to 1.04199, saving model to best.model\n",
      "0s - loss: 1.1423 - acc: 0.3933 - val_loss: 1.0420 - val_acc: 0.3913\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.04199 to 1.03778, saving model to best.model\n",
      "0s - loss: 1.0713 - acc: 0.4719 - val_loss: 1.0378 - val_acc: 0.3913\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.03778 to 1.03364, saving model to best.model\n",
      "0s - loss: 1.0533 - acc: 0.3933 - val_loss: 1.0336 - val_acc: 0.3913\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.03364 to 1.02970, saving model to best.model\n",
      "0s - loss: 1.1033 - acc: 0.4719 - val_loss: 1.0297 - val_acc: 0.3913\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.02970 to 1.02583, saving model to best.model\n",
      "0s - loss: 1.0361 - acc: 0.5056 - val_loss: 1.0258 - val_acc: 0.3913\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.02583 to 1.02153, saving model to best.model\n",
      "0s - loss: 1.0740 - acc: 0.4382 - val_loss: 1.0215 - val_acc: 0.3913\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.02153 to 1.01726, saving model to best.model\n",
      "0s - loss: 1.0339 - acc: 0.4270 - val_loss: 1.0173 - val_acc: 0.3913\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.01726 to 1.01278, saving model to best.model\n",
      "0s - loss: 1.0683 - acc: 0.4607 - val_loss: 1.0128 - val_acc: 0.3913\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.01278 to 1.00842, saving model to best.model\n",
      "0s - loss: 1.1253 - acc: 0.4270 - val_loss: 1.0084 - val_acc: 0.3913\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.00842 to 1.00477, saving model to best.model\n",
      "0s - loss: 1.1195 - acc: 0.4270 - val_loss: 1.0048 - val_acc: 0.3913\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.00477 to 1.00069, saving model to best.model\n",
      "0s - loss: 1.0942 - acc: 0.3933 - val_loss: 1.0007 - val_acc: 0.3913\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.00069 to 0.99661, saving model to best.model\n",
      "0s - loss: 1.0133 - acc: 0.5169 - val_loss: 0.9966 - val_acc: 0.3913\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.99661 to 0.99157, saving model to best.model\n",
      "0s - loss: 1.0184 - acc: 0.4607 - val_loss: 0.9916 - val_acc: 0.3913\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.99157 to 0.98688, saving model to best.model\n",
      "0s - loss: 1.0786 - acc: 0.4045 - val_loss: 0.9869 - val_acc: 0.3913\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.98688 to 0.98258, saving model to best.model\n",
      "0s - loss: 1.0512 - acc: 0.4719 - val_loss: 0.9826 - val_acc: 0.3913\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.98258 to 0.97830, saving model to best.model\n",
      "0s - loss: 1.0595 - acc: 0.4607 - val_loss: 0.9783 - val_acc: 0.3913\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.97830 to 0.97348, saving model to best.model\n",
      "0s - loss: 1.0559 - acc: 0.4494 - val_loss: 0.9735 - val_acc: 0.3913\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.97348 to 0.96810, saving model to best.model\n",
      "0s - loss: 1.0299 - acc: 0.4382 - val_loss: 0.9681 - val_acc: 0.3913\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.96810 to 0.96293, saving model to best.model\n",
      "0s - loss: 1.0478 - acc: 0.4607 - val_loss: 0.9629 - val_acc: 0.3913\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.96293 to 0.95750, saving model to best.model\n",
      "0s - loss: 0.9666 - acc: 0.5506 - val_loss: 0.9575 - val_acc: 0.3913\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.95750 to 0.95167, saving model to best.model\n",
      "0s - loss: 0.9932 - acc: 0.5618 - val_loss: 0.9517 - val_acc: 0.3913\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.95167 to 0.94581, saving model to best.model\n",
      "0s - loss: 0.9882 - acc: 0.5056 - val_loss: 0.9458 - val_acc: 0.3913\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.94581 to 0.93914, saving model to best.model\n",
      "0s - loss: 1.0668 - acc: 0.4045 - val_loss: 0.9391 - val_acc: 0.4348\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.93914 to 0.93272, saving model to best.model\n",
      "0s - loss: 1.0396 - acc: 0.5056 - val_loss: 0.9327 - val_acc: 0.4348\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.93272 to 0.92610, saving model to best.model\n",
      "0s - loss: 0.9944 - acc: 0.5506 - val_loss: 0.9261 - val_acc: 0.4348\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.92610 to 0.91974, saving model to best.model\n",
      "0s - loss: 1.0162 - acc: 0.4831 - val_loss: 0.9197 - val_acc: 0.4783\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.91974 to 0.91337, saving model to best.model\n",
      "0s - loss: 1.0253 - acc: 0.5169 - val_loss: 0.9134 - val_acc: 0.4783\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.91337 to 0.90632, saving model to best.model\n",
      "0s - loss: 0.9418 - acc: 0.5618 - val_loss: 0.9063 - val_acc: 0.4783\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.90632 to 0.89898, saving model to best.model\n",
      "0s - loss: 0.9710 - acc: 0.4831 - val_loss: 0.8990 - val_acc: 0.4348\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.89898 to 0.89109, saving model to best.model\n",
      "0s - loss: 1.0503 - acc: 0.4045 - val_loss: 0.8911 - val_acc: 0.4348\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.89109 to 0.88314, saving model to best.model\n",
      "0s - loss: 0.9535 - acc: 0.5506 - val_loss: 0.8831 - val_acc: 0.4348\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.88314 to 0.87467, saving model to best.model\n",
      "0s - loss: 0.8832 - acc: 0.5281 - val_loss: 0.8747 - val_acc: 0.4348\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.87467 to 0.86502, saving model to best.model\n",
      "0s - loss: 0.9853 - acc: 0.5393 - val_loss: 0.8650 - val_acc: 0.4783\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.86502 to 0.85486, saving model to best.model\n",
      "0s - loss: 1.0073 - acc: 0.4719 - val_loss: 0.8549 - val_acc: 0.5217\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.85486 to 0.84451, saving model to best.model\n",
      "0s - loss: 0.9178 - acc: 0.6180 - val_loss: 0.8445 - val_acc: 0.5652\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.84451 to 0.83454, saving model to best.model\n",
      "0s - loss: 0.9830 - acc: 0.5618 - val_loss: 0.8345 - val_acc: 0.5652\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.83454 to 0.82377, saving model to best.model\n",
      "0s - loss: 0.9349 - acc: 0.5843 - val_loss: 0.8238 - val_acc: 0.5652\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.82377 to 0.81273, saving model to best.model\n",
      "0s - loss: 0.9561 - acc: 0.5281 - val_loss: 0.8127 - val_acc: 0.5652\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.81273 to 0.80087, saving model to best.model\n",
      "0s - loss: 0.8443 - acc: 0.6742 - val_loss: 0.8009 - val_acc: 0.6522\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.80087 to 0.78929, saving model to best.model\n",
      "0s - loss: 0.9341 - acc: 0.5843 - val_loss: 0.7893 - val_acc: 0.6522\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.78929 to 0.77818, saving model to best.model\n",
      "0s - loss: 0.9216 - acc: 0.5169 - val_loss: 0.7782 - val_acc: 0.6522\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.77818 to 0.76659, saving model to best.model\n",
      "0s - loss: 0.8669 - acc: 0.6067 - val_loss: 0.7666 - val_acc: 0.6522\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.76659 to 0.75517, saving model to best.model\n",
      "0s - loss: 0.9322 - acc: 0.5281 - val_loss: 0.7552 - val_acc: 0.6522\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.75517 to 0.74421, saving model to best.model\n",
      "0s - loss: 0.8998 - acc: 0.5843 - val_loss: 0.7442 - val_acc: 0.6522\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.74421 to 0.73336, saving model to best.model\n",
      "0s - loss: 0.8046 - acc: 0.6292 - val_loss: 0.7334 - val_acc: 0.6522\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.73336 to 0.72212, saving model to best.model\n",
      "0s - loss: 0.8347 - acc: 0.5730 - val_loss: 0.7221 - val_acc: 0.6957\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.72212 to 0.71065, saving model to best.model\n",
      "0s - loss: 0.8068 - acc: 0.6629 - val_loss: 0.7106 - val_acc: 0.6957\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.71065 to 0.69835, saving model to best.model\n",
      "0s - loss: 0.7982 - acc: 0.6404 - val_loss: 0.6984 - val_acc: 0.6957\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69835 to 0.68535, saving model to best.model\n",
      "0s - loss: 0.7976 - acc: 0.6180 - val_loss: 0.6854 - val_acc: 0.6957\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68535 to 0.67234, saving model to best.model\n",
      "0s - loss: 0.7777 - acc: 0.6404 - val_loss: 0.6723 - val_acc: 0.7391\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67234 to 0.65866, saving model to best.model\n",
      "0s - loss: 0.8092 - acc: 0.6404 - val_loss: 0.6587 - val_acc: 0.8261\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.65866 to 0.64490, saving model to best.model\n",
      "0s - loss: 0.7741 - acc: 0.6180 - val_loss: 0.6449 - val_acc: 0.8696\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.64490 to 0.63111, saving model to best.model\n",
      "0s - loss: 0.7818 - acc: 0.6404 - val_loss: 0.6311 - val_acc: 0.8696\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.63111 to 0.61797, saving model to best.model\n",
      "0s - loss: 0.8046 - acc: 0.6854 - val_loss: 0.6180 - val_acc: 0.9130\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.61797 to 0.60509, saving model to best.model\n",
      "0s - loss: 0.7584 - acc: 0.6404 - val_loss: 0.6051 - val_acc: 0.9565\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.60509 to 0.59208, saving model to best.model\n",
      "0s - loss: 0.7574 - acc: 0.6404 - val_loss: 0.5921 - val_acc: 0.9565\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.59208 to 0.57848, saving model to best.model\n",
      "0s - loss: 0.7458 - acc: 0.7191 - val_loss: 0.5785 - val_acc: 0.9565\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.57848 to 0.56562, saving model to best.model\n",
      "0s - loss: 0.6358 - acc: 0.8202 - val_loss: 0.5656 - val_acc: 0.9565\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.56562 to 0.55388, saving model to best.model\n",
      "0s - loss: 0.6438 - acc: 0.7640 - val_loss: 0.5539 - val_acc: 0.9565\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.55388 to 0.54304, saving model to best.model\n",
      "0s - loss: 0.7085 - acc: 0.7753 - val_loss: 0.5430 - val_acc: 0.9565\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.54304 to 0.53250, saving model to best.model\n",
      "0s - loss: 0.6915 - acc: 0.7079 - val_loss: 0.5325 - val_acc: 0.9565\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.53250 to 0.52216, saving model to best.model\n",
      "0s - loss: 0.6568 - acc: 0.6966 - val_loss: 0.5222 - val_acc: 0.9565\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.52216 to 0.51197, saving model to best.model\n",
      "0s - loss: 0.6096 - acc: 0.7191 - val_loss: 0.5120 - val_acc: 0.9565\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.51197 to 0.50187, saving model to best.model\n",
      "0s - loss: 0.6288 - acc: 0.7191 - val_loss: 0.5019 - val_acc: 0.9565\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.50187 to 0.49136, saving model to best.model\n",
      "0s - loss: 0.6297 - acc: 0.7528 - val_loss: 0.4914 - val_acc: 0.9565\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.49136 to 0.48111, saving model to best.model\n",
      "0s - loss: 0.6563 - acc: 0.7191 - val_loss: 0.4811 - val_acc: 0.9565\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.48111 to 0.47006, saving model to best.model\n",
      "0s - loss: 0.5655 - acc: 0.7191 - val_loss: 0.4701 - val_acc: 0.9565\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.47006 to 0.45931, saving model to best.model\n",
      "0s - loss: 0.5424 - acc: 0.7978 - val_loss: 0.4593 - val_acc: 0.9565\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.45931 to 0.44853, saving model to best.model\n",
      "0s - loss: 0.6371 - acc: 0.7303 - val_loss: 0.4485 - val_acc: 0.9565\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.44853 to 0.43861, saving model to best.model\n",
      "0s - loss: 0.5849 - acc: 0.7753 - val_loss: 0.4386 - val_acc: 0.9565\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.43861 to 0.43001, saving model to best.model\n",
      "0s - loss: 0.5506 - acc: 0.7640 - val_loss: 0.4300 - val_acc: 0.9565\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.43001 to 0.42132, saving model to best.model\n",
      "0s - loss: 0.5794 - acc: 0.7528 - val_loss: 0.4213 - val_acc: 0.9565\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.42132 to 0.41266, saving model to best.model\n",
      "0s - loss: 0.5199 - acc: 0.7865 - val_loss: 0.4127 - val_acc: 0.9565\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.41266 to 0.40458, saving model to best.model\n",
      "0s - loss: 0.6037 - acc: 0.7079 - val_loss: 0.4046 - val_acc: 0.9565\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.40458 to 0.39686, saving model to best.model\n",
      "0s - loss: 0.4894 - acc: 0.8090 - val_loss: 0.3969 - val_acc: 0.9565\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.39686 to 0.38957, saving model to best.model\n",
      "0s - loss: 0.5415 - acc: 0.7640 - val_loss: 0.3896 - val_acc: 0.9565\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.38957 to 0.38236, saving model to best.model\n",
      "0s - loss: 0.4929 - acc: 0.7640 - val_loss: 0.3824 - val_acc: 0.9565\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.38236 to 0.37464, saving model to best.model\n",
      "0s - loss: 0.5370 - acc: 0.7753 - val_loss: 0.3746 - val_acc: 0.9565\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.37464 to 0.36671, saving model to best.model\n",
      "0s - loss: 0.5170 - acc: 0.7753 - val_loss: 0.3667 - val_acc: 0.9565\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.36671 to 0.35858, saving model to best.model\n",
      "0s - loss: 0.5405 - acc: 0.7416 - val_loss: 0.3586 - val_acc: 0.9565\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.35858 to 0.34917, saving model to best.model\n",
      "0s - loss: 0.5195 - acc: 0.7640 - val_loss: 0.3492 - val_acc: 0.9565\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.34917 to 0.34078, saving model to best.model\n",
      "0s - loss: 0.5268 - acc: 0.7191 - val_loss: 0.3408 - val_acc: 0.9565\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.34078 to 0.33337, saving model to best.model\n",
      "0s - loss: 0.5562 - acc: 0.7865 - val_loss: 0.3334 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.33337 to 0.32630, saving model to best.model\n",
      "0s - loss: 0.4736 - acc: 0.7978 - val_loss: 0.3263 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.32630 to 0.32006, saving model to best.model\n",
      "0s - loss: 0.4990 - acc: 0.7640 - val_loss: 0.3201 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.32006 to 0.31405, saving model to best.model\n",
      "0s - loss: 0.3918 - acc: 0.8876 - val_loss: 0.3140 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.31405 to 0.30798, saving model to best.model\n",
      "0s - loss: 0.4341 - acc: 0.7978 - val_loss: 0.3080 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.30798 to 0.30185, saving model to best.model\n",
      "0s - loss: 0.4014 - acc: 0.8876 - val_loss: 0.3019 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.30185 to 0.29446, saving model to best.model\n",
      "0s - loss: 0.4100 - acc: 0.8315 - val_loss: 0.2945 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.29446 to 0.28744, saving model to best.model\n",
      "0s - loss: 0.3993 - acc: 0.8427 - val_loss: 0.2874 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.28744 to 0.28138, saving model to best.model\n",
      "0s - loss: 0.4351 - acc: 0.8876 - val_loss: 0.2814 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.28138 to 0.27504, saving model to best.model\n",
      "0s - loss: 0.4743 - acc: 0.7753 - val_loss: 0.2750 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.27504 to 0.26882, saving model to best.model\n",
      "0s - loss: 0.4584 - acc: 0.7978 - val_loss: 0.2688 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.26882 to 0.26299, saving model to best.model\n",
      "0s - loss: 0.4174 - acc: 0.8202 - val_loss: 0.2630 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.26299 to 0.25773, saving model to best.model\n",
      "0s - loss: 0.3931 - acc: 0.8427 - val_loss: 0.2577 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.25773 to 0.25277, saving model to best.model\n",
      "0s - loss: 0.4668 - acc: 0.7865 - val_loss: 0.2528 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.25277 to 0.24811, saving model to best.model\n",
      "0s - loss: 0.3457 - acc: 0.9101 - val_loss: 0.2481 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.24811 to 0.24351, saving model to best.model\n",
      "0s - loss: 0.3449 - acc: 0.8652 - val_loss: 0.2435 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.24351 to 0.23890, saving model to best.model\n",
      "0s - loss: 0.4088 - acc: 0.8315 - val_loss: 0.2389 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.23890 to 0.23329, saving model to best.model\n",
      "0s - loss: 0.3894 - acc: 0.8427 - val_loss: 0.2333 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.23329 to 0.22681, saving model to best.model\n",
      "0s - loss: 0.3715 - acc: 0.8090 - val_loss: 0.2268 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.22681 to 0.22023, saving model to best.model\n",
      "0s - loss: 0.3619 - acc: 0.8876 - val_loss: 0.2202 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.22023 to 0.21470, saving model to best.model\n",
      "0s - loss: 0.2904 - acc: 0.9551 - val_loss: 0.2147 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.21470 to 0.20929, saving model to best.model\n",
      "0s - loss: 0.3869 - acc: 0.8764 - val_loss: 0.2093 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.20929 to 0.20521, saving model to best.model\n",
      "0s - loss: 0.4094 - acc: 0.8539 - val_loss: 0.2052 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.20521 to 0.20135, saving model to best.model\n",
      "0s - loss: 0.3982 - acc: 0.8539 - val_loss: 0.2013 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.20135 to 0.19779, saving model to best.model\n",
      "0s - loss: 0.3732 - acc: 0.8202 - val_loss: 0.1978 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.19779 to 0.19393, saving model to best.model\n",
      "0s - loss: 0.3764 - acc: 0.8427 - val_loss: 0.1939 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.19393 to 0.19079, saving model to best.model\n",
      "0s - loss: 0.3073 - acc: 0.8989 - val_loss: 0.1908 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.19079 to 0.18840, saving model to best.model\n",
      "0s - loss: 0.2788 - acc: 0.8989 - val_loss: 0.1884 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.18840 to 0.18464, saving model to best.model\n",
      "0s - loss: 0.3342 - acc: 0.8427 - val_loss: 0.1846 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.18464 to 0.18065, saving model to best.model\n",
      "0s - loss: 0.3547 - acc: 0.8427 - val_loss: 0.1807 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.18065 to 0.17667, saving model to best.model\n",
      "0s - loss: 0.2902 - acc: 0.9326 - val_loss: 0.1767 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.17667 to 0.17311, saving model to best.model\n",
      "0s - loss: 0.3596 - acc: 0.8539 - val_loss: 0.1731 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.17311 to 0.16928, saving model to best.model\n",
      "0s - loss: 0.2858 - acc: 0.9326 - val_loss: 0.1693 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.16928 to 0.16479, saving model to best.model\n",
      "0s - loss: 0.2998 - acc: 0.8764 - val_loss: 0.1648 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.16479 to 0.15970, saving model to best.model\n",
      "0s - loss: 0.2971 - acc: 0.8989 - val_loss: 0.1597 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.15970 to 0.15533, saving model to best.model\n",
      "0s - loss: 0.2376 - acc: 0.9551 - val_loss: 0.1553 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.15533 to 0.15114, saving model to best.model\n",
      "0s - loss: 0.2862 - acc: 0.9213 - val_loss: 0.1511 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.15114 to 0.14680, saving model to best.model\n",
      "0s - loss: 0.3315 - acc: 0.8876 - val_loss: 0.1468 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.14680 to 0.14199, saving model to best.model\n",
      "0s - loss: 0.2893 - acc: 0.8989 - val_loss: 0.1420 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.14199 to 0.13716, saving model to best.model\n",
      "0s - loss: 0.3092 - acc: 0.9213 - val_loss: 0.1372 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.13716 to 0.13238, saving model to best.model\n",
      "0s - loss: 0.2382 - acc: 0.9213 - val_loss: 0.1324 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.13238 to 0.12810, saving model to best.model\n",
      "0s - loss: 0.2757 - acc: 0.9213 - val_loss: 0.1281 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.12810 to 0.12444, saving model to best.model\n",
      "0s - loss: 0.2619 - acc: 0.9213 - val_loss: 0.1244 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.12444 to 0.12073, saving model to best.model\n",
      "0s - loss: 0.2517 - acc: 0.9101 - val_loss: 0.1207 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.12073 to 0.11777, saving model to best.model\n",
      "0s - loss: 0.3054 - acc: 0.9326 - val_loss: 0.1178 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.11777 to 0.11531, saving model to best.model\n",
      "0s - loss: 0.1970 - acc: 0.9775 - val_loss: 0.1153 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.11531 to 0.11299, saving model to best.model\n",
      "0s - loss: 0.2780 - acc: 0.9101 - val_loss: 0.1130 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.11299 to 0.11041, saving model to best.model\n",
      "0s - loss: 0.2182 - acc: 0.9101 - val_loss: 0.1104 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.11041 to 0.10783, saving model to best.model\n",
      "0s - loss: 0.2655 - acc: 0.8876 - val_loss: 0.1078 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.10783 to 0.10520, saving model to best.model\n",
      "0s - loss: 0.2644 - acc: 0.9326 - val_loss: 0.1052 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.10520 to 0.10269, saving model to best.model\n",
      "0s - loss: 0.1922 - acc: 0.9326 - val_loss: 0.1027 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.10269 to 0.10016, saving model to best.model\n",
      "0s - loss: 0.2441 - acc: 0.9213 - val_loss: 0.1002 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.10016 to 0.09788, saving model to best.model\n",
      "0s - loss: 0.2897 - acc: 0.8876 - val_loss: 0.0979 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.09788 to 0.09594, saving model to best.model\n",
      "0s - loss: 0.2536 - acc: 0.9213 - val_loss: 0.0959 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.09594 to 0.09395, saving model to best.model\n",
      "0s - loss: 0.2470 - acc: 0.8989 - val_loss: 0.0939 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.09395 to 0.09210, saving model to best.model\n",
      "0s - loss: 0.2086 - acc: 0.9326 - val_loss: 0.0921 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.09210 to 0.08972, saving model to best.model\n",
      "0s - loss: 0.2278 - acc: 0.9101 - val_loss: 0.0897 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.08972 to 0.08703, saving model to best.model\n",
      "0s - loss: 0.1698 - acc: 0.9775 - val_loss: 0.0870 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.08703 to 0.08412, saving model to best.model\n",
      "0s - loss: 0.2284 - acc: 0.8989 - val_loss: 0.0841 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.08412 to 0.08202, saving model to best.model\n",
      "0s - loss: 0.1953 - acc: 0.9213 - val_loss: 0.0820 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.08202 to 0.08052, saving model to best.model\n",
      "0s - loss: 0.2027 - acc: 0.9213 - val_loss: 0.0805 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.08052 to 0.07896, saving model to best.model\n",
      "0s - loss: 0.2191 - acc: 0.9213 - val_loss: 0.0790 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.07896 to 0.07721, saving model to best.model\n",
      "0s - loss: 0.1743 - acc: 0.9551 - val_loss: 0.0772 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.07721 to 0.07502, saving model to best.model\n",
      "0s - loss: 0.2003 - acc: 0.9213 - val_loss: 0.0750 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.07502 to 0.07325, saving model to best.model\n",
      "0s - loss: 0.2345 - acc: 0.9213 - val_loss: 0.0732 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.07325 to 0.07118, saving model to best.model\n",
      "0s - loss: 0.2196 - acc: 0.9101 - val_loss: 0.0712 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.07118 to 0.06913, saving model to best.model\n",
      "0s - loss: 0.2260 - acc: 0.9213 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.06913 to 0.06734, saving model to best.model\n",
      "0s - loss: 0.2162 - acc: 0.9326 - val_loss: 0.0673 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.06734 to 0.06540, saving model to best.model\n",
      "0s - loss: 0.2257 - acc: 0.8989 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.06540 to 0.06385, saving model to best.model\n",
      "0s - loss: 0.1964 - acc: 0.9551 - val_loss: 0.0638 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.06385 to 0.06204, saving model to best.model\n",
      "0s - loss: 0.1970 - acc: 0.9213 - val_loss: 0.0620 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.06204 to 0.06034, saving model to best.model\n",
      "0s - loss: 0.1671 - acc: 0.9775 - val_loss: 0.0603 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.06034 to 0.05869, saving model to best.model\n",
      "0s - loss: 0.2031 - acc: 0.9326 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.05869 to 0.05731, saving model to best.model\n",
      "0s - loss: 0.2039 - acc: 0.9438 - val_loss: 0.0573 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.05731 to 0.05577, saving model to best.model\n",
      "0s - loss: 0.1961 - acc: 0.9438 - val_loss: 0.0558 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.05577 to 0.05440, saving model to best.model\n",
      "0s - loss: 0.2244 - acc: 0.9213 - val_loss: 0.0544 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.05440 to 0.05328, saving model to best.model\n",
      "0s - loss: 0.2316 - acc: 0.8876 - val_loss: 0.0533 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.05328 to 0.05186, saving model to best.model\n",
      "0s - loss: 0.1617 - acc: 0.9551 - val_loss: 0.0519 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.05186 to 0.05099, saving model to best.model\n",
      "0s - loss: 0.1882 - acc: 0.9326 - val_loss: 0.0510 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.05099 to 0.04987, saving model to best.model\n",
      "0s - loss: 0.2177 - acc: 0.9101 - val_loss: 0.0499 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.04987 to 0.04897, saving model to best.model\n",
      "0s - loss: 0.1271 - acc: 0.9775 - val_loss: 0.0490 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.04897 to 0.04820, saving model to best.model\n",
      "0s - loss: 0.1807 - acc: 0.9438 - val_loss: 0.0482 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.04820 to 0.04731, saving model to best.model\n",
      "0s - loss: 0.1630 - acc: 0.9438 - val_loss: 0.0473 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.04731 to 0.04643, saving model to best.model\n",
      "0s - loss: 0.2333 - acc: 0.8989 - val_loss: 0.0464 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.04643 to 0.04545, saving model to best.model\n",
      "0s - loss: 0.1481 - acc: 0.9663 - val_loss: 0.0455 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.09923, saving model to best.model\n",
      "0s - loss: 1.2973 - acc: 0.4157 - val_loss: 1.0992 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.09923 to 1.08400, saving model to best.model\n",
      "0s - loss: 1.2916 - acc: 0.3483 - val_loss: 1.0840 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.2481 - acc: 0.3371 - val_loss: 1.0912 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2509 - acc: 0.3258 - val_loss: 1.1083 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2458 - acc: 0.3258 - val_loss: 1.1266 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2690 - acc: 0.3596 - val_loss: 1.1414 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2574 - acc: 0.3371 - val_loss: 1.1520 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1553 - acc: 0.4607 - val_loss: 1.1551 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2789 - acc: 0.3596 - val_loss: 1.1510 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2873 - acc: 0.3483 - val_loss: 1.1413 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2829 - acc: 0.3596 - val_loss: 1.1288 - val_acc: 0.3913\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1975 - acc: 0.4045 - val_loss: 1.1147 - val_acc: 0.3913\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.3454 - acc: 0.3034 - val_loss: 1.1019 - val_acc: 0.3913\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2572 - acc: 0.3596 - val_loss: 1.0901 - val_acc: 0.3913\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.08400 to 1.08008, saving model to best.model\n",
      "0s - loss: 1.1001 - acc: 0.4719 - val_loss: 1.0801 - val_acc: 0.3913\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.08008 to 1.07245, saving model to best.model\n",
      "0s - loss: 1.2273 - acc: 0.3258 - val_loss: 1.0724 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.07245 to 1.06674, saving model to best.model\n",
      "0s - loss: 1.2065 - acc: 0.3371 - val_loss: 1.0667 - val_acc: 0.3913\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.06674 to 1.06240, saving model to best.model\n",
      "0s - loss: 1.2092 - acc: 0.4270 - val_loss: 1.0624 - val_acc: 0.3913\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.06240 to 1.05904, saving model to best.model\n",
      "0s - loss: 1.1340 - acc: 0.4045 - val_loss: 1.0590 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.05904 to 1.05686, saving model to best.model\n",
      "0s - loss: 1.1361 - acc: 0.3596 - val_loss: 1.0569 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.05686 to 1.05527, saving model to best.model\n",
      "0s - loss: 1.3054 - acc: 0.2921 - val_loss: 1.0553 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.05527 to 1.05387, saving model to best.model\n",
      "0s - loss: 1.2090 - acc: 0.3820 - val_loss: 1.0539 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.05387 to 1.05287, saving model to best.model\n",
      "0s - loss: 1.1633 - acc: 0.3933 - val_loss: 1.0529 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.05287 to 1.05210, saving model to best.model\n",
      "0s - loss: 1.1121 - acc: 0.4270 - val_loss: 1.0521 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.05210 to 1.05187, saving model to best.model\n",
      "0s - loss: 1.2267 - acc: 0.3483 - val_loss: 1.0519 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.2436 - acc: 0.3596 - val_loss: 1.0521 - val_acc: 0.3913\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1816 - acc: 0.3483 - val_loss: 1.0525 - val_acc: 0.3913\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1118 - acc: 0.4270 - val_loss: 1.0533 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.2120 - acc: 0.4045 - val_loss: 1.0539 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1267 - acc: 0.4157 - val_loss: 1.0543 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1686 - acc: 0.3820 - val_loss: 1.0545 - val_acc: 0.3913\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.0981 - acc: 0.4494 - val_loss: 1.0540 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.1555 - acc: 0.3596 - val_loss: 1.0532 - val_acc: 0.3913\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.05187 to 1.05130, saving model to best.model\n",
      "0s - loss: 1.1527 - acc: 0.4382 - val_loss: 1.0513 - val_acc: 0.3913\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.05130 to 1.04868, saving model to best.model\n",
      "0s - loss: 1.1113 - acc: 0.4494 - val_loss: 1.0487 - val_acc: 0.3913\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.04868 to 1.04539, saving model to best.model\n",
      "0s - loss: 1.1646 - acc: 0.3933 - val_loss: 1.0454 - val_acc: 0.3913\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.04539 to 1.04223, saving model to best.model\n",
      "0s - loss: 1.1047 - acc: 0.4157 - val_loss: 1.0422 - val_acc: 0.3913\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.04223 to 1.03885, saving model to best.model\n",
      "0s - loss: 1.1193 - acc: 0.4157 - val_loss: 1.0388 - val_acc: 0.3913\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.03885 to 1.03513, saving model to best.model\n",
      "0s - loss: 1.1436 - acc: 0.4494 - val_loss: 1.0351 - val_acc: 0.3913\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.03513 to 1.03175, saving model to best.model\n",
      "0s - loss: 1.0605 - acc: 0.4607 - val_loss: 1.0317 - val_acc: 0.3913\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.03175 to 1.02794, saving model to best.model\n",
      "0s - loss: 1.1607 - acc: 0.3820 - val_loss: 1.0279 - val_acc: 0.3913\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.02794 to 1.02391, saving model to best.model\n",
      "0s - loss: 1.0842 - acc: 0.4157 - val_loss: 1.0239 - val_acc: 0.3913\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.02391 to 1.01939, saving model to best.model\n",
      "0s - loss: 1.1351 - acc: 0.3933 - val_loss: 1.0194 - val_acc: 0.3913\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.01939 to 1.01605, saving model to best.model\n",
      "0s - loss: 1.1744 - acc: 0.3146 - val_loss: 1.0161 - val_acc: 0.3913\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.01605 to 1.01269, saving model to best.model\n",
      "0s - loss: 1.1199 - acc: 0.4045 - val_loss: 1.0127 - val_acc: 0.3913\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.01269 to 1.00907, saving model to best.model\n",
      "0s - loss: 1.1515 - acc: 0.4045 - val_loss: 1.0091 - val_acc: 0.3913\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.00907 to 1.00516, saving model to best.model\n",
      "0s - loss: 1.1654 - acc: 0.3483 - val_loss: 1.0052 - val_acc: 0.3913\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.00516 to 1.00195, saving model to best.model\n",
      "0s - loss: 1.1068 - acc: 0.3933 - val_loss: 1.0020 - val_acc: 0.3913\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.00195 to 0.99804, saving model to best.model\n",
      "0s - loss: 1.0128 - acc: 0.4719 - val_loss: 0.9980 - val_acc: 0.3913\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.99804 to 0.99389, saving model to best.model\n",
      "0s - loss: 1.0176 - acc: 0.4045 - val_loss: 0.9939 - val_acc: 0.3913\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.99389 to 0.98984, saving model to best.model\n",
      "0s - loss: 1.0380 - acc: 0.4607 - val_loss: 0.9898 - val_acc: 0.3913\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.98984 to 0.98581, saving model to best.model\n",
      "0s - loss: 1.0401 - acc: 0.4607 - val_loss: 0.9858 - val_acc: 0.3913\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.98581 to 0.98189, saving model to best.model\n",
      "0s - loss: 1.0978 - acc: 0.3933 - val_loss: 0.9819 - val_acc: 0.3913\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.98189 to 0.97800, saving model to best.model\n",
      "0s - loss: 1.1322 - acc: 0.3820 - val_loss: 0.9780 - val_acc: 0.3913\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.97800 to 0.97407, saving model to best.model\n",
      "0s - loss: 1.0180 - acc: 0.5056 - val_loss: 0.9741 - val_acc: 0.3913\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.97407 to 0.96981, saving model to best.model\n",
      "0s - loss: 1.0589 - acc: 0.4270 - val_loss: 0.9698 - val_acc: 0.3913\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.96981 to 0.96497, saving model to best.model\n",
      "0s - loss: 1.0640 - acc: 0.4045 - val_loss: 0.9650 - val_acc: 0.3913\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.96497 to 0.96048, saving model to best.model\n",
      "0s - loss: 1.0027 - acc: 0.4944 - val_loss: 0.9605 - val_acc: 0.3913\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.96048 to 0.95641, saving model to best.model\n",
      "0s - loss: 1.0391 - acc: 0.4494 - val_loss: 0.9564 - val_acc: 0.4348\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.95641 to 0.95154, saving model to best.model\n",
      "0s - loss: 1.0561 - acc: 0.4494 - val_loss: 0.9515 - val_acc: 0.4783\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.95154 to 0.94626, saving model to best.model\n",
      "0s - loss: 0.9966 - acc: 0.4719 - val_loss: 0.9463 - val_acc: 0.5217\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.94626 to 0.94078, saving model to best.model\n",
      "0s - loss: 1.0341 - acc: 0.4607 - val_loss: 0.9408 - val_acc: 0.5652\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.94078 to 0.93488, saving model to best.model\n",
      "0s - loss: 1.0539 - acc: 0.4831 - val_loss: 0.9349 - val_acc: 0.5652\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.93488 to 0.92849, saving model to best.model\n",
      "0s - loss: 0.9554 - acc: 0.5506 - val_loss: 0.9285 - val_acc: 0.6087\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.92849 to 0.92190, saving model to best.model\n",
      "0s - loss: 0.9801 - acc: 0.4944 - val_loss: 0.9219 - val_acc: 0.6087\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.92190 to 0.91497, saving model to best.model\n",
      "0s - loss: 0.8981 - acc: 0.6404 - val_loss: 0.9150 - val_acc: 0.6957\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.91497 to 0.90753, saving model to best.model\n",
      "0s - loss: 0.9562 - acc: 0.5169 - val_loss: 0.9075 - val_acc: 0.6957\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.90753 to 0.89983, saving model to best.model\n",
      "0s - loss: 0.9506 - acc: 0.5281 - val_loss: 0.8998 - val_acc: 0.6957\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.89983 to 0.89235, saving model to best.model\n",
      "0s - loss: 1.0001 - acc: 0.5169 - val_loss: 0.8924 - val_acc: 0.6522\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.89235 to 0.88492, saving model to best.model\n",
      "0s - loss: 0.9369 - acc: 0.4831 - val_loss: 0.8849 - val_acc: 0.6522\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.88492 to 0.87759, saving model to best.model\n",
      "0s - loss: 0.9496 - acc: 0.5281 - val_loss: 0.8776 - val_acc: 0.6522\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.87759 to 0.87012, saving model to best.model\n",
      "0s - loss: 0.9224 - acc: 0.5281 - val_loss: 0.8701 - val_acc: 0.6522\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.87012 to 0.86236, saving model to best.model\n",
      "0s - loss: 0.9125 - acc: 0.5618 - val_loss: 0.8624 - val_acc: 0.6522\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.86236 to 0.85477, saving model to best.model\n",
      "0s - loss: 0.9278 - acc: 0.5506 - val_loss: 0.8548 - val_acc: 0.6522\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.85477 to 0.84705, saving model to best.model\n",
      "0s - loss: 0.9915 - acc: 0.4719 - val_loss: 0.8471 - val_acc: 0.6957\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.84705 to 0.83867, saving model to best.model\n",
      "0s - loss: 0.9535 - acc: 0.4494 - val_loss: 0.8387 - val_acc: 0.6957\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.83867 to 0.83004, saving model to best.model\n",
      "0s - loss: 0.9336 - acc: 0.5955 - val_loss: 0.8300 - val_acc: 0.7391\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.83004 to 0.82091, saving model to best.model\n",
      "0s - loss: 0.8788 - acc: 0.5843 - val_loss: 0.8209 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.82091 to 0.81129, saving model to best.model\n",
      "0s - loss: 0.8602 - acc: 0.5955 - val_loss: 0.8113 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.81129 to 0.80171, saving model to best.model\n",
      "0s - loss: 0.9322 - acc: 0.5281 - val_loss: 0.8017 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.80171 to 0.79152, saving model to best.model\n",
      "0s - loss: 0.8757 - acc: 0.5506 - val_loss: 0.7915 - val_acc: 0.8261\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.79152 to 0.78082, saving model to best.model\n",
      "0s - loss: 0.8058 - acc: 0.6292 - val_loss: 0.7808 - val_acc: 0.8261\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.78082 to 0.77010, saving model to best.model\n",
      "0s - loss: 0.7890 - acc: 0.6292 - val_loss: 0.7701 - val_acc: 0.8696\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.77010 to 0.75922, saving model to best.model\n",
      "0s - loss: 0.9048 - acc: 0.5730 - val_loss: 0.7592 - val_acc: 0.9130\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.75922 to 0.74854, saving model to best.model\n",
      "0s - loss: 0.8673 - acc: 0.5730 - val_loss: 0.7485 - val_acc: 0.9130\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.74854 to 0.73811, saving model to best.model\n",
      "0s - loss: 0.8171 - acc: 0.6742 - val_loss: 0.7381 - val_acc: 0.9130\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.73811 to 0.72717, saving model to best.model\n",
      "0s - loss: 0.8272 - acc: 0.5730 - val_loss: 0.7272 - val_acc: 0.9130\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.72717 to 0.71641, saving model to best.model\n",
      "0s - loss: 0.8239 - acc: 0.6629 - val_loss: 0.7164 - val_acc: 0.9130\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.71641 to 0.70504, saving model to best.model\n",
      "0s - loss: 0.7127 - acc: 0.6854 - val_loss: 0.7050 - val_acc: 0.9130\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.70504 to 0.69362, saving model to best.model\n",
      "0s - loss: 0.7913 - acc: 0.6067 - val_loss: 0.6936 - val_acc: 0.9130\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.69362 to 0.68203, saving model to best.model\n",
      "0s - loss: 0.8358 - acc: 0.6966 - val_loss: 0.6820 - val_acc: 0.9130\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68203 to 0.67057, saving model to best.model\n",
      "0s - loss: 0.7928 - acc: 0.6517 - val_loss: 0.6706 - val_acc: 0.9130\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67057 to 0.65901, saving model to best.model\n",
      "0s - loss: 0.7725 - acc: 0.6292 - val_loss: 0.6590 - val_acc: 0.9130\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.65901 to 0.64734, saving model to best.model\n",
      "0s - loss: 0.7514 - acc: 0.7191 - val_loss: 0.6473 - val_acc: 0.9130\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.64734 to 0.63591, saving model to best.model\n",
      "0s - loss: 0.7545 - acc: 0.6629 - val_loss: 0.6359 - val_acc: 1.0000\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.63591 to 0.62491, saving model to best.model\n",
      "0s - loss: 0.6616 - acc: 0.7640 - val_loss: 0.6249 - val_acc: 1.0000\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.62491 to 0.61412, saving model to best.model\n",
      "0s - loss: 0.7165 - acc: 0.6629 - val_loss: 0.6141 - val_acc: 1.0000\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.61412 to 0.60349, saving model to best.model\n",
      "0s - loss: 0.7279 - acc: 0.6854 - val_loss: 0.6035 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.60349 to 0.59296, saving model to best.model\n",
      "0s - loss: 0.7155 - acc: 0.6966 - val_loss: 0.5930 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.59296 to 0.58302, saving model to best.model\n",
      "0s - loss: 0.6473 - acc: 0.7528 - val_loss: 0.5830 - val_acc: 1.0000\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.58302 to 0.57326, saving model to best.model\n",
      "0s - loss: 0.7549 - acc: 0.6292 - val_loss: 0.5733 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.57326 to 0.56399, saving model to best.model\n",
      "0s - loss: 0.6999 - acc: 0.6854 - val_loss: 0.5640 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.56399 to 0.55459, saving model to best.model\n",
      "0s - loss: 0.6693 - acc: 0.7079 - val_loss: 0.5546 - val_acc: 0.9130\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.55459 to 0.54513, saving model to best.model\n",
      "0s - loss: 0.6287 - acc: 0.7191 - val_loss: 0.5451 - val_acc: 0.9130\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.54513 to 0.53600, saving model to best.model\n",
      "0s - loss: 0.6177 - acc: 0.7079 - val_loss: 0.5360 - val_acc: 0.9130\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.53600 to 0.52653, saving model to best.model\n",
      "0s - loss: 0.7163 - acc: 0.6629 - val_loss: 0.5265 - val_acc: 0.9130\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.52653 to 0.51688, saving model to best.model\n",
      "0s - loss: 0.5821 - acc: 0.7865 - val_loss: 0.5169 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.51688 to 0.50755, saving model to best.model\n",
      "0s - loss: 0.6173 - acc: 0.6966 - val_loss: 0.5076 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.50755 to 0.49799, saving model to best.model\n",
      "0s - loss: 0.6114 - acc: 0.7528 - val_loss: 0.4980 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.49799 to 0.48832, saving model to best.model\n",
      "0s - loss: 0.6372 - acc: 0.6854 - val_loss: 0.4883 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.48832 to 0.47905, saving model to best.model\n",
      "0s - loss: 0.6634 - acc: 0.6404 - val_loss: 0.4791 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.47905 to 0.47044, saving model to best.model\n",
      "0s - loss: 0.6049 - acc: 0.7416 - val_loss: 0.4704 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.47044 to 0.46225, saving model to best.model\n",
      "0s - loss: 0.5891 - acc: 0.7753 - val_loss: 0.4622 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.46225 to 0.45423, saving model to best.model\n",
      "0s - loss: 0.5427 - acc: 0.7978 - val_loss: 0.4542 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.45423 to 0.44645, saving model to best.model\n",
      "0s - loss: 0.5872 - acc: 0.7640 - val_loss: 0.4465 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.44645 to 0.43880, saving model to best.model\n",
      "0s - loss: 0.6199 - acc: 0.7303 - val_loss: 0.4388 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.43880 to 0.43160, saving model to best.model\n",
      "0s - loss: 0.5591 - acc: 0.7753 - val_loss: 0.4316 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.43160 to 0.42471, saving model to best.model\n",
      "0s - loss: 0.5966 - acc: 0.7528 - val_loss: 0.4247 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.42471 to 0.41776, saving model to best.model\n",
      "0s - loss: 0.5572 - acc: 0.7640 - val_loss: 0.4178 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.41776 to 0.41033, saving model to best.model\n",
      "0s - loss: 0.5047 - acc: 0.8202 - val_loss: 0.4103 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.41033 to 0.40310, saving model to best.model\n",
      "0s - loss: 0.5489 - acc: 0.7640 - val_loss: 0.4031 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.40310 to 0.39542, saving model to best.model\n",
      "0s - loss: 0.5175 - acc: 0.8315 - val_loss: 0.3954 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.39542 to 0.38804, saving model to best.model\n",
      "0s - loss: 0.5066 - acc: 0.8090 - val_loss: 0.3880 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.38804 to 0.38024, saving model to best.model\n",
      "0s - loss: 0.5265 - acc: 0.7640 - val_loss: 0.3802 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.38024 to 0.37266, saving model to best.model\n",
      "0s - loss: 0.4699 - acc: 0.8090 - val_loss: 0.3727 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.37266 to 0.36478, saving model to best.model\n",
      "0s - loss: 0.4992 - acc: 0.8202 - val_loss: 0.3648 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.36478 to 0.35725, saving model to best.model\n",
      "0s - loss: 0.4578 - acc: 0.8539 - val_loss: 0.3572 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.35725 to 0.34971, saving model to best.model\n",
      "0s - loss: 0.4102 - acc: 0.8315 - val_loss: 0.3497 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.34971 to 0.34252, saving model to best.model\n",
      "0s - loss: 0.4760 - acc: 0.7865 - val_loss: 0.3425 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.34252 to 0.33541, saving model to best.model\n",
      "0s - loss: 0.4479 - acc: 0.8090 - val_loss: 0.3354 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.33541 to 0.32853, saving model to best.model\n",
      "0s - loss: 0.4154 - acc: 0.8315 - val_loss: 0.3285 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.32853 to 0.32190, saving model to best.model\n",
      "0s - loss: 0.3832 - acc: 0.8539 - val_loss: 0.3219 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.32190 to 0.31546, saving model to best.model\n",
      "0s - loss: 0.3984 - acc: 0.8427 - val_loss: 0.3155 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.31546 to 0.30920, saving model to best.model\n",
      "0s - loss: 0.3684 - acc: 0.8876 - val_loss: 0.3092 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.30920 to 0.30311, saving model to best.model\n",
      "0s - loss: 0.4446 - acc: 0.8652 - val_loss: 0.3031 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.30311 to 0.29724, saving model to best.model\n",
      "0s - loss: 0.4124 - acc: 0.8652 - val_loss: 0.2972 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.29724 to 0.29153, saving model to best.model\n",
      "0s - loss: 0.5120 - acc: 0.7640 - val_loss: 0.2915 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.29153 to 0.28599, saving model to best.model\n",
      "0s - loss: 0.4327 - acc: 0.8876 - val_loss: 0.2860 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.28599 to 0.28064, saving model to best.model\n",
      "0s - loss: 0.4140 - acc: 0.8876 - val_loss: 0.2806 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.28064 to 0.27531, saving model to best.model\n",
      "0s - loss: 0.4584 - acc: 0.8315 - val_loss: 0.2753 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.27531 to 0.27005, saving model to best.model\n",
      "0s - loss: 0.3424 - acc: 0.8764 - val_loss: 0.2701 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.27005 to 0.26495, saving model to best.model\n",
      "0s - loss: 0.3287 - acc: 0.9326 - val_loss: 0.2650 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.26495 to 0.25981, saving model to best.model\n",
      "0s - loss: 0.3677 - acc: 0.8764 - val_loss: 0.2598 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.25981 to 0.25495, saving model to best.model\n",
      "0s - loss: 0.3470 - acc: 0.8764 - val_loss: 0.2550 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.25495 to 0.25023, saving model to best.model\n",
      "0s - loss: 0.3619 - acc: 0.8764 - val_loss: 0.2502 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.25023 to 0.24506, saving model to best.model\n",
      "0s - loss: 0.4047 - acc: 0.8539 - val_loss: 0.2451 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.24506 to 0.23959, saving model to best.model\n",
      "0s - loss: 0.3711 - acc: 0.8427 - val_loss: 0.2396 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.23959 to 0.23500, saving model to best.model\n",
      "0s - loss: 0.3869 - acc: 0.8539 - val_loss: 0.2350 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.23500 to 0.23110, saving model to best.model\n",
      "0s - loss: 0.3414 - acc: 0.8989 - val_loss: 0.2311 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.23110 to 0.22723, saving model to best.model\n",
      "0s - loss: 0.3201 - acc: 0.8764 - val_loss: 0.2272 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.22723 to 0.22353, saving model to best.model\n",
      "0s - loss: 0.3755 - acc: 0.8539 - val_loss: 0.2235 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.22353 to 0.21917, saving model to best.model\n",
      "0s - loss: 0.3784 - acc: 0.8539 - val_loss: 0.2192 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.21917 to 0.21446, saving model to best.model\n",
      "0s - loss: 0.2976 - acc: 0.9213 - val_loss: 0.2145 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.21446 to 0.20976, saving model to best.model\n",
      "0s - loss: 0.3003 - acc: 0.8989 - val_loss: 0.2098 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.20976 to 0.20500, saving model to best.model\n",
      "0s - loss: 0.2665 - acc: 0.9663 - val_loss: 0.2050 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.20500 to 0.20065, saving model to best.model\n",
      "0s - loss: 0.2864 - acc: 0.9101 - val_loss: 0.2006 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.20065 to 0.19613, saving model to best.model\n",
      "0s - loss: 0.3113 - acc: 0.8652 - val_loss: 0.1961 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.19613 to 0.19160, saving model to best.model\n",
      "0s - loss: 0.2335 - acc: 0.9551 - val_loss: 0.1916 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.19160 to 0.18711, saving model to best.model\n",
      "0s - loss: 0.2496 - acc: 0.9326 - val_loss: 0.1871 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.18711 to 0.18291, saving model to best.model\n",
      "0s - loss: 0.3114 - acc: 0.8764 - val_loss: 0.1829 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.18291 to 0.17901, saving model to best.model\n",
      "0s - loss: 0.2713 - acc: 0.9438 - val_loss: 0.1790 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.17901 to 0.17512, saving model to best.model\n",
      "0s - loss: 0.2446 - acc: 0.9213 - val_loss: 0.1751 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.17512 to 0.17100, saving model to best.model\n",
      "0s - loss: 0.2332 - acc: 0.9326 - val_loss: 0.1710 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.17100 to 0.16727, saving model to best.model\n",
      "0s - loss: 0.3119 - acc: 0.8989 - val_loss: 0.1673 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.16727 to 0.16365, saving model to best.model\n",
      "0s - loss: 0.2956 - acc: 0.8876 - val_loss: 0.1636 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.16365 to 0.16053, saving model to best.model\n",
      "0s - loss: 0.2603 - acc: 0.9101 - val_loss: 0.1605 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.16053 to 0.15733, saving model to best.model\n",
      "0s - loss: 0.2540 - acc: 0.9213 - val_loss: 0.1573 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.15733 to 0.15449, saving model to best.model\n",
      "0s - loss: 0.2278 - acc: 0.9326 - val_loss: 0.1545 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.15449 to 0.15146, saving model to best.model\n",
      "0s - loss: 0.2498 - acc: 0.8876 - val_loss: 0.1515 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.15146 to 0.14873, saving model to best.model\n",
      "0s - loss: 0.2592 - acc: 0.9326 - val_loss: 0.1487 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.14873 to 0.14628, saving model to best.model\n",
      "0s - loss: 0.2869 - acc: 0.8876 - val_loss: 0.1463 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.14628 to 0.14386, saving model to best.model\n",
      "0s - loss: 0.2001 - acc: 0.9326 - val_loss: 0.1439 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.14386 to 0.14108, saving model to best.model\n",
      "0s - loss: 0.2351 - acc: 0.9101 - val_loss: 0.1411 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.14108 to 0.13804, saving model to best.model\n",
      "0s - loss: 0.2010 - acc: 0.9551 - val_loss: 0.1380 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.13804 to 0.13537, saving model to best.model\n",
      "0s - loss: 0.3179 - acc: 0.8989 - val_loss: 0.1354 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.13537 to 0.13273, saving model to best.model\n",
      "0s - loss: 0.1688 - acc: 0.9438 - val_loss: 0.1327 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.13273 to 0.13013, saving model to best.model\n",
      "0s - loss: 0.2340 - acc: 0.9551 - val_loss: 0.1301 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.13013 to 0.12743, saving model to best.model\n",
      "0s - loss: 0.1940 - acc: 0.9213 - val_loss: 0.1274 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.12743 to 0.12511, saving model to best.model\n",
      "0s - loss: 0.2751 - acc: 0.9101 - val_loss: 0.1251 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.12511 to 0.12314, saving model to best.model\n",
      "0s - loss: 0.2045 - acc: 0.9438 - val_loss: 0.1231 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.12314 to 0.12134, saving model to best.model\n",
      "0s - loss: 0.1935 - acc: 0.9326 - val_loss: 0.1213 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.12134 to 0.11944, saving model to best.model\n",
      "0s - loss: 0.1661 - acc: 0.9551 - val_loss: 0.1194 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.11944 to 0.11777, saving model to best.model\n",
      "0s - loss: 0.1716 - acc: 0.9438 - val_loss: 0.1178 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.11777 to 0.11617, saving model to best.model\n",
      "0s - loss: 0.1889 - acc: 0.9551 - val_loss: 0.1162 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.11617 to 0.11462, saving model to best.model\n",
      "0s - loss: 0.1537 - acc: 0.9663 - val_loss: 0.1146 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.11462 to 0.11304, saving model to best.model\n",
      "0s - loss: 0.1909 - acc: 0.9438 - val_loss: 0.1130 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.11304 to 0.11136, saving model to best.model\n",
      "0s - loss: 0.1937 - acc: 0.9326 - val_loss: 0.1114 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.11136 to 0.10948, saving model to best.model\n",
      "0s - loss: 0.2015 - acc: 0.9326 - val_loss: 0.1095 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.10948 to 0.10763, saving model to best.model\n",
      "0s - loss: 0.1497 - acc: 0.9663 - val_loss: 0.1076 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.10763 to 0.10576, saving model to best.model\n",
      "0s - loss: 0.1705 - acc: 0.9326 - val_loss: 0.1058 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.10576 to 0.10417, saving model to best.model\n",
      "0s - loss: 0.2066 - acc: 0.9101 - val_loss: 0.1042 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.10417 to 0.10277, saving model to best.model\n",
      "0s - loss: 0.1974 - acc: 0.9326 - val_loss: 0.1028 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.10277 to 0.10138, saving model to best.model\n",
      "0s - loss: 0.1880 - acc: 0.9438 - val_loss: 0.1014 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.10138 to 0.09984, saving model to best.model\n",
      "0s - loss: 0.1965 - acc: 0.9213 - val_loss: 0.0998 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.09984 to 0.09828, saving model to best.model\n",
      "0s - loss: 0.2170 - acc: 0.9438 - val_loss: 0.0983 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.09828 to 0.09692, saving model to best.model\n",
      "0s - loss: 0.1522 - acc: 0.9663 - val_loss: 0.0969 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.09692 to 0.09572, saving model to best.model\n",
      "0s - loss: 0.1732 - acc: 0.9326 - val_loss: 0.0957 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.09572 to 0.09452, saving model to best.model\n",
      "0s - loss: 0.1678 - acc: 0.9551 - val_loss: 0.0945 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.09452 to 0.09331, saving model to best.model\n",
      "0s - loss: 0.1913 - acc: 0.9213 - val_loss: 0.0933 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.09331 to 0.09216, saving model to best.model\n",
      "0s - loss: 0.1500 - acc: 0.9663 - val_loss: 0.0922 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.79379, saving model to best.model\n",
      "0s - loss: 2.1471 - acc: 0.2247 - val_loss: 1.7938 - val_acc: 0.1739\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.79379 to 1.57170, saving model to best.model\n",
      "0s - loss: 1.8960 - acc: 0.2809 - val_loss: 1.5717 - val_acc: 0.1739\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.57170 to 1.39072, saving model to best.model\n",
      "0s - loss: 1.7732 - acc: 0.2360 - val_loss: 1.3907 - val_acc: 0.1739\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.39072 to 1.25196, saving model to best.model\n",
      "0s - loss: 1.6679 - acc: 0.2247 - val_loss: 1.2520 - val_acc: 0.1739\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.25196 to 1.14967, saving model to best.model\n",
      "0s - loss: 1.6805 - acc: 0.3034 - val_loss: 1.1497 - val_acc: 0.5217\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.14967 to 1.07902, saving model to best.model\n",
      "0s - loss: 1.4256 - acc: 0.2921 - val_loss: 1.0790 - val_acc: 0.5217\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.07902 to 1.03469, saving model to best.model\n",
      "0s - loss: 1.5370 - acc: 0.3034 - val_loss: 1.0347 - val_acc: 0.5217\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.03469 to 1.01177, saving model to best.model\n",
      "0s - loss: 1.3133 - acc: 0.3933 - val_loss: 1.0118 - val_acc: 0.5217\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.01177 to 1.00512, saving model to best.model\n",
      "0s - loss: 1.3442 - acc: 0.3258 - val_loss: 1.0051 - val_acc: 0.5217\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1894 - acc: 0.4045 - val_loss: 1.0125 - val_acc: 0.5217\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.3364 - acc: 0.3371 - val_loss: 1.0304 - val_acc: 0.7391\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2203 - acc: 0.3371 - val_loss: 1.0544 - val_acc: 0.3043\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2581 - acc: 0.3258 - val_loss: 1.0795 - val_acc: 0.3043\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2959 - acc: 0.3034 - val_loss: 1.1021 - val_acc: 0.3043\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.3284 - acc: 0.3708 - val_loss: 1.1182 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1655 - acc: 0.4382 - val_loss: 1.1301 - val_acc: 0.3043\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.3431 - acc: 0.3258 - val_loss: 1.1345 - val_acc: 0.3043\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.3468 - acc: 0.3258 - val_loss: 1.1299 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.3425 - acc: 0.3483 - val_loss: 1.1199 - val_acc: 0.3043\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.3019 - acc: 0.3596 - val_loss: 1.1061 - val_acc: 0.3043\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.3240 - acc: 0.3483 - val_loss: 1.0895 - val_acc: 0.3043\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.3470 - acc: 0.3371 - val_loss: 1.0708 - val_acc: 0.3043\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2430 - acc: 0.3483 - val_loss: 1.0535 - val_acc: 0.3043\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1894 - acc: 0.3708 - val_loss: 1.0377 - val_acc: 0.3043\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.2725 - acc: 0.3258 - val_loss: 1.0259 - val_acc: 0.3043\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.2368 - acc: 0.3708 - val_loss: 1.0164 - val_acc: 0.3043\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.2396 - acc: 0.2809 - val_loss: 1.0088 - val_acc: 0.7826\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.00512 to 1.00285, saving model to best.model\n",
      "0s - loss: 1.2532 - acc: 0.2022 - val_loss: 1.0029 - val_acc: 0.7826\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.00285 to 0.99994, saving model to best.model\n",
      "0s - loss: 1.1013 - acc: 0.5056 - val_loss: 0.9999 - val_acc: 0.6522\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.99994 to 0.99808, saving model to best.model\n",
      "0s - loss: 1.1846 - acc: 0.3483 - val_loss: 0.9981 - val_acc: 0.5217\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.99808 to 0.99705, saving model to best.model\n",
      "0s - loss: 1.2239 - acc: 0.3371 - val_loss: 0.9970 - val_acc: 0.5217\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.99705 to 0.99652, saving model to best.model\n",
      "0s - loss: 1.2226 - acc: 0.3820 - val_loss: 0.9965 - val_acc: 0.5217\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.99652 to 0.99643, saving model to best.model\n",
      "0s - loss: 1.1830 - acc: 0.3146 - val_loss: 0.9964 - val_acc: 0.5217\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1771 - acc: 0.3596 - val_loss: 0.9977 - val_acc: 0.5217\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1212 - acc: 0.3933 - val_loss: 0.9986 - val_acc: 0.5217\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.2165 - acc: 0.3820 - val_loss: 0.9991 - val_acc: 0.5217\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 1.1727 - acc: 0.4382 - val_loss: 0.9993 - val_acc: 0.5217\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 1.1127 - acc: 0.4045 - val_loss: 0.9990 - val_acc: 0.5217\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 1.1169 - acc: 0.4157 - val_loss: 0.9986 - val_acc: 0.5652\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 1.1720 - acc: 0.3371 - val_loss: 0.9984 - val_acc: 0.6522\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 1.0933 - acc: 0.4270 - val_loss: 0.9977 - val_acc: 0.6522\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 1.1631 - acc: 0.4157 - val_loss: 0.9965 - val_acc: 0.7826\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.99643 to 0.99523, saving model to best.model\n",
      "0s - loss: 1.0588 - acc: 0.4719 - val_loss: 0.9952 - val_acc: 0.8261\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.99523 to 0.99358, saving model to best.model\n",
      "0s - loss: 1.2064 - acc: 0.3708 - val_loss: 0.9936 - val_acc: 0.8261\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.99358 to 0.99213, saving model to best.model\n",
      "0s - loss: 1.1738 - acc: 0.3483 - val_loss: 0.9921 - val_acc: 0.8261\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.99213 to 0.99010, saving model to best.model\n",
      "0s - loss: 1.1319 - acc: 0.4045 - val_loss: 0.9901 - val_acc: 0.7826\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.99010 to 0.98812, saving model to best.model\n",
      "0s - loss: 1.0437 - acc: 0.4494 - val_loss: 0.9881 - val_acc: 0.7826\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.98812 to 0.98602, saving model to best.model\n",
      "0s - loss: 1.1639 - acc: 0.3596 - val_loss: 0.9860 - val_acc: 0.7826\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.98602 to 0.98404, saving model to best.model\n",
      "0s - loss: 1.1413 - acc: 0.3820 - val_loss: 0.9840 - val_acc: 0.7391\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.98404 to 0.98158, saving model to best.model\n",
      "0s - loss: 1.1125 - acc: 0.3708 - val_loss: 0.9816 - val_acc: 0.7391\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.98158 to 0.97880, saving model to best.model\n",
      "0s - loss: 1.0955 - acc: 0.4157 - val_loss: 0.9788 - val_acc: 0.6957\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.97880 to 0.97640, saving model to best.model\n",
      "0s - loss: 1.1628 - acc: 0.3146 - val_loss: 0.9764 - val_acc: 0.6957\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.97640 to 0.97337, saving model to best.model\n",
      "0s - loss: 1.0402 - acc: 0.4157 - val_loss: 0.9734 - val_acc: 0.6957\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.97337 to 0.97119, saving model to best.model\n",
      "0s - loss: 1.0749 - acc: 0.4045 - val_loss: 0.9712 - val_acc: 0.6957\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.97119 to 0.96903, saving model to best.model\n",
      "0s - loss: 1.0926 - acc: 0.4494 - val_loss: 0.9690 - val_acc: 0.6957\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.96903 to 0.96634, saving model to best.model\n",
      "0s - loss: 1.0590 - acc: 0.4607 - val_loss: 0.9663 - val_acc: 0.6957\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.96634 to 0.96420, saving model to best.model\n",
      "0s - loss: 1.0823 - acc: 0.3820 - val_loss: 0.9642 - val_acc: 0.6957\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.96420 to 0.96203, saving model to best.model\n",
      "0s - loss: 1.0328 - acc: 0.4607 - val_loss: 0.9620 - val_acc: 0.6957\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.96203 to 0.95938, saving model to best.model\n",
      "0s - loss: 1.0904 - acc: 0.4270 - val_loss: 0.9594 - val_acc: 0.6957\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.95938 to 0.95628, saving model to best.model\n",
      "0s - loss: 1.0550 - acc: 0.4270 - val_loss: 0.9563 - val_acc: 0.6957\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.95628 to 0.95256, saving model to best.model\n",
      "0s - loss: 1.0345 - acc: 0.4494 - val_loss: 0.9526 - val_acc: 0.6957\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.95256 to 0.94784, saving model to best.model\n",
      "0s - loss: 1.1049 - acc: 0.4157 - val_loss: 0.9478 - val_acc: 0.6957\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.94784 to 0.94365, saving model to best.model\n",
      "0s - loss: 1.0987 - acc: 0.4270 - val_loss: 0.9436 - val_acc: 0.7391\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.94365 to 0.93973, saving model to best.model\n",
      "0s - loss: 1.0248 - acc: 0.4382 - val_loss: 0.9397 - val_acc: 0.7391\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.93973 to 0.93541, saving model to best.model\n",
      "0s - loss: 1.0441 - acc: 0.4494 - val_loss: 0.9354 - val_acc: 0.7391\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.93541 to 0.93128, saving model to best.model\n",
      "0s - loss: 1.1127 - acc: 0.3933 - val_loss: 0.9313 - val_acc: 0.7826\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.93128 to 0.92665, saving model to best.model\n",
      "0s - loss: 1.0504 - acc: 0.4607 - val_loss: 0.9266 - val_acc: 0.7826\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.92665 to 0.92177, saving model to best.model\n",
      "0s - loss: 1.0683 - acc: 0.4045 - val_loss: 0.9218 - val_acc: 0.7826\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.92177 to 0.91727, saving model to best.model\n",
      "0s - loss: 1.0493 - acc: 0.4382 - val_loss: 0.9173 - val_acc: 0.7826\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.91727 to 0.91306, saving model to best.model\n",
      "0s - loss: 1.0177 - acc: 0.4831 - val_loss: 0.9131 - val_acc: 0.7826\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.91306 to 0.90908, saving model to best.model\n",
      "0s - loss: 1.0781 - acc: 0.3483 - val_loss: 0.9091 - val_acc: 0.7826\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.90908 to 0.90505, saving model to best.model\n",
      "0s - loss: 1.1147 - acc: 0.4382 - val_loss: 0.9050 - val_acc: 0.7826\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.90505 to 0.90153, saving model to best.model\n",
      "0s - loss: 0.9489 - acc: 0.5618 - val_loss: 0.9015 - val_acc: 0.7826\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.90153 to 0.89776, saving model to best.model\n",
      "0s - loss: 0.9521 - acc: 0.5281 - val_loss: 0.8978 - val_acc: 0.7826\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.89776 to 0.89401, saving model to best.model\n",
      "0s - loss: 1.0651 - acc: 0.4494 - val_loss: 0.8940 - val_acc: 0.7826\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.89401 to 0.89013, saving model to best.model\n",
      "0s - loss: 0.9517 - acc: 0.5169 - val_loss: 0.8901 - val_acc: 0.7826\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.89013 to 0.88623, saving model to best.model\n",
      "0s - loss: 0.9608 - acc: 0.5393 - val_loss: 0.8862 - val_acc: 0.7826\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.88623 to 0.88174, saving model to best.model\n",
      "0s - loss: 0.9796 - acc: 0.5281 - val_loss: 0.8817 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.88174 to 0.87638, saving model to best.model\n",
      "0s - loss: 0.9461 - acc: 0.5955 - val_loss: 0.8764 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.87638 to 0.87157, saving model to best.model\n",
      "0s - loss: 1.0005 - acc: 0.4944 - val_loss: 0.8716 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.87157 to 0.86615, saving model to best.model\n",
      "0s - loss: 0.9936 - acc: 0.4944 - val_loss: 0.8662 - val_acc: 0.7826\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.86615 to 0.86085, saving model to best.model\n",
      "0s - loss: 1.0295 - acc: 0.4607 - val_loss: 0.8608 - val_acc: 0.7826\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.86085 to 0.85565, saving model to best.model\n",
      "0s - loss: 0.9252 - acc: 0.5393 - val_loss: 0.8556 - val_acc: 0.7391\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.85565 to 0.84995, saving model to best.model\n",
      "0s - loss: 0.9259 - acc: 0.5506 - val_loss: 0.8499 - val_acc: 0.7391\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.84995 to 0.84380, saving model to best.model\n",
      "0s - loss: 0.9394 - acc: 0.5281 - val_loss: 0.8438 - val_acc: 0.7391\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.84380 to 0.83638, saving model to best.model\n",
      "0s - loss: 0.9550 - acc: 0.5393 - val_loss: 0.8364 - val_acc: 0.7391\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.83638 to 0.82838, saving model to best.model\n",
      "0s - loss: 0.9065 - acc: 0.5730 - val_loss: 0.8284 - val_acc: 0.7391\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.82838 to 0.82040, saving model to best.model\n",
      "0s - loss: 0.9209 - acc: 0.5618 - val_loss: 0.8204 - val_acc: 0.7826\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.82040 to 0.81188, saving model to best.model\n",
      "0s - loss: 0.9245 - acc: 0.5843 - val_loss: 0.8119 - val_acc: 0.7826\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.81188 to 0.80288, saving model to best.model\n",
      "0s - loss: 0.8207 - acc: 0.6854 - val_loss: 0.8029 - val_acc: 0.7826\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.80288 to 0.79397, saving model to best.model\n",
      "0s - loss: 0.9606 - acc: 0.5169 - val_loss: 0.7940 - val_acc: 0.7826\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.79397 to 0.78487, saving model to best.model\n",
      "0s - loss: 0.8968 - acc: 0.5169 - val_loss: 0.7849 - val_acc: 0.7826\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.78487 to 0.77613, saving model to best.model\n",
      "0s - loss: 0.9141 - acc: 0.5730 - val_loss: 0.7761 - val_acc: 0.7826\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.77613 to 0.76721, saving model to best.model\n",
      "0s - loss: 0.8540 - acc: 0.6180 - val_loss: 0.7672 - val_acc: 0.7826\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.76721 to 0.75806, saving model to best.model\n",
      "0s - loss: 0.8068 - acc: 0.6292 - val_loss: 0.7581 - val_acc: 0.7826\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.75806 to 0.74901, saving model to best.model\n",
      "0s - loss: 0.8502 - acc: 0.5955 - val_loss: 0.7490 - val_acc: 0.7826\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.74901 to 0.73996, saving model to best.model\n",
      "0s - loss: 0.8174 - acc: 0.6966 - val_loss: 0.7400 - val_acc: 0.7826\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.73996 to 0.73161, saving model to best.model\n",
      "0s - loss: 0.8037 - acc: 0.6629 - val_loss: 0.7316 - val_acc: 0.7826\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.73161 to 0.72332, saving model to best.model\n",
      "0s - loss: 0.8741 - acc: 0.5506 - val_loss: 0.7233 - val_acc: 0.7826\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.72332 to 0.71546, saving model to best.model\n",
      "0s - loss: 0.8158 - acc: 0.6292 - val_loss: 0.7155 - val_acc: 0.7826\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.71546 to 0.70727, saving model to best.model\n",
      "0s - loss: 0.8008 - acc: 0.6629 - val_loss: 0.7073 - val_acc: 0.7826\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.70727 to 0.69922, saving model to best.model\n",
      "0s - loss: 0.7961 - acc: 0.7303 - val_loss: 0.6992 - val_acc: 0.7826\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.69922 to 0.69119, saving model to best.model\n",
      "0s - loss: 0.8290 - acc: 0.6292 - val_loss: 0.6912 - val_acc: 0.8261\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.69119 to 0.68295, saving model to best.model\n",
      "0s - loss: 0.7631 - acc: 0.6517 - val_loss: 0.6829 - val_acc: 0.8261\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.68295 to 0.67508, saving model to best.model\n",
      "0s - loss: 0.7585 - acc: 0.6404 - val_loss: 0.6751 - val_acc: 0.8261\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.67508 to 0.66640, saving model to best.model\n",
      "0s - loss: 0.7270 - acc: 0.7079 - val_loss: 0.6664 - val_acc: 0.8261\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66640 to 0.65664, saving model to best.model\n",
      "0s - loss: 0.7400 - acc: 0.6629 - val_loss: 0.6566 - val_acc: 0.8261\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65664 to 0.64665, saving model to best.model\n",
      "0s - loss: 0.7594 - acc: 0.6517 - val_loss: 0.6467 - val_acc: 0.8261\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64665 to 0.63680, saving model to best.model\n",
      "0s - loss: 0.6718 - acc: 0.7753 - val_loss: 0.6368 - val_acc: 0.8261\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.63680 to 0.62739, saving model to best.model\n",
      "0s - loss: 0.7588 - acc: 0.6517 - val_loss: 0.6274 - val_acc: 0.8261\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.62739 to 0.61760, saving model to best.model\n",
      "0s - loss: 0.7729 - acc: 0.6742 - val_loss: 0.6176 - val_acc: 0.8261\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.61760 to 0.60817, saving model to best.model\n",
      "0s - loss: 0.7066 - acc: 0.7416 - val_loss: 0.6082 - val_acc: 0.8261\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.60817 to 0.59810, saving model to best.model\n",
      "0s - loss: 0.6998 - acc: 0.6517 - val_loss: 0.5981 - val_acc: 0.8261\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.59810 to 0.58841, saving model to best.model\n",
      "0s - loss: 0.7025 - acc: 0.6629 - val_loss: 0.5884 - val_acc: 0.8261\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.58841 to 0.57799, saving model to best.model\n",
      "0s - loss: 0.6240 - acc: 0.7753 - val_loss: 0.5780 - val_acc: 0.8261\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.57799 to 0.56803, saving model to best.model\n",
      "0s - loss: 0.7044 - acc: 0.6854 - val_loss: 0.5680 - val_acc: 0.8261\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.56803 to 0.55820, saving model to best.model\n",
      "0s - loss: 0.6313 - acc: 0.7978 - val_loss: 0.5582 - val_acc: 0.8261\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.55820 to 0.54831, saving model to best.model\n",
      "0s - loss: 0.7373 - acc: 0.6629 - val_loss: 0.5483 - val_acc: 0.8261\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.54831 to 0.53846, saving model to best.model\n",
      "0s - loss: 0.6299 - acc: 0.7416 - val_loss: 0.5385 - val_acc: 0.8261\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.53846 to 0.52917, saving model to best.model\n",
      "0s - loss: 0.6088 - acc: 0.7753 - val_loss: 0.5292 - val_acc: 0.8261\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.52917 to 0.52029, saving model to best.model\n",
      "0s - loss: 0.6115 - acc: 0.7079 - val_loss: 0.5203 - val_acc: 0.8261\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.52029 to 0.51181, saving model to best.model\n",
      "0s - loss: 0.6047 - acc: 0.7640 - val_loss: 0.5118 - val_acc: 0.8261\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.51181 to 0.50373, saving model to best.model\n",
      "0s - loss: 0.5757 - acc: 0.8427 - val_loss: 0.5037 - val_acc: 0.8261\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.50373 to 0.49577, saving model to best.model\n",
      "0s - loss: 0.5529 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.8696\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.49577 to 0.48857, saving model to best.model\n",
      "0s - loss: 0.6200 - acc: 0.7528 - val_loss: 0.4886 - val_acc: 0.8696\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.48857 to 0.48190, saving model to best.model\n",
      "0s - loss: 0.5530 - acc: 0.7978 - val_loss: 0.4819 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.48190 to 0.47382, saving model to best.model\n",
      "0s - loss: 0.6452 - acc: 0.6517 - val_loss: 0.4738 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.47382 to 0.46546, saving model to best.model\n",
      "0s - loss: 0.5300 - acc: 0.7640 - val_loss: 0.4655 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.46546 to 0.45662, saving model to best.model\n",
      "0s - loss: 0.5560 - acc: 0.7528 - val_loss: 0.4566 - val_acc: 0.9565\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.45662 to 0.44837, saving model to best.model\n",
      "0s - loss: 0.5324 - acc: 0.8427 - val_loss: 0.4484 - val_acc: 0.9565\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.44837 to 0.44037, saving model to best.model\n",
      "0s - loss: 0.5496 - acc: 0.7865 - val_loss: 0.4404 - val_acc: 0.9565\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.44037 to 0.43255, saving model to best.model\n",
      "0s - loss: 0.5127 - acc: 0.7978 - val_loss: 0.4325 - val_acc: 0.9565\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.43255 to 0.42455, saving model to best.model\n",
      "0s - loss: 0.5315 - acc: 0.7865 - val_loss: 0.4246 - val_acc: 0.9565\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.42455 to 0.41726, saving model to best.model\n",
      "0s - loss: 0.4801 - acc: 0.8315 - val_loss: 0.4173 - val_acc: 0.9565\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.41726 to 0.41033, saving model to best.model\n",
      "0s - loss: 0.5306 - acc: 0.7978 - val_loss: 0.4103 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.41033 to 0.40388, saving model to best.model\n",
      "0s - loss: 0.5702 - acc: 0.7079 - val_loss: 0.4039 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.40388 to 0.39782, saving model to best.model\n",
      "0s - loss: 0.5095 - acc: 0.8090 - val_loss: 0.3978 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.39782 to 0.39163, saving model to best.model\n",
      "0s - loss: 0.5783 - acc: 0.7528 - val_loss: 0.3916 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.39163 to 0.38530, saving model to best.model\n",
      "0s - loss: 0.4623 - acc: 0.8315 - val_loss: 0.3853 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.38530 to 0.37883, saving model to best.model\n",
      "0s - loss: 0.4917 - acc: 0.7865 - val_loss: 0.3788 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.37883 to 0.37225, saving model to best.model\n",
      "0s - loss: 0.4586 - acc: 0.8427 - val_loss: 0.3723 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.37225 to 0.36635, saving model to best.model\n",
      "0s - loss: 0.3877 - acc: 0.8764 - val_loss: 0.3663 - val_acc: 0.9565\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.36635 to 0.36069, saving model to best.model\n",
      "0s - loss: 0.4675 - acc: 0.8090 - val_loss: 0.3607 - val_acc: 0.9565\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.36069 to 0.35427, saving model to best.model\n",
      "0s - loss: 0.5036 - acc: 0.7978 - val_loss: 0.3543 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.35427 to 0.34859, saving model to best.model\n",
      "0s - loss: 0.4893 - acc: 0.7978 - val_loss: 0.3486 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.34859 to 0.34347, saving model to best.model\n",
      "0s - loss: 0.4787 - acc: 0.8090 - val_loss: 0.3435 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.34347 to 0.33907, saving model to best.model\n",
      "0s - loss: 0.4611 - acc: 0.8427 - val_loss: 0.3391 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.33907 to 0.33599, saving model to best.model\n",
      "0s - loss: 0.4538 - acc: 0.8427 - val_loss: 0.3360 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.33599 to 0.33302, saving model to best.model\n",
      "0s - loss: 0.4989 - acc: 0.8090 - val_loss: 0.3330 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.33302 to 0.33042, saving model to best.model\n",
      "0s - loss: 0.4573 - acc: 0.8090 - val_loss: 0.3304 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.33042 to 0.32821, saving model to best.model\n",
      "0s - loss: 0.4289 - acc: 0.8315 - val_loss: 0.3282 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.32821 to 0.32568, saving model to best.model\n",
      "0s - loss: 0.3853 - acc: 0.8427 - val_loss: 0.3257 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.32568 to 0.32333, saving model to best.model\n",
      "0s - loss: 0.4445 - acc: 0.8090 - val_loss: 0.3233 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.32333 to 0.32091, saving model to best.model\n",
      "0s - loss: 0.4090 - acc: 0.8652 - val_loss: 0.3209 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.32091 to 0.31773, saving model to best.model\n",
      "0s - loss: 0.4395 - acc: 0.8090 - val_loss: 0.3177 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.31773 to 0.31422, saving model to best.model\n",
      "0s - loss: 0.4616 - acc: 0.7865 - val_loss: 0.3142 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.31422 to 0.31008, saving model to best.model\n",
      "0s - loss: 0.4077 - acc: 0.8315 - val_loss: 0.3101 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.31008 to 0.30463, saving model to best.model\n",
      "0s - loss: 0.3712 - acc: 0.9326 - val_loss: 0.3046 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.30463 to 0.29909, saving model to best.model\n",
      "0s - loss: 0.3930 - acc: 0.8764 - val_loss: 0.2991 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.29909 to 0.29384, saving model to best.model\n",
      "0s - loss: 0.4520 - acc: 0.7978 - val_loss: 0.2938 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.29384 to 0.28776, saving model to best.model\n",
      "0s - loss: 0.3492 - acc: 0.8989 - val_loss: 0.2878 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.28776 to 0.28213, saving model to best.model\n",
      "0s - loss: 0.3590 - acc: 0.8539 - val_loss: 0.2821 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.28213 to 0.27571, saving model to best.model\n",
      "0s - loss: 0.3925 - acc: 0.8090 - val_loss: 0.2757 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.27571 to 0.26928, saving model to best.model\n",
      "0s - loss: 0.3147 - acc: 0.8876 - val_loss: 0.2693 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.26928 to 0.26331, saving model to best.model\n",
      "0s - loss: 0.3516 - acc: 0.9213 - val_loss: 0.2633 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.26331 to 0.25782, saving model to best.model\n",
      "0s - loss: 0.3255 - acc: 0.9101 - val_loss: 0.2578 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.25782 to 0.25283, saving model to best.model\n",
      "0s - loss: 0.2898 - acc: 0.8989 - val_loss: 0.2528 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.25283 to 0.24805, saving model to best.model\n",
      "0s - loss: 0.3220 - acc: 0.8427 - val_loss: 0.2481 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.24805 to 0.24380, saving model to best.model\n",
      "0s - loss: 0.3268 - acc: 0.8764 - val_loss: 0.2438 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.24380 to 0.23961, saving model to best.model\n",
      "0s - loss: 0.2951 - acc: 0.9213 - val_loss: 0.2396 - val_acc: 0.9565\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.23961 to 0.23600, saving model to best.model\n",
      "0s - loss: 0.3333 - acc: 0.8652 - val_loss: 0.2360 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.23600 to 0.23253, saving model to best.model\n",
      "0s - loss: 0.3163 - acc: 0.8989 - val_loss: 0.2325 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.23253 to 0.22926, saving model to best.model\n",
      "0s - loss: 0.2382 - acc: 0.9326 - val_loss: 0.2293 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.22926 to 0.22684, saving model to best.model\n",
      "0s - loss: 0.3072 - acc: 0.8989 - val_loss: 0.2268 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.22684 to 0.22422, saving model to best.model\n",
      "0s - loss: 0.2462 - acc: 0.9326 - val_loss: 0.2242 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.22422 to 0.22240, saving model to best.model\n",
      "0s - loss: 0.3030 - acc: 0.8989 - val_loss: 0.2224 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.22240 to 0.22087, saving model to best.model\n",
      "0s - loss: 0.3049 - acc: 0.8764 - val_loss: 0.2209 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.22087 to 0.21965, saving model to best.model\n",
      "0s - loss: 0.2639 - acc: 0.9101 - val_loss: 0.2196 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.21965 to 0.21828, saving model to best.model\n",
      "0s - loss: 0.2689 - acc: 0.9326 - val_loss: 0.2183 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.21828 to 0.21707, saving model to best.model\n",
      "0s - loss: 0.2284 - acc: 0.9663 - val_loss: 0.2171 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.21707 to 0.21604, saving model to best.model\n",
      "0s - loss: 0.2039 - acc: 0.9438 - val_loss: 0.2160 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.21604 to 0.21480, saving model to best.model\n",
      "0s - loss: 0.2752 - acc: 0.9101 - val_loss: 0.2148 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.21480 to 0.21248, saving model to best.model\n",
      "0s - loss: 0.2676 - acc: 0.9213 - val_loss: 0.2125 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.21248 to 0.20956, saving model to best.model\n",
      "0s - loss: 0.2400 - acc: 0.9213 - val_loss: 0.2096 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.20956 to 0.20717, saving model to best.model\n",
      "0s - loss: 0.2864 - acc: 0.8876 - val_loss: 0.2072 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.20717 to 0.20395, saving model to best.model\n",
      "0s - loss: 0.2693 - acc: 0.8989 - val_loss: 0.2040 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.20395 to 0.20004, saving model to best.model\n",
      "0s - loss: 0.2274 - acc: 0.9326 - val_loss: 0.2000 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.20004 to 0.19568, saving model to best.model\n",
      "0s - loss: 0.2302 - acc: 0.9551 - val_loss: 0.1957 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.19568 to 0.19221, saving model to best.model\n",
      "0s - loss: 0.2159 - acc: 0.9438 - val_loss: 0.1922 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.19221 to 0.18878, saving model to best.model\n",
      "0s - loss: 0.2249 - acc: 0.9326 - val_loss: 0.1888 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.18878 to 0.18724, saving model to best.model\n",
      "0s - loss: 0.2536 - acc: 0.9438 - val_loss: 0.1872 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.18724 to 0.18544, saving model to best.model\n",
      "0s - loss: 0.2155 - acc: 0.9326 - val_loss: 0.1854 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.18544 to 0.18406, saving model to best.model\n",
      "0s - loss: 0.2442 - acc: 0.9213 - val_loss: 0.1841 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.18406 to 0.18352, saving model to best.model\n",
      "0s - loss: 0.2327 - acc: 0.9438 - val_loss: 0.1835 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.18352 to 0.18314, saving model to best.model\n",
      "0s - loss: 0.1915 - acc: 0.9663 - val_loss: 0.1831 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.18314 to 0.18302, saving model to best.model\n",
      "0s - loss: 0.2581 - acc: 0.9326 - val_loss: 0.1830 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.18302 to 0.18221, saving model to best.model\n",
      "0s - loss: 0.2101 - acc: 0.9326 - val_loss: 0.1822 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.18221 to 0.18129, saving model to best.model\n",
      "0s - loss: 0.2011 - acc: 0.9438 - val_loss: 0.1813 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.18129 to 0.17915, saving model to best.model\n",
      "0s - loss: 0.2157 - acc: 0.9326 - val_loss: 0.1792 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.17915 to 0.17650, saving model to best.model\n",
      "0s - loss: 0.2291 - acc: 0.9213 - val_loss: 0.1765 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.58917, saving model to best.model\n",
      "0s - loss: 1.4000 - acc: 0.3820 - val_loss: 1.5892 - val_acc: 0.3043\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.58917 to 1.46125, saving model to best.model\n",
      "0s - loss: 1.3585 - acc: 0.3820 - val_loss: 1.4613 - val_acc: 0.3043\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.46125 to 1.35570, saving model to best.model\n",
      "0s - loss: 1.2694 - acc: 0.3146 - val_loss: 1.3557 - val_acc: 0.1739\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.35570 to 1.26304, saving model to best.model\n",
      "0s - loss: 1.4667 - acc: 0.3034 - val_loss: 1.2630 - val_acc: 0.1739\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.26304 to 1.18502, saving model to best.model\n",
      "0s - loss: 1.4269 - acc: 0.3146 - val_loss: 1.1850 - val_acc: 0.1739\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.18502 to 1.13244, saving model to best.model\n",
      "0s - loss: 1.2624 - acc: 0.3708 - val_loss: 1.1324 - val_acc: 0.1739\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.13244 to 1.10114, saving model to best.model\n",
      "0s - loss: 1.2685 - acc: 0.2697 - val_loss: 1.1011 - val_acc: 0.1739\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.10114 to 1.08440, saving model to best.model\n",
      "0s - loss: 1.3264 - acc: 0.3483 - val_loss: 1.0844 - val_acc: 0.5217\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.08440 to 1.07396, saving model to best.model\n",
      "0s - loss: 1.1675 - acc: 0.3371 - val_loss: 1.0740 - val_acc: 0.5217\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.07396 to 1.07125, saving model to best.model\n",
      "0s - loss: 1.2880 - acc: 0.3258 - val_loss: 1.0712 - val_acc: 0.5217\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.07125 to 1.06971, saving model to best.model\n",
      "0s - loss: 1.3716 - acc: 0.3596 - val_loss: 1.0697 - val_acc: 0.5217\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.06971 to 1.06701, saving model to best.model\n",
      "0s - loss: 1.2313 - acc: 0.3596 - val_loss: 1.0670 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.06701 to 1.06616, saving model to best.model\n",
      "0s - loss: 1.4183 - acc: 0.2472 - val_loss: 1.0662 - val_acc: 0.5217\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2526 - acc: 0.3483 - val_loss: 1.0678 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1852 - acc: 0.3820 - val_loss: 1.0724 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.3298 - acc: 0.3708 - val_loss: 1.0820 - val_acc: 0.5217\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2453 - acc: 0.3034 - val_loss: 1.0919 - val_acc: 0.1739\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1375 - acc: 0.4045 - val_loss: 1.1041 - val_acc: 0.1739\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2416 - acc: 0.3146 - val_loss: 1.1184 - val_acc: 0.1739\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2834 - acc: 0.3483 - val_loss: 1.1341 - val_acc: 0.3043\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.2509 - acc: 0.3820 - val_loss: 1.1486 - val_acc: 0.3478\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.2116 - acc: 0.3483 - val_loss: 1.1616 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2599 - acc: 0.3483 - val_loss: 1.1707 - val_acc: 0.4783\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.2636 - acc: 0.4157 - val_loss: 1.1746 - val_acc: 0.4783\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1066 - acc: 0.4382 - val_loss: 1.1760 - val_acc: 0.4783\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.2275 - acc: 0.3258 - val_loss: 1.1762 - val_acc: 0.4783\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1484 - acc: 0.4045 - val_loss: 1.1749 - val_acc: 0.4783\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1706 - acc: 0.3933 - val_loss: 1.1705 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1767 - acc: 0.3708 - val_loss: 1.1638 - val_acc: 0.3478\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1826 - acc: 0.3820 - val_loss: 1.1576 - val_acc: 0.3043\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1106 - acc: 0.3933 - val_loss: 1.1506 - val_acc: 0.3043\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1539 - acc: 0.4157 - val_loss: 1.1435 - val_acc: 0.2609\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.0785 - acc: 0.4157 - val_loss: 1.1360 - val_acc: 0.2609\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1310 - acc: 0.3933 - val_loss: 1.1280 - val_acc: 0.2609\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.0705 - acc: 0.5281 - val_loss: 1.1174 - val_acc: 0.2609\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.1044 - acc: 0.4270 - val_loss: 1.1089 - val_acc: 0.3043\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 1.1865 - acc: 0.3371 - val_loss: 1.1016 - val_acc: 0.3043\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 1.1201 - acc: 0.4157 - val_loss: 1.0933 - val_acc: 0.3043\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 1.0796 - acc: 0.4719 - val_loss: 1.0857 - val_acc: 0.3478\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.28644, saving model to best.model\n",
      "0s - loss: 1.4446 - acc: 0.3820 - val_loss: 1.2864 - val_acc: 0.1739\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.28644 to 1.16438, saving model to best.model\n",
      "0s - loss: 1.4092 - acc: 0.3483 - val_loss: 1.1644 - val_acc: 0.1739\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.16438 to 1.08589, saving model to best.model\n",
      "0s - loss: 1.3723 - acc: 0.3596 - val_loss: 1.0859 - val_acc: 0.4783\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.08589 to 1.04319, saving model to best.model\n",
      "0s - loss: 1.2428 - acc: 0.3596 - val_loss: 1.0432 - val_acc: 0.5652\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.04319 to 1.02901, saving model to best.model\n",
      "0s - loss: 1.2118 - acc: 0.4831 - val_loss: 1.0290 - val_acc: 0.5652\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2558 - acc: 0.4045 - val_loss: 1.0387 - val_acc: 0.4348\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2960 - acc: 0.2809 - val_loss: 1.0602 - val_acc: 0.2609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2016 - acc: 0.4045 - val_loss: 1.0877 - val_acc: 0.2609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.3962 - acc: 0.3146 - val_loss: 1.1143 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2583 - acc: 0.3933 - val_loss: 1.1314 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2449 - acc: 0.3708 - val_loss: 1.1429 - val_acc: 0.2609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1964 - acc: 0.3371 - val_loss: 1.1481 - val_acc: 0.2609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.3222 - acc: 0.3708 - val_loss: 1.1440 - val_acc: 0.2609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.3361 - acc: 0.4157 - val_loss: 1.1347 - val_acc: 0.2609\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2316 - acc: 0.4157 - val_loss: 1.1239 - val_acc: 0.2609\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.3371 - acc: 0.2921 - val_loss: 1.1152 - val_acc: 0.2609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1525 - acc: 0.3708 - val_loss: 1.1083 - val_acc: 0.2609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2150 - acc: 0.3146 - val_loss: 1.1033 - val_acc: 0.2609\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2155 - acc: 0.4270 - val_loss: 1.1008 - val_acc: 0.2609\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2422 - acc: 0.2360 - val_loss: 1.0993 - val_acc: 0.2609\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.2053 - acc: 0.3371 - val_loss: 1.0958 - val_acc: 0.2609\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.3057 - acc: 0.3034 - val_loss: 1.0894 - val_acc: 0.2609\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1631 - acc: 0.4157 - val_loss: 1.0823 - val_acc: 0.2609\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1060 - acc: 0.4157 - val_loss: 1.0741 - val_acc: 0.5652\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1733 - acc: 0.4270 - val_loss: 1.0672 - val_acc: 0.7391\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1964 - acc: 0.3596 - val_loss: 1.0613 - val_acc: 0.8696\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1863 - acc: 0.3371 - val_loss: 1.0568 - val_acc: 0.8696\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.2482 - acc: 0.3034 - val_loss: 1.0531 - val_acc: 0.8261\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1706 - acc: 0.3034 - val_loss: 1.0494 - val_acc: 0.8261\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1822 - acc: 0.3708 - val_loss: 1.0469 - val_acc: 0.7826\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1574 - acc: 0.3820 - val_loss: 1.0453 - val_acc: 0.6957\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.13458, saving model to best.model\n",
      "0s - loss: 1.3362 - acc: 0.2697 - val_loss: 1.1346 - val_acc: 0.3043\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.4476 - acc: 0.2472 - val_loss: 1.1377 - val_acc: 0.3043\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.1535 - acc: 0.4270 - val_loss: 1.1539 - val_acc: 0.1739\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2375 - acc: 0.3146 - val_loss: 1.1751 - val_acc: 0.1739\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2977 - acc: 0.2921 - val_loss: 1.1880 - val_acc: 0.1739\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1931 - acc: 0.3258 - val_loss: 1.1915 - val_acc: 0.1739\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2169 - acc: 0.3371 - val_loss: 1.1937 - val_acc: 0.1739\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1976 - acc: 0.4270 - val_loss: 1.1865 - val_acc: 0.1739\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.3815 - acc: 0.3146 - val_loss: 1.1738 - val_acc: 0.1739\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.3102 - acc: 0.2247 - val_loss: 1.1588 - val_acc: 0.1739\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2076 - acc: 0.3933 - val_loss: 1.1436 - val_acc: 0.1739\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.13458 to 1.13235, saving model to best.model\n",
      "0s - loss: 1.2461 - acc: 0.3146 - val_loss: 1.1323 - val_acc: 0.1739\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.13235 to 1.12625, saving model to best.model\n",
      "0s - loss: 1.3388 - acc: 0.3034 - val_loss: 1.1263 - val_acc: 0.1739\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.12625 to 1.12224, saving model to best.model\n",
      "0s - loss: 1.1005 - acc: 0.3933 - val_loss: 1.1222 - val_acc: 0.1739\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.12224 to 1.11657, saving model to best.model\n",
      "0s - loss: 1.2894 - acc: 0.3146 - val_loss: 1.1166 - val_acc: 0.1739\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.11657 to 1.10989, saving model to best.model\n",
      "0s - loss: 1.1858 - acc: 0.3708 - val_loss: 1.1099 - val_acc: 0.1739\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.10989 to 1.10506, saving model to best.model\n",
      "0s - loss: 1.1700 - acc: 0.3258 - val_loss: 1.1051 - val_acc: 0.1739\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.10506 to 1.10023, saving model to best.model\n",
      "0s - loss: 1.1836 - acc: 0.3483 - val_loss: 1.1002 - val_acc: 0.1739\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.10023 to 1.09629, saving model to best.model\n",
      "0s - loss: 1.1669 - acc: 0.4045 - val_loss: 1.0963 - val_acc: 0.1739\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.09629 to 1.09572, saving model to best.model\n",
      "0s - loss: 1.3009 - acc: 0.2697 - val_loss: 1.0957 - val_acc: 0.1739\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1344 - acc: 0.4045 - val_loss: 1.0987 - val_acc: 0.1739\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1857 - acc: 0.3146 - val_loss: 1.1031 - val_acc: 0.1739\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1928 - acc: 0.3708 - val_loss: 1.1061 - val_acc: 0.1739\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1174 - acc: 0.3820 - val_loss: 1.1110 - val_acc: 0.1739\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.2096 - acc: 0.3146 - val_loss: 1.1135 - val_acc: 0.1739\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1292 - acc: 0.4494 - val_loss: 1.1128 - val_acc: 0.1739\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.2432 - acc: 0.3820 - val_loss: 1.1106 - val_acc: 0.1739\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1131 - acc: 0.4157 - val_loss: 1.1090 - val_acc: 0.1739\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.0847 - acc: 0.4157 - val_loss: 1.1070 - val_acc: 0.1739\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1978 - acc: 0.2584 - val_loss: 1.1025 - val_acc: 0.1739\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1605 - acc: 0.3483 - val_loss: 1.0981 - val_acc: 0.1739\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.09572 to 1.09280, saving model to best.model\n",
      "0s - loss: 1.1778 - acc: 0.4045 - val_loss: 1.0928 - val_acc: 0.1739\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.09280 to 1.08777, saving model to best.model\n",
      "0s - loss: 1.2173 - acc: 0.3708 - val_loss: 1.0878 - val_acc: 0.1739\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.08777 to 1.08170, saving model to best.model\n",
      "0s - loss: 1.1629 - acc: 0.3820 - val_loss: 1.0817 - val_acc: 0.1739\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.08170 to 1.07335, saving model to best.model\n",
      "0s - loss: 1.1499 - acc: 0.3820 - val_loss: 1.0734 - val_acc: 0.1739\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.07335 to 1.06548, saving model to best.model\n",
      "0s - loss: 1.1083 - acc: 0.3483 - val_loss: 1.0655 - val_acc: 0.1739\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.06548 to 1.06040, saving model to best.model\n",
      "0s - loss: 1.1837 - acc: 0.3146 - val_loss: 1.0604 - val_acc: 0.1739\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.06040 to 1.05562, saving model to best.model\n",
      "0s - loss: 1.1443 - acc: 0.3708 - val_loss: 1.0556 - val_acc: 0.2609\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.05562 to 1.05110, saving model to best.model\n",
      "0s - loss: 1.1187 - acc: 0.4382 - val_loss: 1.0511 - val_acc: 0.2609\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.05110 to 1.04894, saving model to best.model\n",
      "0s - loss: 1.0525 - acc: 0.4944 - val_loss: 1.0489 - val_acc: 0.3478\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.04894 to 1.04792, saving model to best.model\n",
      "0s - loss: 1.0457 - acc: 0.3933 - val_loss: 1.0479 - val_acc: 0.3478\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.04792 to 1.04764, saving model to best.model\n",
      "0s - loss: 1.1057 - acc: 0.4045 - val_loss: 1.0476 - val_acc: 0.3478\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.04764 to 1.04586, saving model to best.model\n",
      "0s - loss: 1.1252 - acc: 0.3933 - val_loss: 1.0459 - val_acc: 0.3478\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.04586 to 1.04407, saving model to best.model\n",
      "0s - loss: 1.0822 - acc: 0.4270 - val_loss: 1.0441 - val_acc: 0.3478\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 1.0785 - acc: 0.3708 - val_loss: 1.0441 - val_acc: 0.2609\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.04407 to 1.04318, saving model to best.model\n",
      "0s - loss: 1.0588 - acc: 0.4382 - val_loss: 1.0432 - val_acc: 0.2609\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.04318 to 1.04151, saving model to best.model\n",
      "0s - loss: 1.0179 - acc: 0.4831 - val_loss: 1.0415 - val_acc: 0.2609\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.04151 to 1.03960, saving model to best.model\n",
      "0s - loss: 1.0924 - acc: 0.4382 - val_loss: 1.0396 - val_acc: 0.2609\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.03960 to 1.03679, saving model to best.model\n",
      "0s - loss: 1.0917 - acc: 0.4157 - val_loss: 1.0368 - val_acc: 0.2609\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.03679 to 1.03398, saving model to best.model\n",
      "0s - loss: 1.0456 - acc: 0.4607 - val_loss: 1.0340 - val_acc: 0.2609\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.03398 to 1.03046, saving model to best.model\n",
      "0s - loss: 1.0939 - acc: 0.4045 - val_loss: 1.0305 - val_acc: 0.3478\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.03046 to 1.02635, saving model to best.model\n",
      "0s - loss: 1.0155 - acc: 0.4831 - val_loss: 1.0264 - val_acc: 0.3478\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.02635 to 1.02069, saving model to best.model\n",
      "0s - loss: 1.0865 - acc: 0.3820 - val_loss: 1.0207 - val_acc: 0.3478\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.02069 to 1.01220, saving model to best.model\n",
      "0s - loss: 1.0270 - acc: 0.4719 - val_loss: 1.0122 - val_acc: 0.3478\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 1.01220 to 1.00154, saving model to best.model\n",
      "0s - loss: 0.9988 - acc: 0.4382 - val_loss: 1.0015 - val_acc: 0.3478\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 1.00154 to 0.98906, saving model to best.model\n",
      "0s - loss: 1.0316 - acc: 0.4944 - val_loss: 0.9891 - val_acc: 0.3478\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.98906 to 0.97650, saving model to best.model\n",
      "0s - loss: 1.0267 - acc: 0.5281 - val_loss: 0.9765 - val_acc: 0.3913\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.97650 to 0.96409, saving model to best.model\n",
      "0s - loss: 1.0968 - acc: 0.4382 - val_loss: 0.9641 - val_acc: 0.5217\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.96409 to 0.95082, saving model to best.model\n",
      "0s - loss: 1.0098 - acc: 0.4944 - val_loss: 0.9508 - val_acc: 0.5652\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.95082 to 0.93813, saving model to best.model\n",
      "0s - loss: 0.9821 - acc: 0.4944 - val_loss: 0.9381 - val_acc: 0.7391\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.93813 to 0.92571, saving model to best.model\n",
      "0s - loss: 0.9914 - acc: 0.4719 - val_loss: 0.9257 - val_acc: 0.9565\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.92571 to 0.91356, saving model to best.model\n",
      "0s - loss: 1.0299 - acc: 0.4494 - val_loss: 0.9136 - val_acc: 0.9565\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.91356 to 0.90191, saving model to best.model\n",
      "0s - loss: 1.0657 - acc: 0.4382 - val_loss: 0.9019 - val_acc: 0.9565\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.90191 to 0.89069, saving model to best.model\n",
      "0s - loss: 0.9950 - acc: 0.4831 - val_loss: 0.8907 - val_acc: 1.0000\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.89069 to 0.87907, saving model to best.model\n",
      "0s - loss: 1.0991 - acc: 0.3708 - val_loss: 0.8791 - val_acc: 1.0000\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.87907 to 0.86730, saving model to best.model\n",
      "0s - loss: 1.0008 - acc: 0.4719 - val_loss: 0.8673 - val_acc: 1.0000\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.86730 to 0.85763, saving model to best.model\n",
      "0s - loss: 0.9635 - acc: 0.5281 - val_loss: 0.8576 - val_acc: 1.0000\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.85763 to 0.84808, saving model to best.model\n",
      "0s - loss: 0.9538 - acc: 0.5393 - val_loss: 0.8481 - val_acc: 1.0000\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.84808 to 0.84009, saving model to best.model\n",
      "0s - loss: 0.9468 - acc: 0.4944 - val_loss: 0.8401 - val_acc: 1.0000\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.84009 to 0.83179, saving model to best.model\n",
      "0s - loss: 0.9029 - acc: 0.5730 - val_loss: 0.8318 - val_acc: 1.0000\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.83179 to 0.82338, saving model to best.model\n",
      "0s - loss: 0.9070 - acc: 0.5843 - val_loss: 0.8234 - val_acc: 1.0000\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.82338 to 0.81520, saving model to best.model\n",
      "0s - loss: 0.9292 - acc: 0.5169 - val_loss: 0.8152 - val_acc: 1.0000\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.81520 to 0.80820, saving model to best.model\n",
      "0s - loss: 0.8939 - acc: 0.5506 - val_loss: 0.8082 - val_acc: 1.0000\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.80820 to 0.80112, saving model to best.model\n",
      "0s - loss: 0.9502 - acc: 0.4831 - val_loss: 0.8011 - val_acc: 1.0000\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.80112 to 0.79262, saving model to best.model\n",
      "0s - loss: 0.8982 - acc: 0.6180 - val_loss: 0.7926 - val_acc: 1.0000\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.79262 to 0.78225, saving model to best.model\n",
      "0s - loss: 0.9279 - acc: 0.5056 - val_loss: 0.7822 - val_acc: 1.0000\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.78225 to 0.76997, saving model to best.model\n",
      "0s - loss: 0.9490 - acc: 0.4944 - val_loss: 0.7700 - val_acc: 1.0000\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.76997 to 0.75717, saving model to best.model\n",
      "0s - loss: 0.8359 - acc: 0.5843 - val_loss: 0.7572 - val_acc: 1.0000\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.75717 to 0.74340, saving model to best.model\n",
      "0s - loss: 0.8766 - acc: 0.5393 - val_loss: 0.7434 - val_acc: 1.0000\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.74340 to 0.72802, saving model to best.model\n",
      "0s - loss: 0.8347 - acc: 0.6629 - val_loss: 0.7280 - val_acc: 1.0000\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.72802 to 0.71032, saving model to best.model\n",
      "0s - loss: 0.8984 - acc: 0.5730 - val_loss: 0.7103 - val_acc: 1.0000\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.71032 to 0.69186, saving model to best.model\n",
      "0s - loss: 0.8181 - acc: 0.6742 - val_loss: 0.6919 - val_acc: 1.0000\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69186 to 0.67500, saving model to best.model\n",
      "0s - loss: 0.8210 - acc: 0.6966 - val_loss: 0.6750 - val_acc: 1.0000\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67500 to 0.65890, saving model to best.model\n",
      "0s - loss: 0.8272 - acc: 0.6292 - val_loss: 0.6589 - val_acc: 1.0000\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.65890 to 0.64127, saving model to best.model\n",
      "0s - loss: 0.8112 - acc: 0.6517 - val_loss: 0.6413 - val_acc: 1.0000\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.64127 to 0.62532, saving model to best.model\n",
      "0s - loss: 0.7593 - acc: 0.7079 - val_loss: 0.6253 - val_acc: 1.0000\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.62532 to 0.61033, saving model to best.model\n",
      "0s - loss: 0.7643 - acc: 0.6517 - val_loss: 0.6103 - val_acc: 1.0000\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.61033 to 0.59558, saving model to best.model\n",
      "0s - loss: 0.8446 - acc: 0.5281 - val_loss: 0.5956 - val_acc: 1.0000\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.59558 to 0.58205, saving model to best.model\n",
      "0s - loss: 0.7547 - acc: 0.6629 - val_loss: 0.5821 - val_acc: 1.0000\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.58205 to 0.56972, saving model to best.model\n",
      "0s - loss: 0.7749 - acc: 0.6292 - val_loss: 0.5697 - val_acc: 1.0000\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.56972 to 0.55918, saving model to best.model\n",
      "0s - loss: 0.7368 - acc: 0.6517 - val_loss: 0.5592 - val_acc: 1.0000\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.55918 to 0.54889, saving model to best.model\n",
      "0s - loss: 0.7142 - acc: 0.7191 - val_loss: 0.5489 - val_acc: 1.0000\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.54889 to 0.53906, saving model to best.model\n",
      "0s - loss: 0.7071 - acc: 0.7528 - val_loss: 0.5391 - val_acc: 1.0000\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.53906 to 0.52968, saving model to best.model\n",
      "0s - loss: 0.6803 - acc: 0.7753 - val_loss: 0.5297 - val_acc: 1.0000\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.52968 to 0.51980, saving model to best.model\n",
      "0s - loss: 0.6811 - acc: 0.7416 - val_loss: 0.5198 - val_acc: 1.0000\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.51980 to 0.50963, saving model to best.model\n",
      "0s - loss: 0.6692 - acc: 0.7528 - val_loss: 0.5096 - val_acc: 1.0000\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.50963 to 0.49927, saving model to best.model\n",
      "0s - loss: 0.6491 - acc: 0.7640 - val_loss: 0.4993 - val_acc: 1.0000\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.49927 to 0.48960, saving model to best.model\n",
      "0s - loss: 0.6501 - acc: 0.7191 - val_loss: 0.4896 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.48960 to 0.47951, saving model to best.model\n",
      "0s - loss: 0.6770 - acc: 0.6966 - val_loss: 0.4795 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.47951 to 0.46837, saving model to best.model\n",
      "0s - loss: 0.6631 - acc: 0.7191 - val_loss: 0.4684 - val_acc: 1.0000\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.46837 to 0.45633, saving model to best.model\n",
      "0s - loss: 0.6216 - acc: 0.7640 - val_loss: 0.4563 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.45633 to 0.44445, saving model to best.model\n",
      "0s - loss: 0.5930 - acc: 0.7640 - val_loss: 0.4444 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.44445 to 0.43077, saving model to best.model\n",
      "0s - loss: 0.6054 - acc: 0.7978 - val_loss: 0.4308 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.43077 to 0.41640, saving model to best.model\n",
      "0s - loss: 0.6370 - acc: 0.7528 - val_loss: 0.4164 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.41640 to 0.40143, saving model to best.model\n",
      "0s - loss: 0.6175 - acc: 0.7528 - val_loss: 0.4014 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.40143 to 0.38729, saving model to best.model\n",
      "0s - loss: 0.6060 - acc: 0.7528 - val_loss: 0.3873 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.38729 to 0.37503, saving model to best.model\n",
      "0s - loss: 0.5735 - acc: 0.7978 - val_loss: 0.3750 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.37503 to 0.36356, saving model to best.model\n",
      "0s - loss: 0.5813 - acc: 0.7753 - val_loss: 0.3636 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.36356 to 0.35306, saving model to best.model\n",
      "0s - loss: 0.6594 - acc: 0.7303 - val_loss: 0.3531 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.35306 to 0.34401, saving model to best.model\n",
      "0s - loss: 0.6065 - acc: 0.7753 - val_loss: 0.3440 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.34401 to 0.33660, saving model to best.model\n",
      "0s - loss: 0.6011 - acc: 0.7640 - val_loss: 0.3366 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.33660 to 0.33015, saving model to best.model\n",
      "0s - loss: 0.5666 - acc: 0.7865 - val_loss: 0.3301 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.33015 to 0.32405, saving model to best.model\n",
      "0s - loss: 0.5559 - acc: 0.7865 - val_loss: 0.3241 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.32405 to 0.31788, saving model to best.model\n",
      "0s - loss: 0.5123 - acc: 0.8202 - val_loss: 0.3179 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.31788 to 0.31207, saving model to best.model\n",
      "0s - loss: 0.5055 - acc: 0.8090 - val_loss: 0.3121 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.31207 to 0.30897, saving model to best.model\n",
      "0s - loss: 0.4894 - acc: 0.7865 - val_loss: 0.3090 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.30897 to 0.30578, saving model to best.model\n",
      "0s - loss: 0.5295 - acc: 0.7640 - val_loss: 0.3058 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.30578 to 0.30269, saving model to best.model\n",
      "0s - loss: 0.5079 - acc: 0.8427 - val_loss: 0.3027 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.30269 to 0.29906, saving model to best.model\n",
      "0s - loss: 0.4535 - acc: 0.8652 - val_loss: 0.2991 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.29906 to 0.29538, saving model to best.model\n",
      "0s - loss: 0.5242 - acc: 0.7978 - val_loss: 0.2954 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.29538 to 0.29205, saving model to best.model\n",
      "0s - loss: 0.4565 - acc: 0.8427 - val_loss: 0.2920 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.29205 to 0.28806, saving model to best.model\n",
      "0s - loss: 0.4717 - acc: 0.8539 - val_loss: 0.2881 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.28806 to 0.28335, saving model to best.model\n",
      "0s - loss: 0.4518 - acc: 0.8427 - val_loss: 0.2834 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.28335 to 0.27770, saving model to best.model\n",
      "0s - loss: 0.4354 - acc: 0.8652 - val_loss: 0.2777 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.27770 to 0.27027, saving model to best.model\n",
      "0s - loss: 0.5041 - acc: 0.8202 - val_loss: 0.2703 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.27027 to 0.26101, saving model to best.model\n",
      "0s - loss: 0.3881 - acc: 0.8652 - val_loss: 0.2610 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.26101 to 0.25189, saving model to best.model\n",
      "0s - loss: 0.4242 - acc: 0.8539 - val_loss: 0.2519 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.25189 to 0.24287, saving model to best.model\n",
      "0s - loss: 0.4720 - acc: 0.8427 - val_loss: 0.2429 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.24287 to 0.23374, saving model to best.model\n",
      "0s - loss: 0.4662 - acc: 0.8202 - val_loss: 0.2337 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.23374 to 0.22606, saving model to best.model\n",
      "0s - loss: 0.4421 - acc: 0.8090 - val_loss: 0.2261 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.22606 to 0.21845, saving model to best.model\n",
      "0s - loss: 0.4413 - acc: 0.8764 - val_loss: 0.2185 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.21845 to 0.21227, saving model to best.model\n",
      "0s - loss: 0.4379 - acc: 0.8315 - val_loss: 0.2123 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.21227 to 0.20662, saving model to best.model\n",
      "0s - loss: 0.3756 - acc: 0.9101 - val_loss: 0.2066 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.20662 to 0.20230, saving model to best.model\n",
      "0s - loss: 0.3662 - acc: 0.8652 - val_loss: 0.2023 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.20230 to 0.19997, saving model to best.model\n",
      "0s - loss: 0.4271 - acc: 0.8539 - val_loss: 0.2000 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.19997 to 0.19799, saving model to best.model\n",
      "0s - loss: 0.3676 - acc: 0.9101 - val_loss: 0.1980 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.19799 to 0.19679, saving model to best.model\n",
      "0s - loss: 0.3678 - acc: 0.8652 - val_loss: 0.1968 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.19679 to 0.19619, saving model to best.model\n",
      "0s - loss: 0.3431 - acc: 0.8876 - val_loss: 0.1962 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.3784 - acc: 0.8539 - val_loss: 0.1968 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.3520 - acc: 0.8652 - val_loss: 0.1973 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.3854 - acc: 0.8989 - val_loss: 0.1974 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.3427 - acc: 0.8764 - val_loss: 0.1971 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.19619 to 0.19523, saving model to best.model\n",
      "0s - loss: 0.3332 - acc: 0.8876 - val_loss: 0.1952 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.19523 to 0.19261, saving model to best.model\n",
      "0s - loss: 0.3491 - acc: 0.8652 - val_loss: 0.1926 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19261 to 0.18907, saving model to best.model\n",
      "0s - loss: 0.3508 - acc: 0.8652 - val_loss: 0.1891 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.18907 to 0.18477, saving model to best.model\n",
      "0s - loss: 0.3260 - acc: 0.9101 - val_loss: 0.1848 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.18477 to 0.17866, saving model to best.model\n",
      "0s - loss: 0.3353 - acc: 0.8989 - val_loss: 0.1787 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.17866 to 0.17155, saving model to best.model\n",
      "0s - loss: 0.3576 - acc: 0.8989 - val_loss: 0.1715 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.17155 to 0.16385, saving model to best.model\n",
      "0s - loss: 0.3200 - acc: 0.9213 - val_loss: 0.1639 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.16385 to 0.15712, saving model to best.model\n",
      "0s - loss: 0.3334 - acc: 0.8876 - val_loss: 0.1571 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.15712 to 0.15071, saving model to best.model\n",
      "0s - loss: 0.2736 - acc: 0.9326 - val_loss: 0.1507 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.15071 to 0.14534, saving model to best.model\n",
      "0s - loss: 0.3493 - acc: 0.8876 - val_loss: 0.1453 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.14534 to 0.14036, saving model to best.model\n",
      "0s - loss: 0.3423 - acc: 0.9213 - val_loss: 0.1404 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.14036 to 0.13552, saving model to best.model\n",
      "0s - loss: 0.2910 - acc: 0.8876 - val_loss: 0.1355 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.13552 to 0.13132, saving model to best.model\n",
      "0s - loss: 0.2715 - acc: 0.9438 - val_loss: 0.1313 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.13132 to 0.12776, saving model to best.model\n",
      "0s - loss: 0.3871 - acc: 0.8202 - val_loss: 0.1278 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.12776 to 0.12411, saving model to best.model\n",
      "0s - loss: 0.3386 - acc: 0.8764 - val_loss: 0.1241 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.12411 to 0.12131, saving model to best.model\n",
      "0s - loss: 0.3830 - acc: 0.8652 - val_loss: 0.1213 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.12131 to 0.11888, saving model to best.model\n",
      "0s - loss: 0.3844 - acc: 0.8427 - val_loss: 0.1189 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.11888 to 0.11743, saving model to best.model\n",
      "0s - loss: 0.2925 - acc: 0.8876 - val_loss: 0.1174 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.11743 to 0.11639, saving model to best.model\n",
      "0s - loss: 0.2186 - acc: 0.9551 - val_loss: 0.1164 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.11639 to 0.11535, saving model to best.model\n",
      "0s - loss: 0.2332 - acc: 0.9326 - val_loss: 0.1153 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.11535 to 0.11480, saving model to best.model\n",
      "0s - loss: 0.2717 - acc: 0.8989 - val_loss: 0.1148 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.11480 to 0.11397, saving model to best.model\n",
      "0s - loss: 0.2958 - acc: 0.9213 - val_loss: 0.1140 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.11397 to 0.11262, saving model to best.model\n",
      "0s - loss: 0.2797 - acc: 0.8989 - val_loss: 0.1126 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.11262 to 0.11124, saving model to best.model\n",
      "0s - loss: 0.2485 - acc: 0.9101 - val_loss: 0.1112 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.11124 to 0.11040, saving model to best.model\n",
      "0s - loss: 0.2426 - acc: 0.9101 - val_loss: 0.1104 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.11040 to 0.10927, saving model to best.model\n",
      "0s - loss: 0.2171 - acc: 0.9326 - val_loss: 0.1093 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.10927 to 0.10764, saving model to best.model\n",
      "0s - loss: 0.2230 - acc: 0.9551 - val_loss: 0.1076 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.10764 to 0.10574, saving model to best.model\n",
      "0s - loss: 0.2208 - acc: 0.9326 - val_loss: 0.1057 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.10574 to 0.10374, saving model to best.model\n",
      "0s - loss: 0.2173 - acc: 0.9551 - val_loss: 0.1037 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.10374 to 0.10176, saving model to best.model\n",
      "0s - loss: 0.2652 - acc: 0.8876 - val_loss: 0.1018 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.10176 to 0.09948, saving model to best.model\n",
      "0s - loss: 0.2202 - acc: 0.9326 - val_loss: 0.0995 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.09948 to 0.09800, saving model to best.model\n",
      "0s - loss: 0.1981 - acc: 0.9326 - val_loss: 0.0980 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.09800 to 0.09625, saving model to best.model\n",
      "0s - loss: 0.1865 - acc: 0.9551 - val_loss: 0.0962 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.09625 to 0.09460, saving model to best.model\n",
      "0s - loss: 0.2784 - acc: 0.9213 - val_loss: 0.0946 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.09460 to 0.09255, saving model to best.model\n",
      "0s - loss: 0.2154 - acc: 0.9213 - val_loss: 0.0926 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.09255 to 0.09070, saving model to best.model\n",
      "0s - loss: 0.2008 - acc: 0.9438 - val_loss: 0.0907 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.09070 to 0.08956, saving model to best.model\n",
      "0s - loss: 0.2739 - acc: 0.9213 - val_loss: 0.0896 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.08956 to 0.08846, saving model to best.model\n",
      "0s - loss: 0.2598 - acc: 0.8764 - val_loss: 0.0885 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.08846 to 0.08737, saving model to best.model\n",
      "0s - loss: 0.2459 - acc: 0.8989 - val_loss: 0.0874 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.08737 to 0.08550, saving model to best.model\n",
      "0s - loss: 0.2408 - acc: 0.8989 - val_loss: 0.0855 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.08550 to 0.08297, saving model to best.model\n",
      "0s - loss: 0.2196 - acc: 0.9438 - val_loss: 0.0830 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.08297 to 0.08038, saving model to best.model\n",
      "0s - loss: 0.2251 - acc: 0.9326 - val_loss: 0.0804 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.08038 to 0.07767, saving model to best.model\n",
      "0s - loss: 0.1798 - acc: 0.9663 - val_loss: 0.0777 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.07767 to 0.07495, saving model to best.model\n",
      "0s - loss: 0.2080 - acc: 0.9438 - val_loss: 0.0749 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.07495 to 0.07218, saving model to best.model\n",
      "0s - loss: 0.1980 - acc: 0.9326 - val_loss: 0.0722 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.07218 to 0.07020, saving model to best.model\n",
      "0s - loss: 0.2466 - acc: 0.9326 - val_loss: 0.0702 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.07020 to 0.06854, saving model to best.model\n",
      "0s - loss: 0.1696 - acc: 0.9551 - val_loss: 0.0685 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.06854 to 0.06764, saving model to best.model\n",
      "0s - loss: 0.2076 - acc: 0.9438 - val_loss: 0.0676 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.06764 to 0.06639, saving model to best.model\n",
      "0s - loss: 0.2381 - acc: 0.9101 - val_loss: 0.0664 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.06639 to 0.06541, saving model to best.model\n",
      "0s - loss: 0.2616 - acc: 0.8876 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.06541 to 0.06397, saving model to best.model\n",
      "0s - loss: 0.2049 - acc: 0.8989 - val_loss: 0.0640 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.06397 to 0.06272, saving model to best.model\n",
      "0s - loss: 0.2450 - acc: 0.9101 - val_loss: 0.0627 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.06272 to 0.06251, saving model to best.model\n",
      "0s - loss: 0.1772 - acc: 0.9326 - val_loss: 0.0625 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.2059 - acc: 0.9551 - val_loss: 0.0626 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.06251 to 0.06233, saving model to best.model\n",
      "0s - loss: 0.1507 - acc: 0.9775 - val_loss: 0.0623 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.06233 to 0.06232, saving model to best.model\n",
      "0s - loss: 0.2126 - acc: 0.9101 - val_loss: 0.0623 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.1861 - acc: 0.9213 - val_loss: 0.0625 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.1973 - acc: 0.9326 - val_loss: 0.0626 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.28663, saving model to best.model\n",
      "0s - loss: 1.5419 - acc: 0.2697 - val_loss: 1.2866 - val_acc: 0.2174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.28663 to 1.18863, saving model to best.model\n",
      "0s - loss: 1.4159 - acc: 0.2697 - val_loss: 1.1886 - val_acc: 0.2174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.18863 to 1.12456, saving model to best.model\n",
      "0s - loss: 1.3301 - acc: 0.3034 - val_loss: 1.1246 - val_acc: 0.2174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.12456 to 1.08613, saving model to best.model\n",
      "0s - loss: 1.2579 - acc: 0.3034 - val_loss: 1.0861 - val_acc: 0.3478\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.08613 to 1.06750, saving model to best.model\n",
      "0s - loss: 1.2905 - acc: 0.2921 - val_loss: 1.0675 - val_acc: 0.3478\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.06750 to 1.06013, saving model to best.model\n",
      "0s - loss: 1.2263 - acc: 0.3820 - val_loss: 1.0601 - val_acc: 0.7391\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.06013 to 1.05986, saving model to best.model\n",
      "0s - loss: 1.3651 - acc: 0.2809 - val_loss: 1.0599 - val_acc: 0.4348\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2981 - acc: 0.3483 - val_loss: 1.0640 - val_acc: 0.4348\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2995 - acc: 0.3258 - val_loss: 1.0698 - val_acc: 0.4348\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2332 - acc: 0.3820 - val_loss: 1.0762 - val_acc: 0.4348\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.3253 - acc: 0.3258 - val_loss: 1.0817 - val_acc: 0.4348\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1029 - acc: 0.4944 - val_loss: 1.0857 - val_acc: 0.4348\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2424 - acc: 0.4494 - val_loss: 1.0869 - val_acc: 0.4348\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2499 - acc: 0.3483 - val_loss: 1.0840 - val_acc: 0.4348\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2829 - acc: 0.3933 - val_loss: 1.0779 - val_acc: 0.4348\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2363 - acc: 0.4494 - val_loss: 1.0699 - val_acc: 0.4348\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2492 - acc: 0.4607 - val_loss: 1.0613 - val_acc: 0.4348\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.05986 to 1.05320, saving model to best.model\n",
      "0s - loss: 1.2204 - acc: 0.4382 - val_loss: 1.0532 - val_acc: 0.4348\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.05320 to 1.04626, saving model to best.model\n",
      "0s - loss: 1.2783 - acc: 0.3596 - val_loss: 1.0463 - val_acc: 0.4348\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.04626 to 1.04119, saving model to best.model\n",
      "0s - loss: 1.1018 - acc: 0.4157 - val_loss: 1.0412 - val_acc: 0.4348\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.04119 to 1.03746, saving model to best.model\n",
      "0s - loss: 1.1484 - acc: 0.3371 - val_loss: 1.0375 - val_acc: 0.4348\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.03746 to 1.03460, saving model to best.model\n",
      "0s - loss: 1.0784 - acc: 0.4157 - val_loss: 1.0346 - val_acc: 0.4348\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.03460 to 1.03252, saving model to best.model\n",
      "0s - loss: 1.1324 - acc: 0.4157 - val_loss: 1.0325 - val_acc: 0.4783\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.03252 to 1.03075, saving model to best.model\n",
      "0s - loss: 1.2408 - acc: 0.4157 - val_loss: 1.0307 - val_acc: 0.6087\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.03075 to 1.02963, saving model to best.model\n",
      "0s - loss: 1.2628 - acc: 0.3483 - val_loss: 1.0296 - val_acc: 0.6957\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.02963 to 1.02851, saving model to best.model\n",
      "0s - loss: 1.0774 - acc: 0.3933 - val_loss: 1.0285 - val_acc: 0.6957\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.02851 to 1.02763, saving model to best.model\n",
      "0s - loss: 1.1574 - acc: 0.3371 - val_loss: 1.0276 - val_acc: 0.6957\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.02763 to 1.02667, saving model to best.model\n",
      "0s - loss: 1.0488 - acc: 0.4494 - val_loss: 1.0267 - val_acc: 0.6087\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.02667 to 1.02580, saving model to best.model\n",
      "0s - loss: 1.1858 - acc: 0.4494 - val_loss: 1.0258 - val_acc: 0.5652\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.02580 to 1.02455, saving model to best.model\n",
      "0s - loss: 1.1186 - acc: 0.3596 - val_loss: 1.0245 - val_acc: 0.5217\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.02455 to 1.02285, saving model to best.model\n",
      "0s - loss: 1.1709 - acc: 0.3708 - val_loss: 1.0228 - val_acc: 0.4783\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.02285 to 1.02049, saving model to best.model\n",
      "0s - loss: 1.1253 - acc: 0.3820 - val_loss: 1.0205 - val_acc: 0.4348\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.02049 to 1.01766, saving model to best.model\n",
      "0s - loss: 1.1843 - acc: 0.3596 - val_loss: 1.0177 - val_acc: 0.4348\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.01766 to 1.01443, saving model to best.model\n",
      "0s - loss: 1.0491 - acc: 0.4607 - val_loss: 1.0144 - val_acc: 0.4348\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.01443 to 1.01120, saving model to best.model\n",
      "0s - loss: 1.2134 - acc: 0.3708 - val_loss: 1.0112 - val_acc: 0.4348\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.01120 to 1.00801, saving model to best.model\n",
      "0s - loss: 1.1047 - acc: 0.3933 - val_loss: 1.0080 - val_acc: 0.4783\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.00801 to 1.00491, saving model to best.model\n",
      "0s - loss: 1.1496 - acc: 0.3820 - val_loss: 1.0049 - val_acc: 0.4783\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.00491 to 1.00178, saving model to best.model\n",
      "0s - loss: 1.1518 - acc: 0.3933 - val_loss: 1.0018 - val_acc: 0.4783\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.00178 to 0.99893, saving model to best.model\n",
      "0s - loss: 0.9730 - acc: 0.5281 - val_loss: 0.9989 - val_acc: 0.4783\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.99893 to 0.99622, saving model to best.model\n",
      "0s - loss: 1.1228 - acc: 0.4382 - val_loss: 0.9962 - val_acc: 0.4783\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.99622 to 0.99382, saving model to best.model\n",
      "0s - loss: 1.0816 - acc: 0.4157 - val_loss: 0.9938 - val_acc: 0.4783\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.99382 to 0.99153, saving model to best.model\n",
      "0s - loss: 1.1231 - acc: 0.3820 - val_loss: 0.9915 - val_acc: 0.4783\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.99153 to 0.98931, saving model to best.model\n",
      "0s - loss: 1.0014 - acc: 0.4607 - val_loss: 0.9893 - val_acc: 0.5217\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.98931 to 0.98696, saving model to best.model\n",
      "0s - loss: 1.0865 - acc: 0.4494 - val_loss: 0.9870 - val_acc: 0.6087\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.98696 to 0.98431, saving model to best.model\n",
      "0s - loss: 1.0627 - acc: 0.4270 - val_loss: 0.9843 - val_acc: 0.6087\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.98431 to 0.98119, saving model to best.model\n",
      "0s - loss: 1.0830 - acc: 0.4607 - val_loss: 0.9812 - val_acc: 0.6087\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.98119 to 0.97765, saving model to best.model\n",
      "0s - loss: 1.1023 - acc: 0.5393 - val_loss: 0.9777 - val_acc: 0.6087\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.97765 to 0.97399, saving model to best.model\n",
      "0s - loss: 1.1121 - acc: 0.4607 - val_loss: 0.9740 - val_acc: 0.6087\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.97399 to 0.97003, saving model to best.model\n",
      "0s - loss: 1.0724 - acc: 0.4382 - val_loss: 0.9700 - val_acc: 0.6087\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.97003 to 0.96585, saving model to best.model\n",
      "0s - loss: 1.1584 - acc: 0.4382 - val_loss: 0.9658 - val_acc: 0.6087\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.96585 to 0.96149, saving model to best.model\n",
      "0s - loss: 1.0998 - acc: 0.4382 - val_loss: 0.9615 - val_acc: 0.5217\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.96149 to 0.95714, saving model to best.model\n",
      "0s - loss: 0.9882 - acc: 0.4831 - val_loss: 0.9571 - val_acc: 0.5652\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.95714 to 0.95284, saving model to best.model\n",
      "0s - loss: 1.0890 - acc: 0.4157 - val_loss: 0.9528 - val_acc: 0.5652\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.95284 to 0.94847, saving model to best.model\n",
      "0s - loss: 1.0673 - acc: 0.4157 - val_loss: 0.9485 - val_acc: 0.6087\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.94847 to 0.94399, saving model to best.model\n",
      "0s - loss: 1.0451 - acc: 0.4494 - val_loss: 0.9440 - val_acc: 0.6957\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.94399 to 0.93938, saving model to best.model\n",
      "0s - loss: 1.0626 - acc: 0.4494 - val_loss: 0.9394 - val_acc: 0.6957\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.93938 to 0.93466, saving model to best.model\n",
      "0s - loss: 1.1026 - acc: 0.4382 - val_loss: 0.9347 - val_acc: 0.7391\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.93466 to 0.92989, saving model to best.model\n",
      "0s - loss: 1.0139 - acc: 0.4382 - val_loss: 0.9299 - val_acc: 0.7826\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.92989 to 0.92505, saving model to best.model\n",
      "0s - loss: 0.9952 - acc: 0.5169 - val_loss: 0.9251 - val_acc: 0.7826\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.92505 to 0.92000, saving model to best.model\n",
      "0s - loss: 0.9822 - acc: 0.5169 - val_loss: 0.9200 - val_acc: 0.7826\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.92000 to 0.91462, saving model to best.model\n",
      "0s - loss: 1.0450 - acc: 0.4270 - val_loss: 0.9146 - val_acc: 0.7826\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.91462 to 0.90884, saving model to best.model\n",
      "0s - loss: 0.9915 - acc: 0.5506 - val_loss: 0.9088 - val_acc: 0.7826\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.90884 to 0.90275, saving model to best.model\n",
      "0s - loss: 0.9882 - acc: 0.5056 - val_loss: 0.9027 - val_acc: 0.7826\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.90275 to 0.89628, saving model to best.model\n",
      "0s - loss: 1.0084 - acc: 0.4607 - val_loss: 0.8963 - val_acc: 0.7826\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.89628 to 0.88958, saving model to best.model\n",
      "0s - loss: 1.0226 - acc: 0.4719 - val_loss: 0.8896 - val_acc: 0.7826\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.88958 to 0.88272, saving model to best.model\n",
      "0s - loss: 1.0209 - acc: 0.4270 - val_loss: 0.8827 - val_acc: 0.7826\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.88272 to 0.87554, saving model to best.model\n",
      "0s - loss: 0.9788 - acc: 0.5281 - val_loss: 0.8755 - val_acc: 0.7826\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.87554 to 0.86818, saving model to best.model\n",
      "0s - loss: 1.0436 - acc: 0.4494 - val_loss: 0.8682 - val_acc: 0.7826\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.86818 to 0.86069, saving model to best.model\n",
      "0s - loss: 1.0030 - acc: 0.4944 - val_loss: 0.8607 - val_acc: 0.7826\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.86069 to 0.85304, saving model to best.model\n",
      "0s - loss: 0.9794 - acc: 0.5506 - val_loss: 0.8530 - val_acc: 0.7826\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.85304 to 0.84536, saving model to best.model\n",
      "0s - loss: 0.9245 - acc: 0.6404 - val_loss: 0.8454 - val_acc: 0.7826\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.84536 to 0.83737, saving model to best.model\n",
      "0s - loss: 0.9497 - acc: 0.5618 - val_loss: 0.8374 - val_acc: 0.7391\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.83737 to 0.82915, saving model to best.model\n",
      "0s - loss: 0.9769 - acc: 0.4944 - val_loss: 0.8292 - val_acc: 0.7391\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.82915 to 0.82073, saving model to best.model\n",
      "0s - loss: 0.9200 - acc: 0.5393 - val_loss: 0.8207 - val_acc: 0.7391\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.82073 to 0.81190, saving model to best.model\n",
      "0s - loss: 0.8705 - acc: 0.6067 - val_loss: 0.8119 - val_acc: 0.7391\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.81190 to 0.80258, saving model to best.model\n",
      "0s - loss: 0.8973 - acc: 0.5730 - val_loss: 0.8026 - val_acc: 0.7391\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.80258 to 0.79300, saving model to best.model\n",
      "0s - loss: 0.9543 - acc: 0.5169 - val_loss: 0.7930 - val_acc: 0.7391\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.79300 to 0.78350, saving model to best.model\n",
      "0s - loss: 0.9184 - acc: 0.4831 - val_loss: 0.7835 - val_acc: 0.7391\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.78350 to 0.77362, saving model to best.model\n",
      "0s - loss: 0.9365 - acc: 0.5169 - val_loss: 0.7736 - val_acc: 0.7391\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.77362 to 0.76380, saving model to best.model\n",
      "0s - loss: 0.9046 - acc: 0.5843 - val_loss: 0.7638 - val_acc: 0.7391\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.76380 to 0.75408, saving model to best.model\n",
      "0s - loss: 0.8476 - acc: 0.6067 - val_loss: 0.7541 - val_acc: 0.7391\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.75408 to 0.74427, saving model to best.model\n",
      "0s - loss: 0.8767 - acc: 0.6517 - val_loss: 0.7443 - val_acc: 0.7391\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.74427 to 0.73487, saving model to best.model\n",
      "0s - loss: 0.9117 - acc: 0.5955 - val_loss: 0.7349 - val_acc: 0.7391\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.73487 to 0.72554, saving model to best.model\n",
      "0s - loss: 0.8326 - acc: 0.6629 - val_loss: 0.7255 - val_acc: 0.7391\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.72554 to 0.71538, saving model to best.model\n",
      "0s - loss: 0.8551 - acc: 0.5730 - val_loss: 0.7154 - val_acc: 0.6957\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.71538 to 0.70479, saving model to best.model\n",
      "0s - loss: 0.8488 - acc: 0.6404 - val_loss: 0.7048 - val_acc: 0.6957\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.70479 to 0.69419, saving model to best.model\n",
      "0s - loss: 0.8157 - acc: 0.6629 - val_loss: 0.6942 - val_acc: 0.6957\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.69419 to 0.68321, saving model to best.model\n",
      "0s - loss: 0.8118 - acc: 0.6966 - val_loss: 0.6832 - val_acc: 0.6957\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68321 to 0.67272, saving model to best.model\n",
      "0s - loss: 0.7688 - acc: 0.6517 - val_loss: 0.6727 - val_acc: 0.6957\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67272 to 0.66181, saving model to best.model\n",
      "0s - loss: 0.7681 - acc: 0.6854 - val_loss: 0.6618 - val_acc: 0.6957\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.66181 to 0.65053, saving model to best.model\n",
      "0s - loss: 0.8601 - acc: 0.5169 - val_loss: 0.6505 - val_acc: 0.6957\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.65053 to 0.63931, saving model to best.model\n",
      "0s - loss: 0.8087 - acc: 0.6517 - val_loss: 0.6393 - val_acc: 0.6957\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.63931 to 0.62797, saving model to best.model\n",
      "0s - loss: 0.6996 - acc: 0.7416 - val_loss: 0.6280 - val_acc: 0.6957\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.62797 to 0.61673, saving model to best.model\n",
      "0s - loss: 0.7772 - acc: 0.6067 - val_loss: 0.6167 - val_acc: 0.6957\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.61673 to 0.60530, saving model to best.model\n",
      "0s - loss: 0.6908 - acc: 0.7079 - val_loss: 0.6053 - val_acc: 0.6957\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.60530 to 0.59388, saving model to best.model\n",
      "0s - loss: 0.6987 - acc: 0.6629 - val_loss: 0.5939 - val_acc: 0.6957\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.59388 to 0.58256, saving model to best.model\n",
      "0s - loss: 0.7120 - acc: 0.7079 - val_loss: 0.5826 - val_acc: 0.6957\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.58256 to 0.57156, saving model to best.model\n",
      "0s - loss: 0.6796 - acc: 0.7416 - val_loss: 0.5716 - val_acc: 0.6957\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.57156 to 0.56046, saving model to best.model\n",
      "0s - loss: 0.7123 - acc: 0.7528 - val_loss: 0.5605 - val_acc: 0.6957\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.56046 to 0.54963, saving model to best.model\n",
      "0s - loss: 0.6663 - acc: 0.6854 - val_loss: 0.5496 - val_acc: 0.6957\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.54963 to 0.53901, saving model to best.model\n",
      "0s - loss: 0.6191 - acc: 0.7753 - val_loss: 0.5390 - val_acc: 0.7391\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.53901 to 0.52918, saving model to best.model\n",
      "0s - loss: 0.6650 - acc: 0.7191 - val_loss: 0.5292 - val_acc: 0.7391\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.52918 to 0.51977, saving model to best.model\n",
      "0s - loss: 0.6469 - acc: 0.7416 - val_loss: 0.5198 - val_acc: 0.7391\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.51977 to 0.51032, saving model to best.model\n",
      "0s - loss: 0.6174 - acc: 0.7303 - val_loss: 0.5103 - val_acc: 0.7826\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.51032 to 0.50138, saving model to best.model\n",
      "0s - loss: 0.6183 - acc: 0.7528 - val_loss: 0.5014 - val_acc: 0.7826\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.50138 to 0.49335, saving model to best.model\n",
      "0s - loss: 0.6811 - acc: 0.6517 - val_loss: 0.4933 - val_acc: 0.7826\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.49335 to 0.48548, saving model to best.model\n",
      "0s - loss: 0.5815 - acc: 0.7753 - val_loss: 0.4855 - val_acc: 0.7826\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.48548 to 0.47811, saving model to best.model\n",
      "0s - loss: 0.6138 - acc: 0.7079 - val_loss: 0.4781 - val_acc: 0.8696\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.47811 to 0.47092, saving model to best.model\n",
      "0s - loss: 0.5738 - acc: 0.7528 - val_loss: 0.4709 - val_acc: 0.8696\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.47092 to 0.46359, saving model to best.model\n",
      "0s - loss: 0.5884 - acc: 0.7528 - val_loss: 0.4636 - val_acc: 0.8696\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.46359 to 0.45607, saving model to best.model\n",
      "0s - loss: 0.5745 - acc: 0.7978 - val_loss: 0.4561 - val_acc: 0.7826\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.45607 to 0.44849, saving model to best.model\n",
      "0s - loss: 0.5590 - acc: 0.7978 - val_loss: 0.4485 - val_acc: 0.8696\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.44849 to 0.44056, saving model to best.model\n",
      "0s - loss: 0.5497 - acc: 0.7978 - val_loss: 0.4406 - val_acc: 0.8696\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.44056 to 0.43352, saving model to best.model\n",
      "0s - loss: 0.5645 - acc: 0.7978 - val_loss: 0.4335 - val_acc: 0.8696\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.43352 to 0.42595, saving model to best.model\n",
      "0s - loss: 0.5409 - acc: 0.7416 - val_loss: 0.4260 - val_acc: 0.8696\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.42595 to 0.41874, saving model to best.model\n",
      "0s - loss: 0.5992 - acc: 0.7640 - val_loss: 0.4187 - val_acc: 0.8696\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.41874 to 0.41217, saving model to best.model\n",
      "0s - loss: 0.6035 - acc: 0.7191 - val_loss: 0.4122 - val_acc: 0.8696\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.41217 to 0.40584, saving model to best.model\n",
      "0s - loss: 0.5278 - acc: 0.7978 - val_loss: 0.4058 - val_acc: 0.8696\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.40584 to 0.39989, saving model to best.model\n",
      "0s - loss: 0.5192 - acc: 0.8315 - val_loss: 0.3999 - val_acc: 0.8696\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.39989 to 0.39338, saving model to best.model\n",
      "0s - loss: 0.4125 - acc: 0.8427 - val_loss: 0.3934 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.39338 to 0.38731, saving model to best.model\n",
      "0s - loss: 0.4614 - acc: 0.8539 - val_loss: 0.3873 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.38731 to 0.38069, saving model to best.model\n",
      "0s - loss: 0.4364 - acc: 0.8539 - val_loss: 0.3807 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.38069 to 0.37450, saving model to best.model\n",
      "0s - loss: 0.4884 - acc: 0.8202 - val_loss: 0.3745 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.37450 to 0.36852, saving model to best.model\n",
      "0s - loss: 0.5051 - acc: 0.7865 - val_loss: 0.3685 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.36852 to 0.36199, saving model to best.model\n",
      "0s - loss: 0.4729 - acc: 0.8427 - val_loss: 0.3620 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.36199 to 0.35615, saving model to best.model\n",
      "0s - loss: 0.4750 - acc: 0.7865 - val_loss: 0.3561 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.35615 to 0.35179, saving model to best.model\n",
      "0s - loss: 0.4366 - acc: 0.8764 - val_loss: 0.3518 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.35179 to 0.34774, saving model to best.model\n",
      "0s - loss: 0.4413 - acc: 0.8427 - val_loss: 0.3477 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.34774 to 0.34328, saving model to best.model\n",
      "0s - loss: 0.4149 - acc: 0.8202 - val_loss: 0.3433 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.34328 to 0.33937, saving model to best.model\n",
      "0s - loss: 0.4716 - acc: 0.8315 - val_loss: 0.3394 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.33937 to 0.33562, saving model to best.model\n",
      "0s - loss: 0.4684 - acc: 0.8315 - val_loss: 0.3356 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.33562 to 0.33301, saving model to best.model\n",
      "0s - loss: 0.4527 - acc: 0.8315 - val_loss: 0.3330 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.33301 to 0.33031, saving model to best.model\n",
      "0s - loss: 0.3807 - acc: 0.8989 - val_loss: 0.3303 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.33031 to 0.32729, saving model to best.model\n",
      "0s - loss: 0.3691 - acc: 0.8539 - val_loss: 0.3273 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.32729 to 0.32251, saving model to best.model\n",
      "0s - loss: 0.4124 - acc: 0.8315 - val_loss: 0.3225 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.32251 to 0.31753, saving model to best.model\n",
      "0s - loss: 0.3378 - acc: 0.8989 - val_loss: 0.3175 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.31753 to 0.31243, saving model to best.model\n",
      "0s - loss: 0.3961 - acc: 0.8427 - val_loss: 0.3124 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.31243 to 0.30762, saving model to best.model\n",
      "0s - loss: 0.3862 - acc: 0.8539 - val_loss: 0.3076 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.30762 to 0.30218, saving model to best.model\n",
      "0s - loss: 0.3587 - acc: 0.8652 - val_loss: 0.3022 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.30218 to 0.29780, saving model to best.model\n",
      "0s - loss: 0.4010 - acc: 0.8652 - val_loss: 0.2978 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.29780 to 0.29450, saving model to best.model\n",
      "0s - loss: 0.4118 - acc: 0.8876 - val_loss: 0.2945 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.29450 to 0.29008, saving model to best.model\n",
      "0s - loss: 0.3810 - acc: 0.8539 - val_loss: 0.2901 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.29008 to 0.28559, saving model to best.model\n",
      "0s - loss: 0.4138 - acc: 0.8315 - val_loss: 0.2856 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.28559 to 0.28170, saving model to best.model\n",
      "0s - loss: 0.3451 - acc: 0.8876 - val_loss: 0.2817 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.28170 to 0.27930, saving model to best.model\n",
      "0s - loss: 0.3078 - acc: 0.8989 - val_loss: 0.2793 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.27930 to 0.27588, saving model to best.model\n",
      "0s - loss: 0.3383 - acc: 0.8876 - val_loss: 0.2759 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.27588 to 0.27243, saving model to best.model\n",
      "0s - loss: 0.3402 - acc: 0.8989 - val_loss: 0.2724 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.27243 to 0.26872, saving model to best.model\n",
      "0s - loss: 0.2906 - acc: 0.9101 - val_loss: 0.2687 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.26872 to 0.26416, saving model to best.model\n",
      "0s - loss: 0.3062 - acc: 0.8989 - val_loss: 0.2642 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.26416 to 0.25973, saving model to best.model\n",
      "0s - loss: 0.2899 - acc: 0.9213 - val_loss: 0.2597 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.25973 to 0.25394, saving model to best.model\n",
      "0s - loss: 0.3265 - acc: 0.8876 - val_loss: 0.2539 - val_acc: 0.9130\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.25394 to 0.24831, saving model to best.model\n",
      "0s - loss: 0.2788 - acc: 0.9101 - val_loss: 0.2483 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.24831 to 0.24220, saving model to best.model\n",
      "0s - loss: 0.2992 - acc: 0.9101 - val_loss: 0.2422 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.24220 to 0.23638, saving model to best.model\n",
      "0s - loss: 0.2562 - acc: 0.9101 - val_loss: 0.2364 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.23638 to 0.23122, saving model to best.model\n",
      "0s - loss: 0.2477 - acc: 0.9326 - val_loss: 0.2312 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.23122 to 0.22776, saving model to best.model\n",
      "0s - loss: 0.2929 - acc: 0.9438 - val_loss: 0.2278 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.22776 to 0.22625, saving model to best.model\n",
      "0s - loss: 0.2553 - acc: 0.9213 - val_loss: 0.2263 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.22625 to 0.22499, saving model to best.model\n",
      "0s - loss: 0.2744 - acc: 0.8989 - val_loss: 0.2250 - val_acc: 0.9130\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.22499 to 0.22416, saving model to best.model\n",
      "0s - loss: 0.2046 - acc: 0.9663 - val_loss: 0.2242 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.22416 to 0.22193, saving model to best.model\n",
      "0s - loss: 0.2925 - acc: 0.8989 - val_loss: 0.2219 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.22193 to 0.21982, saving model to best.model\n",
      "0s - loss: 0.2562 - acc: 0.9438 - val_loss: 0.2198 - val_acc: 0.9130\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.21982 to 0.21705, saving model to best.model\n",
      "0s - loss: 0.2036 - acc: 0.9213 - val_loss: 0.2171 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.21705 to 0.21388, saving model to best.model\n",
      "0s - loss: 0.2374 - acc: 0.9326 - val_loss: 0.2139 - val_acc: 0.9130\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.21388 to 0.21094, saving model to best.model\n",
      "0s - loss: 0.2563 - acc: 0.9101 - val_loss: 0.2109 - val_acc: 0.9130\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.21094 to 0.20787, saving model to best.model\n",
      "0s - loss: 0.2288 - acc: 0.9326 - val_loss: 0.2079 - val_acc: 0.9130\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.20787 to 0.20508, saving model to best.model\n",
      "0s - loss: 0.2068 - acc: 0.9438 - val_loss: 0.2051 - val_acc: 0.9130\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.20508 to 0.20200, saving model to best.model\n",
      "0s - loss: 0.1828 - acc: 0.9326 - val_loss: 0.2020 - val_acc: 0.9130\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.20200 to 0.19912, saving model to best.model\n",
      "0s - loss: 0.2515 - acc: 0.8764 - val_loss: 0.1991 - val_acc: 0.9130\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.19912 to 0.19513, saving model to best.model\n",
      "0s - loss: 0.2148 - acc: 0.9326 - val_loss: 0.1951 - val_acc: 0.9130\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.19513 to 0.19139, saving model to best.model\n",
      "0s - loss: 0.2281 - acc: 0.9326 - val_loss: 0.1914 - val_acc: 0.9130\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.19139 to 0.18721, saving model to best.model\n",
      "0s - loss: 0.2619 - acc: 0.9101 - val_loss: 0.1872 - val_acc: 0.9130\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.18721 to 0.18375, saving model to best.model\n",
      "0s - loss: 0.2121 - acc: 0.9326 - val_loss: 0.1837 - val_acc: 0.9130\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.18375 to 0.18103, saving model to best.model\n",
      "0s - loss: 0.1997 - acc: 0.9438 - val_loss: 0.1810 - val_acc: 0.9130\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.18103 to 0.17778, saving model to best.model\n",
      "0s - loss: 0.2057 - acc: 0.9663 - val_loss: 0.1778 - val_acc: 0.9130\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.17778 to 0.17412, saving model to best.model\n",
      "0s - loss: 0.2115 - acc: 0.9438 - val_loss: 0.1741 - val_acc: 0.9130\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.17412 to 0.17010, saving model to best.model\n",
      "0s - loss: 0.2306 - acc: 0.9326 - val_loss: 0.1701 - val_acc: 0.9130\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.17010 to 0.16692, saving model to best.model\n",
      "0s - loss: 0.2377 - acc: 0.9213 - val_loss: 0.1669 - val_acc: 0.9130\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.16692 to 0.16346, saving model to best.model\n",
      "0s - loss: 0.1607 - acc: 0.9551 - val_loss: 0.1635 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.16346 to 0.16048, saving model to best.model\n",
      "0s - loss: 0.2137 - acc: 0.9101 - val_loss: 0.1605 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.16048 to 0.15778, saving model to best.model\n",
      "0s - loss: 0.2312 - acc: 0.8764 - val_loss: 0.1578 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.15778 to 0.15491, saving model to best.model\n",
      "0s - loss: 0.1857 - acc: 0.9551 - val_loss: 0.1549 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.15491 to 0.15323, saving model to best.model\n",
      "0s - loss: 0.2197 - acc: 0.9213 - val_loss: 0.1532 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.15323 to 0.15163, saving model to best.model\n",
      "0s - loss: 0.1662 - acc: 0.9438 - val_loss: 0.1516 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.15163 to 0.14963, saving model to best.model\n",
      "0s - loss: 0.1762 - acc: 0.9663 - val_loss: 0.1496 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.14963 to 0.14823, saving model to best.model\n",
      "0s - loss: 0.1767 - acc: 0.9438 - val_loss: 0.1482 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.14823 to 0.14679, saving model to best.model\n",
      "0s - loss: 0.1435 - acc: 0.9775 - val_loss: 0.1468 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.14679 to 0.14443, saving model to best.model\n",
      "0s - loss: 0.1781 - acc: 0.9663 - val_loss: 0.1444 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.14443 to 0.14321, saving model to best.model\n",
      "0s - loss: 0.1834 - acc: 0.9326 - val_loss: 0.1432 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.14321 to 0.14320, saving model to best.model\n",
      "0s - loss: 0.1676 - acc: 0.9775 - val_loss: 0.1432 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.1676 - acc: 0.9663 - val_loss: 0.1436 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.2078 - acc: 0.9326 - val_loss: 0.1436 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.1777 - acc: 0.9326 - val_loss: 0.1450 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.2118 - acc: 0.9326 - val_loss: 0.1463 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.1667 - acc: 0.9326 - val_loss: 0.1473 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.1845 - acc: 0.9438 - val_loss: 0.1467 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.1859 - acc: 0.9663 - val_loss: 0.1456 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.1350 - acc: 0.9775 - val_loss: 0.1434 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.14320 to 0.14101, saving model to best.model\n",
      "0s - loss: 0.1975 - acc: 0.9213 - val_loss: 0.1410 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.14101 to 0.13860, saving model to best.model\n",
      "0s - loss: 0.1795 - acc: 0.9438 - val_loss: 0.1386 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.13860 to 0.13418, saving model to best.model\n",
      "0s - loss: 0.1809 - acc: 0.9663 - val_loss: 0.1342 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.11055, saving model to best.model\n",
      "0s - loss: 1.3618 - acc: 0.3146 - val_loss: 1.1106 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.11055 to 1.08657, saving model to best.model\n",
      "0s - loss: 1.3873 - acc: 0.2809 - val_loss: 1.0866 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.08657 to 1.08576, saving model to best.model\n",
      "0s - loss: 1.2133 - acc: 0.3371 - val_loss: 1.0858 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2592 - acc: 0.3258 - val_loss: 1.0997 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2019 - acc: 0.4494 - val_loss: 1.1185 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2069 - acc: 0.4494 - val_loss: 1.1341 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2662 - acc: 0.3371 - val_loss: 1.1433 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1324 - acc: 0.4270 - val_loss: 1.1437 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1019 - acc: 0.4719 - val_loss: 1.1393 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1618 - acc: 0.3820 - val_loss: 1.1295 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1754 - acc: 0.4157 - val_loss: 1.1186 - val_acc: 0.3913\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2585 - acc: 0.4270 - val_loss: 1.1087 - val_acc: 0.3913\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2229 - acc: 0.3258 - val_loss: 1.0999 - val_acc: 0.3913\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.1631 - acc: 0.3820 - val_loss: 1.0917 - val_acc: 0.3913\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.08576 to 1.08431, saving model to best.model\n",
      "0s - loss: 1.2937 - acc: 0.3820 - val_loss: 1.0843 - val_acc: 0.3913\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.08431 to 1.07786, saving model to best.model\n",
      "0s - loss: 1.3259 - acc: 0.3371 - val_loss: 1.0779 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.07786 to 1.07209, saving model to best.model\n",
      "0s - loss: 1.1576 - acc: 0.4607 - val_loss: 1.0721 - val_acc: 0.3913\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.07209 to 1.06755, saving model to best.model\n",
      "0s - loss: 1.1374 - acc: 0.4045 - val_loss: 1.0676 - val_acc: 0.3913\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.06755 to 1.06356, saving model to best.model\n",
      "0s - loss: 1.2480 - acc: 0.3596 - val_loss: 1.0636 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.06356 to 1.06062, saving model to best.model\n",
      "0s - loss: 1.2495 - acc: 0.3708 - val_loss: 1.0606 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.06062 to 1.05851, saving model to best.model\n",
      "0s - loss: 1.1425 - acc: 0.3933 - val_loss: 1.0585 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.05851 to 1.05731, saving model to best.model\n",
      "0s - loss: 1.1055 - acc: 0.4045 - val_loss: 1.0573 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.05731 to 1.05651, saving model to best.model\n",
      "0s - loss: 1.1130 - acc: 0.4270 - val_loss: 1.0565 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.05651 to 1.05632, saving model to best.model\n",
      "0s - loss: 1.0689 - acc: 0.4607 - val_loss: 1.0563 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.05632 to 1.05616, saving model to best.model\n",
      "0s - loss: 1.1793 - acc: 0.3371 - val_loss: 1.0562 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.05616 to 1.05611, saving model to best.model\n",
      "0s - loss: 1.0633 - acc: 0.4831 - val_loss: 1.0561 - val_acc: 0.3913\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1352 - acc: 0.4607 - val_loss: 1.0561 - val_acc: 0.3913\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.0484 - acc: 0.5281 - val_loss: 1.0563 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.05611 to 1.05592, saving model to best.model\n",
      "0s - loss: 1.0639 - acc: 0.5393 - val_loss: 1.0559 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.05592 to 1.05535, saving model to best.model\n",
      "0s - loss: 1.1211 - acc: 0.3708 - val_loss: 1.0553 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.05535 to 1.05402, saving model to best.model\n",
      "0s - loss: 1.0909 - acc: 0.5169 - val_loss: 1.0540 - val_acc: 0.3913\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.05402 to 1.05180, saving model to best.model\n",
      "0s - loss: 1.1995 - acc: 0.4157 - val_loss: 1.0518 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.05180 to 1.05016, saving model to best.model\n",
      "0s - loss: 1.1711 - acc: 0.3820 - val_loss: 1.0502 - val_acc: 0.3913\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.05016 to 1.04874, saving model to best.model\n",
      "0s - loss: 1.0896 - acc: 0.4607 - val_loss: 1.0487 - val_acc: 0.3913\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.04874 to 1.04721, saving model to best.model\n",
      "0s - loss: 1.0650 - acc: 0.4719 - val_loss: 1.0472 - val_acc: 0.3913\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.04721 to 1.04509, saving model to best.model\n",
      "0s - loss: 1.1340 - acc: 0.4157 - val_loss: 1.0451 - val_acc: 0.3913\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.04509 to 1.04308, saving model to best.model\n",
      "0s - loss: 1.1164 - acc: 0.3708 - val_loss: 1.0431 - val_acc: 0.3913\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.04308 to 1.04094, saving model to best.model\n",
      "0s - loss: 1.0919 - acc: 0.4607 - val_loss: 1.0409 - val_acc: 0.3913\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.04094 to 1.03850, saving model to best.model\n",
      "0s - loss: 1.1550 - acc: 0.3708 - val_loss: 1.0385 - val_acc: 0.3913\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.03850 to 1.03542, saving model to best.model\n",
      "0s - loss: 1.1630 - acc: 0.3933 - val_loss: 1.0354 - val_acc: 0.3913\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.03542 to 1.03168, saving model to best.model\n",
      "0s - loss: 1.0721 - acc: 0.5169 - val_loss: 1.0317 - val_acc: 0.3913\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.03168 to 1.02766, saving model to best.model\n",
      "0s - loss: 1.0620 - acc: 0.4831 - val_loss: 1.0277 - val_acc: 0.3913\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.02766 to 1.02396, saving model to best.model\n",
      "0s - loss: 1.1127 - acc: 0.4157 - val_loss: 1.0240 - val_acc: 0.3913\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.02396 to 1.02032, saving model to best.model\n",
      "0s - loss: 1.0938 - acc: 0.4607 - val_loss: 1.0203 - val_acc: 0.3913\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.02032 to 1.01716, saving model to best.model\n",
      "0s - loss: 1.1270 - acc: 0.3708 - val_loss: 1.0172 - val_acc: 0.3913\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.01716 to 1.01366, saving model to best.model\n",
      "0s - loss: 1.1445 - acc: 0.3933 - val_loss: 1.0137 - val_acc: 0.3913\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.01366 to 1.01048, saving model to best.model\n",
      "0s - loss: 1.0007 - acc: 0.5393 - val_loss: 1.0105 - val_acc: 0.3913\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.01048 to 1.00722, saving model to best.model\n",
      "0s - loss: 1.0410 - acc: 0.4944 - val_loss: 1.0072 - val_acc: 0.3913\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.00722 to 1.00396, saving model to best.model\n",
      "0s - loss: 1.1194 - acc: 0.4045 - val_loss: 1.0040 - val_acc: 0.4348\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.00396 to 1.00070, saving model to best.model\n",
      "0s - loss: 0.9931 - acc: 0.4944 - val_loss: 1.0007 - val_acc: 0.4783\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.00070 to 0.99776, saving model to best.model\n",
      "0s - loss: 1.0243 - acc: 0.4607 - val_loss: 0.9978 - val_acc: 0.4783\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.99776 to 0.99493, saving model to best.model\n",
      "0s - loss: 1.0083 - acc: 0.5056 - val_loss: 0.9949 - val_acc: 0.4783\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.99493 to 0.99199, saving model to best.model\n",
      "0s - loss: 1.0170 - acc: 0.4944 - val_loss: 0.9920 - val_acc: 0.4783\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.99199 to 0.98898, saving model to best.model\n",
      "0s - loss: 1.0805 - acc: 0.4719 - val_loss: 0.9890 - val_acc: 0.4783\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.98898 to 0.98559, saving model to best.model\n",
      "0s - loss: 1.0312 - acc: 0.5169 - val_loss: 0.9856 - val_acc: 0.4783\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.98559 to 0.98251, saving model to best.model\n",
      "0s - loss: 1.0043 - acc: 0.4382 - val_loss: 0.9825 - val_acc: 0.4783\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.98251 to 0.97913, saving model to best.model\n",
      "0s - loss: 0.9942 - acc: 0.5393 - val_loss: 0.9791 - val_acc: 0.4783\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.97913 to 0.97521, saving model to best.model\n",
      "0s - loss: 1.0106 - acc: 0.4719 - val_loss: 0.9752 - val_acc: 0.4783\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.97521 to 0.97075, saving model to best.model\n",
      "0s - loss: 1.0329 - acc: 0.4831 - val_loss: 0.9708 - val_acc: 0.4783\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.97075 to 0.96602, saving model to best.model\n",
      "0s - loss: 1.0059 - acc: 0.4494 - val_loss: 0.9660 - val_acc: 0.5652\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.96602 to 0.96156, saving model to best.model\n",
      "0s - loss: 0.9905 - acc: 0.4831 - val_loss: 0.9616 - val_acc: 0.6087\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.96156 to 0.95658, saving model to best.model\n",
      "0s - loss: 1.0343 - acc: 0.4382 - val_loss: 0.9566 - val_acc: 0.6087\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.95658 to 0.95197, saving model to best.model\n",
      "0s - loss: 1.0357 - acc: 0.4382 - val_loss: 0.9520 - val_acc: 0.6522\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.95197 to 0.94706, saving model to best.model\n",
      "0s - loss: 0.9379 - acc: 0.5056 - val_loss: 0.9471 - val_acc: 0.6522\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.94706 to 0.94173, saving model to best.model\n",
      "0s - loss: 1.0292 - acc: 0.5281 - val_loss: 0.9417 - val_acc: 0.6522\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.94173 to 0.93588, saving model to best.model\n",
      "0s - loss: 0.9302 - acc: 0.5506 - val_loss: 0.9359 - val_acc: 0.6522\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.93588 to 0.92969, saving model to best.model\n",
      "0s - loss: 0.9168 - acc: 0.5730 - val_loss: 0.9297 - val_acc: 0.6522\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.92969 to 0.92282, saving model to best.model\n",
      "0s - loss: 1.0196 - acc: 0.4494 - val_loss: 0.9228 - val_acc: 0.6522\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.92282 to 0.91548, saving model to best.model\n",
      "0s - loss: 0.9349 - acc: 0.5730 - val_loss: 0.9155 - val_acc: 0.6522\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.91548 to 0.90752, saving model to best.model\n",
      "0s - loss: 1.0087 - acc: 0.5169 - val_loss: 0.9075 - val_acc: 0.6522\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.90752 to 0.89964, saving model to best.model\n",
      "0s - loss: 1.0024 - acc: 0.4831 - val_loss: 0.8996 - val_acc: 0.6522\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.89964 to 0.89147, saving model to best.model\n",
      "0s - loss: 0.9037 - acc: 0.5730 - val_loss: 0.8915 - val_acc: 0.6522\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.89147 to 0.88331, saving model to best.model\n",
      "0s - loss: 0.9418 - acc: 0.5506 - val_loss: 0.8833 - val_acc: 0.6522\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.88331 to 0.87567, saving model to best.model\n",
      "0s - loss: 0.9259 - acc: 0.5618 - val_loss: 0.8757 - val_acc: 0.6522\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.87567 to 0.86791, saving model to best.model\n",
      "0s - loss: 0.9564 - acc: 0.5056 - val_loss: 0.8679 - val_acc: 0.6522\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.86791 to 0.86034, saving model to best.model\n",
      "0s - loss: 0.9526 - acc: 0.5281 - val_loss: 0.8603 - val_acc: 0.6522\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.86034 to 0.85262, saving model to best.model\n",
      "0s - loss: 0.9091 - acc: 0.5506 - val_loss: 0.8526 - val_acc: 0.6522\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.85262 to 0.84508, saving model to best.model\n",
      "0s - loss: 0.8475 - acc: 0.6629 - val_loss: 0.8451 - val_acc: 0.6522\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.84508 to 0.83778, saving model to best.model\n",
      "0s - loss: 0.8733 - acc: 0.5955 - val_loss: 0.8378 - val_acc: 0.6522\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.83778 to 0.83031, saving model to best.model\n",
      "0s - loss: 0.9361 - acc: 0.5393 - val_loss: 0.8303 - val_acc: 0.6957\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.83031 to 0.82216, saving model to best.model\n",
      "0s - loss: 0.8127 - acc: 0.6854 - val_loss: 0.8222 - val_acc: 0.6957\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.82216 to 0.81339, saving model to best.model\n",
      "0s - loss: 0.9035 - acc: 0.5506 - val_loss: 0.8134 - val_acc: 0.6522\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.81339 to 0.80455, saving model to best.model\n",
      "0s - loss: 0.8942 - acc: 0.6067 - val_loss: 0.8045 - val_acc: 0.6522\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.80455 to 0.79545, saving model to best.model\n",
      "0s - loss: 0.9347 - acc: 0.5506 - val_loss: 0.7954 - val_acc: 0.6522\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.79545 to 0.78672, saving model to best.model\n",
      "0s - loss: 0.7786 - acc: 0.6180 - val_loss: 0.7867 - val_acc: 0.6522\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.78672 to 0.77784, saving model to best.model\n",
      "0s - loss: 0.8855 - acc: 0.5730 - val_loss: 0.7778 - val_acc: 0.6522\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.77784 to 0.76847, saving model to best.model\n",
      "0s - loss: 0.8381 - acc: 0.6292 - val_loss: 0.7685 - val_acc: 0.6522\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.76847 to 0.75922, saving model to best.model\n",
      "0s - loss: 0.8258 - acc: 0.6292 - val_loss: 0.7592 - val_acc: 0.6522\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.75922 to 0.75007, saving model to best.model\n",
      "0s - loss: 0.7966 - acc: 0.6742 - val_loss: 0.7501 - val_acc: 0.6522\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.75007 to 0.74056, saving model to best.model\n",
      "0s - loss: 0.8290 - acc: 0.6292 - val_loss: 0.7406 - val_acc: 0.6522\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.74056 to 0.73155, saving model to best.model\n",
      "0s - loss: 0.7756 - acc: 0.6629 - val_loss: 0.7316 - val_acc: 0.6522\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.73155 to 0.72186, saving model to best.model\n",
      "0s - loss: 0.7719 - acc: 0.7303 - val_loss: 0.7219 - val_acc: 0.6522\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.72186 to 0.71172, saving model to best.model\n",
      "0s - loss: 0.7158 - acc: 0.6966 - val_loss: 0.7117 - val_acc: 0.6522\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.71172 to 0.70189, saving model to best.model\n",
      "0s - loss: 0.7437 - acc: 0.6854 - val_loss: 0.7019 - val_acc: 0.6957\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.70189 to 0.69218, saving model to best.model\n",
      "0s - loss: 0.7144 - acc: 0.7416 - val_loss: 0.6922 - val_acc: 0.6957\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.69218 to 0.68263, saving model to best.model\n",
      "0s - loss: 0.7216 - acc: 0.6966 - val_loss: 0.6826 - val_acc: 0.6957\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.68263 to 0.67342, saving model to best.model\n",
      "0s - loss: 0.7540 - acc: 0.6742 - val_loss: 0.6734 - val_acc: 0.6957\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67342 to 0.66411, saving model to best.model\n",
      "0s - loss: 0.7452 - acc: 0.6854 - val_loss: 0.6641 - val_acc: 0.6957\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66411 to 0.65527, saving model to best.model\n",
      "0s - loss: 0.6606 - acc: 0.7191 - val_loss: 0.6553 - val_acc: 0.6957\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.65527 to 0.64596, saving model to best.model\n",
      "0s - loss: 0.7122 - acc: 0.7416 - val_loss: 0.6460 - val_acc: 0.6957\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.64596 to 0.63727, saving model to best.model\n",
      "0s - loss: 0.6620 - acc: 0.7528 - val_loss: 0.6373 - val_acc: 0.6957\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.63727 to 0.62829, saving model to best.model\n",
      "0s - loss: 0.7143 - acc: 0.6966 - val_loss: 0.6283 - val_acc: 0.6957\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.62829 to 0.61955, saving model to best.model\n",
      "0s - loss: 0.7435 - acc: 0.6966 - val_loss: 0.6195 - val_acc: 0.6957\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.61955 to 0.61055, saving model to best.model\n",
      "0s - loss: 0.7137 - acc: 0.7079 - val_loss: 0.6106 - val_acc: 0.6957\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.61055 to 0.60169, saving model to best.model\n",
      "0s - loss: 0.7182 - acc: 0.6629 - val_loss: 0.6017 - val_acc: 0.6957\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.60169 to 0.59296, saving model to best.model\n",
      "0s - loss: 0.6756 - acc: 0.7528 - val_loss: 0.5930 - val_acc: 0.6957\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.59296 to 0.58490, saving model to best.model\n",
      "0s - loss: 0.5983 - acc: 0.7865 - val_loss: 0.5849 - val_acc: 0.6957\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.58490 to 0.57704, saving model to best.model\n",
      "0s - loss: 0.5845 - acc: 0.7978 - val_loss: 0.5770 - val_acc: 0.6957\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.57704 to 0.56879, saving model to best.model\n",
      "0s - loss: 0.5814 - acc: 0.7528 - val_loss: 0.5688 - val_acc: 0.6957\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.56879 to 0.56136, saving model to best.model\n",
      "0s - loss: 0.6871 - acc: 0.6517 - val_loss: 0.5614 - val_acc: 0.6957\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.56136 to 0.55395, saving model to best.model\n",
      "0s - loss: 0.5844 - acc: 0.7416 - val_loss: 0.5540 - val_acc: 0.6957\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.55395 to 0.54746, saving model to best.model\n",
      "0s - loss: 0.6333 - acc: 0.7079 - val_loss: 0.5475 - val_acc: 0.6957\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.54746 to 0.54121, saving model to best.model\n",
      "0s - loss: 0.5704 - acc: 0.8202 - val_loss: 0.5412 - val_acc: 0.6957\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.54121 to 0.53569, saving model to best.model\n",
      "0s - loss: 0.5899 - acc: 0.8090 - val_loss: 0.5357 - val_acc: 0.6957\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.53569 to 0.52991, saving model to best.model\n",
      "0s - loss: 0.5945 - acc: 0.7753 - val_loss: 0.5299 - val_acc: 0.6957\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.52991 to 0.52393, saving model to best.model\n",
      "0s - loss: 0.5674 - acc: 0.8090 - val_loss: 0.5239 - val_acc: 0.6957\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.52393 to 0.51796, saving model to best.model\n",
      "0s - loss: 0.5148 - acc: 0.8202 - val_loss: 0.5180 - val_acc: 0.6957\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.51796 to 0.51181, saving model to best.model\n",
      "0s - loss: 0.5374 - acc: 0.7865 - val_loss: 0.5118 - val_acc: 0.6957\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.51181 to 0.50586, saving model to best.model\n",
      "0s - loss: 0.5151 - acc: 0.7978 - val_loss: 0.5059 - val_acc: 0.6957\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.50586 to 0.49969, saving model to best.model\n",
      "0s - loss: 0.5553 - acc: 0.7528 - val_loss: 0.4997 - val_acc: 0.7391\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.49969 to 0.49395, saving model to best.model\n",
      "0s - loss: 0.5480 - acc: 0.7528 - val_loss: 0.4940 - val_acc: 0.7826\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.49395 to 0.48768, saving model to best.model\n",
      "0s - loss: 0.5320 - acc: 0.8090 - val_loss: 0.4877 - val_acc: 0.7826\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.48768 to 0.48155, saving model to best.model\n",
      "0s - loss: 0.5458 - acc: 0.7753 - val_loss: 0.4816 - val_acc: 0.7826\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.48155 to 0.47526, saving model to best.model\n",
      "0s - loss: 0.5355 - acc: 0.8202 - val_loss: 0.4753 - val_acc: 0.7826\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.47526 to 0.46981, saving model to best.model\n",
      "0s - loss: 0.4885 - acc: 0.8202 - val_loss: 0.4698 - val_acc: 0.7826\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.46981 to 0.46437, saving model to best.model\n",
      "0s - loss: 0.4836 - acc: 0.8427 - val_loss: 0.4644 - val_acc: 0.7826\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.46437 to 0.45955, saving model to best.model\n",
      "0s - loss: 0.4852 - acc: 0.8090 - val_loss: 0.4596 - val_acc: 0.7826\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.45955 to 0.45520, saving model to best.model\n",
      "0s - loss: 0.4836 - acc: 0.8202 - val_loss: 0.4552 - val_acc: 0.7826\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.45520 to 0.45081, saving model to best.model\n",
      "0s - loss: 0.4308 - acc: 0.8427 - val_loss: 0.4508 - val_acc: 0.7826\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.45081 to 0.44683, saving model to best.model\n",
      "0s - loss: 0.4561 - acc: 0.8652 - val_loss: 0.4468 - val_acc: 0.7826\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.44683 to 0.44336, saving model to best.model\n",
      "0s - loss: 0.4552 - acc: 0.8090 - val_loss: 0.4434 - val_acc: 0.8261\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.44336 to 0.44002, saving model to best.model\n",
      "0s - loss: 0.4820 - acc: 0.8315 - val_loss: 0.4400 - val_acc: 0.8261\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.44002 to 0.43674, saving model to best.model\n",
      "0s - loss: 0.4170 - acc: 0.7753 - val_loss: 0.4367 - val_acc: 0.8261\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.43674 to 0.43306, saving model to best.model\n",
      "0s - loss: 0.3953 - acc: 0.8764 - val_loss: 0.4331 - val_acc: 0.8261\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.43306 to 0.42882, saving model to best.model\n",
      "0s - loss: 0.3984 - acc: 0.8427 - val_loss: 0.4288 - val_acc: 0.8261\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.42882 to 0.42390, saving model to best.model\n",
      "0s - loss: 0.4583 - acc: 0.8090 - val_loss: 0.4239 - val_acc: 0.8261\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.42390 to 0.41895, saving model to best.model\n",
      "0s - loss: 0.4240 - acc: 0.8652 - val_loss: 0.4190 - val_acc: 0.8261\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.41895 to 0.41368, saving model to best.model\n",
      "0s - loss: 0.4239 - acc: 0.7978 - val_loss: 0.4137 - val_acc: 0.8261\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.41368 to 0.40764, saving model to best.model\n",
      "0s - loss: 0.4008 - acc: 0.8202 - val_loss: 0.4076 - val_acc: 0.8261\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.40764 to 0.40153, saving model to best.model\n",
      "0s - loss: 0.4265 - acc: 0.8652 - val_loss: 0.4015 - val_acc: 0.8261\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.40153 to 0.39535, saving model to best.model\n",
      "0s - loss: 0.3728 - acc: 0.8764 - val_loss: 0.3953 - val_acc: 0.8261\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.39535 to 0.39012, saving model to best.model\n",
      "0s - loss: 0.3107 - acc: 0.9438 - val_loss: 0.3901 - val_acc: 0.8261\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.39012 to 0.38501, saving model to best.model\n",
      "0s - loss: 0.3900 - acc: 0.7978 - val_loss: 0.3850 - val_acc: 0.8261\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.38501 to 0.38105, saving model to best.model\n",
      "0s - loss: 0.3186 - acc: 0.9213 - val_loss: 0.3810 - val_acc: 0.8261\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.38105 to 0.37733, saving model to best.model\n",
      "0s - loss: 0.3076 - acc: 0.8876 - val_loss: 0.3773 - val_acc: 0.8261\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.37733 to 0.37367, saving model to best.model\n",
      "0s - loss: 0.3593 - acc: 0.9101 - val_loss: 0.3737 - val_acc: 0.8261\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.37367 to 0.36964, saving model to best.model\n",
      "0s - loss: 0.3892 - acc: 0.8427 - val_loss: 0.3696 - val_acc: 0.8261\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.36964 to 0.36626, saving model to best.model\n",
      "0s - loss: 0.3962 - acc: 0.8427 - val_loss: 0.3663 - val_acc: 0.8261\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.36626 to 0.36286, saving model to best.model\n",
      "0s - loss: 0.3472 - acc: 0.8764 - val_loss: 0.3629 - val_acc: 0.8261\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.36286 to 0.35947, saving model to best.model\n",
      "0s - loss: 0.3970 - acc: 0.7865 - val_loss: 0.3595 - val_acc: 0.8261\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.35947 to 0.35631, saving model to best.model\n",
      "0s - loss: 0.3236 - acc: 0.8539 - val_loss: 0.3563 - val_acc: 0.8261\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.35631 to 0.35203, saving model to best.model\n",
      "0s - loss: 0.3238 - acc: 0.8876 - val_loss: 0.3520 - val_acc: 0.8261\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.35203 to 0.34821, saving model to best.model\n",
      "0s - loss: 0.3259 - acc: 0.8989 - val_loss: 0.3482 - val_acc: 0.8261\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.34821 to 0.34477, saving model to best.model\n",
      "0s - loss: 0.2973 - acc: 0.9101 - val_loss: 0.3448 - val_acc: 0.8696\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.34477 to 0.34149, saving model to best.model\n",
      "0s - loss: 0.2986 - acc: 0.8989 - val_loss: 0.3415 - val_acc: 0.8696\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.34149 to 0.33800, saving model to best.model\n",
      "0s - loss: 0.3345 - acc: 0.8315 - val_loss: 0.3380 - val_acc: 0.8696\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.33800 to 0.33419, saving model to best.model\n",
      "0s - loss: 0.2703 - acc: 0.9101 - val_loss: 0.3342 - val_acc: 0.8696\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.33419 to 0.33154, saving model to best.model\n",
      "0s - loss: 0.2397 - acc: 0.9438 - val_loss: 0.3315 - val_acc: 0.8696\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.33154 to 0.32943, saving model to best.model\n",
      "0s - loss: 0.2776 - acc: 0.9438 - val_loss: 0.3294 - val_acc: 0.8261\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.32943 to 0.32772, saving model to best.model\n",
      "0s - loss: 0.2915 - acc: 0.8876 - val_loss: 0.3277 - val_acc: 0.8261\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.32772 to 0.32509, saving model to best.model\n",
      "0s - loss: 0.3057 - acc: 0.8764 - val_loss: 0.3251 - val_acc: 0.8261\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.32509 to 0.32137, saving model to best.model\n",
      "0s - loss: 0.3174 - acc: 0.8539 - val_loss: 0.3214 - val_acc: 0.8261\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.32137 to 0.31827, saving model to best.model\n",
      "0s - loss: 0.2365 - acc: 0.9551 - val_loss: 0.3183 - val_acc: 0.8261\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.31827 to 0.31528, saving model to best.model\n",
      "0s - loss: 0.2967 - acc: 0.8876 - val_loss: 0.3153 - val_acc: 0.8261\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.31528 to 0.31225, saving model to best.model\n",
      "0s - loss: 0.2914 - acc: 0.8989 - val_loss: 0.3123 - val_acc: 0.8261\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.31225 to 0.30901, saving model to best.model\n",
      "0s - loss: 0.2448 - acc: 0.8989 - val_loss: 0.3090 - val_acc: 0.8261\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.30901 to 0.30572, saving model to best.model\n",
      "0s - loss: 0.2565 - acc: 0.9213 - val_loss: 0.3057 - val_acc: 0.8261\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.30572 to 0.30175, saving model to best.model\n",
      "0s - loss: 0.2747 - acc: 0.8764 - val_loss: 0.3017 - val_acc: 0.8261\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.30175 to 0.29851, saving model to best.model\n",
      "0s - loss: 0.2060 - acc: 0.9101 - val_loss: 0.2985 - val_acc: 0.8696\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.29851 to 0.29530, saving model to best.model\n",
      "0s - loss: 0.2385 - acc: 0.9438 - val_loss: 0.2953 - val_acc: 0.8696\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.29530 to 0.29190, saving model to best.model\n",
      "0s - loss: 0.3034 - acc: 0.8539 - val_loss: 0.2919 - val_acc: 0.8696\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.29190 to 0.28710, saving model to best.model\n",
      "0s - loss: 0.2507 - acc: 0.9101 - val_loss: 0.2871 - val_acc: 0.8696\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.28710 to 0.28287, saving model to best.model\n",
      "0s - loss: 0.2537 - acc: 0.9438 - val_loss: 0.2829 - val_acc: 0.8696\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.28287 to 0.27980, saving model to best.model\n",
      "0s - loss: 0.1978 - acc: 0.9551 - val_loss: 0.2798 - val_acc: 0.8696\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.27980 to 0.27756, saving model to best.model\n",
      "0s - loss: 0.2666 - acc: 0.9101 - val_loss: 0.2776 - val_acc: 0.8696\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.27756 to 0.27584, saving model to best.model\n",
      "0s - loss: 0.2233 - acc: 0.9326 - val_loss: 0.2758 - val_acc: 0.8696\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.27584 to 0.27386, saving model to best.model\n",
      "0s - loss: 0.2190 - acc: 0.8989 - val_loss: 0.2739 - val_acc: 0.8696\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.27386 to 0.27283, saving model to best.model\n",
      "0s - loss: 0.1994 - acc: 0.9551 - val_loss: 0.2728 - val_acc: 0.8696\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.2706 - acc: 0.9101 - val_loss: 0.2731 - val_acc: 0.8696\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.27283 to 0.27279, saving model to best.model\n",
      "0s - loss: 0.2077 - acc: 0.9326 - val_loss: 0.2728 - val_acc: 0.8696\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.1988 - acc: 0.9438 - val_loss: 0.2734 - val_acc: 0.8696\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.2373 - acc: 0.9101 - val_loss: 0.2736 - val_acc: 0.8696\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.2132 - acc: 0.9438 - val_loss: 0.2737 - val_acc: 0.8696\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.2423 - acc: 0.9213 - val_loss: 0.2733 - val_acc: 0.8696\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.1708 - acc: 0.9551 - val_loss: 0.2729 - val_acc: 0.8696\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.2095 - acc: 0.8876 - val_loss: 0.2732 - val_acc: 0.8696\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.1586 - acc: 0.9551 - val_loss: 0.2728 - val_acc: 0.8696\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.27279 to 0.27242, saving model to best.model\n",
      "0s - loss: 0.1636 - acc: 0.9438 - val_loss: 0.2724 - val_acc: 0.8696\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.27242 to 0.27168, saving model to best.model\n",
      "0s - loss: 0.2005 - acc: 0.9213 - val_loss: 0.2717 - val_acc: 0.8696\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.27168 to 0.27144, saving model to best.model\n",
      "0s - loss: 0.1698 - acc: 0.9551 - val_loss: 0.2714 - val_acc: 0.8696\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.27144 to 0.26952, saving model to best.model\n",
      "0s - loss: 0.2108 - acc: 0.9326 - val_loss: 0.2695 - val_acc: 0.8696\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.26952 to 0.26880, saving model to best.model\n",
      "0s - loss: 0.1782 - acc: 0.9551 - val_loss: 0.2688 - val_acc: 0.8696\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.26880 to 0.26675, saving model to best.model\n",
      "0s - loss: 0.2080 - acc: 0.9213 - val_loss: 0.2668 - val_acc: 0.8696\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.26675 to 0.26427, saving model to best.model\n",
      "0s - loss: 0.2110 - acc: 0.9213 - val_loss: 0.2643 - val_acc: 0.8696\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.26427 to 0.26127, saving model to best.model\n",
      "0s - loss: 0.1565 - acc: 0.9551 - val_loss: 0.2613 - val_acc: 0.8696\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.26127 to 0.25871, saving model to best.model\n",
      "0s - loss: 0.2206 - acc: 0.8764 - val_loss: 0.2587 - val_acc: 0.8696\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.25871 to 0.25744, saving model to best.model\n",
      "0s - loss: 0.1704 - acc: 0.9438 - val_loss: 0.2574 - val_acc: 0.8696\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.25744 to 0.25581, saving model to best.model\n",
      "0s - loss: 0.2142 - acc: 0.9213 - val_loss: 0.2558 - val_acc: 0.8696\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.25581 to 0.25393, saving model to best.model\n",
      "0s - loss: 0.1200 - acc: 0.9663 - val_loss: 0.2539 - val_acc: 0.8696\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.25393 to 0.25202, saving model to best.model\n",
      "0s - loss: 0.2152 - acc: 0.8876 - val_loss: 0.2520 - val_acc: 0.8696\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.22164, saving model to best.model\n",
      "0s - loss: 1.4995 - acc: 0.2360 - val_loss: 1.2216 - val_acc: 0.2174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.22164 to 1.13184, saving model to best.model\n",
      "0s - loss: 1.2697 - acc: 0.3258 - val_loss: 1.1318 - val_acc: 0.2174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.13184 to 1.08541, saving model to best.model\n",
      "0s - loss: 1.2042 - acc: 0.3820 - val_loss: 1.0854 - val_acc: 0.4348\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.08541 to 1.07286, saving model to best.model\n",
      "0s - loss: 1.2507 - acc: 0.3483 - val_loss: 1.0729 - val_acc: 0.4348\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.1098 - acc: 0.4157 - val_loss: 1.0814 - val_acc: 0.4348\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2036 - acc: 0.4382 - val_loss: 1.0967 - val_acc: 0.4348\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2507 - acc: 0.3933 - val_loss: 1.1153 - val_acc: 0.4348\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2235 - acc: 0.3933 - val_loss: 1.1312 - val_acc: 0.4348\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2673 - acc: 0.4270 - val_loss: 1.1392 - val_acc: 0.4348\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2378 - acc: 0.4270 - val_loss: 1.1413 - val_acc: 0.4348\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2953 - acc: 0.4494 - val_loss: 1.1374 - val_acc: 0.4348\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1966 - acc: 0.4382 - val_loss: 1.1288 - val_acc: 0.4348\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2373 - acc: 0.3596 - val_loss: 1.1192 - val_acc: 0.4348\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2282 - acc: 0.4157 - val_loss: 1.1078 - val_acc: 0.4348\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1876 - acc: 0.4719 - val_loss: 1.0949 - val_acc: 0.4348\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2124 - acc: 0.3820 - val_loss: 1.0816 - val_acc: 0.4348\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.07286 to 1.06881, saving model to best.model\n",
      "0s - loss: 1.1304 - acc: 0.4719 - val_loss: 1.0688 - val_acc: 0.4348\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.06881 to 1.05640, saving model to best.model\n",
      "0s - loss: 1.2682 - acc: 0.4045 - val_loss: 1.0564 - val_acc: 0.4348\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.05640 to 1.04705, saving model to best.model\n",
      "0s - loss: 1.0742 - acc: 0.4607 - val_loss: 1.0470 - val_acc: 0.4348\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.04705 to 1.03990, saving model to best.model\n",
      "0s - loss: 1.1416 - acc: 0.5056 - val_loss: 1.0399 - val_acc: 0.4348\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.03990 to 1.03533, saving model to best.model\n",
      "0s - loss: 1.0966 - acc: 0.4607 - val_loss: 1.0353 - val_acc: 0.4348\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.03533 to 1.03287, saving model to best.model\n",
      "0s - loss: 1.1080 - acc: 0.4607 - val_loss: 1.0329 - val_acc: 0.4348\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.03287 to 1.03215, saving model to best.model\n",
      "0s - loss: 1.2198 - acc: 0.3933 - val_loss: 1.0322 - val_acc: 0.4348\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1579 - acc: 0.4270 - val_loss: 1.0327 - val_acc: 0.4348\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.2379 - acc: 0.3258 - val_loss: 1.0345 - val_acc: 0.4348\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1290 - acc: 0.4157 - val_loss: 1.0360 - val_acc: 0.4348\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1395 - acc: 0.3820 - val_loss: 1.0372 - val_acc: 0.4348\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.0929 - acc: 0.4719 - val_loss: 1.0373 - val_acc: 0.4348\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1352 - acc: 0.4382 - val_loss: 1.0361 - val_acc: 0.4348\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.0883 - acc: 0.4944 - val_loss: 1.0342 - val_acc: 0.4348\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.03215 to 1.03107, saving model to best.model\n",
      "0s - loss: 1.1176 - acc: 0.3933 - val_loss: 1.0311 - val_acc: 0.4348\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.03107 to 1.02754, saving model to best.model\n",
      "0s - loss: 1.0412 - acc: 0.4831 - val_loss: 1.0275 - val_acc: 0.4348\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.02754 to 1.02402, saving model to best.model\n",
      "0s - loss: 1.2710 - acc: 0.3258 - val_loss: 1.0240 - val_acc: 0.4348\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.02402 to 1.02056, saving model to best.model\n",
      "0s - loss: 1.1744 - acc: 0.4944 - val_loss: 1.0206 - val_acc: 0.4348\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.02056 to 1.01821, saving model to best.model\n",
      "0s - loss: 1.1658 - acc: 0.3708 - val_loss: 1.0182 - val_acc: 0.4348\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.01821 to 1.01619, saving model to best.model\n",
      "0s - loss: 1.1037 - acc: 0.4270 - val_loss: 1.0162 - val_acc: 0.4348\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.01619 to 1.01485, saving model to best.model\n",
      "0s - loss: 1.2091 - acc: 0.3146 - val_loss: 1.0149 - val_acc: 0.4348\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.01485 to 1.01279, saving model to best.model\n",
      "0s - loss: 1.1842 - acc: 0.4382 - val_loss: 1.0128 - val_acc: 0.4348\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.01279 to 1.01079, saving model to best.model\n",
      "0s - loss: 1.1097 - acc: 0.4270 - val_loss: 1.0108 - val_acc: 0.4348\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.01079 to 1.00844, saving model to best.model\n",
      "0s - loss: 1.0411 - acc: 0.4719 - val_loss: 1.0084 - val_acc: 0.4348\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.00844 to 1.00554, saving model to best.model\n",
      "0s - loss: 1.1950 - acc: 0.3708 - val_loss: 1.0055 - val_acc: 0.4348\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.00554 to 1.00165, saving model to best.model\n",
      "0s - loss: 1.0997 - acc: 0.4607 - val_loss: 1.0016 - val_acc: 0.4348\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.00165 to 0.99729, saving model to best.model\n",
      "0s - loss: 1.0084 - acc: 0.5730 - val_loss: 0.9973 - val_acc: 0.4348\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.99729 to 0.99320, saving model to best.model\n",
      "0s - loss: 1.1231 - acc: 0.4270 - val_loss: 0.9932 - val_acc: 0.4348\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.99320 to 0.98854, saving model to best.model\n",
      "0s - loss: 1.1298 - acc: 0.4045 - val_loss: 0.9885 - val_acc: 0.4348\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.98854 to 0.98364, saving model to best.model\n",
      "0s - loss: 1.0099 - acc: 0.5618 - val_loss: 0.9836 - val_acc: 0.4348\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.98364 to 0.97892, saving model to best.model\n",
      "0s - loss: 1.0224 - acc: 0.5056 - val_loss: 0.9789 - val_acc: 0.4348\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.97892 to 0.97407, saving model to best.model\n",
      "0s - loss: 1.0120 - acc: 0.4944 - val_loss: 0.9741 - val_acc: 0.4348\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.97407 to 0.96968, saving model to best.model\n",
      "0s - loss: 1.0822 - acc: 0.4831 - val_loss: 0.9697 - val_acc: 0.4348\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.96968 to 0.96536, saving model to best.model\n",
      "0s - loss: 1.1378 - acc: 0.4045 - val_loss: 0.9654 - val_acc: 0.4348\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.96536 to 0.96159, saving model to best.model\n",
      "0s - loss: 1.1041 - acc: 0.4831 - val_loss: 0.9616 - val_acc: 0.4348\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.96159 to 0.95800, saving model to best.model\n",
      "0s - loss: 1.0226 - acc: 0.4944 - val_loss: 0.9580 - val_acc: 0.4348\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.95800 to 0.95477, saving model to best.model\n",
      "0s - loss: 1.0475 - acc: 0.5281 - val_loss: 0.9548 - val_acc: 0.4348\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.95477 to 0.95188, saving model to best.model\n",
      "0s - loss: 1.0360 - acc: 0.4607 - val_loss: 0.9519 - val_acc: 0.4348\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.95188 to 0.94897, saving model to best.model\n",
      "0s - loss: 0.9547 - acc: 0.5506 - val_loss: 0.9490 - val_acc: 0.4348\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.94897 to 0.94632, saving model to best.model\n",
      "0s - loss: 1.1058 - acc: 0.3596 - val_loss: 0.9463 - val_acc: 0.4348\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.94632 to 0.94379, saving model to best.model\n",
      "0s - loss: 1.0460 - acc: 0.5056 - val_loss: 0.9438 - val_acc: 0.4348\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.94379 to 0.94052, saving model to best.model\n",
      "0s - loss: 0.9803 - acc: 0.5169 - val_loss: 0.9405 - val_acc: 0.4348\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.94052 to 0.93685, saving model to best.model\n",
      "0s - loss: 0.9897 - acc: 0.5056 - val_loss: 0.9369 - val_acc: 0.4348\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.93685 to 0.93307, saving model to best.model\n",
      "0s - loss: 1.0432 - acc: 0.5056 - val_loss: 0.9331 - val_acc: 0.4348\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.93307 to 0.92877, saving model to best.model\n",
      "0s - loss: 0.9839 - acc: 0.5618 - val_loss: 0.9288 - val_acc: 0.4348\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.92877 to 0.92402, saving model to best.model\n",
      "0s - loss: 1.0136 - acc: 0.5169 - val_loss: 0.9240 - val_acc: 0.4348\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.92402 to 0.91875, saving model to best.model\n",
      "0s - loss: 1.0451 - acc: 0.4719 - val_loss: 0.9188 - val_acc: 0.4348\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.91875 to 0.91266, saving model to best.model\n",
      "0s - loss: 1.0262 - acc: 0.4831 - val_loss: 0.9127 - val_acc: 0.4348\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.91266 to 0.90630, saving model to best.model\n",
      "0s - loss: 0.9954 - acc: 0.5393 - val_loss: 0.9063 - val_acc: 0.4348\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.90630 to 0.89945, saving model to best.model\n",
      "0s - loss: 1.0231 - acc: 0.4382 - val_loss: 0.8995 - val_acc: 0.4348\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.89945 to 0.89226, saving model to best.model\n",
      "0s - loss: 0.9892 - acc: 0.4494 - val_loss: 0.8923 - val_acc: 0.4348\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.89226 to 0.88491, saving model to best.model\n",
      "0s - loss: 0.9919 - acc: 0.5843 - val_loss: 0.8849 - val_acc: 0.4348\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.88491 to 0.87691, saving model to best.model\n",
      "0s - loss: 0.9171 - acc: 0.5843 - val_loss: 0.8769 - val_acc: 0.4348\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.87691 to 0.86873, saving model to best.model\n",
      "0s - loss: 0.9101 - acc: 0.5955 - val_loss: 0.8687 - val_acc: 0.5652\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.86873 to 0.86008, saving model to best.model\n",
      "0s - loss: 0.9895 - acc: 0.5056 - val_loss: 0.8601 - val_acc: 0.5652\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.86008 to 0.85188, saving model to best.model\n",
      "0s - loss: 0.8885 - acc: 0.5730 - val_loss: 0.8519 - val_acc: 0.6522\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.85188 to 0.84333, saving model to best.model\n",
      "0s - loss: 0.9366 - acc: 0.5618 - val_loss: 0.8433 - val_acc: 0.6522\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.84333 to 0.83460, saving model to best.model\n",
      "0s - loss: 0.8902 - acc: 0.6067 - val_loss: 0.8346 - val_acc: 0.6522\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.83460 to 0.82570, saving model to best.model\n",
      "0s - loss: 0.9345 - acc: 0.5730 - val_loss: 0.8257 - val_acc: 0.6522\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.82570 to 0.81640, saving model to best.model\n",
      "0s - loss: 0.9326 - acc: 0.5618 - val_loss: 0.8164 - val_acc: 0.6957\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.81640 to 0.80725, saving model to best.model\n",
      "0s - loss: 0.8954 - acc: 0.6517 - val_loss: 0.8072 - val_acc: 0.6957\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.80725 to 0.79817, saving model to best.model\n",
      "0s - loss: 0.9990 - acc: 0.5393 - val_loss: 0.7982 - val_acc: 0.6957\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.79817 to 0.78944, saving model to best.model\n",
      "0s - loss: 0.9670 - acc: 0.5730 - val_loss: 0.7894 - val_acc: 0.7391\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.78944 to 0.78046, saving model to best.model\n",
      "0s - loss: 0.8683 - acc: 0.5506 - val_loss: 0.7805 - val_acc: 0.7391\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.78046 to 0.77093, saving model to best.model\n",
      "0s - loss: 0.8692 - acc: 0.6067 - val_loss: 0.7709 - val_acc: 0.7391\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.77093 to 0.76121, saving model to best.model\n",
      "0s - loss: 0.9002 - acc: 0.5843 - val_loss: 0.7612 - val_acc: 0.7391\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.76121 to 0.75125, saving model to best.model\n",
      "0s - loss: 0.8243 - acc: 0.6517 - val_loss: 0.7512 - val_acc: 0.7391\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.75125 to 0.74149, saving model to best.model\n",
      "0s - loss: 0.9677 - acc: 0.5618 - val_loss: 0.7415 - val_acc: 0.7391\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.74149 to 0.73132, saving model to best.model\n",
      "0s - loss: 0.8042 - acc: 0.6629 - val_loss: 0.7313 - val_acc: 0.7391\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.73132 to 0.72082, saving model to best.model\n",
      "0s - loss: 0.8018 - acc: 0.6404 - val_loss: 0.7208 - val_acc: 0.7391\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.72082 to 0.71005, saving model to best.model\n",
      "0s - loss: 0.8339 - acc: 0.6404 - val_loss: 0.7100 - val_acc: 0.7391\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.71005 to 0.69883, saving model to best.model\n",
      "0s - loss: 0.8125 - acc: 0.6404 - val_loss: 0.6988 - val_acc: 0.7391\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.69883 to 0.68725, saving model to best.model\n",
      "0s - loss: 0.8135 - acc: 0.6629 - val_loss: 0.6873 - val_acc: 0.7826\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68725 to 0.67566, saving model to best.model\n",
      "0s - loss: 0.7688 - acc: 0.7079 - val_loss: 0.6757 - val_acc: 0.7391\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67566 to 0.66411, saving model to best.model\n",
      "0s - loss: 0.7205 - acc: 0.7416 - val_loss: 0.6641 - val_acc: 0.7391\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66411 to 0.65251, saving model to best.model\n",
      "0s - loss: 0.7628 - acc: 0.7191 - val_loss: 0.6525 - val_acc: 0.7391\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.65251 to 0.64042, saving model to best.model\n",
      "0s - loss: 0.7413 - acc: 0.7191 - val_loss: 0.6404 - val_acc: 0.7391\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.64042 to 0.62815, saving model to best.model\n",
      "0s - loss: 0.7981 - acc: 0.6742 - val_loss: 0.6282 - val_acc: 0.7826\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.62815 to 0.61596, saving model to best.model\n",
      "0s - loss: 0.7196 - acc: 0.6966 - val_loss: 0.6160 - val_acc: 0.8261\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.61596 to 0.60383, saving model to best.model\n",
      "0s - loss: 0.7990 - acc: 0.6517 - val_loss: 0.6038 - val_acc: 0.8261\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.60383 to 0.59167, saving model to best.model\n",
      "0s - loss: 0.7523 - acc: 0.6854 - val_loss: 0.5917 - val_acc: 0.8261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.59167 to 0.57965, saving model to best.model\n",
      "0s - loss: 0.7196 - acc: 0.7079 - val_loss: 0.5797 - val_acc: 0.8261\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.57965 to 0.56789, saving model to best.model\n",
      "0s - loss: 0.6938 - acc: 0.7191 - val_loss: 0.5679 - val_acc: 0.8261\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.56789 to 0.55596, saving model to best.model\n",
      "0s - loss: 0.6764 - acc: 0.7303 - val_loss: 0.5560 - val_acc: 0.8261\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.55596 to 0.54409, saving model to best.model\n",
      "0s - loss: 0.6736 - acc: 0.7640 - val_loss: 0.5441 - val_acc: 0.8261\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.54409 to 0.53234, saving model to best.model\n",
      "0s - loss: 0.7145 - acc: 0.6742 - val_loss: 0.5323 - val_acc: 0.8261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.53234 to 0.52082, saving model to best.model\n",
      "0s - loss: 0.6030 - acc: 0.7753 - val_loss: 0.5208 - val_acc: 0.8696\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.52082 to 0.50962, saving model to best.model\n",
      "0s - loss: 0.6659 - acc: 0.7416 - val_loss: 0.5096 - val_acc: 0.8696\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.50962 to 0.49856, saving model to best.model\n",
      "0s - loss: 0.5825 - acc: 0.7640 - val_loss: 0.4986 - val_acc: 0.8696\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.49856 to 0.48776, saving model to best.model\n",
      "0s - loss: 0.6391 - acc: 0.7753 - val_loss: 0.4878 - val_acc: 0.8696\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.48776 to 0.47717, saving model to best.model\n",
      "0s - loss: 0.6296 - acc: 0.7865 - val_loss: 0.4772 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.47717 to 0.46702, saving model to best.model\n",
      "0s - loss: 0.6139 - acc: 0.7303 - val_loss: 0.4670 - val_acc: 0.9130\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.46702 to 0.45731, saving model to best.model\n",
      "0s - loss: 0.5550 - acc: 0.7978 - val_loss: 0.4573 - val_acc: 0.9130\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.45731 to 0.44784, saving model to best.model\n",
      "0s - loss: 0.5970 - acc: 0.7978 - val_loss: 0.4478 - val_acc: 0.9130\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.44784 to 0.43833, saving model to best.model\n",
      "0s - loss: 0.4790 - acc: 0.8539 - val_loss: 0.4383 - val_acc: 0.9130\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.43833 to 0.42948, saving model to best.model\n",
      "0s - loss: 0.5627 - acc: 0.7865 - val_loss: 0.4295 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.42948 to 0.42055, saving model to best.model\n",
      "0s - loss: 0.5167 - acc: 0.8315 - val_loss: 0.4206 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.42055 to 0.41162, saving model to best.model\n",
      "0s - loss: 0.5111 - acc: 0.7753 - val_loss: 0.4116 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.41162 to 0.40283, saving model to best.model\n",
      "0s - loss: 0.4945 - acc: 0.7978 - val_loss: 0.4028 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.40283 to 0.39372, saving model to best.model\n",
      "0s - loss: 0.4680 - acc: 0.8427 - val_loss: 0.3937 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.39372 to 0.38448, saving model to best.model\n",
      "0s - loss: 0.5023 - acc: 0.8652 - val_loss: 0.3845 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.38448 to 0.37516, saving model to best.model\n",
      "0s - loss: 0.5328 - acc: 0.7640 - val_loss: 0.3752 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.37516 to 0.36535, saving model to best.model\n",
      "0s - loss: 0.5124 - acc: 0.7865 - val_loss: 0.3654 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.36535 to 0.35597, saving model to best.model\n",
      "0s - loss: 0.5163 - acc: 0.7978 - val_loss: 0.3560 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.35597 to 0.34727, saving model to best.model\n",
      "0s - loss: 0.4608 - acc: 0.8427 - val_loss: 0.3473 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.34727 to 0.33892, saving model to best.model\n",
      "0s - loss: 0.3663 - acc: 0.8876 - val_loss: 0.3389 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.33892 to 0.33089, saving model to best.model\n",
      "0s - loss: 0.4508 - acc: 0.8427 - val_loss: 0.3309 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.33089 to 0.32373, saving model to best.model\n",
      "0s - loss: 0.3736 - acc: 0.8539 - val_loss: 0.3237 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.32373 to 0.31678, saving model to best.model\n",
      "0s - loss: 0.4637 - acc: 0.8315 - val_loss: 0.3168 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.31678 to 0.31003, saving model to best.model\n",
      "0s - loss: 0.3989 - acc: 0.8315 - val_loss: 0.3100 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.31003 to 0.30385, saving model to best.model\n",
      "0s - loss: 0.4247 - acc: 0.8539 - val_loss: 0.3039 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.30385 to 0.29811, saving model to best.model\n",
      "0s - loss: 0.4521 - acc: 0.7753 - val_loss: 0.2981 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.29811 to 0.29243, saving model to best.model\n",
      "0s - loss: 0.4659 - acc: 0.7865 - val_loss: 0.2924 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.29243 to 0.28692, saving model to best.model\n",
      "0s - loss: 0.4401 - acc: 0.8315 - val_loss: 0.2869 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.28692 to 0.28173, saving model to best.model\n",
      "0s - loss: 0.4310 - acc: 0.8427 - val_loss: 0.2817 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.28173 to 0.27670, saving model to best.model\n",
      "0s - loss: 0.3657 - acc: 0.8764 - val_loss: 0.2767 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.27670 to 0.27212, saving model to best.model\n",
      "0s - loss: 0.4140 - acc: 0.8427 - val_loss: 0.2721 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.27212 to 0.26714, saving model to best.model\n",
      "0s - loss: 0.3246 - acc: 0.9326 - val_loss: 0.2671 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.26714 to 0.26191, saving model to best.model\n",
      "0s - loss: 0.3553 - acc: 0.8764 - val_loss: 0.2619 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.26191 to 0.25705, saving model to best.model\n",
      "0s - loss: 0.3320 - acc: 0.8764 - val_loss: 0.2571 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.25705 to 0.25270, saving model to best.model\n",
      "0s - loss: 0.3153 - acc: 0.8989 - val_loss: 0.2527 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.25270 to 0.24857, saving model to best.model\n",
      "0s - loss: 0.3269 - acc: 0.8989 - val_loss: 0.2486 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.24857 to 0.24461, saving model to best.model\n",
      "0s - loss: 0.3421 - acc: 0.8764 - val_loss: 0.2446 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.24461 to 0.24049, saving model to best.model\n",
      "0s - loss: 0.2928 - acc: 0.9438 - val_loss: 0.2405 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.24049 to 0.23563, saving model to best.model\n",
      "0s - loss: 0.3185 - acc: 0.8539 - val_loss: 0.2356 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.23563 to 0.23075, saving model to best.model\n",
      "0s - loss: 0.3260 - acc: 0.8876 - val_loss: 0.2307 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.23075 to 0.22575, saving model to best.model\n",
      "0s - loss: 0.2899 - acc: 0.9213 - val_loss: 0.2258 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.22575 to 0.22145, saving model to best.model\n",
      "0s - loss: 0.3201 - acc: 0.9213 - val_loss: 0.2214 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.22145 to 0.21726, saving model to best.model\n",
      "0s - loss: 0.3310 - acc: 0.8876 - val_loss: 0.2173 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.21726 to 0.21319, saving model to best.model\n",
      "0s - loss: 0.2858 - acc: 0.8989 - val_loss: 0.2132 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.21319 to 0.20937, saving model to best.model\n",
      "0s - loss: 0.3278 - acc: 0.8764 - val_loss: 0.2094 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.20937 to 0.20566, saving model to best.model\n",
      "0s - loss: 0.2952 - acc: 0.8989 - val_loss: 0.2057 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.20566 to 0.20217, saving model to best.model\n",
      "0s - loss: 0.3147 - acc: 0.8989 - val_loss: 0.2022 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.20217 to 0.19929, saving model to best.model\n",
      "0s - loss: 0.2743 - acc: 0.9438 - val_loss: 0.1993 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.19929 to 0.19675, saving model to best.model\n",
      "0s - loss: 0.2295 - acc: 0.9326 - val_loss: 0.1967 - val_acc: 0.9130\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.19675 to 0.19462, saving model to best.model\n",
      "0s - loss: 0.2630 - acc: 0.9326 - val_loss: 0.1946 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.19462 to 0.19250, saving model to best.model\n",
      "0s - loss: 0.2791 - acc: 0.9101 - val_loss: 0.1925 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.19250 to 0.18982, saving model to best.model\n",
      "0s - loss: 0.2229 - acc: 0.9326 - val_loss: 0.1898 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.18982 to 0.18763, saving model to best.model\n",
      "0s - loss: 0.2561 - acc: 0.9101 - val_loss: 0.1876 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.18763 to 0.18508, saving model to best.model\n",
      "0s - loss: 0.3147 - acc: 0.8989 - val_loss: 0.1851 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.18508 to 0.18311, saving model to best.model\n",
      "0s - loss: 0.2367 - acc: 0.9101 - val_loss: 0.1831 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.18311 to 0.18188, saving model to best.model\n",
      "0s - loss: 0.3006 - acc: 0.8989 - val_loss: 0.1819 - val_acc: 0.9130\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.18188 to 0.18052, saving model to best.model\n",
      "0s - loss: 0.1935 - acc: 0.9551 - val_loss: 0.1805 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.18052 to 0.17953, saving model to best.model\n",
      "0s - loss: 0.1819 - acc: 0.9663 - val_loss: 0.1795 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.17953 to 0.17808, saving model to best.model\n",
      "0s - loss: 0.2044 - acc: 0.9551 - val_loss: 0.1781 - val_acc: 0.9130\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.17808 to 0.17702, saving model to best.model\n",
      "0s - loss: 0.1780 - acc: 0.9888 - val_loss: 0.1770 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.17702 to 0.17654, saving model to best.model\n",
      "0s - loss: 0.2076 - acc: 0.9438 - val_loss: 0.1765 - val_acc: 0.9130\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.17654 to 0.17482, saving model to best.model\n",
      "0s - loss: 0.1820 - acc: 0.9663 - val_loss: 0.1748 - val_acc: 0.9130\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.17482 to 0.17272, saving model to best.model\n",
      "0s - loss: 0.1967 - acc: 0.9101 - val_loss: 0.1727 - val_acc: 0.9130\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.17272 to 0.17120, saving model to best.model\n",
      "0s - loss: 0.1646 - acc: 0.9663 - val_loss: 0.1712 - val_acc: 0.9130\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.17120 to 0.16918, saving model to best.model\n",
      "0s - loss: 0.1799 - acc: 0.9551 - val_loss: 0.1692 - val_acc: 0.9130\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.16918 to 0.16823, saving model to best.model\n",
      "0s - loss: 0.2616 - acc: 0.9213 - val_loss: 0.1682 - val_acc: 0.9130\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.16823 to 0.16720, saving model to best.model\n",
      "0s - loss: 0.2102 - acc: 0.9438 - val_loss: 0.1672 - val_acc: 0.9130\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.16720 to 0.16608, saving model to best.model\n",
      "0s - loss: 0.1951 - acc: 0.9438 - val_loss: 0.1661 - val_acc: 0.9130\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.16608 to 0.16517, saving model to best.model\n",
      "0s - loss: 0.1509 - acc: 0.9663 - val_loss: 0.1652 - val_acc: 0.9130\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.16517 to 0.16477, saving model to best.model\n",
      "0s - loss: 0.1323 - acc: 0.9775 - val_loss: 0.1648 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.16477 to 0.16391, saving model to best.model\n",
      "0s - loss: 0.1520 - acc: 0.9551 - val_loss: 0.1639 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.16391 to 0.16269, saving model to best.model\n",
      "0s - loss: 0.1668 - acc: 0.9663 - val_loss: 0.1627 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.16269 to 0.16176, saving model to best.model\n",
      "0s - loss: 0.1862 - acc: 0.9663 - val_loss: 0.1618 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.16176 to 0.16159, saving model to best.model\n",
      "0s - loss: 0.2191 - acc: 0.9213 - val_loss: 0.1616 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.16159 to 0.16135, saving model to best.model\n",
      "0s - loss: 0.1332 - acc: 0.9888 - val_loss: 0.1613 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.16135 to 0.16058, saving model to best.model\n",
      "0s - loss: 0.1768 - acc: 0.9438 - val_loss: 0.1606 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.16058 to 0.15961, saving model to best.model\n",
      "0s - loss: 0.1404 - acc: 0.9775 - val_loss: 0.1596 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.15961 to 0.15880, saving model to best.model\n",
      "0s - loss: 0.2056 - acc: 0.9213 - val_loss: 0.1588 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.15880 to 0.15797, saving model to best.model\n",
      "0s - loss: 0.1427 - acc: 0.9551 - val_loss: 0.1580 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.15797 to 0.15674, saving model to best.model\n",
      "0s - loss: 0.1463 - acc: 0.9438 - val_loss: 0.1567 - val_acc: 0.9130\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.15674 to 0.15518, saving model to best.model\n",
      "0s - loss: 0.1614 - acc: 0.9438 - val_loss: 0.1552 - val_acc: 0.9130\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.15518 to 0.15357, saving model to best.model\n",
      "0s - loss: 0.1143 - acc: 0.9888 - val_loss: 0.1536 - val_acc: 0.9130\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.15357 to 0.15165, saving model to best.model\n",
      "0s - loss: 0.1523 - acc: 0.9551 - val_loss: 0.1517 - val_acc: 0.9130\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.15165 to 0.15011, saving model to best.model\n",
      "0s - loss: 0.1477 - acc: 0.9438 - val_loss: 0.1501 - val_acc: 0.9130\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.15011 to 0.14931, saving model to best.model\n",
      "0s - loss: 0.1537 - acc: 0.9551 - val_loss: 0.1493 - val_acc: 0.9130\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.14931 to 0.14840, saving model to best.model\n",
      "0s - loss: 0.1161 - acc: 0.9663 - val_loss: 0.1484 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.14840 to 0.14715, saving model to best.model\n",
      "0s - loss: 0.1505 - acc: 0.9551 - val_loss: 0.1471 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.14715 to 0.14592, saving model to best.model\n",
      "0s - loss: 0.1777 - acc: 0.9326 - val_loss: 0.1459 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.14592 to 0.14469, saving model to best.model\n",
      "0s - loss: 0.1392 - acc: 0.9551 - val_loss: 0.1447 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.14469 to 0.14399, saving model to best.model\n",
      "0s - loss: 0.1592 - acc: 0.9438 - val_loss: 0.1440 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.14399 to 0.14313, saving model to best.model\n",
      "0s - loss: 0.1268 - acc: 0.9663 - val_loss: 0.1431 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.14313 to 0.14201, saving model to best.model\n",
      "0s - loss: 0.1424 - acc: 0.9438 - val_loss: 0.1420 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.14201 to 0.14063, saving model to best.model\n",
      "0s - loss: 0.1269 - acc: 0.9663 - val_loss: 0.1406 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.14063 to 0.13956, saving model to best.model\n",
      "0s - loss: 0.1163 - acc: 0.9663 - val_loss: 0.1396 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.13956 to 0.13872, saving model to best.model\n",
      "0s - loss: 0.1175 - acc: 0.9663 - val_loss: 0.1387 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.13872 to 0.13710, saving model to best.model\n",
      "0s - loss: 0.1515 - acc: 0.9551 - val_loss: 0.1371 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.13710 to 0.13563, saving model to best.model\n",
      "0s - loss: 0.1257 - acc: 0.9663 - val_loss: 0.1356 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.13563 to 0.13443, saving model to best.model\n",
      "0s - loss: 0.1203 - acc: 0.9775 - val_loss: 0.1344 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.10869, saving model to best.model\n",
      "0s - loss: 1.3139 - acc: 0.3146 - val_loss: 1.1087 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.10869 to 1.10603, saving model to best.model\n",
      "0s - loss: 1.4410 - acc: 0.3820 - val_loss: 1.1060 - val_acc: 0.2609\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.2449 - acc: 0.3483 - val_loss: 1.1235 - val_acc: 0.2609\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.3683 - acc: 0.2809 - val_loss: 1.1517 - val_acc: 0.2609\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.3488 - acc: 0.2360 - val_loss: 1.1716 - val_acc: 0.2609\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.3269 - acc: 0.3146 - val_loss: 1.1829 - val_acc: 0.2609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1672 - acc: 0.4607 - val_loss: 1.1835 - val_acc: 0.2609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1979 - acc: 0.3371 - val_loss: 1.1786 - val_acc: 0.2609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2304 - acc: 0.3596 - val_loss: 1.1698 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.3508 - acc: 0.2921 - val_loss: 1.1575 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2283 - acc: 0.3371 - val_loss: 1.1435 - val_acc: 0.2609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1919 - acc: 0.3933 - val_loss: 1.1296 - val_acc: 0.2609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.3145 - acc: 0.2697 - val_loss: 1.1156 - val_acc: 0.3478\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.10603 to 1.10357, saving model to best.model\n",
      "0s - loss: 1.2077 - acc: 0.3258 - val_loss: 1.1036 - val_acc: 0.3913\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.10357 to 1.09562, saving model to best.model\n",
      "0s - loss: 1.2011 - acc: 0.3708 - val_loss: 1.0956 - val_acc: 0.3478\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.09562 to 1.09077, saving model to best.model\n",
      "0s - loss: 1.2802 - acc: 0.2584 - val_loss: 1.0908 - val_acc: 0.2609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.09077 to 1.08799, saving model to best.model\n",
      "0s - loss: 1.2027 - acc: 0.4157 - val_loss: 1.0880 - val_acc: 0.2609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.08799 to 1.08763, saving model to best.model\n",
      "0s - loss: 1.2071 - acc: 0.3371 - val_loss: 1.0876 - val_acc: 0.2609\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1772 - acc: 0.3258 - val_loss: 1.0882 - val_acc: 0.2609\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2521 - acc: 0.3483 - val_loss: 1.0881 - val_acc: 0.2609\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.08763 to 1.08762, saving model to best.model\n",
      "0s - loss: 1.1785 - acc: 0.3933 - val_loss: 1.0876 - val_acc: 0.2609\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.08762 to 1.08694, saving model to best.model\n",
      "0s - loss: 1.2623 - acc: 0.3371 - val_loss: 1.0869 - val_acc: 0.2609\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.08694 to 1.08484, saving model to best.model\n",
      "0s - loss: 1.1866 - acc: 0.3820 - val_loss: 1.0848 - val_acc: 0.2609\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.08484 to 1.08282, saving model to best.model\n",
      "0s - loss: 1.1682 - acc: 0.4157 - val_loss: 1.0828 - val_acc: 0.2609\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.08282 to 1.08070, saving model to best.model\n",
      "0s - loss: 1.2008 - acc: 0.3708 - val_loss: 1.0807 - val_acc: 0.2609\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.08070 to 1.07824, saving model to best.model\n",
      "0s - loss: 1.0983 - acc: 0.4157 - val_loss: 1.0782 - val_acc: 0.2609\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.07824 to 1.07622, saving model to best.model\n",
      "0s - loss: 1.1758 - acc: 0.3371 - val_loss: 1.0762 - val_acc: 0.2609\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.07622 to 1.07412, saving model to best.model\n",
      "0s - loss: 1.0967 - acc: 0.4719 - val_loss: 1.0741 - val_acc: 0.2609\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.07412 to 1.07259, saving model to best.model\n",
      "0s - loss: 1.1622 - acc: 0.3933 - val_loss: 1.0726 - val_acc: 0.2609\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.07259 to 1.07177, saving model to best.model\n",
      "0s - loss: 1.2385 - acc: 0.2472 - val_loss: 1.0718 - val_acc: 0.3043\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.07177 to 1.07133, saving model to best.model\n",
      "0s - loss: 1.1818 - acc: 0.3708 - val_loss: 1.0713 - val_acc: 0.3478\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.07133 to 1.06984, saving model to best.model\n",
      "0s - loss: 1.1674 - acc: 0.3483 - val_loss: 1.0698 - val_acc: 0.4348\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.06984 to 1.06639, saving model to best.model\n",
      "0s - loss: 1.1583 - acc: 0.3258 - val_loss: 1.0664 - val_acc: 0.4783\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.06639 to 1.06234, saving model to best.model\n",
      "0s - loss: 1.2181 - acc: 0.4045 - val_loss: 1.0623 - val_acc: 0.5652\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.06234 to 1.05920, saving model to best.model\n",
      "0s - loss: 1.1530 - acc: 0.3596 - val_loss: 1.0592 - val_acc: 0.5652\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.05920 to 1.05622, saving model to best.model\n",
      "0s - loss: 1.0996 - acc: 0.4494 - val_loss: 1.0562 - val_acc: 0.5652\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.05622 to 1.05315, saving model to best.model\n",
      "0s - loss: 1.1333 - acc: 0.3371 - val_loss: 1.0532 - val_acc: 0.5652\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.05315 to 1.04939, saving model to best.model\n",
      "0s - loss: 1.0783 - acc: 0.5056 - val_loss: 1.0494 - val_acc: 0.5652\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.04939 to 1.04462, saving model to best.model\n",
      "0s - loss: 1.1520 - acc: 0.4607 - val_loss: 1.0446 - val_acc: 0.5652\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.04462 to 1.04095, saving model to best.model\n",
      "0s - loss: 1.1525 - acc: 0.4045 - val_loss: 1.0409 - val_acc: 0.5652\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.04095 to 1.03667, saving model to best.model\n",
      "0s - loss: 1.1142 - acc: 0.3933 - val_loss: 1.0367 - val_acc: 0.5652\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.03667 to 1.03199, saving model to best.model\n",
      "0s - loss: 1.0898 - acc: 0.4270 - val_loss: 1.0320 - val_acc: 0.5652\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.03199 to 1.02676, saving model to best.model\n",
      "0s - loss: 1.0624 - acc: 0.4494 - val_loss: 1.0268 - val_acc: 0.5652\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.02676 to 1.02133, saving model to best.model\n",
      "0s - loss: 1.0458 - acc: 0.4944 - val_loss: 1.0213 - val_acc: 0.5652\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.02133 to 1.01636, saving model to best.model\n",
      "0s - loss: 1.1553 - acc: 0.4270 - val_loss: 1.0164 - val_acc: 0.5652\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.01636 to 1.01163, saving model to best.model\n",
      "0s - loss: 1.0213 - acc: 0.4719 - val_loss: 1.0116 - val_acc: 0.5652\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.01163 to 1.00845, saving model to best.model\n",
      "0s - loss: 1.1335 - acc: 0.3371 - val_loss: 1.0085 - val_acc: 0.5652\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.00845 to 1.00541, saving model to best.model\n",
      "0s - loss: 1.1225 - acc: 0.4045 - val_loss: 1.0054 - val_acc: 0.5652\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.00541 to 1.00155, saving model to best.model\n",
      "0s - loss: 1.0957 - acc: 0.3820 - val_loss: 1.0016 - val_acc: 0.5217\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.00155 to 0.99728, saving model to best.model\n",
      "0s - loss: 1.0035 - acc: 0.4831 - val_loss: 0.9973 - val_acc: 0.4783\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.99728 to 0.99312, saving model to best.model\n",
      "0s - loss: 1.0645 - acc: 0.4719 - val_loss: 0.9931 - val_acc: 0.5217\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.99312 to 0.98893, saving model to best.model\n",
      "0s - loss: 1.0495 - acc: 0.4719 - val_loss: 0.9889 - val_acc: 0.5652\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.98893 to 0.98444, saving model to best.model\n",
      "0s - loss: 1.0229 - acc: 0.4494 - val_loss: 0.9844 - val_acc: 0.5652\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.98444 to 0.98026, saving model to best.model\n",
      "0s - loss: 1.1004 - acc: 0.4270 - val_loss: 0.9803 - val_acc: 0.5652\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.98026 to 0.97495, saving model to best.model\n",
      "0s - loss: 1.0249 - acc: 0.4831 - val_loss: 0.9749 - val_acc: 0.5652\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.97495 to 0.96897, saving model to best.model\n",
      "0s - loss: 1.0030 - acc: 0.5056 - val_loss: 0.9690 - val_acc: 0.5652\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.96897 to 0.96228, saving model to best.model\n",
      "0s - loss: 1.0249 - acc: 0.4831 - val_loss: 0.9623 - val_acc: 0.6087\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.96228 to 0.95519, saving model to best.model\n",
      "0s - loss: 0.9386 - acc: 0.5393 - val_loss: 0.9552 - val_acc: 0.6087\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.95519 to 0.94758, saving model to best.model\n",
      "0s - loss: 0.9788 - acc: 0.4831 - val_loss: 0.9476 - val_acc: 0.6087\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.94758 to 0.93942, saving model to best.model\n",
      "0s - loss: 1.0228 - acc: 0.4944 - val_loss: 0.9394 - val_acc: 0.6087\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.93942 to 0.93009, saving model to best.model\n",
      "0s - loss: 0.9567 - acc: 0.5506 - val_loss: 0.9301 - val_acc: 0.6087\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.93009 to 0.92055, saving model to best.model\n",
      "0s - loss: 0.9432 - acc: 0.5506 - val_loss: 0.9206 - val_acc: 0.6087\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.92055 to 0.91085, saving model to best.model\n",
      "0s - loss: 0.9607 - acc: 0.5506 - val_loss: 0.9108 - val_acc: 0.7391\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.91085 to 0.90138, saving model to best.model\n",
      "0s - loss: 1.0642 - acc: 0.4270 - val_loss: 0.9014 - val_acc: 0.7391\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.90138 to 0.89194, saving model to best.model\n",
      "0s - loss: 0.8829 - acc: 0.6180 - val_loss: 0.8919 - val_acc: 0.7391\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.89194 to 0.88240, saving model to best.model\n",
      "0s - loss: 0.9764 - acc: 0.5281 - val_loss: 0.8824 - val_acc: 0.7391\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.88240 to 0.87298, saving model to best.model\n",
      "0s - loss: 0.8717 - acc: 0.6404 - val_loss: 0.8730 - val_acc: 0.7391\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.87298 to 0.86372, saving model to best.model\n",
      "0s - loss: 0.9355 - acc: 0.5618 - val_loss: 0.8637 - val_acc: 0.7391\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.86372 to 0.85475, saving model to best.model\n",
      "0s - loss: 0.8718 - acc: 0.6180 - val_loss: 0.8548 - val_acc: 0.7391\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.85475 to 0.84589, saving model to best.model\n",
      "0s - loss: 0.9423 - acc: 0.5730 - val_loss: 0.8459 - val_acc: 0.7391\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.84589 to 0.83684, saving model to best.model\n",
      "0s - loss: 0.8592 - acc: 0.6629 - val_loss: 0.8368 - val_acc: 0.7391\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.83684 to 0.82772, saving model to best.model\n",
      "0s - loss: 0.9048 - acc: 0.5730 - val_loss: 0.8277 - val_acc: 0.7826\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.82772 to 0.81914, saving model to best.model\n",
      "0s - loss: 0.9245 - acc: 0.5393 - val_loss: 0.8191 - val_acc: 0.8261\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.81914 to 0.81085, saving model to best.model\n",
      "0s - loss: 0.8935 - acc: 0.5843 - val_loss: 0.8109 - val_acc: 0.8696\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.81085 to 0.80213, saving model to best.model\n",
      "0s - loss: 0.8763 - acc: 0.6067 - val_loss: 0.8021 - val_acc: 0.8696\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.80213 to 0.79329, saving model to best.model\n",
      "0s - loss: 0.7909 - acc: 0.6742 - val_loss: 0.7933 - val_acc: 0.8696\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.79329 to 0.78359, saving model to best.model\n",
      "0s - loss: 0.7861 - acc: 0.6629 - val_loss: 0.7836 - val_acc: 0.9130\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.78359 to 0.77344, saving model to best.model\n",
      "0s - loss: 0.8181 - acc: 0.6517 - val_loss: 0.7734 - val_acc: 0.9130\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.77344 to 0.76323, saving model to best.model\n",
      "0s - loss: 0.8342 - acc: 0.6180 - val_loss: 0.7632 - val_acc: 0.9130\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.76323 to 0.75253, saving model to best.model\n",
      "0s - loss: 0.8121 - acc: 0.6180 - val_loss: 0.7525 - val_acc: 0.9130\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.75253 to 0.74297, saving model to best.model\n",
      "0s - loss: 0.7504 - acc: 0.6742 - val_loss: 0.7430 - val_acc: 0.9130\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.74297 to 0.73271, saving model to best.model\n",
      "0s - loss: 0.7472 - acc: 0.6966 - val_loss: 0.7327 - val_acc: 0.9130\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.73271 to 0.72164, saving model to best.model\n",
      "0s - loss: 0.8080 - acc: 0.6292 - val_loss: 0.7216 - val_acc: 0.9130\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.72164 to 0.70992, saving model to best.model\n",
      "0s - loss: 0.8025 - acc: 0.6404 - val_loss: 0.7099 - val_acc: 0.9130\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.70992 to 0.69880, saving model to best.model\n",
      "0s - loss: 0.8085 - acc: 0.6292 - val_loss: 0.6988 - val_acc: 0.9130\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69880 to 0.68788, saving model to best.model\n",
      "0s - loss: 0.7407 - acc: 0.6629 - val_loss: 0.6879 - val_acc: 0.9130\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68788 to 0.67721, saving model to best.model\n",
      "0s - loss: 0.8094 - acc: 0.5955 - val_loss: 0.6772 - val_acc: 0.9130\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67721 to 0.66592, saving model to best.model\n",
      "0s - loss: 0.7127 - acc: 0.6742 - val_loss: 0.6659 - val_acc: 0.9130\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.66592 to 0.65511, saving model to best.model\n",
      "0s - loss: 0.7522 - acc: 0.6742 - val_loss: 0.6551 - val_acc: 0.9130\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.65511 to 0.64530, saving model to best.model\n",
      "0s - loss: 0.7188 - acc: 0.6966 - val_loss: 0.6453 - val_acc: 0.9130\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.64530 to 0.63618, saving model to best.model\n",
      "0s - loss: 0.6758 - acc: 0.7079 - val_loss: 0.6362 - val_acc: 0.9130\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.63618 to 0.62706, saving model to best.model\n",
      "0s - loss: 0.6402 - acc: 0.7416 - val_loss: 0.6271 - val_acc: 0.9130\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.62706 to 0.61826, saving model to best.model\n",
      "0s - loss: 0.6350 - acc: 0.7978 - val_loss: 0.6183 - val_acc: 0.9130\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.61826 to 0.60954, saving model to best.model\n",
      "0s - loss: 0.6889 - acc: 0.7416 - val_loss: 0.6095 - val_acc: 0.9130\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.60954 to 0.60089, saving model to best.model\n",
      "0s - loss: 0.6745 - acc: 0.6966 - val_loss: 0.6009 - val_acc: 0.9130\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.60089 to 0.59178, saving model to best.model\n",
      "0s - loss: 0.6984 - acc: 0.6629 - val_loss: 0.5918 - val_acc: 0.9130\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.59178 to 0.58292, saving model to best.model\n",
      "0s - loss: 0.6527 - acc: 0.7640 - val_loss: 0.5829 - val_acc: 0.9130\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.58292 to 0.57447, saving model to best.model\n",
      "0s - loss: 0.6235 - acc: 0.7865 - val_loss: 0.5745 - val_acc: 0.9130\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.57447 to 0.56706, saving model to best.model\n",
      "0s - loss: 0.6196 - acc: 0.7528 - val_loss: 0.5671 - val_acc: 0.9130\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.56706 to 0.56019, saving model to best.model\n",
      "0s - loss: 0.6037 - acc: 0.7753 - val_loss: 0.5602 - val_acc: 0.9130\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.56019 to 0.55339, saving model to best.model\n",
      "0s - loss: 0.6126 - acc: 0.7416 - val_loss: 0.5534 - val_acc: 0.9130\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.55339 to 0.54659, saving model to best.model\n",
      "0s - loss: 0.6573 - acc: 0.7303 - val_loss: 0.5466 - val_acc: 0.9130\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.54659 to 0.53955, saving model to best.model\n",
      "0s - loss: 0.5551 - acc: 0.7528 - val_loss: 0.5396 - val_acc: 0.9130\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.53955 to 0.53257, saving model to best.model\n",
      "0s - loss: 0.6677 - acc: 0.7416 - val_loss: 0.5326 - val_acc: 0.9130\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.53257 to 0.52579, saving model to best.model\n",
      "0s - loss: 0.5632 - acc: 0.7640 - val_loss: 0.5258 - val_acc: 0.9130\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.52579 to 0.51898, saving model to best.model\n",
      "0s - loss: 0.5414 - acc: 0.7865 - val_loss: 0.5190 - val_acc: 0.9130\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.51898 to 0.51244, saving model to best.model\n",
      "0s - loss: 0.5348 - acc: 0.8202 - val_loss: 0.5124 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.51244 to 0.50604, saving model to best.model\n",
      "0s - loss: 0.5740 - acc: 0.7753 - val_loss: 0.5060 - val_acc: 0.9130\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.50604 to 0.49978, saving model to best.model\n",
      "0s - loss: 0.5638 - acc: 0.7640 - val_loss: 0.4998 - val_acc: 0.9130\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.49978 to 0.49392, saving model to best.model\n",
      "0s - loss: 0.5276 - acc: 0.7865 - val_loss: 0.4939 - val_acc: 0.9130\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.49392 to 0.48815, saving model to best.model\n",
      "0s - loss: 0.4520 - acc: 0.8427 - val_loss: 0.4881 - val_acc: 0.9130\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.48815 to 0.48224, saving model to best.model\n",
      "0s - loss: 0.5182 - acc: 0.7640 - val_loss: 0.4822 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.48224 to 0.47590, saving model to best.model\n",
      "0s - loss: 0.4887 - acc: 0.7978 - val_loss: 0.4759 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.47590 to 0.46946, saving model to best.model\n",
      "0s - loss: 0.4887 - acc: 0.7978 - val_loss: 0.4695 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.46946 to 0.46353, saving model to best.model\n",
      "0s - loss: 0.5107 - acc: 0.7865 - val_loss: 0.4635 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.46353 to 0.45760, saving model to best.model\n",
      "0s - loss: 0.5392 - acc: 0.7416 - val_loss: 0.4576 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.45760 to 0.45194, saving model to best.model\n",
      "0s - loss: 0.4840 - acc: 0.7640 - val_loss: 0.4519 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.45194 to 0.44707, saving model to best.model\n",
      "0s - loss: 0.4791 - acc: 0.8652 - val_loss: 0.4471 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.44707 to 0.44262, saving model to best.model\n",
      "0s - loss: 0.4775 - acc: 0.8090 - val_loss: 0.4426 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.44262 to 0.43834, saving model to best.model\n",
      "0s - loss: 0.5136 - acc: 0.7865 - val_loss: 0.4383 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.43834 to 0.43417, saving model to best.model\n",
      "0s - loss: 0.5168 - acc: 0.7753 - val_loss: 0.4342 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.43417 to 0.42989, saving model to best.model\n",
      "0s - loss: 0.4157 - acc: 0.8090 - val_loss: 0.4299 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.42989 to 0.42542, saving model to best.model\n",
      "0s - loss: 0.4102 - acc: 0.8652 - val_loss: 0.4254 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.42542 to 0.42050, saving model to best.model\n",
      "0s - loss: 0.4036 - acc: 0.8315 - val_loss: 0.4205 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.42050 to 0.41556, saving model to best.model\n",
      "0s - loss: 0.4918 - acc: 0.7753 - val_loss: 0.4156 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.41556 to 0.41064, saving model to best.model\n",
      "0s - loss: 0.4211 - acc: 0.8876 - val_loss: 0.4106 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.41064 to 0.40579, saving model to best.model\n",
      "0s - loss: 0.4145 - acc: 0.8427 - val_loss: 0.4058 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.40579 to 0.40049, saving model to best.model\n",
      "0s - loss: 0.4811 - acc: 0.7640 - val_loss: 0.4005 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.40049 to 0.39566, saving model to best.model\n",
      "0s - loss: 0.4804 - acc: 0.7978 - val_loss: 0.3957 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.39566 to 0.39064, saving model to best.model\n",
      "0s - loss: 0.4225 - acc: 0.8315 - val_loss: 0.3906 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.39064 to 0.38526, saving model to best.model\n",
      "0s - loss: 0.3749 - acc: 0.8876 - val_loss: 0.3853 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.38526 to 0.38030, saving model to best.model\n",
      "0s - loss: 0.3698 - acc: 0.8652 - val_loss: 0.3803 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.38030 to 0.37396, saving model to best.model\n",
      "0s - loss: 0.4527 - acc: 0.7978 - val_loss: 0.3740 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.37396 to 0.36724, saving model to best.model\n",
      "0s - loss: 0.4038 - acc: 0.8539 - val_loss: 0.3672 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.36724 to 0.36069, saving model to best.model\n",
      "0s - loss: 0.4147 - acc: 0.8876 - val_loss: 0.3607 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.36069 to 0.35484, saving model to best.model\n",
      "0s - loss: 0.3580 - acc: 0.8989 - val_loss: 0.3548 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.35484 to 0.34852, saving model to best.model\n",
      "0s - loss: 0.3904 - acc: 0.8315 - val_loss: 0.3485 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.34852 to 0.34290, saving model to best.model\n",
      "0s - loss: 0.4110 - acc: 0.8539 - val_loss: 0.3429 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.34290 to 0.33731, saving model to best.model\n",
      "0s - loss: 0.4074 - acc: 0.7640 - val_loss: 0.3373 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.33731 to 0.33142, saving model to best.model\n",
      "0s - loss: 0.4042 - acc: 0.8315 - val_loss: 0.3314 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.33142 to 0.32549, saving model to best.model\n",
      "0s - loss: 0.4135 - acc: 0.8090 - val_loss: 0.3255 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.32549 to 0.32043, saving model to best.model\n",
      "0s - loss: 0.3025 - acc: 0.8989 - val_loss: 0.3204 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.32043 to 0.31596, saving model to best.model\n",
      "0s - loss: 0.3668 - acc: 0.8427 - val_loss: 0.3160 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.31596 to 0.31067, saving model to best.model\n",
      "0s - loss: 0.4201 - acc: 0.8315 - val_loss: 0.3107 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.31067 to 0.30576, saving model to best.model\n",
      "0s - loss: 0.3396 - acc: 0.8989 - val_loss: 0.3058 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.30576 to 0.30102, saving model to best.model\n",
      "0s - loss: 0.3110 - acc: 0.8764 - val_loss: 0.3010 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.30102 to 0.29736, saving model to best.model\n",
      "0s - loss: 0.3248 - acc: 0.8652 - val_loss: 0.2974 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.29736 to 0.29343, saving model to best.model\n",
      "0s - loss: 0.3382 - acc: 0.8989 - val_loss: 0.2934 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.29343 to 0.28913, saving model to best.model\n",
      "0s - loss: 0.3878 - acc: 0.8315 - val_loss: 0.2891 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.28913 to 0.28506, saving model to best.model\n",
      "0s - loss: 0.2881 - acc: 0.8876 - val_loss: 0.2851 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.28506 to 0.28147, saving model to best.model\n",
      "0s - loss: 0.2759 - acc: 0.9438 - val_loss: 0.2815 - val_acc: 0.9130\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.28147 to 0.27757, saving model to best.model\n",
      "0s - loss: 0.2939 - acc: 0.8876 - val_loss: 0.2776 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.27757 to 0.27330, saving model to best.model\n",
      "0s - loss: 0.3010 - acc: 0.8652 - val_loss: 0.2733 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.27330 to 0.26894, saving model to best.model\n",
      "0s - loss: 0.3582 - acc: 0.8764 - val_loss: 0.2689 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.26894 to 0.26483, saving model to best.model\n",
      "0s - loss: 0.3128 - acc: 0.8764 - val_loss: 0.2648 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.26483 to 0.26031, saving model to best.model\n",
      "0s - loss: 0.3070 - acc: 0.9213 - val_loss: 0.2603 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.26031 to 0.25529, saving model to best.model\n",
      "0s - loss: 0.2698 - acc: 0.9213 - val_loss: 0.2553 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.25529 to 0.25124, saving model to best.model\n",
      "0s - loss: 0.3298 - acc: 0.8539 - val_loss: 0.2512 - val_acc: 0.9130\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.25124 to 0.24762, saving model to best.model\n",
      "0s - loss: 0.2531 - acc: 0.9551 - val_loss: 0.2476 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.24762 to 0.24505, saving model to best.model\n",
      "0s - loss: 0.2358 - acc: 0.9326 - val_loss: 0.2451 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.24505 to 0.24251, saving model to best.model\n",
      "0s - loss: 0.2607 - acc: 0.9101 - val_loss: 0.2425 - val_acc: 0.9130\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.24251 to 0.24009, saving model to best.model\n",
      "0s - loss: 0.2539 - acc: 0.9213 - val_loss: 0.2401 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.24009 to 0.23794, saving model to best.model\n",
      "0s - loss: 0.2947 - acc: 0.8876 - val_loss: 0.2379 - val_acc: 0.9130\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.23794 to 0.23610, saving model to best.model\n",
      "0s - loss: 0.3112 - acc: 0.8315 - val_loss: 0.2361 - val_acc: 0.9130\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.23610 to 0.23389, saving model to best.model\n",
      "0s - loss: 0.2211 - acc: 0.9213 - val_loss: 0.2339 - val_acc: 0.9130\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.23389 to 0.23196, saving model to best.model\n",
      "0s - loss: 0.2991 - acc: 0.8876 - val_loss: 0.2320 - val_acc: 0.9130\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.23196 to 0.23052, saving model to best.model\n",
      "0s - loss: 0.3097 - acc: 0.8652 - val_loss: 0.2305 - val_acc: 0.9130\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.23052 to 0.22829, saving model to best.model\n",
      "0s - loss: 0.2705 - acc: 0.8989 - val_loss: 0.2283 - val_acc: 0.9130\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.22829 to 0.22570, saving model to best.model\n",
      "0s - loss: 0.2865 - acc: 0.9101 - val_loss: 0.2257 - val_acc: 0.9130\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.22570 to 0.22107, saving model to best.model\n",
      "0s - loss: 0.2691 - acc: 0.8989 - val_loss: 0.2211 - val_acc: 0.9130\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.22107 to 0.21590, saving model to best.model\n",
      "0s - loss: 0.2195 - acc: 0.9438 - val_loss: 0.2159 - val_acc: 0.9130\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.21590 to 0.21039, saving model to best.model\n",
      "0s - loss: 0.1837 - acc: 0.9438 - val_loss: 0.2104 - val_acc: 0.9130\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.21039 to 0.20495, saving model to best.model\n",
      "0s - loss: 0.2934 - acc: 0.8539 - val_loss: 0.2049 - val_acc: 0.9130\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.20495 to 0.19934, saving model to best.model\n",
      "0s - loss: 0.1729 - acc: 0.9438 - val_loss: 0.1993 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.19934 to 0.19391, saving model to best.model\n",
      "0s - loss: 0.2204 - acc: 0.9438 - val_loss: 0.1939 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.19391 to 0.18897, saving model to best.model\n",
      "0s - loss: 0.2438 - acc: 0.8876 - val_loss: 0.1890 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.18897 to 0.18492, saving model to best.model\n",
      "0s - loss: 0.2062 - acc: 0.9438 - val_loss: 0.1849 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.18492 to 0.18168, saving model to best.model\n",
      "0s - loss: 0.1962 - acc: 0.9326 - val_loss: 0.1817 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.18168 to 0.17884, saving model to best.model\n",
      "0s - loss: 0.1958 - acc: 0.9213 - val_loss: 0.1788 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.17884 to 0.17553, saving model to best.model\n",
      "0s - loss: 0.1662 - acc: 0.9326 - val_loss: 0.1755 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.17553 to 0.17300, saving model to best.model\n",
      "0s - loss: 0.1819 - acc: 0.9551 - val_loss: 0.1730 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.17300 to 0.17002, saving model to best.model\n",
      "0s - loss: 0.2077 - acc: 0.9213 - val_loss: 0.1700 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.17002 to 0.16745, saving model to best.model\n",
      "0s - loss: 0.2060 - acc: 0.9213 - val_loss: 0.1674 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.16745 to 0.16469, saving model to best.model\n",
      "0s - loss: 0.2052 - acc: 0.9101 - val_loss: 0.1647 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.16469 to 0.16243, saving model to best.model\n",
      "0s - loss: 0.2507 - acc: 0.8876 - val_loss: 0.1624 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.16243 to 0.16001, saving model to best.model\n",
      "0s - loss: 0.1967 - acc: 0.9326 - val_loss: 0.1600 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.16001 to 0.15740, saving model to best.model\n",
      "0s - loss: 0.1816 - acc: 0.9101 - val_loss: 0.1574 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.15740 to 0.15538, saving model to best.model\n",
      "0s - loss: 0.1927 - acc: 0.9438 - val_loss: 0.1554 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.15538 to 0.15395, saving model to best.model\n",
      "0s - loss: 0.1839 - acc: 0.9551 - val_loss: 0.1540 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.15395 to 0.15196, saving model to best.model\n",
      "0s - loss: 0.2679 - acc: 0.8989 - val_loss: 0.1520 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.15196 to 0.15055, saving model to best.model\n",
      "0s - loss: 0.1782 - acc: 0.9551 - val_loss: 0.1506 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.15055 to 0.14946, saving model to best.model\n",
      "0s - loss: 0.2248 - acc: 0.9101 - val_loss: 0.1495 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.14946 to 0.14821, saving model to best.model\n",
      "0s - loss: 0.2119 - acc: 0.9438 - val_loss: 0.1482 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.14821 to 0.14711, saving model to best.model\n",
      "0s - loss: 0.1753 - acc: 0.9551 - val_loss: 0.1471 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.14711 to 0.14572, saving model to best.model\n",
      "0s - loss: 0.2561 - acc: 0.9213 - val_loss: 0.1457 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.14572 to 0.14482, saving model to best.model\n",
      "0s - loss: 0.2317 - acc: 0.9101 - val_loss: 0.1448 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.14482 to 0.14461, saving model to best.model\n",
      "0s - loss: 0.1910 - acc: 0.8989 - val_loss: 0.1446 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.1931 - acc: 0.9438 - val_loss: 0.1449 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.1708 - acc: 0.9438 - val_loss: 0.1453 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.1740 - acc: 0.9101 - val_loss: 0.1448 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.00421, saving model to best.model\n",
      "0s - loss: 1.3594 - acc: 0.3596 - val_loss: 1.0042 - val_acc: 0.5217\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.1993 - acc: 0.4157 - val_loss: 1.0134 - val_acc: 0.5217\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.3161 - acc: 0.3371 - val_loss: 1.0409 - val_acc: 0.5217\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1692 - acc: 0.3596 - val_loss: 1.0746 - val_acc: 0.3478\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2258 - acc: 0.4157 - val_loss: 1.1063 - val_acc: 0.3478\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2994 - acc: 0.3596 - val_loss: 1.1269 - val_acc: 0.3478\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1352 - acc: 0.4270 - val_loss: 1.1317 - val_acc: 0.3478\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.3615 - acc: 0.2697 - val_loss: 1.1231 - val_acc: 0.3478\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.3091 - acc: 0.3146 - val_loss: 1.1063 - val_acc: 0.3478\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1659 - acc: 0.3596 - val_loss: 1.0876 - val_acc: 0.3478\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2381 - acc: 0.3708 - val_loss: 1.0695 - val_acc: 0.8696\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2415 - acc: 0.3483 - val_loss: 1.0481 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1573 - acc: 0.4494 - val_loss: 1.0270 - val_acc: 0.5217\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.1900 - acc: 0.3820 - val_loss: 1.0093 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.00421 to 0.99540, saving model to best.model\n",
      "0s - loss: 1.2670 - acc: 0.3258 - val_loss: 0.9954 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.99540 to 0.98453, saving model to best.model\n",
      "0s - loss: 1.0884 - acc: 0.4382 - val_loss: 0.9845 - val_acc: 0.5217\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.98453 to 0.97672, saving model to best.model\n",
      "0s - loss: 1.1991 - acc: 0.4157 - val_loss: 0.9767 - val_acc: 0.5217\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.97672 to 0.97030, saving model to best.model\n",
      "0s - loss: 1.2264 - acc: 0.3034 - val_loss: 0.9703 - val_acc: 0.5217\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.97030 to 0.96552, saving model to best.model\n",
      "0s - loss: 1.1475 - acc: 0.4157 - val_loss: 0.9655 - val_acc: 0.5217\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.96552 to 0.96321, saving model to best.model\n",
      "0s - loss: 1.1240 - acc: 0.3933 - val_loss: 0.9632 - val_acc: 0.5217\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.96321 to 0.96194, saving model to best.model\n",
      "0s - loss: 1.2132 - acc: 0.3596 - val_loss: 0.9619 - val_acc: 0.5217\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.2575 - acc: 0.3371 - val_loss: 0.9624 - val_acc: 0.5217\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2211 - acc: 0.3146 - val_loss: 0.9643 - val_acc: 0.5217\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1915 - acc: 0.3820 - val_loss: 0.9667 - val_acc: 0.5217\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1753 - acc: 0.3146 - val_loss: 0.9691 - val_acc: 0.5217\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.2211 - acc: 0.3483 - val_loss: 0.9720 - val_acc: 0.6087\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1285 - acc: 0.4045 - val_loss: 0.9753 - val_acc: 0.7826\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.2075 - acc: 0.3371 - val_loss: 0.9788 - val_acc: 0.8261\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.2010 - acc: 0.4045 - val_loss: 0.9817 - val_acc: 0.8261\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1223 - acc: 0.3933 - val_loss: 0.9830 - val_acc: 0.8261\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.2189 - acc: 0.3596 - val_loss: 0.9839 - val_acc: 0.8261\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1010 - acc: 0.4270 - val_loss: 0.9828 - val_acc: 0.8261\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.1460 - acc: 0.4157 - val_loss: 0.9810 - val_acc: 0.8261\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1893 - acc: 0.4157 - val_loss: 0.9770 - val_acc: 0.8261\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1498 - acc: 0.3596 - val_loss: 0.9713 - val_acc: 0.8261\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.1630 - acc: 0.4494 - val_loss: 0.9655 - val_acc: 0.8261\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.96194 to 0.95968, saving model to best.model\n",
      "0s - loss: 1.1271 - acc: 0.4494 - val_loss: 0.9597 - val_acc: 0.8261\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.95968 to 0.95408, saving model to best.model\n",
      "0s - loss: 1.1369 - acc: 0.3933 - val_loss: 0.9541 - val_acc: 0.8261\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.95408 to 0.94801, saving model to best.model\n",
      "0s - loss: 1.1383 - acc: 0.4270 - val_loss: 0.9480 - val_acc: 0.8261\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.94801 to 0.94211, saving model to best.model\n",
      "0s - loss: 1.0868 - acc: 0.4045 - val_loss: 0.9421 - val_acc: 0.8261\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.94211 to 0.93610, saving model to best.model\n",
      "0s - loss: 1.1045 - acc: 0.3820 - val_loss: 0.9361 - val_acc: 0.8261\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.93610 to 0.93074, saving model to best.model\n",
      "0s - loss: 1.0343 - acc: 0.4719 - val_loss: 0.9307 - val_acc: 0.8261\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.93074 to 0.92573, saving model to best.model\n",
      "0s - loss: 1.0845 - acc: 0.4494 - val_loss: 0.9257 - val_acc: 0.8261\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.92573 to 0.92077, saving model to best.model\n",
      "0s - loss: 1.0934 - acc: 0.4494 - val_loss: 0.9208 - val_acc: 0.8261\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.92077 to 0.91565, saving model to best.model\n",
      "0s - loss: 1.0671 - acc: 0.4270 - val_loss: 0.9156 - val_acc: 0.8261\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.91565 to 0.90993, saving model to best.model\n",
      "0s - loss: 1.1425 - acc: 0.4270 - val_loss: 0.9099 - val_acc: 0.8261\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.90993 to 0.90347, saving model to best.model\n",
      "0s - loss: 1.1212 - acc: 0.4045 - val_loss: 0.9035 - val_acc: 0.8261\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.90347 to 0.89719, saving model to best.model\n",
      "0s - loss: 1.1135 - acc: 0.3820 - val_loss: 0.8972 - val_acc: 0.8261\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.89719 to 0.89164, saving model to best.model\n",
      "0s - loss: 1.1210 - acc: 0.4045 - val_loss: 0.8916 - val_acc: 0.8261\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.89164 to 0.88553, saving model to best.model\n",
      "0s - loss: 0.9769 - acc: 0.5618 - val_loss: 0.8855 - val_acc: 0.8261\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.88553 to 0.87954, saving model to best.model\n",
      "0s - loss: 1.0878 - acc: 0.4719 - val_loss: 0.8795 - val_acc: 0.8261\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.87954 to 0.87391, saving model to best.model\n",
      "0s - loss: 1.0958 - acc: 0.4719 - val_loss: 0.8739 - val_acc: 0.8696\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.87391 to 0.86850, saving model to best.model\n",
      "0s - loss: 1.0390 - acc: 0.4719 - val_loss: 0.8685 - val_acc: 0.8696\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.86850 to 0.86192, saving model to best.model\n",
      "0s - loss: 1.1015 - acc: 0.4270 - val_loss: 0.8619 - val_acc: 0.8696\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.86192 to 0.85531, saving model to best.model\n",
      "0s - loss: 1.0431 - acc: 0.4382 - val_loss: 0.8553 - val_acc: 0.8696\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.85531 to 0.84851, saving model to best.model\n",
      "0s - loss: 1.0556 - acc: 0.5281 - val_loss: 0.8485 - val_acc: 0.8696\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.84851 to 0.84182, saving model to best.model\n",
      "0s - loss: 0.9606 - acc: 0.5618 - val_loss: 0.8418 - val_acc: 0.8696\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.84182 to 0.83432, saving model to best.model\n",
      "0s - loss: 0.9975 - acc: 0.5618 - val_loss: 0.8343 - val_acc: 0.8696\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.83432 to 0.82656, saving model to best.model\n",
      "0s - loss: 1.0559 - acc: 0.4831 - val_loss: 0.8266 - val_acc: 0.8696\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.82656 to 0.81898, saving model to best.model\n",
      "0s - loss: 1.0138 - acc: 0.5506 - val_loss: 0.8190 - val_acc: 0.8696\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.81898 to 0.81129, saving model to best.model\n",
      "0s - loss: 1.0343 - acc: 0.4944 - val_loss: 0.8113 - val_acc: 0.8696\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.81129 to 0.80293, saving model to best.model\n",
      "0s - loss: 0.9410 - acc: 0.5730 - val_loss: 0.8029 - val_acc: 0.8696\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.80293 to 0.79313, saving model to best.model\n",
      "0s - loss: 1.0335 - acc: 0.5169 - val_loss: 0.7931 - val_acc: 0.8696\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.79313 to 0.78268, saving model to best.model\n",
      "0s - loss: 0.9599 - acc: 0.5169 - val_loss: 0.7827 - val_acc: 0.8696\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.78268 to 0.77144, saving model to best.model\n",
      "0s - loss: 1.0646 - acc: 0.4494 - val_loss: 0.7714 - val_acc: 0.8696\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.77144 to 0.75891, saving model to best.model\n",
      "0s - loss: 0.9506 - acc: 0.5730 - val_loss: 0.7589 - val_acc: 0.8696\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.75891 to 0.74481, saving model to best.model\n",
      "0s - loss: 0.9127 - acc: 0.5618 - val_loss: 0.7448 - val_acc: 0.8696\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.74481 to 0.73048, saving model to best.model\n",
      "0s - loss: 0.9341 - acc: 0.5618 - val_loss: 0.7305 - val_acc: 0.8696\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.73048 to 0.71528, saving model to best.model\n",
      "0s - loss: 0.8429 - acc: 0.6404 - val_loss: 0.7153 - val_acc: 0.8696\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71528 to 0.70030, saving model to best.model\n",
      "0s - loss: 0.8224 - acc: 0.7303 - val_loss: 0.7003 - val_acc: 0.8696\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70030 to 0.68541, saving model to best.model\n",
      "0s - loss: 0.9586 - acc: 0.5169 - val_loss: 0.6854 - val_acc: 0.8696\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.68541 to 0.67122, saving model to best.model\n",
      "0s - loss: 0.8624 - acc: 0.6517 - val_loss: 0.6712 - val_acc: 0.8696\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.67122 to 0.65731, saving model to best.model\n",
      "0s - loss: 0.8986 - acc: 0.6292 - val_loss: 0.6573 - val_acc: 0.8696\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.65731 to 0.64354, saving model to best.model\n",
      "0s - loss: 0.8701 - acc: 0.6180 - val_loss: 0.6435 - val_acc: 0.8696\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.64354 to 0.63048, saving model to best.model\n",
      "0s - loss: 0.8703 - acc: 0.6180 - val_loss: 0.6305 - val_acc: 0.8696\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.63048 to 0.61758, saving model to best.model\n",
      "0s - loss: 0.8836 - acc: 0.6180 - val_loss: 0.6176 - val_acc: 0.8696\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.61758 to 0.60490, saving model to best.model\n",
      "0s - loss: 0.8410 - acc: 0.6517 - val_loss: 0.6049 - val_acc: 0.8696\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.60490 to 0.59267, saving model to best.model\n",
      "0s - loss: 0.8329 - acc: 0.6067 - val_loss: 0.5927 - val_acc: 0.8696\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.59267 to 0.58055, saving model to best.model\n",
      "0s - loss: 0.8008 - acc: 0.6629 - val_loss: 0.5806 - val_acc: 0.8696\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.58055 to 0.56791, saving model to best.model\n",
      "0s - loss: 0.8281 - acc: 0.6517 - val_loss: 0.5679 - val_acc: 0.8696\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.56791 to 0.55554, saving model to best.model\n",
      "0s - loss: 0.8499 - acc: 0.6292 - val_loss: 0.5555 - val_acc: 0.8696\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.55554 to 0.54359, saving model to best.model\n",
      "0s - loss: 0.8180 - acc: 0.6517 - val_loss: 0.5436 - val_acc: 0.8696\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.54359 to 0.53218, saving model to best.model\n",
      "0s - loss: 0.8083 - acc: 0.6629 - val_loss: 0.5322 - val_acc: 0.8696\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.53218 to 0.52138, saving model to best.model\n",
      "0s - loss: 0.7903 - acc: 0.6742 - val_loss: 0.5214 - val_acc: 0.8696\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.52138 to 0.51082, saving model to best.model\n",
      "0s - loss: 0.7673 - acc: 0.6854 - val_loss: 0.5108 - val_acc: 0.8696\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.51082 to 0.50023, saving model to best.model\n",
      "0s - loss: 0.7003 - acc: 0.7528 - val_loss: 0.5002 - val_acc: 0.8696\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.50023 to 0.48924, saving model to best.model\n",
      "0s - loss: 0.7052 - acc: 0.7303 - val_loss: 0.4892 - val_acc: 0.8696\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.48924 to 0.47879, saving model to best.model\n",
      "0s - loss: 0.7027 - acc: 0.7079 - val_loss: 0.4788 - val_acc: 0.8696\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.47879 to 0.46837, saving model to best.model\n",
      "0s - loss: 0.6442 - acc: 0.8202 - val_loss: 0.4684 - val_acc: 0.8696\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.46837 to 0.45794, saving model to best.model\n",
      "0s - loss: 0.6788 - acc: 0.6966 - val_loss: 0.4579 - val_acc: 0.8696\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.45794 to 0.44791, saving model to best.model\n",
      "0s - loss: 0.6527 - acc: 0.7753 - val_loss: 0.4479 - val_acc: 0.8696\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.44791 to 0.43842, saving model to best.model\n",
      "0s - loss: 0.6371 - acc: 0.7303 - val_loss: 0.4384 - val_acc: 0.8696\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.43842 to 0.42897, saving model to best.model\n",
      "0s - loss: 0.6545 - acc: 0.7528 - val_loss: 0.4290 - val_acc: 0.8696\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.42897 to 0.41930, saving model to best.model\n",
      "0s - loss: 0.6255 - acc: 0.7416 - val_loss: 0.4193 - val_acc: 0.8696\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.41930 to 0.40983, saving model to best.model\n",
      "0s - loss: 0.6193 - acc: 0.7303 - val_loss: 0.4098 - val_acc: 0.8696\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.40983 to 0.40099, saving model to best.model\n",
      "0s - loss: 0.6172 - acc: 0.7978 - val_loss: 0.4010 - val_acc: 0.8696\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.40099 to 0.39311, saving model to best.model\n",
      "0s - loss: 0.5436 - acc: 0.7865 - val_loss: 0.3931 - val_acc: 0.8696\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.39311 to 0.38555, saving model to best.model\n",
      "0s - loss: 0.6368 - acc: 0.7416 - val_loss: 0.3855 - val_acc: 0.8696\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.38555 to 0.37785, saving model to best.model\n",
      "0s - loss: 0.5797 - acc: 0.7528 - val_loss: 0.3778 - val_acc: 0.8696\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.37785 to 0.37063, saving model to best.model\n",
      "0s - loss: 0.6259 - acc: 0.7640 - val_loss: 0.3706 - val_acc: 0.8696\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.37063 to 0.36322, saving model to best.model\n",
      "0s - loss: 0.5966 - acc: 0.7303 - val_loss: 0.3632 - val_acc: 0.8696\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.36322 to 0.35651, saving model to best.model\n",
      "0s - loss: 0.6169 - acc: 0.7753 - val_loss: 0.3565 - val_acc: 0.8696\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.35651 to 0.35038, saving model to best.model\n",
      "0s - loss: 0.5503 - acc: 0.7416 - val_loss: 0.3504 - val_acc: 0.8696\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.35038 to 0.34427, saving model to best.model\n",
      "0s - loss: 0.6198 - acc: 0.7978 - val_loss: 0.3443 - val_acc: 0.8696\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.34427 to 0.33828, saving model to best.model\n",
      "0s - loss: 0.5588 - acc: 0.7865 - val_loss: 0.3383 - val_acc: 0.8696\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.33828 to 0.33265, saving model to best.model\n",
      "0s - loss: 0.5662 - acc: 0.7753 - val_loss: 0.3327 - val_acc: 0.8696\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.33265 to 0.32756, saving model to best.model\n",
      "0s - loss: 0.5131 - acc: 0.7865 - val_loss: 0.3276 - val_acc: 0.8696\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.32756 to 0.32327, saving model to best.model\n",
      "0s - loss: 0.5403 - acc: 0.8090 - val_loss: 0.3233 - val_acc: 0.8696\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.32327 to 0.31942, saving model to best.model\n",
      "0s - loss: 0.5720 - acc: 0.7753 - val_loss: 0.3194 - val_acc: 0.9130\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.31942 to 0.31572, saving model to best.model\n",
      "0s - loss: 0.5532 - acc: 0.7416 - val_loss: 0.3157 - val_acc: 0.9565\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.31572 to 0.31157, saving model to best.model\n",
      "0s - loss: 0.4725 - acc: 0.7865 - val_loss: 0.3116 - val_acc: 0.9565\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.31157 to 0.30691, saving model to best.model\n",
      "0s - loss: 0.5269 - acc: 0.7865 - val_loss: 0.3069 - val_acc: 0.9565\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.30691 to 0.30221, saving model to best.model\n",
      "0s - loss: 0.4993 - acc: 0.8539 - val_loss: 0.3022 - val_acc: 0.9565\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.30221 to 0.29757, saving model to best.model\n",
      "0s - loss: 0.4582 - acc: 0.8539 - val_loss: 0.2976 - val_acc: 0.9565\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.29757 to 0.29354, saving model to best.model\n",
      "0s - loss: 0.5370 - acc: 0.7640 - val_loss: 0.2935 - val_acc: 0.9565\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.29354 to 0.28916, saving model to best.model\n",
      "0s - loss: 0.5246 - acc: 0.7640 - val_loss: 0.2892 - val_acc: 0.9565\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.28916 to 0.28558, saving model to best.model\n",
      "0s - loss: 0.4642 - acc: 0.8202 - val_loss: 0.2856 - val_acc: 0.9565\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.28558 to 0.28237, saving model to best.model\n",
      "0s - loss: 0.5272 - acc: 0.8090 - val_loss: 0.2824 - val_acc: 0.9565\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.28237 to 0.27905, saving model to best.model\n",
      "0s - loss: 0.4770 - acc: 0.7640 - val_loss: 0.2790 - val_acc: 0.9565\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.27905 to 0.27629, saving model to best.model\n",
      "0s - loss: 0.5024 - acc: 0.7865 - val_loss: 0.2763 - val_acc: 0.9565\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.27629 to 0.27292, saving model to best.model\n",
      "0s - loss: 0.4049 - acc: 0.8539 - val_loss: 0.2729 - val_acc: 0.9565\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.27292 to 0.26938, saving model to best.model\n",
      "0s - loss: 0.4465 - acc: 0.8315 - val_loss: 0.2694 - val_acc: 0.9565\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.26938 to 0.26505, saving model to best.model\n",
      "0s - loss: 0.5278 - acc: 0.7640 - val_loss: 0.2651 - val_acc: 0.9565\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.26505 to 0.26127, saving model to best.model\n",
      "0s - loss: 0.4367 - acc: 0.8427 - val_loss: 0.2613 - val_acc: 0.9565\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.26127 to 0.25788, saving model to best.model\n",
      "0s - loss: 0.4292 - acc: 0.8090 - val_loss: 0.2579 - val_acc: 0.9565\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.25788 to 0.25464, saving model to best.model\n",
      "0s - loss: 0.4301 - acc: 0.8427 - val_loss: 0.2546 - val_acc: 0.9565\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.25464 to 0.25153, saving model to best.model\n",
      "0s - loss: 0.4147 - acc: 0.8652 - val_loss: 0.2515 - val_acc: 0.9565\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.25153 to 0.24887, saving model to best.model\n",
      "0s - loss: 0.4492 - acc: 0.8315 - val_loss: 0.2489 - val_acc: 0.9565\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.24887 to 0.24616, saving model to best.model\n",
      "0s - loss: 0.4397 - acc: 0.8539 - val_loss: 0.2462 - val_acc: 0.9565\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.24616 to 0.24366, saving model to best.model\n",
      "0s - loss: 0.5115 - acc: 0.7865 - val_loss: 0.2437 - val_acc: 0.9565\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.24366 to 0.24114, saving model to best.model\n",
      "0s - loss: 0.4882 - acc: 0.8202 - val_loss: 0.2411 - val_acc: 0.9565\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.24114 to 0.23848, saving model to best.model\n",
      "0s - loss: 0.3969 - acc: 0.8539 - val_loss: 0.2385 - val_acc: 0.9565\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.23848 to 0.23589, saving model to best.model\n",
      "0s - loss: 0.4048 - acc: 0.8427 - val_loss: 0.2359 - val_acc: 0.9565\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.23589 to 0.23304, saving model to best.model\n",
      "0s - loss: 0.4498 - acc: 0.7753 - val_loss: 0.2330 - val_acc: 0.9565\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.23304 to 0.23029, saving model to best.model\n",
      "0s - loss: 0.3483 - acc: 0.8876 - val_loss: 0.2303 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.23029 to 0.22766, saving model to best.model\n",
      "0s - loss: 0.4216 - acc: 0.8202 - val_loss: 0.2277 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.22766 to 0.22488, saving model to best.model\n",
      "0s - loss: 0.4358 - acc: 0.8202 - val_loss: 0.2249 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.22488 to 0.22245, saving model to best.model\n",
      "0s - loss: 0.3831 - acc: 0.8539 - val_loss: 0.2225 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.22245 to 0.21996, saving model to best.model\n",
      "0s - loss: 0.3833 - acc: 0.8315 - val_loss: 0.2200 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.21996 to 0.21757, saving model to best.model\n",
      "0s - loss: 0.4516 - acc: 0.7978 - val_loss: 0.2176 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.21757 to 0.21508, saving model to best.model\n",
      "0s - loss: 0.3787 - acc: 0.8652 - val_loss: 0.2151 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.21508 to 0.21206, saving model to best.model\n",
      "0s - loss: 0.3441 - acc: 0.8989 - val_loss: 0.2121 - val_acc: 0.9565\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.21206 to 0.20895, saving model to best.model\n",
      "0s - loss: 0.3859 - acc: 0.8539 - val_loss: 0.2090 - val_acc: 0.9565\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.20895 to 0.20628, saving model to best.model\n",
      "0s - loss: 0.3717 - acc: 0.8652 - val_loss: 0.2063 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.20628 to 0.20327, saving model to best.model\n",
      "0s - loss: 0.4007 - acc: 0.8202 - val_loss: 0.2033 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.20327 to 0.20073, saving model to best.model\n",
      "0s - loss: 0.4061 - acc: 0.8427 - val_loss: 0.2007 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.20073 to 0.19790, saving model to best.model\n",
      "0s - loss: 0.3980 - acc: 0.8202 - val_loss: 0.1979 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.19790 to 0.19495, saving model to best.model\n",
      "0s - loss: 0.3481 - acc: 0.8652 - val_loss: 0.1950 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.19495 to 0.19131, saving model to best.model\n",
      "0s - loss: 0.3648 - acc: 0.8652 - val_loss: 0.1913 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.19131 to 0.18803, saving model to best.model\n",
      "0s - loss: 0.3765 - acc: 0.8427 - val_loss: 0.1880 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.18803 to 0.18445, saving model to best.model\n",
      "0s - loss: 0.3438 - acc: 0.8539 - val_loss: 0.1844 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.18445 to 0.18071, saving model to best.model\n",
      "0s - loss: 0.3217 - acc: 0.8764 - val_loss: 0.1807 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.18071 to 0.17695, saving model to best.model\n",
      "0s - loss: 0.3380 - acc: 0.8764 - val_loss: 0.1769 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.17695 to 0.17323, saving model to best.model\n",
      "0s - loss: 0.3143 - acc: 0.9101 - val_loss: 0.1732 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.17323 to 0.16934, saving model to best.model\n",
      "0s - loss: 0.3333 - acc: 0.8989 - val_loss: 0.1693 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.16934 to 0.16600, saving model to best.model\n",
      "0s - loss: 0.3428 - acc: 0.8652 - val_loss: 0.1660 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.16600 to 0.16282, saving model to best.model\n",
      "0s - loss: 0.3480 - acc: 0.8876 - val_loss: 0.1628 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.16282 to 0.15979, saving model to best.model\n",
      "0s - loss: 0.3095 - acc: 0.8989 - val_loss: 0.1598 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.15979 to 0.15705, saving model to best.model\n",
      "0s - loss: 0.3210 - acc: 0.9213 - val_loss: 0.1570 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.15705 to 0.15409, saving model to best.model\n",
      "0s - loss: 0.3671 - acc: 0.8427 - val_loss: 0.1541 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.15409 to 0.15136, saving model to best.model\n",
      "0s - loss: 0.3266 - acc: 0.8876 - val_loss: 0.1514 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.15136 to 0.14897, saving model to best.model\n",
      "0s - loss: 0.3354 - acc: 0.8764 - val_loss: 0.1490 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.14897 to 0.14675, saving model to best.model\n",
      "0s - loss: 0.3133 - acc: 0.9101 - val_loss: 0.1468 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.14675 to 0.14449, saving model to best.model\n",
      "0s - loss: 0.3110 - acc: 0.8989 - val_loss: 0.1445 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.14449 to 0.14227, saving model to best.model\n",
      "0s - loss: 0.3612 - acc: 0.8764 - val_loss: 0.1423 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.14227 to 0.14032, saving model to best.model\n",
      "0s - loss: 0.3291 - acc: 0.8539 - val_loss: 0.1403 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.14032 to 0.13845, saving model to best.model\n",
      "0s - loss: 0.2993 - acc: 0.8989 - val_loss: 0.1384 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.13845 to 0.13676, saving model to best.model\n",
      "0s - loss: 0.3041 - acc: 0.8764 - val_loss: 0.1368 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.13676 to 0.13542, saving model to best.model\n",
      "0s - loss: 0.3228 - acc: 0.8764 - val_loss: 0.1354 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.13542 to 0.13429, saving model to best.model\n",
      "0s - loss: 0.2643 - acc: 0.9213 - val_loss: 0.1343 - val_acc: 0.9565\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.13429 to 0.13278, saving model to best.model\n",
      "0s - loss: 0.3015 - acc: 0.8764 - val_loss: 0.1328 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.13278 to 0.13141, saving model to best.model\n",
      "0s - loss: 0.2334 - acc: 0.9326 - val_loss: 0.1314 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.13141 to 0.13026, saving model to best.model\n",
      "0s - loss: 0.2909 - acc: 0.8764 - val_loss: 0.1303 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.13026 to 0.12877, saving model to best.model\n",
      "0s - loss: 0.2697 - acc: 0.9213 - val_loss: 0.1288 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.12877 to 0.12663, saving model to best.model\n",
      "0s - loss: 0.3284 - acc: 0.8876 - val_loss: 0.1266 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.12663 to 0.12458, saving model to best.model\n",
      "0s - loss: 0.2466 - acc: 0.9101 - val_loss: 0.1246 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.12458 to 0.12201, saving model to best.model\n",
      "0s - loss: 0.3011 - acc: 0.9326 - val_loss: 0.1220 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.12201 to 0.11867, saving model to best.model\n",
      "0s - loss: 0.3221 - acc: 0.8876 - val_loss: 0.1187 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.11867 to 0.11546, saving model to best.model\n",
      "0s - loss: 0.2398 - acc: 0.9213 - val_loss: 0.1155 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.11546 to 0.11258, saving model to best.model\n",
      "0s - loss: 0.2807 - acc: 0.8989 - val_loss: 0.1126 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.11258 to 0.10938, saving model to best.model\n",
      "0s - loss: 0.2678 - acc: 0.9326 - val_loss: 0.1094 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.10938 to 0.10628, saving model to best.model\n",
      "0s - loss: 0.2121 - acc: 0.9326 - val_loss: 0.1063 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.10628 to 0.10386, saving model to best.model\n",
      "0s - loss: 0.3015 - acc: 0.8989 - val_loss: 0.1039 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.10386 to 0.10185, saving model to best.model\n",
      "0s - loss: 0.2266 - acc: 0.8989 - val_loss: 0.1019 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.10185 to 0.10019, saving model to best.model\n",
      "0s - loss: 0.2620 - acc: 0.9213 - val_loss: 0.1002 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.10019 to 0.09860, saving model to best.model\n",
      "0s - loss: 0.2321 - acc: 0.9438 - val_loss: 0.0986 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.09860 to 0.09745, saving model to best.model\n",
      "0s - loss: 0.2301 - acc: 0.8989 - val_loss: 0.0975 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.09745 to 0.09623, saving model to best.model\n",
      "0s - loss: 0.2349 - acc: 0.9101 - val_loss: 0.0962 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.09623 to 0.09457, saving model to best.model\n",
      "0s - loss: 0.2450 - acc: 0.8989 - val_loss: 0.0946 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.09457 to 0.09294, saving model to best.model\n",
      "0s - loss: 0.2229 - acc: 0.9101 - val_loss: 0.0929 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.09294 to 0.09101, saving model to best.model\n",
      "0s - loss: 0.2178 - acc: 0.9551 - val_loss: 0.0910 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.09101 to 0.08921, saving model to best.model\n",
      "0s - loss: 0.2370 - acc: 0.9213 - val_loss: 0.0892 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.08921 to 0.08763, saving model to best.model\n",
      "0s - loss: 0.2474 - acc: 0.9213 - val_loss: 0.0876 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.08763 to 0.08601, saving model to best.model\n",
      "0s - loss: 0.1816 - acc: 0.9551 - val_loss: 0.0860 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.08601 to 0.08443, saving model to best.model\n",
      "0s - loss: 0.2142 - acc: 0.9438 - val_loss: 0.0844 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.08443 to 0.08294, saving model to best.model\n",
      "0s - loss: 0.2186 - acc: 0.9326 - val_loss: 0.0829 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.08294 to 0.08125, saving model to best.model\n",
      "0s - loss: 0.2513 - acc: 0.8989 - val_loss: 0.0813 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.08125 to 0.07951, saving model to best.model\n",
      "0s - loss: 0.2327 - acc: 0.9438 - val_loss: 0.0795 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.07951 to 0.07760, saving model to best.model\n",
      "0s - loss: 0.2194 - acc: 0.9213 - val_loss: 0.0776 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.07760 to 0.07575, saving model to best.model\n",
      "0s - loss: 0.2448 - acc: 0.8989 - val_loss: 0.0758 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.12351, saving model to best.model\n",
      "0s - loss: 1.3938 - acc: 0.3371 - val_loss: 1.1235 - val_acc: 0.2609\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.12351 to 1.08392, saving model to best.model\n",
      "0s - loss: 1.3776 - acc: 0.2809 - val_loss: 1.0839 - val_acc: 0.4783\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.08392 to 1.06664, saving model to best.model\n",
      "0s - loss: 1.2142 - acc: 0.4045 - val_loss: 1.0666 - val_acc: 0.4783\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.06664 to 1.05937, saving model to best.model\n",
      "0s - loss: 1.2963 - acc: 0.4157 - val_loss: 1.0594 - val_acc: 0.4783\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.05937 to 1.05762, saving model to best.model\n",
      "0s - loss: 1.2548 - acc: 0.2584 - val_loss: 1.0576 - val_acc: 0.4783\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.05762 to 1.05738, saving model to best.model\n",
      "0s - loss: 1.3069 - acc: 0.3258 - val_loss: 1.0574 - val_acc: 0.4783\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.3242 - acc: 0.3483 - val_loss: 1.0582 - val_acc: 0.4783\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2720 - acc: 0.4494 - val_loss: 1.0610 - val_acc: 0.4783\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2364 - acc: 0.3596 - val_loss: 1.0639 - val_acc: 0.4783\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2373 - acc: 0.3483 - val_loss: 1.0677 - val_acc: 0.4783\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1939 - acc: 0.3933 - val_loss: 1.0688 - val_acc: 0.6087\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1935 - acc: 0.3933 - val_loss: 1.0677 - val_acc: 0.7391\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.3035 - acc: 0.3034 - val_loss: 1.0682 - val_acc: 0.6522\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2018 - acc: 0.3820 - val_loss: 1.0678 - val_acc: 0.6087\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1660 - acc: 0.4045 - val_loss: 1.0656 - val_acc: 0.6087\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2639 - acc: 0.4157 - val_loss: 1.0624 - val_acc: 0.6957\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1868 - acc: 0.3146 - val_loss: 1.0574 - val_acc: 0.7391\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.05738 to 1.05072, saving model to best.model\n",
      "0s - loss: 1.2542 - acc: 0.3258 - val_loss: 1.0507 - val_acc: 0.6087\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.05072 to 1.04600, saving model to best.model\n",
      "0s - loss: 1.1184 - acc: 0.4494 - val_loss: 1.0460 - val_acc: 0.4783\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.04600 to 1.04207, saving model to best.model\n",
      "0s - loss: 1.2265 - acc: 0.3146 - val_loss: 1.0421 - val_acc: 0.4783\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.04207 to 1.03922, saving model to best.model\n",
      "0s - loss: 1.1562 - acc: 0.3708 - val_loss: 1.0392 - val_acc: 0.4783\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.03922 to 1.03660, saving model to best.model\n",
      "0s - loss: 1.1681 - acc: 0.4382 - val_loss: 1.0366 - val_acc: 0.4783\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.03660 to 1.03438, saving model to best.model\n",
      "0s - loss: 1.2716 - acc: 0.3258 - val_loss: 1.0344 - val_acc: 0.4783\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.03438 to 1.03319, saving model to best.model\n",
      "0s - loss: 1.1557 - acc: 0.4157 - val_loss: 1.0332 - val_acc: 0.4783\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.03319 to 1.03285, saving model to best.model\n",
      "0s - loss: 1.1684 - acc: 0.3933 - val_loss: 1.0328 - val_acc: 0.4783\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.03285 to 1.03249, saving model to best.model\n",
      "0s - loss: 1.1305 - acc: 0.3371 - val_loss: 1.0325 - val_acc: 0.6522\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.03249 to 1.03182, saving model to best.model\n",
      "0s - loss: 1.1434 - acc: 0.4045 - val_loss: 1.0318 - val_acc: 0.6522\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1338 - acc: 0.4382 - val_loss: 1.0319 - val_acc: 0.7391\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.03182 to 1.03146, saving model to best.model\n",
      "0s - loss: 1.1575 - acc: 0.3483 - val_loss: 1.0315 - val_acc: 0.7391\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.03146 to 1.03126, saving model to best.model\n",
      "0s - loss: 1.2208 - acc: 0.3596 - val_loss: 1.0313 - val_acc: 0.7391\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.03126 to 1.03110, saving model to best.model\n",
      "0s - loss: 1.1795 - acc: 0.3258 - val_loss: 1.0311 - val_acc: 0.6522\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.03110 to 1.02973, saving model to best.model\n",
      "0s - loss: 1.1682 - acc: 0.3371 - val_loss: 1.0297 - val_acc: 0.6522\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.02973 to 1.02741, saving model to best.model\n",
      "0s - loss: 1.1605 - acc: 0.3371 - val_loss: 1.0274 - val_acc: 0.6522\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.02741 to 1.02489, saving model to best.model\n",
      "0s - loss: 1.1885 - acc: 0.3371 - val_loss: 1.0249 - val_acc: 0.6522\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.02489 to 1.02063, saving model to best.model\n",
      "0s - loss: 1.1466 - acc: 0.3596 - val_loss: 1.0206 - val_acc: 0.6522\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.02063 to 1.01613, saving model to best.model\n",
      "0s - loss: 1.0902 - acc: 0.4944 - val_loss: 1.0161 - val_acc: 0.7391\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.01613 to 1.01201, saving model to best.model\n",
      "0s - loss: 1.0730 - acc: 0.4382 - val_loss: 1.0120 - val_acc: 0.7391\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.01201 to 1.00732, saving model to best.model\n",
      "0s - loss: 1.2264 - acc: 0.4382 - val_loss: 1.0073 - val_acc: 0.7391\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.00732 to 1.00270, saving model to best.model\n",
      "0s - loss: 1.1615 - acc: 0.3596 - val_loss: 1.0027 - val_acc: 0.7391\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.00270 to 0.99743, saving model to best.model\n",
      "0s - loss: 1.0695 - acc: 0.4494 - val_loss: 0.9974 - val_acc: 0.7391\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.99743 to 0.99208, saving model to best.model\n",
      "0s - loss: 1.0714 - acc: 0.4382 - val_loss: 0.9921 - val_acc: 0.7391\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.99208 to 0.98722, saving model to best.model\n",
      "0s - loss: 1.0818 - acc: 0.3483 - val_loss: 0.9872 - val_acc: 0.6522\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.98722 to 0.98322, saving model to best.model\n",
      "0s - loss: 1.0991 - acc: 0.3933 - val_loss: 0.9832 - val_acc: 0.6522\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.98322 to 0.97981, saving model to best.model\n",
      "0s - loss: 1.1670 - acc: 0.3708 - val_loss: 0.9798 - val_acc: 0.7391\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.97981 to 0.97629, saving model to best.model\n",
      "0s - loss: 1.1047 - acc: 0.3034 - val_loss: 0.9763 - val_acc: 0.7391\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.97629 to 0.97230, saving model to best.model\n",
      "0s - loss: 1.0668 - acc: 0.4831 - val_loss: 0.9723 - val_acc: 0.7391\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.97230 to 0.96806, saving model to best.model\n",
      "0s - loss: 1.0702 - acc: 0.4494 - val_loss: 0.9681 - val_acc: 0.7391\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.96806 to 0.96386, saving model to best.model\n",
      "0s - loss: 1.0309 - acc: 0.5056 - val_loss: 0.9639 - val_acc: 0.7391\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.96386 to 0.96036, saving model to best.model\n",
      "0s - loss: 0.9532 - acc: 0.5393 - val_loss: 0.9604 - val_acc: 0.7391\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.96036 to 0.95627, saving model to best.model\n",
      "0s - loss: 1.0673 - acc: 0.4382 - val_loss: 0.9563 - val_acc: 0.7391\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.95627 to 0.95228, saving model to best.model\n",
      "0s - loss: 1.0755 - acc: 0.4045 - val_loss: 0.9523 - val_acc: 0.7391\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.95228 to 0.94804, saving model to best.model\n",
      "0s - loss: 1.0229 - acc: 0.5056 - val_loss: 0.9480 - val_acc: 0.7391\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.94804 to 0.94379, saving model to best.model\n",
      "0s - loss: 1.0613 - acc: 0.4270 - val_loss: 0.9438 - val_acc: 0.7391\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.94379 to 0.93927, saving model to best.model\n",
      "0s - loss: 1.0489 - acc: 0.4045 - val_loss: 0.9393 - val_acc: 0.7391\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.93927 to 0.93460, saving model to best.model\n",
      "0s - loss: 1.0862 - acc: 0.4831 - val_loss: 0.9346 - val_acc: 0.7391\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.93460 to 0.92944, saving model to best.model\n",
      "0s - loss: 1.0755 - acc: 0.3933 - val_loss: 0.9294 - val_acc: 0.7391\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.92944 to 0.92381, saving model to best.model\n",
      "0s - loss: 0.9998 - acc: 0.4831 - val_loss: 0.9238 - val_acc: 0.7391\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.92381 to 0.91724, saving model to best.model\n",
      "0s - loss: 1.0125 - acc: 0.4944 - val_loss: 0.9172 - val_acc: 0.7391\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.91724 to 0.91081, saving model to best.model\n",
      "0s - loss: 0.9787 - acc: 0.5281 - val_loss: 0.9108 - val_acc: 0.7391\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.91081 to 0.90417, saving model to best.model\n",
      "0s - loss: 1.0029 - acc: 0.5393 - val_loss: 0.9042 - val_acc: 0.7391\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.90417 to 0.89681, saving model to best.model\n",
      "0s - loss: 1.0080 - acc: 0.4494 - val_loss: 0.8968 - val_acc: 0.7391\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.89681 to 0.88945, saving model to best.model\n",
      "0s - loss: 0.9626 - acc: 0.5506 - val_loss: 0.8895 - val_acc: 0.7391\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.88945 to 0.88196, saving model to best.model\n",
      "0s - loss: 0.9475 - acc: 0.5843 - val_loss: 0.8820 - val_acc: 0.7391\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.88196 to 0.87432, saving model to best.model\n",
      "0s - loss: 0.9899 - acc: 0.5056 - val_loss: 0.8743 - val_acc: 0.7391\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.87432 to 0.86723, saving model to best.model\n",
      "0s - loss: 1.0334 - acc: 0.4719 - val_loss: 0.8672 - val_acc: 0.7391\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.86723 to 0.85923, saving model to best.model\n",
      "0s - loss: 0.9631 - acc: 0.5056 - val_loss: 0.8592 - val_acc: 0.7391\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.85923 to 0.85106, saving model to best.model\n",
      "0s - loss: 0.9724 - acc: 0.5618 - val_loss: 0.8511 - val_acc: 0.7391\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.85106 to 0.84285, saving model to best.model\n",
      "0s - loss: 1.0326 - acc: 0.4944 - val_loss: 0.8429 - val_acc: 0.7391\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.84285 to 0.83461, saving model to best.model\n",
      "0s - loss: 0.9341 - acc: 0.4831 - val_loss: 0.8346 - val_acc: 0.7391\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.83461 to 0.82660, saving model to best.model\n",
      "0s - loss: 0.8811 - acc: 0.6067 - val_loss: 0.8266 - val_acc: 0.7391\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.82660 to 0.81866, saving model to best.model\n",
      "0s - loss: 0.9071 - acc: 0.5843 - val_loss: 0.8187 - val_acc: 0.7391\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.81866 to 0.81036, saving model to best.model\n",
      "0s - loss: 0.9246 - acc: 0.5506 - val_loss: 0.8104 - val_acc: 0.7391\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.81036 to 0.80224, saving model to best.model\n",
      "0s - loss: 0.9665 - acc: 0.5618 - val_loss: 0.8022 - val_acc: 0.7391\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.80224 to 0.79379, saving model to best.model\n",
      "0s - loss: 0.8538 - acc: 0.6404 - val_loss: 0.7938 - val_acc: 0.6522\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.79379 to 0.78544, saving model to best.model\n",
      "0s - loss: 0.8605 - acc: 0.6067 - val_loss: 0.7854 - val_acc: 0.6522\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.78544 to 0.77696, saving model to best.model\n",
      "0s - loss: 0.9039 - acc: 0.6180 - val_loss: 0.7770 - val_acc: 0.6522\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.77696 to 0.76750, saving model to best.model\n",
      "0s - loss: 0.8292 - acc: 0.6180 - val_loss: 0.7675 - val_acc: 0.6522\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.76750 to 0.75782, saving model to best.model\n",
      "0s - loss: 0.8587 - acc: 0.6404 - val_loss: 0.7578 - val_acc: 0.6522\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.75782 to 0.74762, saving model to best.model\n",
      "0s - loss: 0.7988 - acc: 0.6404 - val_loss: 0.7476 - val_acc: 0.6522\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.74762 to 0.73677, saving model to best.model\n",
      "0s - loss: 0.8069 - acc: 0.6180 - val_loss: 0.7368 - val_acc: 0.6522\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.73677 to 0.72603, saving model to best.model\n",
      "0s - loss: 0.7511 - acc: 0.6742 - val_loss: 0.7260 - val_acc: 0.6522\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.72603 to 0.71521, saving model to best.model\n",
      "0s - loss: 0.8131 - acc: 0.6180 - val_loss: 0.7152 - val_acc: 0.6522\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.71521 to 0.70480, saving model to best.model\n",
      "0s - loss: 0.7633 - acc: 0.6292 - val_loss: 0.7048 - val_acc: 0.6522\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.70480 to 0.69412, saving model to best.model\n",
      "0s - loss: 0.8563 - acc: 0.5955 - val_loss: 0.6941 - val_acc: 0.6522\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69412 to 0.68328, saving model to best.model\n",
      "0s - loss: 0.7896 - acc: 0.6966 - val_loss: 0.6833 - val_acc: 0.6522\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68328 to 0.67275, saving model to best.model\n",
      "0s - loss: 0.8003 - acc: 0.6517 - val_loss: 0.6727 - val_acc: 0.6957\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67275 to 0.66244, saving model to best.model\n",
      "0s - loss: 0.7636 - acc: 0.6517 - val_loss: 0.6624 - val_acc: 0.7391\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.66244 to 0.65298, saving model to best.model\n",
      "0s - loss: 0.7273 - acc: 0.6966 - val_loss: 0.6530 - val_acc: 0.6957\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.65298 to 0.64380, saving model to best.model\n",
      "0s - loss: 0.6905 - acc: 0.6742 - val_loss: 0.6438 - val_acc: 0.6957\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.64380 to 0.63521, saving model to best.model\n",
      "0s - loss: 0.7354 - acc: 0.6180 - val_loss: 0.6352 - val_acc: 0.6957\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.63521 to 0.62703, saving model to best.model\n",
      "0s - loss: 0.6918 - acc: 0.6854 - val_loss: 0.6270 - val_acc: 0.6522\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.62703 to 0.61909, saving model to best.model\n",
      "0s - loss: 0.7059 - acc: 0.7191 - val_loss: 0.6191 - val_acc: 0.6522\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.61909 to 0.61249, saving model to best.model\n",
      "0s - loss: 0.7601 - acc: 0.6517 - val_loss: 0.6125 - val_acc: 0.6522\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.61249 to 0.60628, saving model to best.model\n",
      "0s - loss: 0.6405 - acc: 0.7528 - val_loss: 0.6063 - val_acc: 0.6957\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.60628 to 0.60009, saving model to best.model\n",
      "0s - loss: 0.6644 - acc: 0.6966 - val_loss: 0.6001 - val_acc: 0.6957\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.60009 to 0.59407, saving model to best.model\n",
      "0s - loss: 0.6593 - acc: 0.6966 - val_loss: 0.5941 - val_acc: 0.6957\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.59407 to 0.58828, saving model to best.model\n",
      "0s - loss: 0.6681 - acc: 0.7191 - val_loss: 0.5883 - val_acc: 0.6957\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.58828 to 0.58204, saving model to best.model\n",
      "0s - loss: 0.6589 - acc: 0.6854 - val_loss: 0.5820 - val_acc: 0.6957\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.58204 to 0.57583, saving model to best.model\n",
      "0s - loss: 0.6316 - acc: 0.7865 - val_loss: 0.5758 - val_acc: 0.6957\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.57583 to 0.56795, saving model to best.model\n",
      "0s - loss: 0.6150 - acc: 0.6966 - val_loss: 0.5679 - val_acc: 0.6957\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.56795 to 0.56021, saving model to best.model\n",
      "0s - loss: 0.6185 - acc: 0.7528 - val_loss: 0.5602 - val_acc: 0.6957\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.56021 to 0.55203, saving model to best.model\n",
      "0s - loss: 0.5867 - acc: 0.7640 - val_loss: 0.5520 - val_acc: 0.6957\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.55203 to 0.54307, saving model to best.model\n",
      "0s - loss: 0.5710 - acc: 0.7753 - val_loss: 0.5431 - val_acc: 0.6957\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.54307 to 0.53452, saving model to best.model\n",
      "0s - loss: 0.5912 - acc: 0.7528 - val_loss: 0.5345 - val_acc: 0.6957\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.53452 to 0.52651, saving model to best.model\n",
      "0s - loss: 0.5658 - acc: 0.7640 - val_loss: 0.5265 - val_acc: 0.6957\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.52651 to 0.51881, saving model to best.model\n",
      "0s - loss: 0.6478 - acc: 0.6404 - val_loss: 0.5188 - val_acc: 0.7391\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.51881 to 0.51178, saving model to best.model\n",
      "0s - loss: 0.5974 - acc: 0.6854 - val_loss: 0.5118 - val_acc: 0.7391\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.51178 to 0.50555, saving model to best.model\n",
      "0s - loss: 0.5470 - acc: 0.7753 - val_loss: 0.5055 - val_acc: 0.7391\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.50555 to 0.49961, saving model to best.model\n",
      "0s - loss: 0.5520 - acc: 0.7640 - val_loss: 0.4996 - val_acc: 0.7826\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.49961 to 0.49385, saving model to best.model\n",
      "0s - loss: 0.5599 - acc: 0.7528 - val_loss: 0.4938 - val_acc: 0.7826\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.49385 to 0.48841, saving model to best.model\n",
      "0s - loss: 0.5564 - acc: 0.7303 - val_loss: 0.4884 - val_acc: 0.7826\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.48841 to 0.48292, saving model to best.model\n",
      "0s - loss: 0.5490 - acc: 0.7191 - val_loss: 0.4829 - val_acc: 0.7826\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.48292 to 0.47777, saving model to best.model\n",
      "0s - loss: 0.5190 - acc: 0.7865 - val_loss: 0.4778 - val_acc: 0.7826\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.47777 to 0.47296, saving model to best.model\n",
      "0s - loss: 0.5565 - acc: 0.7416 - val_loss: 0.4730 - val_acc: 0.7826\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.47296 to 0.46821, saving model to best.model\n",
      "0s - loss: 0.4982 - acc: 0.7753 - val_loss: 0.4682 - val_acc: 0.7826\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.46821 to 0.46388, saving model to best.model\n",
      "0s - loss: 0.4809 - acc: 0.7978 - val_loss: 0.4639 - val_acc: 0.7826\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.46388 to 0.45995, saving model to best.model\n",
      "0s - loss: 0.4991 - acc: 0.7978 - val_loss: 0.4599 - val_acc: 0.7826\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.45995 to 0.45617, saving model to best.model\n",
      "0s - loss: 0.5466 - acc: 0.7640 - val_loss: 0.4562 - val_acc: 0.7826\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.45617 to 0.45236, saving model to best.model\n",
      "0s - loss: 0.5167 - acc: 0.7528 - val_loss: 0.4524 - val_acc: 0.8696\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.45236 to 0.44880, saving model to best.model\n",
      "0s - loss: 0.5102 - acc: 0.7416 - val_loss: 0.4488 - val_acc: 0.8696\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.44880 to 0.44512, saving model to best.model\n",
      "0s - loss: 0.5387 - acc: 0.7528 - val_loss: 0.4451 - val_acc: 0.8261\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.44512 to 0.44158, saving model to best.model\n",
      "0s - loss: 0.4527 - acc: 0.7865 - val_loss: 0.4416 - val_acc: 0.8261\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.44158 to 0.43760, saving model to best.model\n",
      "0s - loss: 0.5310 - acc: 0.7416 - val_loss: 0.4376 - val_acc: 0.8261\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.43760 to 0.43318, saving model to best.model\n",
      "0s - loss: 0.4745 - acc: 0.7978 - val_loss: 0.4332 - val_acc: 0.8261\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.43318 to 0.42839, saving model to best.model\n",
      "0s - loss: 0.5203 - acc: 0.7640 - val_loss: 0.4284 - val_acc: 0.8261\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.42839 to 0.42340, saving model to best.model\n",
      "0s - loss: 0.4667 - acc: 0.8090 - val_loss: 0.4234 - val_acc: 0.8261\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.42340 to 0.41896, saving model to best.model\n",
      "0s - loss: 0.4748 - acc: 0.7865 - val_loss: 0.4190 - val_acc: 0.8261\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.41896 to 0.41452, saving model to best.model\n",
      "0s - loss: 0.4269 - acc: 0.8202 - val_loss: 0.4145 - val_acc: 0.8261\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.41452 to 0.40998, saving model to best.model\n",
      "0s - loss: 0.4848 - acc: 0.7865 - val_loss: 0.4100 - val_acc: 0.8261\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.40998 to 0.40524, saving model to best.model\n",
      "0s - loss: 0.4081 - acc: 0.8427 - val_loss: 0.4052 - val_acc: 0.8261\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.40524 to 0.40068, saving model to best.model\n",
      "0s - loss: 0.4816 - acc: 0.7978 - val_loss: 0.4007 - val_acc: 0.8261\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.40068 to 0.39600, saving model to best.model\n",
      "0s - loss: 0.4636 - acc: 0.7978 - val_loss: 0.3960 - val_acc: 0.8261\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.39600 to 0.39130, saving model to best.model\n",
      "0s - loss: 0.4344 - acc: 0.8090 - val_loss: 0.3913 - val_acc: 0.8696\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.39130 to 0.38634, saving model to best.model\n",
      "0s - loss: 0.4683 - acc: 0.7978 - val_loss: 0.3863 - val_acc: 0.8696\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.38634 to 0.38138, saving model to best.model\n",
      "0s - loss: 0.4680 - acc: 0.7978 - val_loss: 0.3814 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.38138 to 0.37662, saving model to best.model\n",
      "0s - loss: 0.4158 - acc: 0.8427 - val_loss: 0.3766 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.37662 to 0.37141, saving model to best.model\n",
      "0s - loss: 0.5179 - acc: 0.7416 - val_loss: 0.3714 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.37141 to 0.36620, saving model to best.model\n",
      "0s - loss: 0.4331 - acc: 0.8652 - val_loss: 0.3662 - val_acc: 0.8696\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.36620 to 0.36135, saving model to best.model\n",
      "0s - loss: 0.4177 - acc: 0.8202 - val_loss: 0.3613 - val_acc: 0.8696\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.36135 to 0.35670, saving model to best.model\n",
      "0s - loss: 0.3741 - acc: 0.8427 - val_loss: 0.3567 - val_acc: 0.8696\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.35670 to 0.35226, saving model to best.model\n",
      "0s - loss: 0.4289 - acc: 0.8202 - val_loss: 0.3523 - val_acc: 0.8696\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.35226 to 0.34809, saving model to best.model\n",
      "0s - loss: 0.3827 - acc: 0.8315 - val_loss: 0.3481 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.34809 to 0.34340, saving model to best.model\n",
      "0s - loss: 0.4149 - acc: 0.8315 - val_loss: 0.3434 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.34340 to 0.33835, saving model to best.model\n",
      "0s - loss: 0.3961 - acc: 0.8427 - val_loss: 0.3383 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.33835 to 0.33311, saving model to best.model\n",
      "0s - loss: 0.3729 - acc: 0.8315 - val_loss: 0.3331 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.33311 to 0.32774, saving model to best.model\n",
      "0s - loss: 0.3858 - acc: 0.8427 - val_loss: 0.3277 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.32774 to 0.32248, saving model to best.model\n",
      "0s - loss: 0.3691 - acc: 0.8539 - val_loss: 0.3225 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.32248 to 0.31751, saving model to best.model\n",
      "0s - loss: 0.3851 - acc: 0.8539 - val_loss: 0.3175 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.31751 to 0.31279, saving model to best.model\n",
      "0s - loss: 0.3793 - acc: 0.8764 - val_loss: 0.3128 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.31279 to 0.30836, saving model to best.model\n",
      "0s - loss: 0.3816 - acc: 0.8315 - val_loss: 0.3084 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.30836 to 0.30398, saving model to best.model\n",
      "0s - loss: 0.3653 - acc: 0.8427 - val_loss: 0.3040 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.30398 to 0.29933, saving model to best.model\n",
      "0s - loss: 0.3297 - acc: 0.8427 - val_loss: 0.2993 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.29933 to 0.29451, saving model to best.model\n",
      "0s - loss: 0.3195 - acc: 0.9101 - val_loss: 0.2945 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.29451 to 0.28967, saving model to best.model\n",
      "0s - loss: 0.3783 - acc: 0.8427 - val_loss: 0.2897 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.28967 to 0.28479, saving model to best.model\n",
      "0s - loss: 0.3415 - acc: 0.8764 - val_loss: 0.2848 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.28479 to 0.28001, saving model to best.model\n",
      "0s - loss: 0.3182 - acc: 0.8652 - val_loss: 0.2800 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.28001 to 0.27509, saving model to best.model\n",
      "0s - loss: 0.3194 - acc: 0.8876 - val_loss: 0.2751 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.27509 to 0.26975, saving model to best.model\n",
      "0s - loss: 0.3462 - acc: 0.8764 - val_loss: 0.2697 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.26975 to 0.26452, saving model to best.model\n",
      "0s - loss: 0.3858 - acc: 0.8090 - val_loss: 0.2645 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.26452 to 0.25961, saving model to best.model\n",
      "0s - loss: 0.3125 - acc: 0.8764 - val_loss: 0.2596 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.25961 to 0.25467, saving model to best.model\n",
      "0s - loss: 0.3467 - acc: 0.8652 - val_loss: 0.2547 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.25467 to 0.24997, saving model to best.model\n",
      "0s - loss: 0.2547 - acc: 0.9326 - val_loss: 0.2500 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.24997 to 0.24496, saving model to best.model\n",
      "0s - loss: 0.2919 - acc: 0.8764 - val_loss: 0.2450 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.24496 to 0.23992, saving model to best.model\n",
      "0s - loss: 0.3336 - acc: 0.8427 - val_loss: 0.2399 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.23992 to 0.23480, saving model to best.model\n",
      "0s - loss: 0.2519 - acc: 0.9213 - val_loss: 0.2348 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.23480 to 0.23008, saving model to best.model\n",
      "0s - loss: 0.3004 - acc: 0.9213 - val_loss: 0.2301 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.23008 to 0.22535, saving model to best.model\n",
      "0s - loss: 0.3238 - acc: 0.8989 - val_loss: 0.2253 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.22535 to 0.22091, saving model to best.model\n",
      "0s - loss: 0.2558 - acc: 0.9551 - val_loss: 0.2209 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.22091 to 0.21669, saving model to best.model\n",
      "0s - loss: 0.3361 - acc: 0.8652 - val_loss: 0.2167 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.21669 to 0.21320, saving model to best.model\n",
      "0s - loss: 0.2957 - acc: 0.8989 - val_loss: 0.2132 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.21320 to 0.20998, saving model to best.model\n",
      "0s - loss: 0.2377 - acc: 0.9213 - val_loss: 0.2100 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.20998 to 0.20630, saving model to best.model\n",
      "0s - loss: 0.2540 - acc: 0.8764 - val_loss: 0.2063 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.20630 to 0.20252, saving model to best.model\n",
      "0s - loss: 0.2958 - acc: 0.8989 - val_loss: 0.2025 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.20252 to 0.19881, saving model to best.model\n",
      "0s - loss: 0.3386 - acc: 0.8427 - val_loss: 0.1988 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.19881 to 0.19491, saving model to best.model\n",
      "0s - loss: 0.3294 - acc: 0.8427 - val_loss: 0.1949 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.19491 to 0.19095, saving model to best.model\n",
      "0s - loss: 0.2786 - acc: 0.9101 - val_loss: 0.1910 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.19095 to 0.18703, saving model to best.model\n",
      "0s - loss: 0.2498 - acc: 0.8876 - val_loss: 0.1870 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.18703 to 0.18277, saving model to best.model\n",
      "0s - loss: 0.2912 - acc: 0.8539 - val_loss: 0.1828 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.18277 to 0.17869, saving model to best.model\n",
      "0s - loss: 0.2355 - acc: 0.9438 - val_loss: 0.1787 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.17869 to 0.17467, saving model to best.model\n",
      "0s - loss: 0.2905 - acc: 0.8989 - val_loss: 0.1747 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.17467 to 0.17051, saving model to best.model\n",
      "0s - loss: 0.1822 - acc: 0.9775 - val_loss: 0.1705 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.17051 to 0.16665, saving model to best.model\n",
      "0s - loss: 0.2475 - acc: 0.8989 - val_loss: 0.1666 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.16665 to 0.16316, saving model to best.model\n",
      "0s - loss: 0.2225 - acc: 0.9213 - val_loss: 0.1632 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.16316 to 0.15998, saving model to best.model\n",
      "0s - loss: 0.1965 - acc: 0.9551 - val_loss: 0.1600 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.15998 to 0.15607, saving model to best.model\n",
      "0s - loss: 0.2027 - acc: 0.9551 - val_loss: 0.1561 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.15607 to 0.15223, saving model to best.model\n",
      "0s - loss: 0.2882 - acc: 0.8989 - val_loss: 0.1522 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.15223 to 0.14850, saving model to best.model\n",
      "0s - loss: 0.2439 - acc: 0.9101 - val_loss: 0.1485 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.14850 to 0.14475, saving model to best.model\n",
      "0s - loss: 0.2223 - acc: 0.9213 - val_loss: 0.1447 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.14475 to 0.14109, saving model to best.model\n",
      "0s - loss: 0.1816 - acc: 0.9438 - val_loss: 0.1411 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.14109 to 0.13726, saving model to best.model\n",
      "0s - loss: 0.2172 - acc: 0.9213 - val_loss: 0.1373 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.13726 to 0.13377, saving model to best.model\n",
      "0s - loss: 0.1757 - acc: 0.9438 - val_loss: 0.1338 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.13377 to 0.13014, saving model to best.model\n",
      "0s - loss: 0.2507 - acc: 0.9101 - val_loss: 0.1301 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.13014 to 0.12721, saving model to best.model\n",
      "0s - loss: 0.1888 - acc: 0.9438 - val_loss: 0.1272 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.12721 to 0.12474, saving model to best.model\n",
      "0s - loss: 0.1908 - acc: 0.9326 - val_loss: 0.1247 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.12474 to 0.12275, saving model to best.model\n",
      "0s - loss: 0.1740 - acc: 0.9775 - val_loss: 0.1227 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.12275 to 0.12092, saving model to best.model\n",
      "0s - loss: 0.1986 - acc: 0.9326 - val_loss: 0.1209 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.12092 to 0.11864, saving model to best.model\n",
      "0s - loss: 0.2323 - acc: 0.9213 - val_loss: 0.1186 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.11864 to 0.11632, saving model to best.model\n",
      "0s - loss: 0.1775 - acc: 0.9775 - val_loss: 0.1163 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.11632 to 0.11422, saving model to best.model\n",
      "0s - loss: 0.1742 - acc: 0.9438 - val_loss: 0.1142 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.11422 to 0.11232, saving model to best.model\n",
      "0s - loss: 0.1744 - acc: 0.9213 - val_loss: 0.1123 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.25528, saving model to best.model\n",
      "0s - loss: 1.3163 - acc: 0.3483 - val_loss: 1.2553 - val_acc: 0.1304\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.25528 to 1.15685, saving model to best.model\n",
      "0s - loss: 1.3531 - acc: 0.3371 - val_loss: 1.1568 - val_acc: 0.1304\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.15685 to 1.08959, saving model to best.model\n",
      "0s - loss: 1.2900 - acc: 0.3034 - val_loss: 1.0896 - val_acc: 0.3478\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.08959 to 1.04546, saving model to best.model\n",
      "0s - loss: 1.2566 - acc: 0.2809 - val_loss: 1.0455 - val_acc: 0.5652\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.04546 to 1.01693, saving model to best.model\n",
      "0s - loss: 1.4594 - acc: 0.1798 - val_loss: 1.0169 - val_acc: 0.5217\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.01693 to 1.00070, saving model to best.model\n",
      "0s - loss: 1.3542 - acc: 0.2584 - val_loss: 1.0007 - val_acc: 0.5217\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.00070 to 0.99200, saving model to best.model\n",
      "0s - loss: 1.3092 - acc: 0.3034 - val_loss: 0.9920 - val_acc: 0.5217\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.99200 to 0.98675, saving model to best.model\n",
      "0s - loss: 1.2678 - acc: 0.2921 - val_loss: 0.9868 - val_acc: 0.5217\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.98675 to 0.98385, saving model to best.model\n",
      "0s - loss: 1.2274 - acc: 0.3258 - val_loss: 0.9839 - val_acc: 0.5217\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.98385 to 0.98360, saving model to best.model\n",
      "0s - loss: 1.2872 - acc: 0.3483 - val_loss: 0.9836 - val_acc: 0.5217\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2155 - acc: 0.3483 - val_loss: 0.9846 - val_acc: 0.5217\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.3652 - acc: 0.2809 - val_loss: 0.9868 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2294 - acc: 0.3708 - val_loss: 0.9910 - val_acc: 0.5217\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2184 - acc: 0.3034 - val_loss: 0.9966 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1833 - acc: 0.3708 - val_loss: 1.0033 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2418 - acc: 0.3258 - val_loss: 1.0117 - val_acc: 0.5217\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1927 - acc: 0.3483 - val_loss: 1.0203 - val_acc: 0.5217\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2480 - acc: 0.2697 - val_loss: 1.0284 - val_acc: 0.5217\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1379 - acc: 0.3708 - val_loss: 1.0369 - val_acc: 0.6957\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2509 - acc: 0.3258 - val_loss: 1.0431 - val_acc: 0.8696\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1301 - acc: 0.3820 - val_loss: 1.0479 - val_acc: 0.8696\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.0888 - acc: 0.4719 - val_loss: 1.0502 - val_acc: 0.8696\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2687 - acc: 0.3146 - val_loss: 1.0494 - val_acc: 0.7826\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.2131 - acc: 0.2809 - val_loss: 1.0477 - val_acc: 0.6087\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1541 - acc: 0.3258 - val_loss: 1.0450 - val_acc: 0.5652\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.2393 - acc: 0.3034 - val_loss: 1.0412 - val_acc: 0.5652\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1835 - acc: 0.3483 - val_loss: 1.0341 - val_acc: 0.5217\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1733 - acc: 0.3933 - val_loss: 1.0257 - val_acc: 0.5217\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1658 - acc: 0.3371 - val_loss: 1.0175 - val_acc: 0.5217\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1640 - acc: 0.3483 - val_loss: 1.0100 - val_acc: 0.5217\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1177 - acc: 0.4157 - val_loss: 1.0047 - val_acc: 0.5652\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1408 - acc: 0.3708 - val_loss: 1.0005 - val_acc: 0.6087\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.1310 - acc: 0.3708 - val_loss: 0.9974 - val_acc: 0.6087\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1656 - acc: 0.3258 - val_loss: 0.9940 - val_acc: 0.6087\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1399 - acc: 0.3146 - val_loss: 0.9926 - val_acc: 0.6087\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.0863 - acc: 0.4719 - val_loss: 0.9914 - val_acc: 0.6522\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.11320, saving model to best.model\n",
      "0s - loss: 1.4893 - acc: 0.2809 - val_loss: 1.1132 - val_acc: 0.4348\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.11320 to 1.08267, saving model to best.model\n",
      "0s - loss: 1.2750 - acc: 0.3596 - val_loss: 1.0827 - val_acc: 0.4348\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.08267 to 1.07006, saving model to best.model\n",
      "0s - loss: 1.2114 - acc: 0.3483 - val_loss: 1.0701 - val_acc: 0.4348\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2872 - acc: 0.3258 - val_loss: 1.0765 - val_acc: 0.4348\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.1633 - acc: 0.4494 - val_loss: 1.0975 - val_acc: 0.3043\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1393 - acc: 0.4157 - val_loss: 1.1289 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1590 - acc: 0.3708 - val_loss: 1.1665 - val_acc: 0.3043\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1439 - acc: 0.3820 - val_loss: 1.2006 - val_acc: 0.3043\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2698 - acc: 0.3371 - val_loss: 1.2266 - val_acc: 0.3043\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2702 - acc: 0.4045 - val_loss: 1.2395 - val_acc: 0.3043\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.3287 - acc: 0.3371 - val_loss: 1.2405 - val_acc: 0.3043\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2095 - acc: 0.4045 - val_loss: 1.2317 - val_acc: 0.3043\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1400 - acc: 0.4831 - val_loss: 1.2165 - val_acc: 0.3043\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.1548 - acc: 0.4494 - val_loss: 1.1964 - val_acc: 0.3043\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2416 - acc: 0.3483 - val_loss: 1.1746 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1728 - acc: 0.4382 - val_loss: 1.1547 - val_acc: 0.3043\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1269 - acc: 0.4157 - val_loss: 1.1374 - val_acc: 0.3043\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2258 - acc: 0.3820 - val_loss: 1.1220 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.0904 - acc: 0.4831 - val_loss: 1.1094 - val_acc: 0.7391\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1898 - acc: 0.4045 - val_loss: 1.0983 - val_acc: 0.4783\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1455 - acc: 0.4270 - val_loss: 1.0881 - val_acc: 0.4783\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1761 - acc: 0.3596 - val_loss: 1.0800 - val_acc: 0.4783\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1144 - acc: 0.4494 - val_loss: 1.0724 - val_acc: 0.4783\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.07006 to 1.06578, saving model to best.model\n",
      "0s - loss: 1.2731 - acc: 0.3483 - val_loss: 1.0658 - val_acc: 0.4783\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.06578 to 1.06033, saving model to best.model\n",
      "0s - loss: 1.0991 - acc: 0.3596 - val_loss: 1.0603 - val_acc: 0.4783\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.06033 to 1.05599, saving model to best.model\n",
      "0s - loss: 1.1248 - acc: 0.3708 - val_loss: 1.0560 - val_acc: 0.4783\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.05599 to 1.05315, saving model to best.model\n",
      "0s - loss: 1.0670 - acc: 0.4270 - val_loss: 1.0532 - val_acc: 0.4783\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.05315 to 1.05151, saving model to best.model\n",
      "0s - loss: 1.1558 - acc: 0.3933 - val_loss: 1.0515 - val_acc: 0.5217\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.05151 to 1.05095, saving model to best.model\n",
      "0s - loss: 1.2170 - acc: 0.3146 - val_loss: 1.0509 - val_acc: 0.6522\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1464 - acc: 0.3933 - val_loss: 1.0516 - val_acc: 0.6957\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1189 - acc: 0.4382 - val_loss: 1.0531 - val_acc: 0.5217\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1377 - acc: 0.4270 - val_loss: 1.0555 - val_acc: 0.3043\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.0551 - acc: 0.4494 - val_loss: 1.0577 - val_acc: 0.3043\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.0816 - acc: 0.4157 - val_loss: 1.0593 - val_acc: 0.3043\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1057 - acc: 0.4494 - val_loss: 1.0611 - val_acc: 0.3043\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.2046 - acc: 0.3820 - val_loss: 1.0629 - val_acc: 0.3043\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 1.0687 - acc: 0.4382 - val_loss: 1.0634 - val_acc: 0.3043\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 1.1383 - acc: 0.4157 - val_loss: 1.0634 - val_acc: 0.3043\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 1.1032 - acc: 0.4157 - val_loss: 1.0629 - val_acc: 0.3043\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 1.0586 - acc: 0.4607 - val_loss: 1.0608 - val_acc: 0.3043\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 1.1107 - acc: 0.4157 - val_loss: 1.0585 - val_acc: 0.3043\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.9699 - acc: 0.5281 - val_loss: 1.0562 - val_acc: 0.3043\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 1.0598 - acc: 0.4494 - val_loss: 1.0533 - val_acc: 0.3043\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.05095 to 1.04952, saving model to best.model\n",
      "0s - loss: 1.0258 - acc: 0.4831 - val_loss: 1.0495 - val_acc: 0.3478\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.04952 to 1.04565, saving model to best.model\n",
      "0s - loss: 0.9514 - acc: 0.5281 - val_loss: 1.0456 - val_acc: 0.4783\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.04565 to 1.04074, saving model to best.model\n",
      "0s - loss: 1.0477 - acc: 0.4831 - val_loss: 1.0407 - val_acc: 0.5652\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.04074 to 1.03606, saving model to best.model\n",
      "0s - loss: 1.1230 - acc: 0.3933 - val_loss: 1.0361 - val_acc: 0.6087\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.03606 to 1.03164, saving model to best.model\n",
      "0s - loss: 1.1286 - acc: 0.4045 - val_loss: 1.0316 - val_acc: 0.6522\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.03164 to 1.02674, saving model to best.model\n",
      "0s - loss: 0.9620 - acc: 0.5393 - val_loss: 1.0267 - val_acc: 0.6522\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.02674 to 1.02170, saving model to best.model\n",
      "0s - loss: 1.0659 - acc: 0.4157 - val_loss: 1.0217 - val_acc: 0.7391\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.02170 to 1.01708, saving model to best.model\n",
      "0s - loss: 0.9920 - acc: 0.4607 - val_loss: 1.0171 - val_acc: 0.7391\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.01708 to 1.01240, saving model to best.model\n",
      "0s - loss: 1.0563 - acc: 0.4494 - val_loss: 1.0124 - val_acc: 0.7391\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.01240 to 1.00757, saving model to best.model\n",
      "0s - loss: 1.0592 - acc: 0.4719 - val_loss: 1.0076 - val_acc: 0.7391\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.00757 to 1.00231, saving model to best.model\n",
      "0s - loss: 1.0074 - acc: 0.5056 - val_loss: 1.0023 - val_acc: 0.7391\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 1.00231 to 0.99693, saving model to best.model\n",
      "0s - loss: 1.0420 - acc: 0.4270 - val_loss: 0.9969 - val_acc: 0.7391\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.99693 to 0.99162, saving model to best.model\n",
      "0s - loss: 1.0785 - acc: 0.4270 - val_loss: 0.9916 - val_acc: 0.7391\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.99162 to 0.98673, saving model to best.model\n",
      "0s - loss: 0.9620 - acc: 0.5730 - val_loss: 0.9867 - val_acc: 0.7391\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.98673 to 0.98217, saving model to best.model\n",
      "0s - loss: 1.0197 - acc: 0.4719 - val_loss: 0.9822 - val_acc: 0.7391\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.98217 to 0.97785, saving model to best.model\n",
      "0s - loss: 1.0643 - acc: 0.4270 - val_loss: 0.9778 - val_acc: 0.7391\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.97785 to 0.97361, saving model to best.model\n",
      "0s - loss: 1.0664 - acc: 0.4607 - val_loss: 0.9736 - val_acc: 0.7391\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.97361 to 0.96945, saving model to best.model\n",
      "0s - loss: 0.9836 - acc: 0.5393 - val_loss: 0.9694 - val_acc: 0.7391\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.96945 to 0.96548, saving model to best.model\n",
      "0s - loss: 1.0169 - acc: 0.4607 - val_loss: 0.9655 - val_acc: 0.7391\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.96548 to 0.96168, saving model to best.model\n",
      "0s - loss: 1.0199 - acc: 0.4045 - val_loss: 0.9617 - val_acc: 0.6957\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.96168 to 0.95766, saving model to best.model\n",
      "0s - loss: 1.0143 - acc: 0.4719 - val_loss: 0.9577 - val_acc: 0.6522\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.95766 to 0.95393, saving model to best.model\n",
      "0s - loss: 1.0083 - acc: 0.4157 - val_loss: 0.9539 - val_acc: 0.6522\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.95393 to 0.94940, saving model to best.model\n",
      "0s - loss: 0.9193 - acc: 0.5730 - val_loss: 0.9494 - val_acc: 0.6522\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.94940 to 0.94505, saving model to best.model\n",
      "0s - loss: 1.0345 - acc: 0.4831 - val_loss: 0.9450 - val_acc: 0.6522\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.94505 to 0.94056, saving model to best.model\n",
      "0s - loss: 0.9918 - acc: 0.4719 - val_loss: 0.9406 - val_acc: 0.6522\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.94056 to 0.93564, saving model to best.model\n",
      "0s - loss: 0.9657 - acc: 0.4719 - val_loss: 0.9356 - val_acc: 0.6522\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.93564 to 0.93016, saving model to best.model\n",
      "0s - loss: 0.9947 - acc: 0.4607 - val_loss: 0.9302 - val_acc: 0.6522\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.93016 to 0.92424, saving model to best.model\n",
      "0s - loss: 0.9761 - acc: 0.5056 - val_loss: 0.9242 - val_acc: 0.6522\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.92424 to 0.91765, saving model to best.model\n",
      "0s - loss: 0.8805 - acc: 0.6517 - val_loss: 0.9177 - val_acc: 0.6522\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.91765 to 0.91084, saving model to best.model\n",
      "0s - loss: 0.9773 - acc: 0.5618 - val_loss: 0.9108 - val_acc: 0.6522\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.91084 to 0.90319, saving model to best.model\n",
      "0s - loss: 1.0245 - acc: 0.4607 - val_loss: 0.9032 - val_acc: 0.6522\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.90319 to 0.89554, saving model to best.model\n",
      "0s - loss: 0.9517 - acc: 0.5393 - val_loss: 0.8955 - val_acc: 0.6957\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.89554 to 0.88743, saving model to best.model\n",
      "0s - loss: 0.9198 - acc: 0.5506 - val_loss: 0.8874 - val_acc: 0.7391\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.88743 to 0.87964, saving model to best.model\n",
      "0s - loss: 0.9215 - acc: 0.5506 - val_loss: 0.8796 - val_acc: 0.7391\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.87964 to 0.87123, saving model to best.model\n",
      "0s - loss: 0.9079 - acc: 0.5955 - val_loss: 0.8712 - val_acc: 0.7391\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.87123 to 0.86212, saving model to best.model\n",
      "0s - loss: 0.9187 - acc: 0.5393 - val_loss: 0.8621 - val_acc: 0.7391\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.86212 to 0.85312, saving model to best.model\n",
      "0s - loss: 0.8359 - acc: 0.6067 - val_loss: 0.8531 - val_acc: 0.7391\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.85312 to 0.84409, saving model to best.model\n",
      "0s - loss: 0.8604 - acc: 0.6067 - val_loss: 0.8441 - val_acc: 0.7391\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.84409 to 0.83476, saving model to best.model\n",
      "0s - loss: 0.9457 - acc: 0.5393 - val_loss: 0.8348 - val_acc: 0.7391\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.83476 to 0.82521, saving model to best.model\n",
      "0s - loss: 0.8069 - acc: 0.6067 - val_loss: 0.8252 - val_acc: 0.7391\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.82521 to 0.81516, saving model to best.model\n",
      "0s - loss: 0.8193 - acc: 0.6292 - val_loss: 0.8152 - val_acc: 0.7391\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.81516 to 0.80501, saving model to best.model\n",
      "0s - loss: 0.8683 - acc: 0.6517 - val_loss: 0.8050 - val_acc: 0.7391\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.80501 to 0.79445, saving model to best.model\n",
      "0s - loss: 0.8139 - acc: 0.6854 - val_loss: 0.7944 - val_acc: 0.7391\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.79445 to 0.78381, saving model to best.model\n",
      "0s - loss: 0.8160 - acc: 0.6854 - val_loss: 0.7838 - val_acc: 0.7826\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.78381 to 0.77277, saving model to best.model\n",
      "0s - loss: 0.8359 - acc: 0.6404 - val_loss: 0.7728 - val_acc: 0.8261\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.77277 to 0.76161, saving model to best.model\n",
      "0s - loss: 0.8314 - acc: 0.6404 - val_loss: 0.7616 - val_acc: 0.8696\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.76161 to 0.75083, saving model to best.model\n",
      "0s - loss: 0.7592 - acc: 0.6517 - val_loss: 0.7508 - val_acc: 0.8696\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.75083 to 0.74017, saving model to best.model\n",
      "0s - loss: 0.7647 - acc: 0.7191 - val_loss: 0.7402 - val_acc: 0.8696\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.74017 to 0.72931, saving model to best.model\n",
      "0s - loss: 0.8147 - acc: 0.6292 - val_loss: 0.7293 - val_acc: 0.8696\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.72931 to 0.71838, saving model to best.model\n",
      "0s - loss: 0.7955 - acc: 0.6067 - val_loss: 0.7184 - val_acc: 0.9130\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.71838 to 0.70747, saving model to best.model\n",
      "0s - loss: 0.8413 - acc: 0.6404 - val_loss: 0.7075 - val_acc: 0.9130\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.70747 to 0.69634, saving model to best.model\n",
      "0s - loss: 0.7084 - acc: 0.7978 - val_loss: 0.6963 - val_acc: 0.9130\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.69634 to 0.68504, saving model to best.model\n",
      "0s - loss: 0.7425 - acc: 0.6966 - val_loss: 0.6850 - val_acc: 0.9130\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.68504 to 0.67348, saving model to best.model\n",
      "0s - loss: 0.6998 - acc: 0.7865 - val_loss: 0.6735 - val_acc: 0.9130\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67348 to 0.66203, saving model to best.model\n",
      "0s - loss: 0.7433 - acc: 0.6742 - val_loss: 0.6620 - val_acc: 0.9130\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66203 to 0.65077, saving model to best.model\n",
      "0s - loss: 0.7254 - acc: 0.7191 - val_loss: 0.6508 - val_acc: 0.9130\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.65077 to 0.63925, saving model to best.model\n",
      "0s - loss: 0.7023 - acc: 0.7079 - val_loss: 0.6392 - val_acc: 0.9130\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.63925 to 0.62746, saving model to best.model\n",
      "0s - loss: 0.6380 - acc: 0.7640 - val_loss: 0.6275 - val_acc: 0.9130\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.62746 to 0.61515, saving model to best.model\n",
      "0s - loss: 0.7313 - acc: 0.6742 - val_loss: 0.6151 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.61515 to 0.60237, saving model to best.model\n",
      "0s - loss: 0.6793 - acc: 0.7416 - val_loss: 0.6024 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.60237 to 0.58955, saving model to best.model\n",
      "0s - loss: 0.7271 - acc: 0.7640 - val_loss: 0.5895 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.58955 to 0.57672, saving model to best.model\n",
      "0s - loss: 0.7078 - acc: 0.7191 - val_loss: 0.5767 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.57672 to 0.56422, saving model to best.model\n",
      "0s - loss: 0.6369 - acc: 0.7191 - val_loss: 0.5642 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.56422 to 0.55222, saving model to best.model\n",
      "0s - loss: 0.6797 - acc: 0.7191 - val_loss: 0.5522 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.55222 to 0.54072, saving model to best.model\n",
      "0s - loss: 0.6252 - acc: 0.7978 - val_loss: 0.5407 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.54072 to 0.52917, saving model to best.model\n",
      "0s - loss: 0.6060 - acc: 0.7753 - val_loss: 0.5292 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.52917 to 0.51766, saving model to best.model\n",
      "0s - loss: 0.6328 - acc: 0.7640 - val_loss: 0.5177 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.51766 to 0.50616, saving model to best.model\n",
      "0s - loss: 0.6642 - acc: 0.7303 - val_loss: 0.5062 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.50616 to 0.49454, saving model to best.model\n",
      "0s - loss: 0.5655 - acc: 0.8315 - val_loss: 0.4945 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.49454 to 0.48332, saving model to best.model\n",
      "0s - loss: 0.5733 - acc: 0.7753 - val_loss: 0.4833 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.48332 to 0.47253, saving model to best.model\n",
      "0s - loss: 0.5523 - acc: 0.8090 - val_loss: 0.4725 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.47253 to 0.46207, saving model to best.model\n",
      "0s - loss: 0.5813 - acc: 0.7640 - val_loss: 0.4621 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.46207 to 0.45125, saving model to best.model\n",
      "0s - loss: 0.5214 - acc: 0.8090 - val_loss: 0.4512 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.45125 to 0.44071, saving model to best.model\n",
      "0s - loss: 0.4669 - acc: 0.8876 - val_loss: 0.4407 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.44071 to 0.42969, saving model to best.model\n",
      "0s - loss: 0.4878 - acc: 0.8090 - val_loss: 0.4297 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.42969 to 0.41883, saving model to best.model\n",
      "0s - loss: 0.4812 - acc: 0.8539 - val_loss: 0.4188 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.41883 to 0.40785, saving model to best.model\n",
      "0s - loss: 0.4266 - acc: 0.9326 - val_loss: 0.4078 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.40785 to 0.39758, saving model to best.model\n",
      "0s - loss: 0.5148 - acc: 0.8652 - val_loss: 0.3976 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.39758 to 0.38745, saving model to best.model\n",
      "0s - loss: 0.4921 - acc: 0.7978 - val_loss: 0.3875 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.38745 to 0.37770, saving model to best.model\n",
      "0s - loss: 0.4193 - acc: 0.8764 - val_loss: 0.3777 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.37770 to 0.36816, saving model to best.model\n",
      "0s - loss: 0.4647 - acc: 0.8764 - val_loss: 0.3682 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.36816 to 0.35942, saving model to best.model\n",
      "0s - loss: 0.4355 - acc: 0.8652 - val_loss: 0.3594 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.35942 to 0.35117, saving model to best.model\n",
      "0s - loss: 0.3950 - acc: 0.8876 - val_loss: 0.3512 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.35117 to 0.34319, saving model to best.model\n",
      "0s - loss: 0.4673 - acc: 0.8315 - val_loss: 0.3432 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.34319 to 0.33453, saving model to best.model\n",
      "0s - loss: 0.4279 - acc: 0.8764 - val_loss: 0.3345 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.33453 to 0.32549, saving model to best.model\n",
      "0s - loss: 0.3942 - acc: 0.8876 - val_loss: 0.3255 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.32549 to 0.31650, saving model to best.model\n",
      "0s - loss: 0.4133 - acc: 0.8652 - val_loss: 0.3165 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.31650 to 0.30904, saving model to best.model\n",
      "0s - loss: 0.3716 - acc: 0.8764 - val_loss: 0.3090 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.30904 to 0.30112, saving model to best.model\n",
      "0s - loss: 0.3973 - acc: 0.8427 - val_loss: 0.3011 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.30112 to 0.29341, saving model to best.model\n",
      "0s - loss: 0.3366 - acc: 0.8989 - val_loss: 0.2934 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.29341 to 0.28594, saving model to best.model\n",
      "0s - loss: 0.3083 - acc: 0.9438 - val_loss: 0.2859 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.28594 to 0.27851, saving model to best.model\n",
      "0s - loss: 0.3267 - acc: 0.9213 - val_loss: 0.2785 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.27851 to 0.27065, saving model to best.model\n",
      "0s - loss: 0.4177 - acc: 0.8539 - val_loss: 0.2706 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.27065 to 0.26252, saving model to best.model\n",
      "0s - loss: 0.3558 - acc: 0.8652 - val_loss: 0.2625 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.26252 to 0.25444, saving model to best.model\n",
      "0s - loss: 0.3406 - acc: 0.8876 - val_loss: 0.2544 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.25444 to 0.24749, saving model to best.model\n",
      "0s - loss: 0.3778 - acc: 0.8652 - val_loss: 0.2475 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.24749 to 0.24108, saving model to best.model\n",
      "0s - loss: 0.3004 - acc: 0.8989 - val_loss: 0.2411 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.24108 to 0.23523, saving model to best.model\n",
      "0s - loss: 0.2511 - acc: 0.9213 - val_loss: 0.2352 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.23523 to 0.23004, saving model to best.model\n",
      "0s - loss: 0.3123 - acc: 0.8989 - val_loss: 0.2300 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.23004 to 0.22564, saving model to best.model\n",
      "0s - loss: 0.2430 - acc: 0.9213 - val_loss: 0.2256 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.22564 to 0.22158, saving model to best.model\n",
      "0s - loss: 0.2658 - acc: 0.9438 - val_loss: 0.2216 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.22158 to 0.21795, saving model to best.model\n",
      "0s - loss: 0.3744 - acc: 0.8427 - val_loss: 0.2180 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.21795 to 0.21433, saving model to best.model\n",
      "0s - loss: 0.2209 - acc: 0.9438 - val_loss: 0.2143 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.21433 to 0.21082, saving model to best.model\n",
      "0s - loss: 0.3521 - acc: 0.8764 - val_loss: 0.2108 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.21082 to 0.20745, saving model to best.model\n",
      "0s - loss: 0.2298 - acc: 0.9326 - val_loss: 0.2075 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.20745 to 0.20373, saving model to best.model\n",
      "0s - loss: 0.3454 - acc: 0.8652 - val_loss: 0.2037 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.20373 to 0.20006, saving model to best.model\n",
      "0s - loss: 0.2291 - acc: 0.9438 - val_loss: 0.2001 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.20006 to 0.19664, saving model to best.model\n",
      "0s - loss: 0.2392 - acc: 0.9663 - val_loss: 0.1966 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.19664 to 0.19364, saving model to best.model\n",
      "0s - loss: 0.3173 - acc: 0.8989 - val_loss: 0.1936 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.19364 to 0.19029, saving model to best.model\n",
      "0s - loss: 0.2314 - acc: 0.9326 - val_loss: 0.1903 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.19029 to 0.18717, saving model to best.model\n",
      "0s - loss: 0.2516 - acc: 0.9213 - val_loss: 0.1872 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.18717 to 0.18399, saving model to best.model\n",
      "0s - loss: 0.2706 - acc: 0.9101 - val_loss: 0.1840 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.18399 to 0.18061, saving model to best.model\n",
      "0s - loss: 0.2248 - acc: 0.9213 - val_loss: 0.1806 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.18061 to 0.17690, saving model to best.model\n",
      "0s - loss: 0.2885 - acc: 0.8764 - val_loss: 0.1769 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.17690 to 0.17433, saving model to best.model\n",
      "0s - loss: 0.1970 - acc: 0.9663 - val_loss: 0.1743 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.17433 to 0.17210, saving model to best.model\n",
      "0s - loss: 0.2273 - acc: 0.9438 - val_loss: 0.1721 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.17210 to 0.16962, saving model to best.model\n",
      "0s - loss: 0.2342 - acc: 0.9326 - val_loss: 0.1696 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.16962 to 0.16616, saving model to best.model\n",
      "0s - loss: 0.2126 - acc: 0.9551 - val_loss: 0.1662 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.16616 to 0.16150, saving model to best.model\n",
      "0s - loss: 0.2633 - acc: 0.9213 - val_loss: 0.1615 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.16150 to 0.15616, saving model to best.model\n",
      "0s - loss: 0.2163 - acc: 0.9551 - val_loss: 0.1562 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.15616 to 0.15158, saving model to best.model\n",
      "0s - loss: 0.1698 - acc: 0.9551 - val_loss: 0.1516 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.15158 to 0.14703, saving model to best.model\n",
      "0s - loss: 0.2374 - acc: 0.9101 - val_loss: 0.1470 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.14703 to 0.14277, saving model to best.model\n",
      "0s - loss: 0.2235 - acc: 0.8989 - val_loss: 0.1428 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.14277 to 0.13840, saving model to best.model\n",
      "0s - loss: 0.2513 - acc: 0.9326 - val_loss: 0.1384 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.13840 to 0.13373, saving model to best.model\n",
      "0s - loss: 0.2421 - acc: 0.9101 - val_loss: 0.1337 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.13373 to 0.12856, saving model to best.model\n",
      "0s - loss: 0.2011 - acc: 0.9213 - val_loss: 0.1286 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.12856 to 0.12424, saving model to best.model\n",
      "0s - loss: 0.1965 - acc: 0.9326 - val_loss: 0.1242 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.12424 to 0.12048, saving model to best.model\n",
      "0s - loss: 0.1907 - acc: 0.9551 - val_loss: 0.1205 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.12048 to 0.11745, saving model to best.model\n",
      "0s - loss: 0.2047 - acc: 0.9326 - val_loss: 0.1174 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.11745 to 0.11518, saving model to best.model\n",
      "0s - loss: 0.2612 - acc: 0.9101 - val_loss: 0.1152 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.11518 to 0.11349, saving model to best.model\n",
      "0s - loss: 0.2058 - acc: 0.9326 - val_loss: 0.1135 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.11349 to 0.11296, saving model to best.model\n",
      "0s - loss: 0.2489 - acc: 0.9101 - val_loss: 0.1130 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.11296 to 0.11258, saving model to best.model\n",
      "0s - loss: 0.1686 - acc: 0.9438 - val_loss: 0.1126 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.2250 - acc: 0.8989 - val_loss: 0.1126 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.2234 - acc: 0.9326 - val_loss: 0.1126 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.11258 to 0.11226, saving model to best.model\n",
      "0s - loss: 0.1879 - acc: 0.9326 - val_loss: 0.1123 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.11226 to 0.11202, saving model to best.model\n",
      "0s - loss: 0.1669 - acc: 0.9438 - val_loss: 0.1120 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.11202 to 0.11162, saving model to best.model\n",
      "0s - loss: 0.2061 - acc: 0.9213 - val_loss: 0.1116 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.11162 to 0.11136, saving model to best.model\n",
      "0s - loss: 0.1860 - acc: 0.9438 - val_loss: 0.1114 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.11136 to 0.11038, saving model to best.model\n",
      "0s - loss: 0.1694 - acc: 0.9326 - val_loss: 0.1104 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.11038 to 0.10814, saving model to best.model\n",
      "0s - loss: 0.1514 - acc: 0.9551 - val_loss: 0.1081 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.10814 to 0.10434, saving model to best.model\n",
      "0s - loss: 0.2379 - acc: 0.9101 - val_loss: 0.1043 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.10434 to 0.09963, saving model to best.model\n",
      "0s - loss: 0.2487 - acc: 0.9213 - val_loss: 0.0996 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.09963 to 0.09535, saving model to best.model\n",
      "0s - loss: 0.1579 - acc: 0.9663 - val_loss: 0.0953 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.09535 to 0.09170, saving model to best.model\n",
      "0s - loss: 0.1379 - acc: 0.9551 - val_loss: 0.0917 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.09170 to 0.08851, saving model to best.model\n",
      "0s - loss: 0.1594 - acc: 0.9551 - val_loss: 0.0885 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.08851 to 0.08605, saving model to best.model\n",
      "0s - loss: 0.1578 - acc: 0.9326 - val_loss: 0.0860 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.08605 to 0.08387, saving model to best.model\n",
      "0s - loss: 0.1690 - acc: 0.9551 - val_loss: 0.0839 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.08387 to 0.08210, saving model to best.model\n",
      "0s - loss: 0.1416 - acc: 0.9551 - val_loss: 0.0821 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.08210 to 0.08026, saving model to best.model\n",
      "0s - loss: 0.1545 - acc: 0.9551 - val_loss: 0.0803 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.08026 to 0.07849, saving model to best.model\n",
      "0s - loss: 0.1730 - acc: 0.9326 - val_loss: 0.0785 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.07849 to 0.07716, saving model to best.model\n",
      "0s - loss: 0.1366 - acc: 0.9438 - val_loss: 0.0772 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.07716 to 0.07584, saving model to best.model\n",
      "0s - loss: 0.1237 - acc: 0.9775 - val_loss: 0.0758 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.07584 to 0.07431, saving model to best.model\n",
      "0s - loss: 0.1337 - acc: 0.9663 - val_loss: 0.0743 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.07431 to 0.07328, saving model to best.model\n",
      "0s - loss: 0.1552 - acc: 0.9551 - val_loss: 0.0733 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.07328 to 0.07201, saving model to best.model\n",
      "0s - loss: 0.1507 - acc: 0.9551 - val_loss: 0.0720 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.07201 to 0.07084, saving model to best.model\n",
      "0s - loss: 0.1464 - acc: 0.9551 - val_loss: 0.0708 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.10893, saving model to best.model\n",
      "0s - loss: 1.3066 - acc: 0.2809 - val_loss: 1.1089 - val_acc: 0.3043\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.2361 - acc: 0.4045 - val_loss: 1.1368 - val_acc: 0.3043\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.1844 - acc: 0.4157 - val_loss: 1.1748 - val_acc: 0.3043\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1682 - acc: 0.3708 - val_loss: 1.2071 - val_acc: 0.3043\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.1508 - acc: 0.3820 - val_loss: 1.2264 - val_acc: 0.3043\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2893 - acc: 0.3483 - val_loss: 1.2278 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2422 - acc: 0.4045 - val_loss: 1.2165 - val_acc: 0.3043\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2870 - acc: 0.3708 - val_loss: 1.1983 - val_acc: 0.3043\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2312 - acc: 0.3483 - val_loss: 1.1743 - val_acc: 0.3043\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1963 - acc: 0.3708 - val_loss: 1.1503 - val_acc: 0.3043\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2325 - acc: 0.3483 - val_loss: 1.1313 - val_acc: 0.3043\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1900 - acc: 0.4045 - val_loss: 1.1171 - val_acc: 0.3043\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.10893 to 1.10675, saving model to best.model\n",
      "0s - loss: 1.1892 - acc: 0.3708 - val_loss: 1.1068 - val_acc: 0.3043\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.10675 to 1.10102, saving model to best.model\n",
      "0s - loss: 1.1249 - acc: 0.4270 - val_loss: 1.1010 - val_acc: 0.3043\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.10102 to 1.09656, saving model to best.model\n",
      "0s - loss: 1.1294 - acc: 0.3708 - val_loss: 1.0966 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.09656 to 1.09355, saving model to best.model\n",
      "0s - loss: 1.1529 - acc: 0.3708 - val_loss: 1.0936 - val_acc: 0.3043\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.09355 to 1.09270, saving model to best.model\n",
      "0s - loss: 1.1345 - acc: 0.3933 - val_loss: 1.0927 - val_acc: 0.3043\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2435 - acc: 0.2921 - val_loss: 1.0928 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2116 - acc: 0.3820 - val_loss: 1.0947 - val_acc: 0.3043\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1731 - acc: 0.3483 - val_loss: 1.0976 - val_acc: 0.3043\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1902 - acc: 0.3483 - val_loss: 1.1027 - val_acc: 0.3043\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1553 - acc: 0.3146 - val_loss: 1.1089 - val_acc: 0.3043\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1142 - acc: 0.4494 - val_loss: 1.1140 - val_acc: 0.3043\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1296 - acc: 0.4157 - val_loss: 1.1209 - val_acc: 0.3043\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.0884 - acc: 0.5056 - val_loss: 1.1264 - val_acc: 0.3043\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1104 - acc: 0.4382 - val_loss: 1.1313 - val_acc: 0.3043\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1914 - acc: 0.4382 - val_loss: 1.1341 - val_acc: 0.3043\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.2648 - acc: 0.3708 - val_loss: 1.1339 - val_acc: 0.3043\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.0818 - acc: 0.4719 - val_loss: 1.1305 - val_acc: 0.3043\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.0549 - acc: 0.4944 - val_loss: 1.1250 - val_acc: 0.3043\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1022 - acc: 0.4494 - val_loss: 1.1197 - val_acc: 0.3043\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1107 - acc: 0.4719 - val_loss: 1.1123 - val_acc: 0.3043\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.1179 - acc: 0.4382 - val_loss: 1.1047 - val_acc: 0.3043\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.0687 - acc: 0.4944 - val_loss: 1.0965 - val_acc: 0.3043\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.09270 to 1.08936, saving model to best.model\n",
      "0s - loss: 1.0446 - acc: 0.4382 - val_loss: 1.0894 - val_acc: 0.3043\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.08936 to 1.08372, saving model to best.model\n",
      "0s - loss: 1.0349 - acc: 0.4944 - val_loss: 1.0837 - val_acc: 0.3043\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.08372 to 1.07883, saving model to best.model\n",
      "0s - loss: 1.1698 - acc: 0.3034 - val_loss: 1.0788 - val_acc: 0.3043\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.07883 to 1.07383, saving model to best.model\n",
      "0s - loss: 1.1292 - acc: 0.3933 - val_loss: 1.0738 - val_acc: 0.3043\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.07383 to 1.06888, saving model to best.model\n",
      "0s - loss: 1.0474 - acc: 0.4719 - val_loss: 1.0689 - val_acc: 0.3043\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.06888 to 1.06321, saving model to best.model\n",
      "0s - loss: 1.0930 - acc: 0.4494 - val_loss: 1.0632 - val_acc: 0.3043\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.06321 to 1.05771, saving model to best.model\n",
      "0s - loss: 1.0411 - acc: 0.4944 - val_loss: 1.0577 - val_acc: 0.3043\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.05771 to 1.05357, saving model to best.model\n",
      "0s - loss: 1.0992 - acc: 0.4270 - val_loss: 1.0536 - val_acc: 0.3043\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.05357 to 1.05057, saving model to best.model\n",
      "0s - loss: 1.0987 - acc: 0.3820 - val_loss: 1.0506 - val_acc: 0.3043\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.05057 to 1.04760, saving model to best.model\n",
      "0s - loss: 1.0633 - acc: 0.4157 - val_loss: 1.0476 - val_acc: 0.3043\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.04760 to 1.04404, saving model to best.model\n",
      "0s - loss: 0.9805 - acc: 0.5169 - val_loss: 1.0440 - val_acc: 0.3043\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.04404 to 1.04090, saving model to best.model\n",
      "0s - loss: 1.0961 - acc: 0.3820 - val_loss: 1.0409 - val_acc: 0.3043\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.04090 to 1.03750, saving model to best.model\n",
      "0s - loss: 1.0228 - acc: 0.5056 - val_loss: 1.0375 - val_acc: 0.3043\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.03750 to 1.03379, saving model to best.model\n",
      "0s - loss: 1.0711 - acc: 0.4157 - val_loss: 1.0338 - val_acc: 0.3043\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.03379 to 1.02914, saving model to best.model\n",
      "0s - loss: 1.0771 - acc: 0.4157 - val_loss: 1.0291 - val_acc: 0.3043\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.02914 to 1.02513, saving model to best.model\n",
      "0s - loss: 1.0540 - acc: 0.3933 - val_loss: 1.0251 - val_acc: 0.3043\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.02513 to 1.02032, saving model to best.model\n",
      "0s - loss: 1.0068 - acc: 0.4944 - val_loss: 1.0203 - val_acc: 0.3043\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.02032 to 1.01811, saving model to best.model\n",
      "0s - loss: 1.0551 - acc: 0.4607 - val_loss: 1.0181 - val_acc: 0.3043\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.01811 to 1.01534, saving model to best.model\n",
      "0s - loss: 1.0199 - acc: 0.4494 - val_loss: 1.0153 - val_acc: 0.3043\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.01534 to 1.01328, saving model to best.model\n",
      "0s - loss: 1.0642 - acc: 0.3933 - val_loss: 1.0133 - val_acc: 0.3043\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 1.01328 to 1.01022, saving model to best.model\n",
      "0s - loss: 1.0409 - acc: 0.4719 - val_loss: 1.0102 - val_acc: 0.3043\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 1.01022 to 1.00593, saving model to best.model\n",
      "0s - loss: 1.0394 - acc: 0.4494 - val_loss: 1.0059 - val_acc: 0.3043\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 1.00593 to 1.00096, saving model to best.model\n",
      "0s - loss: 1.0731 - acc: 0.4382 - val_loss: 1.0010 - val_acc: 0.3043\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 1.00096 to 0.99567, saving model to best.model\n",
      "0s - loss: 0.9868 - acc: 0.4831 - val_loss: 0.9957 - val_acc: 0.3043\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.99567 to 0.99039, saving model to best.model\n",
      "0s - loss: 0.9700 - acc: 0.5056 - val_loss: 0.9904 - val_acc: 0.3043\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.99039 to 0.98463, saving model to best.model\n",
      "0s - loss: 0.9815 - acc: 0.4944 - val_loss: 0.9846 - val_acc: 0.3043\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.98463 to 0.97815, saving model to best.model\n",
      "0s - loss: 0.9709 - acc: 0.4831 - val_loss: 0.9781 - val_acc: 0.3043\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.97815 to 0.97082, saving model to best.model\n",
      "0s - loss: 1.0451 - acc: 0.4607 - val_loss: 0.9708 - val_acc: 0.3043\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.97082 to 0.96183, saving model to best.model\n",
      "0s - loss: 0.9825 - acc: 0.5056 - val_loss: 0.9618 - val_acc: 0.3043\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.96183 to 0.95199, saving model to best.model\n",
      "0s - loss: 1.0448 - acc: 0.4607 - val_loss: 0.9520 - val_acc: 0.3043\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.95199 to 0.94182, saving model to best.model\n",
      "0s - loss: 0.9221 - acc: 0.5169 - val_loss: 0.9418 - val_acc: 0.3478\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.94182 to 0.93154, saving model to best.model\n",
      "0s - loss: 0.9891 - acc: 0.5056 - val_loss: 0.9315 - val_acc: 0.3478\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.93154 to 0.92174, saving model to best.model\n",
      "0s - loss: 0.9880 - acc: 0.4831 - val_loss: 0.9217 - val_acc: 0.3913\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.92174 to 0.91260, saving model to best.model\n",
      "0s - loss: 0.9395 - acc: 0.5169 - val_loss: 0.9126 - val_acc: 0.4783\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.91260 to 0.90364, saving model to best.model\n",
      "0s - loss: 0.9290 - acc: 0.5730 - val_loss: 0.9036 - val_acc: 0.4783\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.90364 to 0.89498, saving model to best.model\n",
      "0s - loss: 0.9123 - acc: 0.5506 - val_loss: 0.8950 - val_acc: 0.4783\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.89498 to 0.88675, saving model to best.model\n",
      "0s - loss: 0.9013 - acc: 0.5730 - val_loss: 0.8868 - val_acc: 0.4783\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.88675 to 0.87729, saving model to best.model\n",
      "0s - loss: 0.9160 - acc: 0.5730 - val_loss: 0.8773 - val_acc: 0.5217\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.87729 to 0.86863, saving model to best.model\n",
      "0s - loss: 0.8486 - acc: 0.6180 - val_loss: 0.8686 - val_acc: 0.5217\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.86863 to 0.85920, saving model to best.model\n",
      "0s - loss: 0.8976 - acc: 0.5843 - val_loss: 0.8592 - val_acc: 0.5217\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.85920 to 0.85029, saving model to best.model\n",
      "0s - loss: 0.9191 - acc: 0.5281 - val_loss: 0.8503 - val_acc: 0.5217\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.85029 to 0.84178, saving model to best.model\n",
      "0s - loss: 0.9401 - acc: 0.5843 - val_loss: 0.8418 - val_acc: 0.5217\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.84178 to 0.83254, saving model to best.model\n",
      "0s - loss: 0.9517 - acc: 0.5056 - val_loss: 0.8325 - val_acc: 0.5217\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.83254 to 0.82308, saving model to best.model\n",
      "0s - loss: 0.9109 - acc: 0.5843 - val_loss: 0.8231 - val_acc: 0.5217\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.82308 to 0.81256, saving model to best.model\n",
      "0s - loss: 0.8972 - acc: 0.5843 - val_loss: 0.8126 - val_acc: 0.5217\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.81256 to 0.80216, saving model to best.model\n",
      "0s - loss: 0.8314 - acc: 0.6966 - val_loss: 0.8022 - val_acc: 0.5652\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.80216 to 0.79151, saving model to best.model\n",
      "0s - loss: 0.8411 - acc: 0.5843 - val_loss: 0.7915 - val_acc: 0.5652\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.79151 to 0.77975, saving model to best.model\n",
      "0s - loss: 0.8002 - acc: 0.6517 - val_loss: 0.7798 - val_acc: 0.5652\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.77975 to 0.76655, saving model to best.model\n",
      "0s - loss: 0.8255 - acc: 0.6404 - val_loss: 0.7665 - val_acc: 0.6522\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.76655 to 0.75345, saving model to best.model\n",
      "0s - loss: 0.8966 - acc: 0.5393 - val_loss: 0.7534 - val_acc: 0.6522\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.75345 to 0.74037, saving model to best.model\n",
      "0s - loss: 0.8178 - acc: 0.5843 - val_loss: 0.7404 - val_acc: 0.6957\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.74037 to 0.72681, saving model to best.model\n",
      "0s - loss: 0.7656 - acc: 0.6966 - val_loss: 0.7268 - val_acc: 0.7391\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.72681 to 0.71318, saving model to best.model\n",
      "0s - loss: 0.7516 - acc: 0.6854 - val_loss: 0.7132 - val_acc: 0.7391\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.71318 to 0.69868, saving model to best.model\n",
      "0s - loss: 0.7402 - acc: 0.6966 - val_loss: 0.6987 - val_acc: 0.8261\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.69868 to 0.68319, saving model to best.model\n",
      "0s - loss: 0.8272 - acc: 0.6180 - val_loss: 0.6832 - val_acc: 0.8261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68319 to 0.66804, saving model to best.model\n",
      "0s - loss: 0.7504 - acc: 0.6966 - val_loss: 0.6680 - val_acc: 0.8261\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.66804 to 0.65236, saving model to best.model\n",
      "0s - loss: 0.7286 - acc: 0.6854 - val_loss: 0.6524 - val_acc: 0.8261\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.65236 to 0.63722, saving model to best.model\n",
      "0s - loss: 0.7527 - acc: 0.6742 - val_loss: 0.6372 - val_acc: 0.8696\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.63722 to 0.62266, saving model to best.model\n",
      "0s - loss: 0.8073 - acc: 0.6404 - val_loss: 0.6227 - val_acc: 0.9565\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.62266 to 0.60709, saving model to best.model\n",
      "0s - loss: 0.7414 - acc: 0.6854 - val_loss: 0.6071 - val_acc: 0.9565\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.60709 to 0.58962, saving model to best.model\n",
      "0s - loss: 0.6710 - acc: 0.7528 - val_loss: 0.5896 - val_acc: 0.9565\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.58962 to 0.57132, saving model to best.model\n",
      "0s - loss: 0.7399 - acc: 0.7191 - val_loss: 0.5713 - val_acc: 0.9565\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.57132 to 0.55370, saving model to best.model\n",
      "0s - loss: 0.6630 - acc: 0.7753 - val_loss: 0.5537 - val_acc: 0.9565\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.55370 to 0.53694, saving model to best.model\n",
      "0s - loss: 0.6204 - acc: 0.7978 - val_loss: 0.5369 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.53694 to 0.52041, saving model to best.model\n",
      "0s - loss: 0.6380 - acc: 0.7865 - val_loss: 0.5204 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.52041 to 0.50648, saving model to best.model\n",
      "0s - loss: 0.6846 - acc: 0.7416 - val_loss: 0.5065 - val_acc: 1.0000\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.50648 to 0.49354, saving model to best.model\n",
      "0s - loss: 0.6679 - acc: 0.6966 - val_loss: 0.4935 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.49354 to 0.48167, saving model to best.model\n",
      "0s - loss: 0.6426 - acc: 0.7079 - val_loss: 0.4817 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.48167 to 0.47012, saving model to best.model\n",
      "0s - loss: 0.6138 - acc: 0.8202 - val_loss: 0.4701 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.47012 to 0.45820, saving model to best.model\n",
      "0s - loss: 0.6062 - acc: 0.7753 - val_loss: 0.4582 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.45820 to 0.44669, saving model to best.model\n",
      "0s - loss: 0.5749 - acc: 0.7865 - val_loss: 0.4467 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.44669 to 0.43394, saving model to best.model\n",
      "0s - loss: 0.5747 - acc: 0.7753 - val_loss: 0.4339 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.43394 to 0.42199, saving model to best.model\n",
      "0s - loss: 0.6475 - acc: 0.7303 - val_loss: 0.4220 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.42199 to 0.41023, saving model to best.model\n",
      "0s - loss: 0.5480 - acc: 0.7640 - val_loss: 0.4102 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.41023 to 0.39980, saving model to best.model\n",
      "0s - loss: 0.5152 - acc: 0.8315 - val_loss: 0.3998 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.39980 to 0.38909, saving model to best.model\n",
      "0s - loss: 0.5380 - acc: 0.7978 - val_loss: 0.3891 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.38909 to 0.37796, saving model to best.model\n",
      "0s - loss: 0.5618 - acc: 0.7753 - val_loss: 0.3780 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.37796 to 0.36752, saving model to best.model\n",
      "0s - loss: 0.5879 - acc: 0.7753 - val_loss: 0.3675 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.36752 to 0.35843, saving model to best.model\n",
      "0s - loss: 0.5285 - acc: 0.8427 - val_loss: 0.3584 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.35843 to 0.34875, saving model to best.model\n",
      "0s - loss: 0.4844 - acc: 0.8427 - val_loss: 0.3487 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.34875 to 0.33842, saving model to best.model\n",
      "0s - loss: 0.5055 - acc: 0.8315 - val_loss: 0.3384 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.33842 to 0.32899, saving model to best.model\n",
      "0s - loss: 0.5126 - acc: 0.7640 - val_loss: 0.3290 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.32899 to 0.31877, saving model to best.model\n",
      "0s - loss: 0.5237 - acc: 0.8652 - val_loss: 0.3188 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.31877 to 0.30814, saving model to best.model\n",
      "0s - loss: 0.4435 - acc: 0.8090 - val_loss: 0.3081 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.30814 to 0.29682, saving model to best.model\n",
      "0s - loss: 0.4193 - acc: 0.8315 - val_loss: 0.2968 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.29682 to 0.28652, saving model to best.model\n",
      "0s - loss: 0.4391 - acc: 0.8427 - val_loss: 0.2865 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.28652 to 0.27746, saving model to best.model\n",
      "0s - loss: 0.4227 - acc: 0.8652 - val_loss: 0.2775 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.27746 to 0.26810, saving model to best.model\n",
      "0s - loss: 0.4286 - acc: 0.8989 - val_loss: 0.2681 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.26810 to 0.25825, saving model to best.model\n",
      "0s - loss: 0.3900 - acc: 0.9101 - val_loss: 0.2583 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.25825 to 0.24939, saving model to best.model\n",
      "0s - loss: 0.4049 - acc: 0.8764 - val_loss: 0.2494 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.24939 to 0.24093, saving model to best.model\n",
      "0s - loss: 0.4267 - acc: 0.8315 - val_loss: 0.2409 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.24093 to 0.23309, saving model to best.model\n",
      "0s - loss: 0.3942 - acc: 0.8539 - val_loss: 0.2331 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.23309 to 0.22504, saving model to best.model\n",
      "0s - loss: 0.4636 - acc: 0.8315 - val_loss: 0.2250 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.22504 to 0.21671, saving model to best.model\n",
      "0s - loss: 0.3803 - acc: 0.8876 - val_loss: 0.2167 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.21671 to 0.20819, saving model to best.model\n",
      "0s - loss: 0.3923 - acc: 0.8539 - val_loss: 0.2082 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.20819 to 0.20103, saving model to best.model\n",
      "0s - loss: 0.3327 - acc: 0.9213 - val_loss: 0.2010 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.20103 to 0.19442, saving model to best.model\n",
      "0s - loss: 0.3725 - acc: 0.8764 - val_loss: 0.1944 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.19442 to 0.18798, saving model to best.model\n",
      "0s - loss: 0.3704 - acc: 0.8876 - val_loss: 0.1880 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.18798 to 0.18330, saving model to best.model\n",
      "0s - loss: 0.3581 - acc: 0.8652 - val_loss: 0.1833 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.18330 to 0.17812, saving model to best.model\n",
      "0s - loss: 0.3090 - acc: 0.9213 - val_loss: 0.1781 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.17812 to 0.17192, saving model to best.model\n",
      "0s - loss: 0.3434 - acc: 0.8989 - val_loss: 0.1719 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.17192 to 0.16682, saving model to best.model\n",
      "0s - loss: 0.3270 - acc: 0.8876 - val_loss: 0.1668 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.16682 to 0.16234, saving model to best.model\n",
      "0s - loss: 0.3621 - acc: 0.8764 - val_loss: 0.1623 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.16234 to 0.15731, saving model to best.model\n",
      "0s - loss: 0.3609 - acc: 0.8652 - val_loss: 0.1573 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.15731 to 0.15240, saving model to best.model\n",
      "0s - loss: 0.2544 - acc: 0.9438 - val_loss: 0.1524 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.15240 to 0.14774, saving model to best.model\n",
      "0s - loss: 0.2853 - acc: 0.9213 - val_loss: 0.1477 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.14774 to 0.14456, saving model to best.model\n",
      "0s - loss: 0.2295 - acc: 0.9438 - val_loss: 0.1446 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.14456 to 0.14195, saving model to best.model\n",
      "0s - loss: 0.2665 - acc: 0.9326 - val_loss: 0.1420 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.14195 to 0.13733, saving model to best.model\n",
      "0s - loss: 0.3225 - acc: 0.8539 - val_loss: 0.1373 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.13733 to 0.13218, saving model to best.model\n",
      "0s - loss: 0.2669 - acc: 0.9213 - val_loss: 0.1322 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.13218 to 0.12733, saving model to best.model\n",
      "0s - loss: 0.2644 - acc: 0.9326 - val_loss: 0.1273 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.12733 to 0.12278, saving model to best.model\n",
      "0s - loss: 0.2779 - acc: 0.9213 - val_loss: 0.1228 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.12278 to 0.11858, saving model to best.model\n",
      "0s - loss: 0.2695 - acc: 0.8989 - val_loss: 0.1186 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.11858 to 0.11407, saving model to best.model\n",
      "0s - loss: 0.3363 - acc: 0.8764 - val_loss: 0.1141 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.11407 to 0.11116, saving model to best.model\n",
      "0s - loss: 0.3079 - acc: 0.8876 - val_loss: 0.1112 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.11116 to 0.10812, saving model to best.model\n",
      "0s - loss: 0.2139 - acc: 0.9438 - val_loss: 0.1081 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.10812 to 0.10543, saving model to best.model\n",
      "0s - loss: 0.2311 - acc: 0.9326 - val_loss: 0.1054 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.10543 to 0.10253, saving model to best.model\n",
      "0s - loss: 0.2611 - acc: 0.9101 - val_loss: 0.1025 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.10253 to 0.09945, saving model to best.model\n",
      "0s - loss: 0.2544 - acc: 0.8989 - val_loss: 0.0994 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.09945 to 0.09595, saving model to best.model\n",
      "0s - loss: 0.2153 - acc: 0.9326 - val_loss: 0.0959 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.09595 to 0.09310, saving model to best.model\n",
      "0s - loss: 0.2482 - acc: 0.9326 - val_loss: 0.0931 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.09310 to 0.08988, saving model to best.model\n",
      "0s - loss: 0.2604 - acc: 0.9213 - val_loss: 0.0899 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.08988 to 0.08680, saving model to best.model\n",
      "0s - loss: 0.1786 - acc: 0.9551 - val_loss: 0.0868 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.08680 to 0.08472, saving model to best.model\n",
      "0s - loss: 0.2391 - acc: 0.8876 - val_loss: 0.0847 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.08472 to 0.08282, saving model to best.model\n",
      "0s - loss: 0.2534 - acc: 0.9213 - val_loss: 0.0828 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.08282 to 0.08206, saving model to best.model\n",
      "0s - loss: 0.2359 - acc: 0.9213 - val_loss: 0.0821 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.08206 to 0.08142, saving model to best.model\n",
      "0s - loss: 0.2230 - acc: 0.9101 - val_loss: 0.0814 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.08142 to 0.08008, saving model to best.model\n",
      "0s - loss: 0.2057 - acc: 0.9101 - val_loss: 0.0801 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.08008 to 0.07940, saving model to best.model\n",
      "0s - loss: 0.2403 - acc: 0.9101 - val_loss: 0.0794 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.07940 to 0.07849, saving model to best.model\n",
      "0s - loss: 0.1619 - acc: 0.9551 - val_loss: 0.0785 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.07849 to 0.07723, saving model to best.model\n",
      "0s - loss: 0.1963 - acc: 0.9551 - val_loss: 0.0772 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.07723 to 0.07521, saving model to best.model\n",
      "0s - loss: 0.2372 - acc: 0.9213 - val_loss: 0.0752 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.07521 to 0.07308, saving model to best.model\n",
      "0s - loss: 0.2085 - acc: 0.9663 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.07308 to 0.07101, saving model to best.model\n",
      "0s - loss: 0.1380 - acc: 0.9775 - val_loss: 0.0710 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.07101 to 0.06892, saving model to best.model\n",
      "0s - loss: 0.1889 - acc: 0.9551 - val_loss: 0.0689 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.06892 to 0.06645, saving model to best.model\n",
      "0s - loss: 0.1671 - acc: 0.9888 - val_loss: 0.0665 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.06645 to 0.06341, saving model to best.model\n",
      "0s - loss: 0.1775 - acc: 0.9438 - val_loss: 0.0634 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.06341 to 0.06099, saving model to best.model\n",
      "0s - loss: 0.1240 - acc: 0.9888 - val_loss: 0.0610 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.06099 to 0.05850, saving model to best.model\n",
      "0s - loss: 0.1443 - acc: 0.9775 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.05850 to 0.05639, saving model to best.model\n",
      "0s - loss: 0.1684 - acc: 0.9326 - val_loss: 0.0564 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.05639 to 0.05412, saving model to best.model\n",
      "0s - loss: 0.2152 - acc: 0.9213 - val_loss: 0.0541 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.05412 to 0.05258, saving model to best.model\n",
      "0s - loss: 0.2333 - acc: 0.8876 - val_loss: 0.0526 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.05258 to 0.05154, saving model to best.model\n",
      "0s - loss: 0.1478 - acc: 0.9775 - val_loss: 0.0515 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.05154 to 0.05039, saving model to best.model\n",
      "0s - loss: 0.1301 - acc: 0.9663 - val_loss: 0.0504 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.05039 to 0.04883, saving model to best.model\n",
      "0s - loss: 0.1790 - acc: 0.9551 - val_loss: 0.0488 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.04883 to 0.04692, saving model to best.model\n",
      "0s - loss: 0.1621 - acc: 0.9438 - val_loss: 0.0469 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.04692 to 0.04497, saving model to best.model\n",
      "0s - loss: 0.2004 - acc: 0.9213 - val_loss: 0.0450 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.04497 to 0.04335, saving model to best.model\n",
      "0s - loss: 0.1345 - acc: 0.9663 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.04335 to 0.04179, saving model to best.model\n",
      "0s - loss: 0.1727 - acc: 0.9551 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.04179 to 0.04062, saving model to best.model\n",
      "0s - loss: 0.1788 - acc: 0.9551 - val_loss: 0.0406 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.04062 to 0.03976, saving model to best.model\n",
      "0s - loss: 0.1121 - acc: 0.9888 - val_loss: 0.0398 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.03976 to 0.03874, saving model to best.model\n",
      "0s - loss: 0.1100 - acc: 0.9775 - val_loss: 0.0387 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.03874 to 0.03824, saving model to best.model\n",
      "0s - loss: 0.1488 - acc: 0.9438 - val_loss: 0.0382 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.03824 to 0.03819, saving model to best.model\n",
      "0s - loss: 0.1539 - acc: 0.9663 - val_loss: 0.0382 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.1636 - acc: 0.9438 - val_loss: 0.0385 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.1899 - acc: 0.9326 - val_loss: 0.0387 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.1514 - acc: 0.9663 - val_loss: 0.0391 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.0919 - acc: 0.9888 - val_loss: 0.0392 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.1442 - acc: 0.9438 - val_loss: 0.0391 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.1263 - acc: 0.9551 - val_loss: 0.0392 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.0748 - acc: 0.9888 - val_loss: 0.0394 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.1579 - acc: 0.9326 - val_loss: 0.0389 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.03819 to 0.03815, saving model to best.model\n",
      "0s - loss: 0.1170 - acc: 0.9663 - val_loss: 0.0381 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.03815 to 0.03792, saving model to best.model\n",
      "0s - loss: 0.0864 - acc: 0.9888 - val_loss: 0.0379 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.1265 - acc: 0.9775 - val_loss: 0.0383 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.1328 - acc: 0.9438 - val_loss: 0.0387 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.10195, saving model to best.model\n",
      "0s - loss: 1.3442 - acc: 0.2247 - val_loss: 1.1019 - val_acc: 0.3043\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.2221 - acc: 0.2809 - val_loss: 1.1144 - val_acc: 0.3043\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.3129 - acc: 0.2809 - val_loss: 1.1330 - val_acc: 0.3043\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2009 - acc: 0.3933 - val_loss: 1.1484 - val_acc: 0.3043\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.1887 - acc: 0.4494 - val_loss: 1.1551 - val_acc: 0.3043\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1125 - acc: 0.4045 - val_loss: 1.1609 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1984 - acc: 0.4157 - val_loss: 1.1651 - val_acc: 0.3043\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2233 - acc: 0.3596 - val_loss: 1.1675 - val_acc: 0.3043\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1208 - acc: 0.4607 - val_loss: 1.1632 - val_acc: 0.3043\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2260 - acc: 0.3371 - val_loss: 1.1562 - val_acc: 0.3043\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1581 - acc: 0.4045 - val_loss: 1.1470 - val_acc: 0.3043\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1292 - acc: 0.4157 - val_loss: 1.1359 - val_acc: 0.3043\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1092 - acc: 0.4045 - val_loss: 1.1251 - val_acc: 0.3043\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2811 - acc: 0.3483 - val_loss: 1.1152 - val_acc: 0.3043\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1762 - acc: 0.3820 - val_loss: 1.1077 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1888 - acc: 0.2921 - val_loss: 1.1021 - val_acc: 0.3043\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.10195 to 1.09794, saving model to best.model\n",
      "0s - loss: 1.2390 - acc: 0.2809 - val_loss: 1.0979 - val_acc: 0.3043\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.09794 to 1.09431, saving model to best.model\n",
      "0s - loss: 1.1230 - acc: 0.3820 - val_loss: 1.0943 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.09431 to 1.09096, saving model to best.model\n",
      "0s - loss: 1.1177 - acc: 0.3933 - val_loss: 1.0910 - val_acc: 0.3043\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.09096 to 1.08807, saving model to best.model\n",
      "0s - loss: 1.1703 - acc: 0.3596 - val_loss: 1.0881 - val_acc: 0.3043\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.08807 to 1.08665, saving model to best.model\n",
      "0s - loss: 1.2960 - acc: 0.2921 - val_loss: 1.0866 - val_acc: 0.3043\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.08665 to 1.08593, saving model to best.model\n",
      "0s - loss: 1.0640 - acc: 0.4607 - val_loss: 1.0859 - val_acc: 0.3043\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2117 - acc: 0.3371 - val_loss: 1.0859 - val_acc: 0.3043\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1847 - acc: 0.3146 - val_loss: 1.0869 - val_acc: 0.3043\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1254 - acc: 0.3483 - val_loss: 1.0882 - val_acc: 0.3043\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.0897 - acc: 0.4157 - val_loss: 1.0892 - val_acc: 0.3043\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1477 - acc: 0.3483 - val_loss: 1.0905 - val_acc: 0.3043\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.0903 - acc: 0.4045 - val_loss: 1.0908 - val_acc: 0.3043\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1147 - acc: 0.4270 - val_loss: 1.0901 - val_acc: 0.3043\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1020 - acc: 0.4157 - val_loss: 1.0872 - val_acc: 0.3043\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.08593 to 1.08347, saving model to best.model\n",
      "0s - loss: 1.1886 - acc: 0.3820 - val_loss: 1.0835 - val_acc: 0.3043\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.08347 to 1.07842, saving model to best.model\n",
      "0s - loss: 1.0225 - acc: 0.4831 - val_loss: 1.0784 - val_acc: 0.3043\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.07842 to 1.07305, saving model to best.model\n",
      "0s - loss: 1.1907 - acc: 0.3933 - val_loss: 1.0730 - val_acc: 0.3043\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.07305 to 1.06857, saving model to best.model\n",
      "0s - loss: 1.0822 - acc: 0.4157 - val_loss: 1.0686 - val_acc: 0.3043\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.06857 to 1.06401, saving model to best.model\n",
      "0s - loss: 1.1212 - acc: 0.4045 - val_loss: 1.0640 - val_acc: 0.3043\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.06401 to 1.05986, saving model to best.model\n",
      "0s - loss: 1.1187 - acc: 0.3820 - val_loss: 1.0599 - val_acc: 0.3043\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.05986 to 1.05609, saving model to best.model\n",
      "0s - loss: 1.1129 - acc: 0.4045 - val_loss: 1.0561 - val_acc: 0.4348\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.05609 to 1.05216, saving model to best.model\n",
      "0s - loss: 1.0279 - acc: 0.4719 - val_loss: 1.0522 - val_acc: 0.5652\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.05216 to 1.04867, saving model to best.model\n",
      "0s - loss: 1.0421 - acc: 0.4270 - val_loss: 1.0487 - val_acc: 0.5652\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.04867 to 1.04541, saving model to best.model\n",
      "0s - loss: 1.1137 - acc: 0.4382 - val_loss: 1.0454 - val_acc: 0.5652\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.04541 to 1.04248, saving model to best.model\n",
      "0s - loss: 1.0830 - acc: 0.4270 - val_loss: 1.0425 - val_acc: 0.5652\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.04248 to 1.03921, saving model to best.model\n",
      "0s - loss: 1.1565 - acc: 0.3708 - val_loss: 1.0392 - val_acc: 0.5652\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.03921 to 1.03584, saving model to best.model\n",
      "0s - loss: 1.0949 - acc: 0.4831 - val_loss: 1.0358 - val_acc: 0.5652\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.03584 to 1.03198, saving model to best.model\n",
      "0s - loss: 1.0757 - acc: 0.3596 - val_loss: 1.0320 - val_acc: 0.5652\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.03198 to 1.02870, saving model to best.model\n",
      "0s - loss: 1.1170 - acc: 0.4270 - val_loss: 1.0287 - val_acc: 0.5652\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.02870 to 1.02609, saving model to best.model\n",
      "0s - loss: 1.0195 - acc: 0.4607 - val_loss: 1.0261 - val_acc: 0.5652\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.02609 to 1.02362, saving model to best.model\n",
      "0s - loss: 1.0392 - acc: 0.4270 - val_loss: 1.0236 - val_acc: 0.5652\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.02362 to 1.02117, saving model to best.model\n",
      "0s - loss: 1.0193 - acc: 0.4494 - val_loss: 1.0212 - val_acc: 0.5652\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.02117 to 1.01811, saving model to best.model\n",
      "0s - loss: 1.0391 - acc: 0.4944 - val_loss: 1.0181 - val_acc: 0.5652\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.01811 to 1.01460, saving model to best.model\n",
      "0s - loss: 1.0366 - acc: 0.4719 - val_loss: 1.0146 - val_acc: 0.5652\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.01460 to 1.01039, saving model to best.model\n",
      "0s - loss: 1.1192 - acc: 0.3933 - val_loss: 1.0104 - val_acc: 0.5652\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.01039 to 1.00567, saving model to best.model\n",
      "0s - loss: 1.0476 - acc: 0.4831 - val_loss: 1.0057 - val_acc: 0.6087\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.00567 to 1.00085, saving model to best.model\n",
      "0s - loss: 1.0064 - acc: 0.4607 - val_loss: 1.0008 - val_acc: 0.6087\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.00085 to 0.99587, saving model to best.model\n",
      "0s - loss: 0.9794 - acc: 0.5618 - val_loss: 0.9959 - val_acc: 0.6087\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.99587 to 0.99089, saving model to best.model\n",
      "0s - loss: 1.0689 - acc: 0.4157 - val_loss: 0.9909 - val_acc: 0.6087\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.99089 to 0.98475, saving model to best.model\n",
      "0s - loss: 1.0520 - acc: 0.4944 - val_loss: 0.9848 - val_acc: 0.6522\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.98475 to 0.97911, saving model to best.model\n",
      "0s - loss: 0.9660 - acc: 0.5393 - val_loss: 0.9791 - val_acc: 0.6522\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.97911 to 0.97320, saving model to best.model\n",
      "0s - loss: 1.0532 - acc: 0.4382 - val_loss: 0.9732 - val_acc: 0.6522\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.97320 to 0.96618, saving model to best.model\n",
      "0s - loss: 0.9192 - acc: 0.6180 - val_loss: 0.9662 - val_acc: 0.6957\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.96618 to 0.95868, saving model to best.model\n",
      "0s - loss: 0.9469 - acc: 0.5393 - val_loss: 0.9587 - val_acc: 0.6957\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.95868 to 0.94963, saving model to best.model\n",
      "0s - loss: 0.9885 - acc: 0.5506 - val_loss: 0.9496 - val_acc: 0.6957\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.94963 to 0.94076, saving model to best.model\n",
      "0s - loss: 0.9336 - acc: 0.5730 - val_loss: 0.9408 - val_acc: 0.6957\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.94076 to 0.93165, saving model to best.model\n",
      "0s - loss: 0.9376 - acc: 0.5843 - val_loss: 0.9317 - val_acc: 0.6957\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.93165 to 0.92207, saving model to best.model\n",
      "0s - loss: 0.9683 - acc: 0.5169 - val_loss: 0.9221 - val_acc: 0.6957\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.92207 to 0.91238, saving model to best.model\n",
      "0s - loss: 0.8948 - acc: 0.6517 - val_loss: 0.9124 - val_acc: 0.6957\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.91238 to 0.90160, saving model to best.model\n",
      "0s - loss: 0.9413 - acc: 0.4944 - val_loss: 0.9016 - val_acc: 0.6957\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.90160 to 0.89140, saving model to best.model\n",
      "0s - loss: 0.9063 - acc: 0.5955 - val_loss: 0.8914 - val_acc: 0.6957\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.89140 to 0.88136, saving model to best.model\n",
      "0s - loss: 0.9577 - acc: 0.5730 - val_loss: 0.8814 - val_acc: 0.6957\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.88136 to 0.87113, saving model to best.model\n",
      "0s - loss: 0.9898 - acc: 0.4831 - val_loss: 0.8711 - val_acc: 0.6957\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.87113 to 0.86041, saving model to best.model\n",
      "0s - loss: 0.8797 - acc: 0.6067 - val_loss: 0.8604 - val_acc: 0.6957\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.86041 to 0.84967, saving model to best.model\n",
      "0s - loss: 0.9249 - acc: 0.6067 - val_loss: 0.8497 - val_acc: 0.6957\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.84967 to 0.83869, saving model to best.model\n",
      "0s - loss: 0.9742 - acc: 0.5056 - val_loss: 0.8387 - val_acc: 0.6957\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.83869 to 0.82792, saving model to best.model\n",
      "0s - loss: 0.9173 - acc: 0.5618 - val_loss: 0.8279 - val_acc: 0.6957\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.82792 to 0.81650, saving model to best.model\n",
      "0s - loss: 0.8580 - acc: 0.5618 - val_loss: 0.8165 - val_acc: 0.6957\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.81650 to 0.80469, saving model to best.model\n",
      "0s - loss: 0.8640 - acc: 0.6404 - val_loss: 0.8047 - val_acc: 0.6957\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.80469 to 0.79243, saving model to best.model\n",
      "0s - loss: 0.8163 - acc: 0.6517 - val_loss: 0.7924 - val_acc: 0.6957\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.79243 to 0.78027, saving model to best.model\n",
      "0s - loss: 0.7581 - acc: 0.7079 - val_loss: 0.7803 - val_acc: 0.7826\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.78027 to 0.76911, saving model to best.model\n",
      "0s - loss: 0.8539 - acc: 0.6404 - val_loss: 0.7691 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.76911 to 0.75787, saving model to best.model\n",
      "0s - loss: 0.8057 - acc: 0.6742 - val_loss: 0.7579 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.75787 to 0.74750, saving model to best.model\n",
      "0s - loss: 0.8115 - acc: 0.6629 - val_loss: 0.7475 - val_acc: 0.6957\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.74750 to 0.73774, saving model to best.model\n",
      "0s - loss: 0.8389 - acc: 0.6180 - val_loss: 0.7377 - val_acc: 0.6957\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.73774 to 0.72782, saving model to best.model\n",
      "0s - loss: 0.8096 - acc: 0.6517 - val_loss: 0.7278 - val_acc: 0.6957\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.72782 to 0.71722, saving model to best.model\n",
      "0s - loss: 0.7846 - acc: 0.6854 - val_loss: 0.7172 - val_acc: 0.6957\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.71722 to 0.70587, saving model to best.model\n",
      "0s - loss: 0.7437 - acc: 0.6629 - val_loss: 0.7059 - val_acc: 0.6957\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.70587 to 0.69444, saving model to best.model\n",
      "0s - loss: 0.7197 - acc: 0.6966 - val_loss: 0.6944 - val_acc: 0.6957\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69444 to 0.68227, saving model to best.model\n",
      "0s - loss: 0.7441 - acc: 0.6629 - val_loss: 0.6823 - val_acc: 0.6957\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68227 to 0.66888, saving model to best.model\n",
      "0s - loss: 0.7618 - acc: 0.7079 - val_loss: 0.6689 - val_acc: 0.7826\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.66888 to 0.65443, saving model to best.model\n",
      "0s - loss: 0.7405 - acc: 0.7416 - val_loss: 0.6544 - val_acc: 0.7826\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.65443 to 0.63950, saving model to best.model\n",
      "0s - loss: 0.7143 - acc: 0.6517 - val_loss: 0.6395 - val_acc: 0.7826\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.63950 to 0.62464, saving model to best.model\n",
      "0s - loss: 0.7397 - acc: 0.6629 - val_loss: 0.6246 - val_acc: 0.8261\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.62464 to 0.61022, saving model to best.model\n",
      "0s - loss: 0.6835 - acc: 0.6966 - val_loss: 0.6102 - val_acc: 0.8261\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.61022 to 0.59703, saving model to best.model\n",
      "0s - loss: 0.6156 - acc: 0.7978 - val_loss: 0.5970 - val_acc: 0.8261\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.59703 to 0.58417, saving model to best.model\n",
      "0s - loss: 0.7149 - acc: 0.6742 - val_loss: 0.5842 - val_acc: 0.8261\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.58417 to 0.57048, saving model to best.model\n",
      "0s - loss: 0.7124 - acc: 0.7079 - val_loss: 0.5705 - val_acc: 0.8261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.57048 to 0.55686, saving model to best.model\n",
      "0s - loss: 0.5972 - acc: 0.7978 - val_loss: 0.5569 - val_acc: 0.8261\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.55686 to 0.54431, saving model to best.model\n",
      "0s - loss: 0.6211 - acc: 0.7528 - val_loss: 0.5443 - val_acc: 0.8261\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.54431 to 0.53243, saving model to best.model\n",
      "0s - loss: 0.5977 - acc: 0.7640 - val_loss: 0.5324 - val_acc: 0.8261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.53243 to 0.52012, saving model to best.model\n",
      "0s - loss: 0.5653 - acc: 0.8090 - val_loss: 0.5201 - val_acc: 0.8261\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.52012 to 0.50979, saving model to best.model\n",
      "0s - loss: 0.5565 - acc: 0.7978 - val_loss: 0.5098 - val_acc: 0.8261\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.50979 to 0.49917, saving model to best.model\n",
      "0s - loss: 0.5176 - acc: 0.8315 - val_loss: 0.4992 - val_acc: 0.8261\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.49917 to 0.48874, saving model to best.model\n",
      "0s - loss: 0.5820 - acc: 0.7753 - val_loss: 0.4887 - val_acc: 0.8261\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.48874 to 0.47916, saving model to best.model\n",
      "0s - loss: 0.5453 - acc: 0.7978 - val_loss: 0.4792 - val_acc: 0.8261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.47916 to 0.47049, saving model to best.model\n",
      "0s - loss: 0.5682 - acc: 0.7978 - val_loss: 0.4705 - val_acc: 0.8261\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.47049 to 0.46133, saving model to best.model\n",
      "0s - loss: 0.5343 - acc: 0.8090 - val_loss: 0.4613 - val_acc: 0.8261\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.46133 to 0.45268, saving model to best.model\n",
      "0s - loss: 0.4992 - acc: 0.8202 - val_loss: 0.4527 - val_acc: 0.8261\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.45268 to 0.44445, saving model to best.model\n",
      "0s - loss: 0.5513 - acc: 0.7303 - val_loss: 0.4444 - val_acc: 0.8261\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.44445 to 0.43662, saving model to best.model\n",
      "0s - loss: 0.5240 - acc: 0.7865 - val_loss: 0.4366 - val_acc: 0.8261\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.43662 to 0.42869, saving model to best.model\n",
      "0s - loss: 0.5528 - acc: 0.7865 - val_loss: 0.4287 - val_acc: 0.8261\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.42869 to 0.42112, saving model to best.model\n",
      "0s - loss: 0.4808 - acc: 0.8652 - val_loss: 0.4211 - val_acc: 0.8261\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.42112 to 0.41425, saving model to best.model\n",
      "0s - loss: 0.4977 - acc: 0.8652 - val_loss: 0.4143 - val_acc: 0.8261\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.41425 to 0.40766, saving model to best.model\n",
      "0s - loss: 0.4712 - acc: 0.8202 - val_loss: 0.4077 - val_acc: 0.8261\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.40766 to 0.40051, saving model to best.model\n",
      "0s - loss: 0.4685 - acc: 0.8539 - val_loss: 0.4005 - val_acc: 0.8261\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.40051 to 0.39381, saving model to best.model\n",
      "0s - loss: 0.4416 - acc: 0.8315 - val_loss: 0.3938 - val_acc: 0.8261\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.39381 to 0.38612, saving model to best.model\n",
      "0s - loss: 0.4437 - acc: 0.8539 - val_loss: 0.3861 - val_acc: 0.8261\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.38612 to 0.37731, saving model to best.model\n",
      "0s - loss: 0.5381 - acc: 0.7416 - val_loss: 0.3773 - val_acc: 0.8696\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.37731 to 0.36872, saving model to best.model\n",
      "0s - loss: 0.4091 - acc: 0.8427 - val_loss: 0.3687 - val_acc: 0.8696\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.36872 to 0.35972, saving model to best.model\n",
      "0s - loss: 0.4182 - acc: 0.8315 - val_loss: 0.3597 - val_acc: 0.8696\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.35972 to 0.35209, saving model to best.model\n",
      "0s - loss: 0.4266 - acc: 0.8539 - val_loss: 0.3521 - val_acc: 0.8696\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.35209 to 0.34572, saving model to best.model\n",
      "0s - loss: 0.3962 - acc: 0.8539 - val_loss: 0.3457 - val_acc: 0.8696\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.34572 to 0.33979, saving model to best.model\n",
      "0s - loss: 0.4165 - acc: 0.8315 - val_loss: 0.3398 - val_acc: 0.8696\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.33979 to 0.33279, saving model to best.model\n",
      "0s - loss: 0.4628 - acc: 0.8315 - val_loss: 0.3328 - val_acc: 0.8696\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.33279 to 0.32761, saving model to best.model\n",
      "0s - loss: 0.4053 - acc: 0.8090 - val_loss: 0.3276 - val_acc: 0.8696\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.32761 to 0.32211, saving model to best.model\n",
      "0s - loss: 0.3534 - acc: 0.8989 - val_loss: 0.3221 - val_acc: 0.8696\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.32211 to 0.31716, saving model to best.model\n",
      "0s - loss: 0.3811 - acc: 0.8876 - val_loss: 0.3172 - val_acc: 0.8696\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.31716 to 0.31275, saving model to best.model\n",
      "0s - loss: 0.3374 - acc: 0.8764 - val_loss: 0.3128 - val_acc: 0.8696\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.31275 to 0.30920, saving model to best.model\n",
      "0s - loss: 0.3667 - acc: 0.8427 - val_loss: 0.3092 - val_acc: 0.8696\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.30920 to 0.30587, saving model to best.model\n",
      "0s - loss: 0.3370 - acc: 0.8876 - val_loss: 0.3059 - val_acc: 0.8696\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.30587 to 0.30318, saving model to best.model\n",
      "0s - loss: 0.3551 - acc: 0.8539 - val_loss: 0.3032 - val_acc: 0.8696\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.30318 to 0.30126, saving model to best.model\n",
      "0s - loss: 0.3168 - acc: 0.8876 - val_loss: 0.3013 - val_acc: 0.8696\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.30126 to 0.29969, saving model to best.model\n",
      "0s - loss: 0.3736 - acc: 0.8427 - val_loss: 0.2997 - val_acc: 0.8696\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.29969 to 0.29766, saving model to best.model\n",
      "0s - loss: 0.3080 - acc: 0.8764 - val_loss: 0.2977 - val_acc: 0.8696\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.29766 to 0.29449, saving model to best.model\n",
      "0s - loss: 0.3188 - acc: 0.8652 - val_loss: 0.2945 - val_acc: 0.8696\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.29449 to 0.29249, saving model to best.model\n",
      "0s - loss: 0.3329 - acc: 0.8876 - val_loss: 0.2925 - val_acc: 0.8696\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.29249 to 0.28865, saving model to best.model\n",
      "0s - loss: 0.3293 - acc: 0.8876 - val_loss: 0.2887 - val_acc: 0.8696\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.28865 to 0.28421, saving model to best.model\n",
      "0s - loss: 0.3040 - acc: 0.8876 - val_loss: 0.2842 - val_acc: 0.8696\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.28421 to 0.27775, saving model to best.model\n",
      "0s - loss: 0.3070 - acc: 0.8764 - val_loss: 0.2777 - val_acc: 0.8696\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.27775 to 0.27131, saving model to best.model\n",
      "0s - loss: 0.2692 - acc: 0.9438 - val_loss: 0.2713 - val_acc: 0.8696\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.27131 to 0.26556, saving model to best.model\n",
      "0s - loss: 0.3006 - acc: 0.9213 - val_loss: 0.2656 - val_acc: 0.8696\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.26556 to 0.26008, saving model to best.model\n",
      "0s - loss: 0.2788 - acc: 0.8652 - val_loss: 0.2601 - val_acc: 0.8696\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.26008 to 0.25471, saving model to best.model\n",
      "0s - loss: 0.2956 - acc: 0.8876 - val_loss: 0.2547 - val_acc: 0.8696\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.25471 to 0.24934, saving model to best.model\n",
      "0s - loss: 0.2944 - acc: 0.9213 - val_loss: 0.2493 - val_acc: 0.8696\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.24934 to 0.24415, saving model to best.model\n",
      "0s - loss: 0.3137 - acc: 0.8876 - val_loss: 0.2441 - val_acc: 0.8696\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.24415 to 0.23922, saving model to best.model\n",
      "0s - loss: 0.2314 - acc: 0.9438 - val_loss: 0.2392 - val_acc: 0.8696\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.23922 to 0.23512, saving model to best.model\n",
      "0s - loss: 0.2823 - acc: 0.8876 - val_loss: 0.2351 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.23512 to 0.23263, saving model to best.model\n",
      "0s - loss: 0.3271 - acc: 0.8764 - val_loss: 0.2326 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.23263 to 0.23141, saving model to best.model\n",
      "0s - loss: 0.2676 - acc: 0.9326 - val_loss: 0.2314 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.2472 - acc: 0.9326 - val_loss: 0.2315 - val_acc: 0.8696\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.2230 - acc: 0.9101 - val_loss: 0.2315 - val_acc: 0.8696\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.3064 - acc: 0.8539 - val_loss: 0.2324 - val_acc: 0.8696\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.2018 - acc: 0.9438 - val_loss: 0.2348 - val_acc: 0.8696\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.2549 - acc: 0.9326 - val_loss: 0.2348 - val_acc: 0.8696\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.2529 - acc: 0.9326 - val_loss: 0.2356 - val_acc: 0.8696\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.2243 - acc: 0.9213 - val_loss: 0.2351 - val_acc: 0.8696\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.2228 - acc: 0.9551 - val_loss: 0.2334 - val_acc: 0.8696\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.23141 to 0.23037, saving model to best.model\n",
      "0s - loss: 0.1839 - acc: 0.9326 - val_loss: 0.2304 - val_acc: 0.8696\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.23037 to 0.22569, saving model to best.model\n",
      "0s - loss: 0.2161 - acc: 0.9326 - val_loss: 0.2257 - val_acc: 0.8696\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.22569 to 0.22148, saving model to best.model\n",
      "0s - loss: 0.2019 - acc: 0.9438 - val_loss: 0.2215 - val_acc: 0.8696\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.22148 to 0.21638, saving model to best.model\n",
      "0s - loss: 0.1929 - acc: 0.9438 - val_loss: 0.2164 - val_acc: 0.8696\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.21638 to 0.21286, saving model to best.model\n",
      "0s - loss: 0.1814 - acc: 0.9775 - val_loss: 0.2129 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.21286 to 0.21082, saving model to best.model\n",
      "0s - loss: 0.1983 - acc: 0.9551 - val_loss: 0.2108 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.21082 to 0.20726, saving model to best.model\n",
      "0s - loss: 0.2229 - acc: 0.9101 - val_loss: 0.2073 - val_acc: 0.9130\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.20726 to 0.20367, saving model to best.model\n",
      "0s - loss: 0.1613 - acc: 0.9438 - val_loss: 0.2037 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.20367 to 0.20075, saving model to best.model\n",
      "0s - loss: 0.1612 - acc: 0.9438 - val_loss: 0.2007 - val_acc: 0.9130\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.20075 to 0.19534, saving model to best.model\n",
      "0s - loss: 0.2189 - acc: 0.9326 - val_loss: 0.1953 - val_acc: 0.9130\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.19534 to 0.19092, saving model to best.model\n",
      "0s - loss: 0.1481 - acc: 0.9663 - val_loss: 0.1909 - val_acc: 0.9130\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.19092 to 0.18663, saving model to best.model\n",
      "0s - loss: 0.1629 - acc: 0.9551 - val_loss: 0.1866 - val_acc: 0.9130\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.18663 to 0.18156, saving model to best.model\n",
      "0s - loss: 0.1982 - acc: 0.9101 - val_loss: 0.1816 - val_acc: 0.9130\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.18156 to 0.17765, saving model to best.model\n",
      "0s - loss: 0.1867 - acc: 0.9326 - val_loss: 0.1776 - val_acc: 0.9130\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.17765 to 0.17503, saving model to best.model\n",
      "0s - loss: 0.1681 - acc: 0.9663 - val_loss: 0.1750 - val_acc: 0.9130\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.17503 to 0.17321, saving model to best.model\n",
      "0s - loss: 0.1987 - acc: 0.9101 - val_loss: 0.1732 - val_acc: 0.9130\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.17321 to 0.17039, saving model to best.model\n",
      "0s - loss: 0.1909 - acc: 0.9438 - val_loss: 0.1704 - val_acc: 0.9130\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.17039 to 0.16828, saving model to best.model\n",
      "0s - loss: 0.1361 - acc: 0.9663 - val_loss: 0.1683 - val_acc: 0.9130\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.16828 to 0.16758, saving model to best.model\n",
      "0s - loss: 0.1707 - acc: 0.9438 - val_loss: 0.1676 - val_acc: 0.9130\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.16758 to 0.16633, saving model to best.model\n",
      "0s - loss: 0.1735 - acc: 0.9551 - val_loss: 0.1663 - val_acc: 0.9130\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.16633 to 0.16480, saving model to best.model\n",
      "0s - loss: 0.1283 - acc: 0.9775 - val_loss: 0.1648 - val_acc: 0.9130\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.16480 to 0.16450, saving model to best.model\n",
      "0s - loss: 0.1845 - acc: 0.9326 - val_loss: 0.1645 - val_acc: 0.9130\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.1707 - acc: 0.9438 - val_loss: 0.1652 - val_acc: 0.9130\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.1470 - acc: 0.9551 - val_loss: 0.1672 - val_acc: 0.9130\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.1060 - acc: 0.9663 - val_loss: 0.1688 - val_acc: 0.9130\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.1803 - acc: 0.9775 - val_loss: 0.1707 - val_acc: 0.9130\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.1663 - acc: 0.9551 - val_loss: 0.1719 - val_acc: 0.9130\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.1342 - acc: 0.9663 - val_loss: 0.1707 - val_acc: 0.9130\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.1692 - acc: 0.9326 - val_loss: 0.1676 - val_acc: 0.9130\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.1533 - acc: 0.9551 - val_loss: 0.1650 - val_acc: 0.9130\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.16450 to 0.16193, saving model to best.model\n",
      "0s - loss: 0.1158 - acc: 0.9775 - val_loss: 0.1619 - val_acc: 0.9130\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.16193 to 0.15921, saving model to best.model\n",
      "0s - loss: 0.1174 - acc: 0.9663 - val_loss: 0.1592 - val_acc: 0.9130\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.15921 to 0.15696, saving model to best.model\n",
      "0s - loss: 0.1128 - acc: 0.9775 - val_loss: 0.1570 - val_acc: 0.9130\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.15696 to 0.15431, saving model to best.model\n",
      "0s - loss: 0.1514 - acc: 0.9551 - val_loss: 0.1543 - val_acc: 0.9130\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.15431 to 0.15129, saving model to best.model\n",
      "0s - loss: 0.1222 - acc: 0.9663 - val_loss: 0.1513 - val_acc: 0.9130\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.15129 to 0.14887, saving model to best.model\n",
      "0s - loss: 0.1228 - acc: 0.9663 - val_loss: 0.1489 - val_acc: 0.9130\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.14887 to 0.14775, saving model to best.model\n",
      "0s - loss: 0.1537 - acc: 0.9551 - val_loss: 0.1477 - val_acc: 0.9130\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.14775 to 0.14509, saving model to best.model\n",
      "0s - loss: 0.1285 - acc: 0.9663 - val_loss: 0.1451 - val_acc: 0.9130\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.14509 to 0.14263, saving model to best.model\n",
      "0s - loss: 0.1223 - acc: 0.9663 - val_loss: 0.1426 - val_acc: 0.9130\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.14263 to 0.13942, saving model to best.model\n",
      "0s - loss: 0.0907 - acc: 0.9775 - val_loss: 0.1394 - val_acc: 0.9130\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.13942 to 0.13660, saving model to best.model\n",
      "0s - loss: 0.1352 - acc: 0.9551 - val_loss: 0.1366 - val_acc: 0.9130\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.13660 to 0.13425, saving model to best.model\n",
      "0s - loss: 0.1708 - acc: 0.8989 - val_loss: 0.1342 - val_acc: 0.9130\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.13425 to 0.13189, saving model to best.model\n",
      "0s - loss: 0.0843 - acc: 1.0000 - val_loss: 0.1319 - val_acc: 0.9130\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.13189 to 0.12956, saving model to best.model\n",
      "0s - loss: 0.1458 - acc: 0.9551 - val_loss: 0.1296 - val_acc: 0.9130\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.12956 to 0.12669, saving model to best.model\n",
      "0s - loss: 0.1081 - acc: 0.9775 - val_loss: 0.1267 - val_acc: 0.9130\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.12669 to 0.12408, saving model to best.model\n",
      "0s - loss: 0.1268 - acc: 0.9663 - val_loss: 0.1241 - val_acc: 0.9130\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.17845, saving model to best.model\n",
      "0s - loss: 1.3550 - acc: 0.2584 - val_loss: 1.1785 - val_acc: 0.2174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.17845 to 1.14846, saving model to best.model\n",
      "0s - loss: 1.3900 - acc: 0.2247 - val_loss: 1.1485 - val_acc: 0.2174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.14846 to 1.12396, saving model to best.model\n",
      "0s - loss: 1.2279 - acc: 0.3146 - val_loss: 1.1240 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.12396 to 1.10836, saving model to best.model\n",
      "0s - loss: 1.2791 - acc: 0.3371 - val_loss: 1.1084 - val_acc: 0.4348\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.10836 to 1.10459, saving model to best.model\n",
      "0s - loss: 1.2544 - acc: 0.3483 - val_loss: 1.1046 - val_acc: 0.3478\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1336 - acc: 0.4045 - val_loss: 1.1055 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.3140 - acc: 0.2921 - val_loss: 1.1087 - val_acc: 0.2174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1624 - acc: 0.4494 - val_loss: 1.1137 - val_acc: 0.2174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1545 - acc: 0.3371 - val_loss: 1.1200 - val_acc: 0.2174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.3472 - acc: 0.3146 - val_loss: 1.1217 - val_acc: 0.2174\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2236 - acc: 0.3933 - val_loss: 1.1234 - val_acc: 0.2174\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1662 - acc: 0.3146 - val_loss: 1.1234 - val_acc: 0.2174\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1609 - acc: 0.4045 - val_loss: 1.1237 - val_acc: 0.2174\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2700 - acc: 0.4157 - val_loss: 1.1250 - val_acc: 0.2174\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2412 - acc: 0.3258 - val_loss: 1.1225 - val_acc: 0.2174\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2011 - acc: 0.4494 - val_loss: 1.1170 - val_acc: 0.2174\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2305 - acc: 0.3371 - val_loss: 1.1152 - val_acc: 0.2174\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1213 - acc: 0.3820 - val_loss: 1.1176 - val_acc: 0.2174\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1498 - acc: 0.3708 - val_loss: 1.1164 - val_acc: 0.2609\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2008 - acc: 0.3596 - val_loss: 1.1115 - val_acc: 0.2609\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.3158 - acc: 0.2584 - val_loss: 1.1090 - val_acc: 0.3478\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.2697 - acc: 0.2697 - val_loss: 1.1064 - val_acc: 0.4348\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.10459 to 1.10258, saving model to best.model\n",
      "0s - loss: 1.3324 - acc: 0.2360 - val_loss: 1.1026 - val_acc: 0.4348\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.10258 to 1.09945, saving model to best.model\n",
      "0s - loss: 1.0891 - acc: 0.4494 - val_loss: 1.0995 - val_acc: 0.4348\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.09945 to 1.09820, saving model to best.model\n",
      "0s - loss: 1.2715 - acc: 0.3258 - val_loss: 1.0982 - val_acc: 0.4348\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.09820 to 1.09330, saving model to best.model\n",
      "0s - loss: 1.2424 - acc: 0.3483 - val_loss: 1.0933 - val_acc: 0.4348\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.09330 to 1.08778, saving model to best.model\n",
      "0s - loss: 1.2510 - acc: 0.3708 - val_loss: 1.0878 - val_acc: 0.4348\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.08778 to 1.08171, saving model to best.model\n",
      "0s - loss: 1.1649 - acc: 0.3820 - val_loss: 1.0817 - val_acc: 0.4348\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.08171 to 1.07754, saving model to best.model\n",
      "0s - loss: 1.0686 - acc: 0.4382 - val_loss: 1.0775 - val_acc: 0.4348\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.07754 to 1.07437, saving model to best.model\n",
      "0s - loss: 1.2111 - acc: 0.3933 - val_loss: 1.0744 - val_acc: 0.4348\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.07437 to 1.07211, saving model to best.model\n",
      "0s - loss: 1.2094 - acc: 0.3820 - val_loss: 1.0721 - val_acc: 0.4348\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.07211 to 1.07099, saving model to best.model\n",
      "0s - loss: 1.0373 - acc: 0.5393 - val_loss: 1.0710 - val_acc: 0.4348\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.07099 to 1.07005, saving model to best.model\n",
      "0s - loss: 1.1239 - acc: 0.4045 - val_loss: 1.0700 - val_acc: 0.4348\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.07005 to 1.06787, saving model to best.model\n",
      "0s - loss: 1.1128 - acc: 0.4270 - val_loss: 1.0679 - val_acc: 0.4348\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.06787 to 1.06460, saving model to best.model\n",
      "0s - loss: 1.0714 - acc: 0.5056 - val_loss: 1.0646 - val_acc: 0.4348\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.06460 to 1.06283, saving model to best.model\n",
      "0s - loss: 1.1272 - acc: 0.3708 - val_loss: 1.0628 - val_acc: 0.4348\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.06283 to 1.06265, saving model to best.model\n",
      "0s - loss: 1.2152 - acc: 0.3034 - val_loss: 1.0626 - val_acc: 0.4348\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.06265 to 1.06137, saving model to best.model\n",
      "0s - loss: 1.1316 - acc: 0.3933 - val_loss: 1.0614 - val_acc: 0.4348\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.06137 to 1.05761, saving model to best.model\n",
      "0s - loss: 1.0185 - acc: 0.4719 - val_loss: 1.0576 - val_acc: 0.4348\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.05761 to 1.05468, saving model to best.model\n",
      "0s - loss: 1.1167 - acc: 0.3820 - val_loss: 1.0547 - val_acc: 0.4348\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.05468 to 1.05251, saving model to best.model\n",
      "0s - loss: 1.1671 - acc: 0.3820 - val_loss: 1.0525 - val_acc: 0.4348\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.05251 to 1.05012, saving model to best.model\n",
      "0s - loss: 1.1315 - acc: 0.3933 - val_loss: 1.0501 - val_acc: 0.4783\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.05012 to 1.05012, saving model to best.model\n",
      "0s - loss: 1.1705 - acc: 0.3596 - val_loss: 1.0501 - val_acc: 0.4783\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.05012 to 1.04868, saving model to best.model\n",
      "0s - loss: 1.1502 - acc: 0.4382 - val_loss: 1.0487 - val_acc: 0.4783\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.04868 to 1.04630, saving model to best.model\n",
      "0s - loss: 1.0138 - acc: 0.5056 - val_loss: 1.0463 - val_acc: 0.4783\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.04630 to 1.04345, saving model to best.model\n",
      "0s - loss: 1.0771 - acc: 0.4719 - val_loss: 1.0434 - val_acc: 0.4783\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.04345 to 1.03936, saving model to best.model\n",
      "0s - loss: 1.0834 - acc: 0.4382 - val_loss: 1.0394 - val_acc: 0.4783\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.03936 to 1.03460, saving model to best.model\n",
      "0s - loss: 1.0655 - acc: 0.4270 - val_loss: 1.0346 - val_acc: 0.4783\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.03460 to 1.03051, saving model to best.model\n",
      "0s - loss: 1.0200 - acc: 0.5056 - val_loss: 1.0305 - val_acc: 0.4783\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.03051 to 1.02389, saving model to best.model\n",
      "0s - loss: 1.0052 - acc: 0.4831 - val_loss: 1.0239 - val_acc: 0.4783\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.02389 to 1.01702, saving model to best.model\n",
      "0s - loss: 1.0737 - acc: 0.4270 - val_loss: 1.0170 - val_acc: 0.4783\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.01702 to 1.01009, saving model to best.model\n",
      "0s - loss: 1.0326 - acc: 0.5056 - val_loss: 1.0101 - val_acc: 0.4783\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.01009 to 1.00358, saving model to best.model\n",
      "0s - loss: 0.9754 - acc: 0.5506 - val_loss: 1.0036 - val_acc: 0.5217\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.00358 to 0.99574, saving model to best.model\n",
      "0s - loss: 1.0334 - acc: 0.4382 - val_loss: 0.9957 - val_acc: 0.6522\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.99574 to 0.98803, saving model to best.model\n",
      "0s - loss: 1.0051 - acc: 0.4719 - val_loss: 0.9880 - val_acc: 0.6522\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.98803 to 0.98059, saving model to best.model\n",
      "0s - loss: 0.9661 - acc: 0.5618 - val_loss: 0.9806 - val_acc: 0.6522\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.98059 to 0.97315, saving model to best.model\n",
      "0s - loss: 1.0083 - acc: 0.5056 - val_loss: 0.9732 - val_acc: 0.6522\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.97315 to 0.96583, saving model to best.model\n",
      "0s - loss: 1.0061 - acc: 0.4831 - val_loss: 0.9658 - val_acc: 0.6522\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.96583 to 0.95983, saving model to best.model\n",
      "0s - loss: 1.0239 - acc: 0.4494 - val_loss: 0.9598 - val_acc: 0.6522\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.95983 to 0.95406, saving model to best.model\n",
      "0s - loss: 0.9299 - acc: 0.5506 - val_loss: 0.9541 - val_acc: 0.6522\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.95406 to 0.94900, saving model to best.model\n",
      "0s - loss: 0.9310 - acc: 0.5618 - val_loss: 0.9490 - val_acc: 0.6522\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.94900 to 0.94294, saving model to best.model\n",
      "0s - loss: 0.9104 - acc: 0.5618 - val_loss: 0.9429 - val_acc: 0.6522\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.94294 to 0.93803, saving model to best.model\n",
      "0s - loss: 0.9760 - acc: 0.5281 - val_loss: 0.9380 - val_acc: 0.6522\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.93803 to 0.93269, saving model to best.model\n",
      "0s - loss: 0.8998 - acc: 0.5955 - val_loss: 0.9327 - val_acc: 0.6522\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.93269 to 0.92716, saving model to best.model\n",
      "0s - loss: 0.9995 - acc: 0.5169 - val_loss: 0.9272 - val_acc: 0.6522\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.92716 to 0.92231, saving model to best.model\n",
      "0s - loss: 0.8907 - acc: 0.6180 - val_loss: 0.9223 - val_acc: 0.6522\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.92231 to 0.91593, saving model to best.model\n",
      "0s - loss: 0.9387 - acc: 0.5730 - val_loss: 0.9159 - val_acc: 0.6522\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.91593 to 0.90902, saving model to best.model\n",
      "0s - loss: 0.8753 - acc: 0.5056 - val_loss: 0.9090 - val_acc: 0.6522\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.90902 to 0.90196, saving model to best.model\n",
      "0s - loss: 0.8498 - acc: 0.5843 - val_loss: 0.9020 - val_acc: 0.6522\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.90196 to 0.89514, saving model to best.model\n",
      "0s - loss: 0.8270 - acc: 0.6742 - val_loss: 0.8951 - val_acc: 0.6522\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.89514 to 0.88687, saving model to best.model\n",
      "0s - loss: 0.8365 - acc: 0.6629 - val_loss: 0.8869 - val_acc: 0.6522\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.88687 to 0.87785, saving model to best.model\n",
      "0s - loss: 0.8420 - acc: 0.6404 - val_loss: 0.8779 - val_acc: 0.6522\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.87785 to 0.86701, saving model to best.model\n",
      "0s - loss: 0.8447 - acc: 0.6067 - val_loss: 0.8670 - val_acc: 0.6522\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.86701 to 0.85606, saving model to best.model\n",
      "0s - loss: 0.9248 - acc: 0.5843 - val_loss: 0.8561 - val_acc: 0.7391\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.85606 to 0.84411, saving model to best.model\n",
      "0s - loss: 0.7733 - acc: 0.6854 - val_loss: 0.8441 - val_acc: 0.7391\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.84411 to 0.83269, saving model to best.model\n",
      "0s - loss: 0.9232 - acc: 0.5843 - val_loss: 0.8327 - val_acc: 0.7391\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.83269 to 0.82314, saving model to best.model\n",
      "0s - loss: 0.8211 - acc: 0.6629 - val_loss: 0.8231 - val_acc: 0.7826\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.82314 to 0.81321, saving model to best.model\n",
      "0s - loss: 0.7994 - acc: 0.6629 - val_loss: 0.8132 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.81321 to 0.80195, saving model to best.model\n",
      "0s - loss: 0.7604 - acc: 0.7191 - val_loss: 0.8020 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.80195 to 0.79109, saving model to best.model\n",
      "0s - loss: 0.7823 - acc: 0.6629 - val_loss: 0.7911 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.79109 to 0.78014, saving model to best.model\n",
      "0s - loss: 0.7431 - acc: 0.6854 - val_loss: 0.7801 - val_acc: 0.7826\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.78014 to 0.77030, saving model to best.model\n",
      "0s - loss: 0.6855 - acc: 0.7416 - val_loss: 0.7703 - val_acc: 0.7826\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.77030 to 0.76074, saving model to best.model\n",
      "0s - loss: 0.7508 - acc: 0.7303 - val_loss: 0.7607 - val_acc: 0.7826\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.76074 to 0.75167, saving model to best.model\n",
      "0s - loss: 0.7127 - acc: 0.6854 - val_loss: 0.7517 - val_acc: 0.7826\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.75167 to 0.74298, saving model to best.model\n",
      "0s - loss: 0.7611 - acc: 0.7640 - val_loss: 0.7430 - val_acc: 0.7826\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.74298 to 0.73447, saving model to best.model\n",
      "0s - loss: 0.6578 - acc: 0.7416 - val_loss: 0.7345 - val_acc: 0.7826\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.73447 to 0.72585, saving model to best.model\n",
      "0s - loss: 0.7481 - acc: 0.6629 - val_loss: 0.7258 - val_acc: 0.7826\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.72585 to 0.71696, saving model to best.model\n",
      "0s - loss: 0.6160 - acc: 0.7640 - val_loss: 0.7170 - val_acc: 0.7826\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.71696 to 0.70813, saving model to best.model\n",
      "0s - loss: 0.6510 - acc: 0.7416 - val_loss: 0.7081 - val_acc: 0.7826\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.70813 to 0.69907, saving model to best.model\n",
      "0s - loss: 0.5772 - acc: 0.7079 - val_loss: 0.6991 - val_acc: 0.7826\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.69907 to 0.69025, saving model to best.model\n",
      "0s - loss: 0.6747 - acc: 0.6742 - val_loss: 0.6903 - val_acc: 0.7826\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.69025 to 0.68114, saving model to best.model\n",
      "0s - loss: 0.5774 - acc: 0.7528 - val_loss: 0.6811 - val_acc: 0.7826\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68114 to 0.67235, saving model to best.model\n",
      "0s - loss: 0.5946 - acc: 0.7865 - val_loss: 0.6723 - val_acc: 0.7826\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67235 to 0.66501, saving model to best.model\n",
      "0s - loss: 0.7272 - acc: 0.6517 - val_loss: 0.6650 - val_acc: 0.7826\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66501 to 0.65647, saving model to best.model\n",
      "0s - loss: 0.6032 - acc: 0.7753 - val_loss: 0.6565 - val_acc: 0.7826\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.65647 to 0.64965, saving model to best.model\n",
      "0s - loss: 0.5634 - acc: 0.8090 - val_loss: 0.6497 - val_acc: 0.7826\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.64965 to 0.64174, saving model to best.model\n",
      "0s - loss: 0.6011 - acc: 0.7303 - val_loss: 0.6417 - val_acc: 0.7826\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.64174 to 0.63365, saving model to best.model\n",
      "0s - loss: 0.5182 - acc: 0.7978 - val_loss: 0.6337 - val_acc: 0.7826\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.63365 to 0.62515, saving model to best.model\n",
      "0s - loss: 0.5420 - acc: 0.8315 - val_loss: 0.6252 - val_acc: 0.7826\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.62515 to 0.61568, saving model to best.model\n",
      "0s - loss: 0.5780 - acc: 0.7303 - val_loss: 0.6157 - val_acc: 0.7826\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.61568 to 0.60628, saving model to best.model\n",
      "0s - loss: 0.5774 - acc: 0.7865 - val_loss: 0.6063 - val_acc: 0.7826\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.60628 to 0.59656, saving model to best.model\n",
      "0s - loss: 0.5573 - acc: 0.8090 - val_loss: 0.5966 - val_acc: 0.7826\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.59656 to 0.58581, saving model to best.model\n",
      "0s - loss: 0.5460 - acc: 0.8090 - val_loss: 0.5858 - val_acc: 0.7826\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.58581 to 0.57577, saving model to best.model\n",
      "0s - loss: 0.5499 - acc: 0.7865 - val_loss: 0.5758 - val_acc: 0.7826\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.57577 to 0.56466, saving model to best.model\n",
      "0s - loss: 0.5134 - acc: 0.8090 - val_loss: 0.5647 - val_acc: 0.7826\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.56466 to 0.55464, saving model to best.model\n",
      "0s - loss: 0.5147 - acc: 0.8090 - val_loss: 0.5546 - val_acc: 0.7826\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.55464 to 0.54555, saving model to best.model\n",
      "0s - loss: 0.4377 - acc: 0.8652 - val_loss: 0.5455 - val_acc: 0.7826\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.54555 to 0.53769, saving model to best.model\n",
      "0s - loss: 0.4972 - acc: 0.8202 - val_loss: 0.5377 - val_acc: 0.7391\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.53769 to 0.53090, saving model to best.model\n",
      "0s - loss: 0.4866 - acc: 0.7640 - val_loss: 0.5309 - val_acc: 0.7391\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.53090 to 0.52610, saving model to best.model\n",
      "0s - loss: 0.4960 - acc: 0.7978 - val_loss: 0.5261 - val_acc: 0.7391\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.52610 to 0.52102, saving model to best.model\n",
      "0s - loss: 0.4747 - acc: 0.8315 - val_loss: 0.5210 - val_acc: 0.7391\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.52102 to 0.51646, saving model to best.model\n",
      "0s - loss: 0.4647 - acc: 0.8090 - val_loss: 0.5165 - val_acc: 0.7391\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.51646 to 0.51111, saving model to best.model\n",
      "0s - loss: 0.4949 - acc: 0.8202 - val_loss: 0.5111 - val_acc: 0.7391\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.51111 to 0.50585, saving model to best.model\n",
      "0s - loss: 0.5141 - acc: 0.7753 - val_loss: 0.5058 - val_acc: 0.7391\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.50585 to 0.50049, saving model to best.model\n",
      "0s - loss: 0.4409 - acc: 0.8315 - val_loss: 0.5005 - val_acc: 0.7391\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.50049 to 0.49465, saving model to best.model\n",
      "0s - loss: 0.4087 - acc: 0.8315 - val_loss: 0.4946 - val_acc: 0.7391\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.49465 to 0.48720, saving model to best.model\n",
      "0s - loss: 0.4344 - acc: 0.8427 - val_loss: 0.4872 - val_acc: 0.7391\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.48720 to 0.47849, saving model to best.model\n",
      "0s - loss: 0.4527 - acc: 0.8315 - val_loss: 0.4785 - val_acc: 0.7391\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.47849 to 0.46896, saving model to best.model\n",
      "0s - loss: 0.4590 - acc: 0.8539 - val_loss: 0.4690 - val_acc: 0.7391\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.46896 to 0.46020, saving model to best.model\n",
      "0s - loss: 0.4267 - acc: 0.8539 - val_loss: 0.4602 - val_acc: 0.7391\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.46020 to 0.45184, saving model to best.model\n",
      "0s - loss: 0.4530 - acc: 0.8090 - val_loss: 0.4518 - val_acc: 0.7826\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.45184 to 0.44430, saving model to best.model\n",
      "0s - loss: 0.3584 - acc: 0.8876 - val_loss: 0.4443 - val_acc: 0.7826\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.44430 to 0.43550, saving model to best.model\n",
      "0s - loss: 0.4055 - acc: 0.8539 - val_loss: 0.4355 - val_acc: 0.8261\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.43550 to 0.42694, saving model to best.model\n",
      "0s - loss: 0.4651 - acc: 0.8539 - val_loss: 0.4269 - val_acc: 0.8261\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.42694 to 0.41902, saving model to best.model\n",
      "0s - loss: 0.4130 - acc: 0.8539 - val_loss: 0.4190 - val_acc: 0.8261\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.41902 to 0.41443, saving model to best.model\n",
      "0s - loss: 0.4322 - acc: 0.8427 - val_loss: 0.4144 - val_acc: 0.8261\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.41443 to 0.41091, saving model to best.model\n",
      "0s - loss: 0.4501 - acc: 0.8090 - val_loss: 0.4109 - val_acc: 0.7826\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.41091 to 0.40677, saving model to best.model\n",
      "0s - loss: 0.3771 - acc: 0.8652 - val_loss: 0.4068 - val_acc: 0.7826\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.40677 to 0.40162, saving model to best.model\n",
      "0s - loss: 0.3648 - acc: 0.8652 - val_loss: 0.4016 - val_acc: 0.7826\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.40162 to 0.39686, saving model to best.model\n",
      "0s - loss: 0.3283 - acc: 0.9101 - val_loss: 0.3969 - val_acc: 0.7826\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.39686 to 0.39302, saving model to best.model\n",
      "0s - loss: 0.4025 - acc: 0.8427 - val_loss: 0.3930 - val_acc: 0.7826\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.39302 to 0.38940, saving model to best.model\n",
      "0s - loss: 0.4065 - acc: 0.8764 - val_loss: 0.3894 - val_acc: 0.7826\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.38940 to 0.38467, saving model to best.model\n",
      "0s - loss: 0.3350 - acc: 0.8539 - val_loss: 0.3847 - val_acc: 0.7826\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.38467 to 0.37956, saving model to best.model\n",
      "0s - loss: 0.3423 - acc: 0.8539 - val_loss: 0.3796 - val_acc: 0.7826\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.37956 to 0.37612, saving model to best.model\n",
      "0s - loss: 0.3446 - acc: 0.8652 - val_loss: 0.3761 - val_acc: 0.7826\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.37612 to 0.37258, saving model to best.model\n",
      "0s - loss: 0.2785 - acc: 0.9326 - val_loss: 0.3726 - val_acc: 0.8261\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.37258 to 0.36740, saving model to best.model\n",
      "0s - loss: 0.3007 - acc: 0.8876 - val_loss: 0.3674 - val_acc: 0.8261\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.36740 to 0.36182, saving model to best.model\n",
      "0s - loss: 0.4249 - acc: 0.8427 - val_loss: 0.3618 - val_acc: 0.8696\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.36182 to 0.35630, saving model to best.model\n",
      "0s - loss: 0.3519 - acc: 0.8876 - val_loss: 0.3563 - val_acc: 0.8696\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.35630 to 0.35141, saving model to best.model\n",
      "0s - loss: 0.3510 - acc: 0.8989 - val_loss: 0.3514 - val_acc: 0.8696\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.35141 to 0.34515, saving model to best.model\n",
      "0s - loss: 0.3702 - acc: 0.8764 - val_loss: 0.3451 - val_acc: 0.8696\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.34515 to 0.33832, saving model to best.model\n",
      "0s - loss: 0.3246 - acc: 0.8876 - val_loss: 0.3383 - val_acc: 0.8696\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.33832 to 0.33198, saving model to best.model\n",
      "0s - loss: 0.2943 - acc: 0.9326 - val_loss: 0.3320 - val_acc: 0.8696\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.33198 to 0.32577, saving model to best.model\n",
      "0s - loss: 0.3080 - acc: 0.9213 - val_loss: 0.3258 - val_acc: 0.8696\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.32577 to 0.31943, saving model to best.model\n",
      "0s - loss: 0.3491 - acc: 0.8989 - val_loss: 0.3194 - val_acc: 0.8696\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.31943 to 0.31249, saving model to best.model\n",
      "0s - loss: 0.3359 - acc: 0.9213 - val_loss: 0.3125 - val_acc: 0.8696\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.31249 to 0.30768, saving model to best.model\n",
      "0s - loss: 0.3526 - acc: 0.8876 - val_loss: 0.3077 - val_acc: 0.8696\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.30768 to 0.30165, saving model to best.model\n",
      "0s - loss: 0.3279 - acc: 0.8539 - val_loss: 0.3017 - val_acc: 0.8696\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.30165 to 0.29493, saving model to best.model\n",
      "0s - loss: 0.2757 - acc: 0.9438 - val_loss: 0.2949 - val_acc: 0.8696\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.29493 to 0.28708, saving model to best.model\n",
      "0s - loss: 0.2811 - acc: 0.9101 - val_loss: 0.2871 - val_acc: 0.8696\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.28708 to 0.27910, saving model to best.model\n",
      "0s - loss: 0.2911 - acc: 0.8989 - val_loss: 0.2791 - val_acc: 0.8696\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.27910 to 0.27235, saving model to best.model\n",
      "0s - loss: 0.3460 - acc: 0.8539 - val_loss: 0.2723 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.27235 to 0.26696, saving model to best.model\n",
      "0s - loss: 0.3001 - acc: 0.8876 - val_loss: 0.2670 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.26696 to 0.26174, saving model to best.model\n",
      "0s - loss: 0.2692 - acc: 0.9213 - val_loss: 0.2617 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.26174 to 0.25711, saving model to best.model\n",
      "0s - loss: 0.2410 - acc: 0.8989 - val_loss: 0.2571 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.25711 to 0.25309, saving model to best.model\n",
      "0s - loss: 0.2870 - acc: 0.8764 - val_loss: 0.2531 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.25309 to 0.25080, saving model to best.model\n",
      "0s - loss: 0.2534 - acc: 0.9101 - val_loss: 0.2508 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.25080 to 0.24963, saving model to best.model\n",
      "0s - loss: 0.2413 - acc: 0.9663 - val_loss: 0.2496 - val_acc: 0.9130\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.24963 to 0.24852, saving model to best.model\n",
      "0s - loss: 0.2385 - acc: 0.9326 - val_loss: 0.2485 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.24852 to 0.24730, saving model to best.model\n",
      "0s - loss: 0.1906 - acc: 0.9663 - val_loss: 0.2473 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.3255 - acc: 0.8989 - val_loss: 0.2476 - val_acc: 0.8696\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.1955 - acc: 0.9551 - val_loss: 0.2478 - val_acc: 0.8696\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.24730 to 0.24681, saving model to best.model\n",
      "0s - loss: 0.2254 - acc: 0.9326 - val_loss: 0.2468 - val_acc: 0.8696\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.24681 to 0.24579, saving model to best.model\n",
      "0s - loss: 0.2070 - acc: 0.9326 - val_loss: 0.2458 - val_acc: 0.8696\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.24579 to 0.24291, saving model to best.model\n",
      "0s - loss: 0.2416 - acc: 0.9326 - val_loss: 0.2429 - val_acc: 0.8696\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.24291 to 0.23881, saving model to best.model\n",
      "0s - loss: 0.2534 - acc: 0.9213 - val_loss: 0.2388 - val_acc: 0.8696\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.23881 to 0.23394, saving model to best.model\n",
      "0s - loss: 0.1993 - acc: 0.9438 - val_loss: 0.2339 - val_acc: 0.8696\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.23394 to 0.22744, saving model to best.model\n",
      "0s - loss: 0.2979 - acc: 0.8652 - val_loss: 0.2274 - val_acc: 0.8696\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.22744 to 0.21899, saving model to best.model\n",
      "0s - loss: 0.2313 - acc: 0.9213 - val_loss: 0.2190 - val_acc: 0.9130\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.21899 to 0.20980, saving model to best.model\n",
      "0s - loss: 0.2386 - acc: 0.8764 - val_loss: 0.2098 - val_acc: 0.9130\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.20980 to 0.20154, saving model to best.model\n",
      "0s - loss: 0.1717 - acc: 0.9775 - val_loss: 0.2015 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.20154 to 0.19563, saving model to best.model\n",
      "0s - loss: 0.3092 - acc: 0.9213 - val_loss: 0.1956 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.19563 to 0.18958, saving model to best.model\n",
      "0s - loss: 0.1901 - acc: 0.9438 - val_loss: 0.1896 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.18958 to 0.18253, saving model to best.model\n",
      "0s - loss: 0.2054 - acc: 0.9213 - val_loss: 0.1825 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.18253 to 0.17738, saving model to best.model\n",
      "0s - loss: 0.1711 - acc: 0.9551 - val_loss: 0.1774 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.17738 to 0.17259, saving model to best.model\n",
      "0s - loss: 0.1802 - acc: 0.9326 - val_loss: 0.1726 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.17259 to 0.16829, saving model to best.model\n",
      "0s - loss: 0.2050 - acc: 0.9101 - val_loss: 0.1683 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.16829 to 0.16233, saving model to best.model\n",
      "0s - loss: 0.2536 - acc: 0.9101 - val_loss: 0.1623 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.16233 to 0.15718, saving model to best.model\n",
      "0s - loss: 0.1558 - acc: 0.9663 - val_loss: 0.1572 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.15718 to 0.15390, saving model to best.model\n",
      "0s - loss: 0.2120 - acc: 0.9101 - val_loss: 0.1539 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.15390 to 0.15102, saving model to best.model\n",
      "0s - loss: 0.1764 - acc: 0.9663 - val_loss: 0.1510 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.15102 to 0.14815, saving model to best.model\n",
      "0s - loss: 0.1852 - acc: 0.9326 - val_loss: 0.1481 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.14815 to 0.14605, saving model to best.model\n",
      "0s - loss: 0.1654 - acc: 0.9438 - val_loss: 0.1461 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.14605 to 0.14399, saving model to best.model\n",
      "0s - loss: 0.2335 - acc: 0.9213 - val_loss: 0.1440 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.14399 to 0.14174, saving model to best.model\n",
      "0s - loss: 0.1814 - acc: 0.9438 - val_loss: 0.1417 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.14174 to 0.13972, saving model to best.model\n",
      "0s - loss: 0.1716 - acc: 0.9663 - val_loss: 0.1397 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.13972 to 0.13896, saving model to best.model\n",
      "0s - loss: 0.1714 - acc: 0.9663 - val_loss: 0.1390 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.13896 to 0.13739, saving model to best.model\n",
      "0s - loss: 0.2035 - acc: 0.9326 - val_loss: 0.1374 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.13739 to 0.13578, saving model to best.model\n",
      "0s - loss: 0.1666 - acc: 0.9326 - val_loss: 0.1358 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.13578 to 0.13377, saving model to best.model\n",
      "0s - loss: 0.1642 - acc: 0.9326 - val_loss: 0.1338 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.13377 to 0.13159, saving model to best.model\n",
      "0s - loss: 0.1692 - acc: 0.9438 - val_loss: 0.1316 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.13159 to 0.12982, saving model to best.model\n",
      "0s - loss: 0.1192 - acc: 0.9775 - val_loss: 0.1298 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.12982 to 0.12753, saving model to best.model\n",
      "0s - loss: 0.1261 - acc: 0.9775 - val_loss: 0.1275 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.12753 to 0.12559, saving model to best.model\n",
      "0s - loss: 0.1763 - acc: 0.9438 - val_loss: 0.1256 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.12559 to 0.12403, saving model to best.model\n",
      "0s - loss: 0.1360 - acc: 0.9775 - val_loss: 0.1240 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.12403 to 0.12111, saving model to best.model\n",
      "0s - loss: 0.1617 - acc: 0.9326 - val_loss: 0.1211 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.12111 to 0.11718, saving model to best.model\n",
      "0s - loss: 0.1487 - acc: 0.9438 - val_loss: 0.1172 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.11718 to 0.11318, saving model to best.model\n",
      "0s - loss: 0.1659 - acc: 0.9438 - val_loss: 0.1132 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.11318 to 0.10901, saving model to best.model\n",
      "0s - loss: 0.1133 - acc: 0.9663 - val_loss: 0.1090 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.10901 to 0.10607, saving model to best.model\n",
      "0s - loss: 0.1462 - acc: 0.9775 - val_loss: 0.1061 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.14627, saving model to best.model\n",
      "0s - loss: 1.4402 - acc: 0.3820 - val_loss: 1.1463 - val_acc: 0.2609\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.14627 to 1.09158, saving model to best.model\n",
      "0s - loss: 1.2823 - acc: 0.3933 - val_loss: 1.0916 - val_acc: 0.2609\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.09158 to 1.06496, saving model to best.model\n",
      "0s - loss: 1.1646 - acc: 0.3933 - val_loss: 1.0650 - val_acc: 0.4783\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.06496 to 1.05162, saving model to best.model\n",
      "0s - loss: 1.2801 - acc: 0.3483 - val_loss: 1.0516 - val_acc: 0.4783\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.05162 to 1.04695, saving model to best.model\n",
      "0s - loss: 1.2160 - acc: 0.3371 - val_loss: 1.0469 - val_acc: 0.4783\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.04695 to 1.04466, saving model to best.model\n",
      "0s - loss: 1.4685 - acc: 0.2584 - val_loss: 1.0447 - val_acc: 0.4783\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.04466 to 1.04282, saving model to best.model\n",
      "0s - loss: 1.2974 - acc: 0.3258 - val_loss: 1.0428 - val_acc: 0.4783\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.04282 to 1.04088, saving model to best.model\n",
      "0s - loss: 1.2088 - acc: 0.3933 - val_loss: 1.0409 - val_acc: 0.4783\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.04088 to 1.03915, saving model to best.model\n",
      "0s - loss: 1.2651 - acc: 0.3258 - val_loss: 1.0392 - val_acc: 0.4783\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.03915 to 1.03810, saving model to best.model\n",
      "0s - loss: 1.2709 - acc: 0.3371 - val_loss: 1.0381 - val_acc: 0.4783\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.3315 - acc: 0.3596 - val_loss: 1.0393 - val_acc: 0.4783\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.4108 - acc: 0.2472 - val_loss: 1.0425 - val_acc: 0.4783\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1811 - acc: 0.4045 - val_loss: 1.0471 - val_acc: 0.4783\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2970 - acc: 0.3258 - val_loss: 1.0504 - val_acc: 0.4783\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1713 - acc: 0.3933 - val_loss: 1.0514 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2111 - acc: 0.4045 - val_loss: 1.0501 - val_acc: 0.5652\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1698 - acc: 0.4045 - val_loss: 1.0483 - val_acc: 0.5652\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2029 - acc: 0.3034 - val_loss: 1.0460 - val_acc: 0.5652\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1784 - acc: 0.4270 - val_loss: 1.0431 - val_acc: 0.5652\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1571 - acc: 0.3933 - val_loss: 1.0390 - val_acc: 0.4783\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.03810 to 1.03439, saving model to best.model\n",
      "0s - loss: 1.1565 - acc: 0.3596 - val_loss: 1.0344 - val_acc: 0.4783\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.03439 to 1.03010, saving model to best.model\n",
      "0s - loss: 1.1019 - acc: 0.4045 - val_loss: 1.0301 - val_acc: 0.4783\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.03010 to 1.02680, saving model to best.model\n",
      "0s - loss: 1.2839 - acc: 0.3596 - val_loss: 1.0268 - val_acc: 0.4783\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.02680 to 1.02398, saving model to best.model\n",
      "0s - loss: 1.1751 - acc: 0.3820 - val_loss: 1.0240 - val_acc: 0.4783\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.02398 to 1.02030, saving model to best.model\n",
      "0s - loss: 1.1565 - acc: 0.3820 - val_loss: 1.0203 - val_acc: 0.4783\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.02030 to 1.01699, saving model to best.model\n",
      "0s - loss: 1.1287 - acc: 0.3708 - val_loss: 1.0170 - val_acc: 0.4783\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.01699 to 1.01408, saving model to best.model\n",
      "0s - loss: 1.2430 - acc: 0.3258 - val_loss: 1.0141 - val_acc: 0.4783\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.01408 to 1.01229, saving model to best.model\n",
      "0s - loss: 1.2090 - acc: 0.3371 - val_loss: 1.0123 - val_acc: 0.4783\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.01229 to 1.00998, saving model to best.model\n",
      "0s - loss: 1.1481 - acc: 0.3034 - val_loss: 1.0100 - val_acc: 0.4783\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.00998 to 1.00782, saving model to best.model\n",
      "0s - loss: 1.2422 - acc: 0.3483 - val_loss: 1.0078 - val_acc: 0.4783\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.00782 to 1.00661, saving model to best.model\n",
      "0s - loss: 1.1088 - acc: 0.4157 - val_loss: 1.0066 - val_acc: 0.4783\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.00661 to 1.00574, saving model to best.model\n",
      "0s - loss: 1.1041 - acc: 0.4157 - val_loss: 1.0057 - val_acc: 0.4783\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.00574 to 1.00521, saving model to best.model\n",
      "0s - loss: 1.2101 - acc: 0.3371 - val_loss: 1.0052 - val_acc: 0.4783\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.00521 to 1.00417, saving model to best.model\n",
      "0s - loss: 1.1536 - acc: 0.3820 - val_loss: 1.0042 - val_acc: 0.4783\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.00417 to 1.00321, saving model to best.model\n",
      "0s - loss: 1.1343 - acc: 0.3708 - val_loss: 1.0032 - val_acc: 0.4783\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.00321 to 1.00244, saving model to best.model\n",
      "0s - loss: 1.1461 - acc: 0.3933 - val_loss: 1.0024 - val_acc: 0.5652\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.00244 to 1.00161, saving model to best.model\n",
      "0s - loss: 1.0800 - acc: 0.4045 - val_loss: 1.0016 - val_acc: 0.6522\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.00161 to 1.00092, saving model to best.model\n",
      "0s - loss: 1.0604 - acc: 0.4382 - val_loss: 1.0009 - val_acc: 0.6957\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.00092 to 0.99939, saving model to best.model\n",
      "0s - loss: 1.0341 - acc: 0.4157 - val_loss: 0.9994 - val_acc: 0.6957\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.99939 to 0.99753, saving model to best.model\n",
      "0s - loss: 1.2235 - acc: 0.2809 - val_loss: 0.9975 - val_acc: 0.6957\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.99753 to 0.99522, saving model to best.model\n",
      "0s - loss: 1.0801 - acc: 0.4607 - val_loss: 0.9952 - val_acc: 0.6957\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.99522 to 0.99179, saving model to best.model\n",
      "0s - loss: 1.0899 - acc: 0.4719 - val_loss: 0.9918 - val_acc: 0.6957\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.99179 to 0.98790, saving model to best.model\n",
      "0s - loss: 1.1802 - acc: 0.3820 - val_loss: 0.9879 - val_acc: 0.6957\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.98790 to 0.98294, saving model to best.model\n",
      "0s - loss: 1.1478 - acc: 0.3258 - val_loss: 0.9829 - val_acc: 0.6957\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.98294 to 0.97833, saving model to best.model\n",
      "0s - loss: 1.0609 - acc: 0.4607 - val_loss: 0.9783 - val_acc: 0.6522\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.97833 to 0.97402, saving model to best.model\n",
      "0s - loss: 1.0662 - acc: 0.3933 - val_loss: 0.9740 - val_acc: 0.6522\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.97402 to 0.96986, saving model to best.model\n",
      "0s - loss: 1.0535 - acc: 0.3820 - val_loss: 0.9699 - val_acc: 0.5652\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.96986 to 0.96620, saving model to best.model\n",
      "0s - loss: 1.0477 - acc: 0.4270 - val_loss: 0.9662 - val_acc: 0.5652\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.96620 to 0.96206, saving model to best.model\n",
      "0s - loss: 1.0586 - acc: 0.4494 - val_loss: 0.9621 - val_acc: 0.5652\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.96206 to 0.95794, saving model to best.model\n",
      "0s - loss: 1.0278 - acc: 0.4831 - val_loss: 0.9579 - val_acc: 0.5652\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.95794 to 0.95351, saving model to best.model\n",
      "0s - loss: 1.0948 - acc: 0.4045 - val_loss: 0.9535 - val_acc: 0.5652\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.95351 to 0.94951, saving model to best.model\n",
      "0s - loss: 1.1205 - acc: 0.4157 - val_loss: 0.9495 - val_acc: 0.5652\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.94951 to 0.94588, saving model to best.model\n",
      "0s - loss: 1.0996 - acc: 0.4494 - val_loss: 0.9459 - val_acc: 0.5652\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.94588 to 0.94235, saving model to best.model\n",
      "0s - loss: 1.0500 - acc: 0.4607 - val_loss: 0.9424 - val_acc: 0.6522\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.94235 to 0.93873, saving model to best.model\n",
      "0s - loss: 0.9910 - acc: 0.4944 - val_loss: 0.9387 - val_acc: 0.6522\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.93873 to 0.93456, saving model to best.model\n",
      "0s - loss: 1.0844 - acc: 0.4157 - val_loss: 0.9346 - val_acc: 0.6957\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.93456 to 0.93021, saving model to best.model\n",
      "0s - loss: 0.9873 - acc: 0.5056 - val_loss: 0.9302 - val_acc: 0.6957\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.93021 to 0.92591, saving model to best.model\n",
      "0s - loss: 0.9996 - acc: 0.5056 - val_loss: 0.9259 - val_acc: 0.6957\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.92591 to 0.92155, saving model to best.model\n",
      "0s - loss: 1.0736 - acc: 0.4607 - val_loss: 0.9216 - val_acc: 0.6957\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.92155 to 0.91709, saving model to best.model\n",
      "0s - loss: 1.0539 - acc: 0.4719 - val_loss: 0.9171 - val_acc: 0.6957\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.91709 to 0.91299, saving model to best.model\n",
      "0s - loss: 1.0753 - acc: 0.4719 - val_loss: 0.9130 - val_acc: 0.6957\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.91299 to 0.90896, saving model to best.model\n",
      "0s - loss: 0.9860 - acc: 0.4831 - val_loss: 0.9090 - val_acc: 0.6957\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.90896 to 0.90405, saving model to best.model\n",
      "0s - loss: 1.0140 - acc: 0.5281 - val_loss: 0.9041 - val_acc: 0.6957\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.90405 to 0.89889, saving model to best.model\n",
      "0s - loss: 1.0882 - acc: 0.4270 - val_loss: 0.8989 - val_acc: 0.6957\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.89889 to 0.89332, saving model to best.model\n",
      "0s - loss: 1.0335 - acc: 0.4045 - val_loss: 0.8933 - val_acc: 0.6957\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.89332 to 0.88738, saving model to best.model\n",
      "0s - loss: 0.9743 - acc: 0.5393 - val_loss: 0.8874 - val_acc: 0.6957\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.88738 to 0.88078, saving model to best.model\n",
      "0s - loss: 0.9335 - acc: 0.5955 - val_loss: 0.8808 - val_acc: 0.6957\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.88078 to 0.87379, saving model to best.model\n",
      "0s - loss: 1.0775 - acc: 0.4831 - val_loss: 0.8738 - val_acc: 0.6957\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.87379 to 0.86661, saving model to best.model\n",
      "0s - loss: 0.9935 - acc: 0.4719 - val_loss: 0.8666 - val_acc: 0.6957\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.86661 to 0.85873, saving model to best.model\n",
      "0s - loss: 0.9662 - acc: 0.4831 - val_loss: 0.8587 - val_acc: 0.6957\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.85873 to 0.85039, saving model to best.model\n",
      "0s - loss: 0.9669 - acc: 0.4944 - val_loss: 0.8504 - val_acc: 0.6957\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.85039 to 0.84204, saving model to best.model\n",
      "0s - loss: 1.0170 - acc: 0.5281 - val_loss: 0.8420 - val_acc: 0.6957\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.84204 to 0.83379, saving model to best.model\n",
      "0s - loss: 0.9681 - acc: 0.5169 - val_loss: 0.8338 - val_acc: 0.7391\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.83379 to 0.82563, saving model to best.model\n",
      "0s - loss: 0.9028 - acc: 0.5618 - val_loss: 0.8256 - val_acc: 0.7391\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.82563 to 0.81713, saving model to best.model\n",
      "0s - loss: 0.9906 - acc: 0.4382 - val_loss: 0.8171 - val_acc: 0.7391\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.81713 to 0.80896, saving model to best.model\n",
      "0s - loss: 1.0419 - acc: 0.5169 - val_loss: 0.8090 - val_acc: 0.7391\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.80896 to 0.80072, saving model to best.model\n",
      "0s - loss: 0.8811 - acc: 0.5955 - val_loss: 0.8007 - val_acc: 0.7391\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.80072 to 0.79209, saving model to best.model\n",
      "0s - loss: 0.8736 - acc: 0.5730 - val_loss: 0.7921 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.79209 to 0.78371, saving model to best.model\n",
      "0s - loss: 0.9301 - acc: 0.5730 - val_loss: 0.7837 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.78371 to 0.77528, saving model to best.model\n",
      "0s - loss: 0.9615 - acc: 0.5169 - val_loss: 0.7753 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.77528 to 0.76654, saving model to best.model\n",
      "0s - loss: 0.9522 - acc: 0.5506 - val_loss: 0.7665 - val_acc: 0.7826\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.76654 to 0.75788, saving model to best.model\n",
      "0s - loss: 0.8866 - acc: 0.6067 - val_loss: 0.7579 - val_acc: 0.8696\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.75788 to 0.74882, saving model to best.model\n",
      "0s - loss: 0.8967 - acc: 0.5843 - val_loss: 0.7488 - val_acc: 0.8696\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.74882 to 0.73995, saving model to best.model\n",
      "0s - loss: 0.8311 - acc: 0.6067 - val_loss: 0.7399 - val_acc: 0.8696\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.73995 to 0.73065, saving model to best.model\n",
      "0s - loss: 0.8263 - acc: 0.6517 - val_loss: 0.7306 - val_acc: 0.8696\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.73065 to 0.72095, saving model to best.model\n",
      "0s - loss: 0.9051 - acc: 0.5506 - val_loss: 0.7209 - val_acc: 0.8696\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.72095 to 0.71086, saving model to best.model\n",
      "0s - loss: 0.7835 - acc: 0.6966 - val_loss: 0.7109 - val_acc: 0.8696\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.71086 to 0.70091, saving model to best.model\n",
      "0s - loss: 0.8386 - acc: 0.6629 - val_loss: 0.7009 - val_acc: 0.9565\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.70091 to 0.69039, saving model to best.model\n",
      "0s - loss: 0.8844 - acc: 0.5506 - val_loss: 0.6904 - val_acc: 1.0000\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69039 to 0.67950, saving model to best.model\n",
      "0s - loss: 0.7673 - acc: 0.6517 - val_loss: 0.6795 - val_acc: 1.0000\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67950 to 0.66786, saving model to best.model\n",
      "0s - loss: 0.7951 - acc: 0.5955 - val_loss: 0.6679 - val_acc: 1.0000\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66786 to 0.65581, saving model to best.model\n",
      "0s - loss: 0.7516 - acc: 0.6742 - val_loss: 0.6558 - val_acc: 1.0000\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.65581 to 0.64413, saving model to best.model\n",
      "0s - loss: 0.7720 - acc: 0.6966 - val_loss: 0.6441 - val_acc: 1.0000\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.64413 to 0.63233, saving model to best.model\n",
      "0s - loss: 0.7780 - acc: 0.5955 - val_loss: 0.6323 - val_acc: 1.0000\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.63233 to 0.62028, saving model to best.model\n",
      "0s - loss: 0.8393 - acc: 0.6629 - val_loss: 0.6203 - val_acc: 1.0000\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.62028 to 0.60781, saving model to best.model\n",
      "0s - loss: 0.6515 - acc: 0.7416 - val_loss: 0.6078 - val_acc: 1.0000\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.60781 to 0.59570, saving model to best.model\n",
      "0s - loss: 0.7924 - acc: 0.6517 - val_loss: 0.5957 - val_acc: 1.0000\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.59570 to 0.58363, saving model to best.model\n",
      "0s - loss: 0.7456 - acc: 0.6966 - val_loss: 0.5836 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.58363 to 0.57166, saving model to best.model\n",
      "0s - loss: 0.7452 - acc: 0.6966 - val_loss: 0.5717 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.57166 to 0.56009, saving model to best.model\n",
      "0s - loss: 0.7756 - acc: 0.6404 - val_loss: 0.5601 - val_acc: 0.9565\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.56009 to 0.54876, saving model to best.model\n",
      "0s - loss: 0.7027 - acc: 0.6854 - val_loss: 0.5488 - val_acc: 0.9565\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.54876 to 0.53760, saving model to best.model\n",
      "0s - loss: 0.6919 - acc: 0.7303 - val_loss: 0.5376 - val_acc: 0.9565\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.53760 to 0.52669, saving model to best.model\n",
      "0s - loss: 0.7174 - acc: 0.6966 - val_loss: 0.5267 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.52669 to 0.51581, saving model to best.model\n",
      "0s - loss: 0.6871 - acc: 0.6966 - val_loss: 0.5158 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.51581 to 0.50508, saving model to best.model\n",
      "0s - loss: 0.6858 - acc: 0.7528 - val_loss: 0.5051 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.50508 to 0.49467, saving model to best.model\n",
      "0s - loss: 0.7096 - acc: 0.7079 - val_loss: 0.4947 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.49467 to 0.48426, saving model to best.model\n",
      "0s - loss: 0.6291 - acc: 0.7528 - val_loss: 0.4843 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.48426 to 0.47400, saving model to best.model\n",
      "0s - loss: 0.6571 - acc: 0.7753 - val_loss: 0.4740 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.47400 to 0.46386, saving model to best.model\n",
      "0s - loss: 0.7470 - acc: 0.6854 - val_loss: 0.4639 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.46386 to 0.45384, saving model to best.model\n",
      "0s - loss: 0.6140 - acc: 0.7753 - val_loss: 0.4538 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.45384 to 0.44365, saving model to best.model\n",
      "0s - loss: 0.5704 - acc: 0.8090 - val_loss: 0.4436 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.44365 to 0.43368, saving model to best.model\n",
      "0s - loss: 0.5636 - acc: 0.7640 - val_loss: 0.4337 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.43368 to 0.42395, saving model to best.model\n",
      "0s - loss: 0.6249 - acc: 0.7416 - val_loss: 0.4239 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.42395 to 0.41437, saving model to best.model\n",
      "0s - loss: 0.5787 - acc: 0.7416 - val_loss: 0.4144 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.41437 to 0.40512, saving model to best.model\n",
      "0s - loss: 0.6599 - acc: 0.7191 - val_loss: 0.4051 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.40512 to 0.39645, saving model to best.model\n",
      "0s - loss: 0.6118 - acc: 0.7303 - val_loss: 0.3964 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.39645 to 0.38769, saving model to best.model\n",
      "0s - loss: 0.6355 - acc: 0.6966 - val_loss: 0.3877 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.38769 to 0.37917, saving model to best.model\n",
      "0s - loss: 0.5403 - acc: 0.7528 - val_loss: 0.3792 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.37917 to 0.37078, saving model to best.model\n",
      "0s - loss: 0.6137 - acc: 0.7753 - val_loss: 0.3708 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.37078 to 0.36253, saving model to best.model\n",
      "0s - loss: 0.5990 - acc: 0.7416 - val_loss: 0.3625 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.36253 to 0.35403, saving model to best.model\n",
      "0s - loss: 0.5894 - acc: 0.7191 - val_loss: 0.3540 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.35403 to 0.34558, saving model to best.model\n",
      "0s - loss: 0.5774 - acc: 0.7640 - val_loss: 0.3456 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.34558 to 0.33720, saving model to best.model\n",
      "0s - loss: 0.4870 - acc: 0.8202 - val_loss: 0.3372 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.33720 to 0.32913, saving model to best.model\n",
      "0s - loss: 0.4789 - acc: 0.8539 - val_loss: 0.3291 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.32913 to 0.32120, saving model to best.model\n",
      "0s - loss: 0.5148 - acc: 0.7978 - val_loss: 0.3212 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.32120 to 0.31342, saving model to best.model\n",
      "0s - loss: 0.5576 - acc: 0.7303 - val_loss: 0.3134 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.31342 to 0.30572, saving model to best.model\n",
      "0s - loss: 0.5692 - acc: 0.7528 - val_loss: 0.3057 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.30572 to 0.29820, saving model to best.model\n",
      "0s - loss: 0.5227 - acc: 0.8090 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.29820 to 0.29080, saving model to best.model\n",
      "0s - loss: 0.4978 - acc: 0.7753 - val_loss: 0.2908 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.29080 to 0.28376, saving model to best.model\n",
      "0s - loss: 0.4257 - acc: 0.8427 - val_loss: 0.2838 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.28376 to 0.27658, saving model to best.model\n",
      "0s - loss: 0.4293 - acc: 0.8652 - val_loss: 0.2766 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.27658 to 0.26976, saving model to best.model\n",
      "0s - loss: 0.4660 - acc: 0.8315 - val_loss: 0.2698 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.26976 to 0.26281, saving model to best.model\n",
      "0s - loss: 0.4666 - acc: 0.8202 - val_loss: 0.2628 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.26281 to 0.25596, saving model to best.model\n",
      "0s - loss: 0.4451 - acc: 0.8764 - val_loss: 0.2560 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.25596 to 0.24932, saving model to best.model\n",
      "0s - loss: 0.4466 - acc: 0.8315 - val_loss: 0.2493 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.24932 to 0.24291, saving model to best.model\n",
      "0s - loss: 0.4269 - acc: 0.8539 - val_loss: 0.2429 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.24291 to 0.23669, saving model to best.model\n",
      "0s - loss: 0.4881 - acc: 0.8315 - val_loss: 0.2367 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.23669 to 0.23063, saving model to best.model\n",
      "0s - loss: 0.4043 - acc: 0.8427 - val_loss: 0.2306 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.23063 to 0.22466, saving model to best.model\n",
      "0s - loss: 0.4144 - acc: 0.8876 - val_loss: 0.2247 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.22466 to 0.21889, saving model to best.model\n",
      "0s - loss: 0.3759 - acc: 0.8764 - val_loss: 0.2189 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.21889 to 0.21324, saving model to best.model\n",
      "0s - loss: 0.3848 - acc: 0.8764 - val_loss: 0.2132 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.21324 to 0.20779, saving model to best.model\n",
      "0s - loss: 0.4313 - acc: 0.8090 - val_loss: 0.2078 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.20779 to 0.20244, saving model to best.model\n",
      "0s - loss: 0.4039 - acc: 0.8539 - val_loss: 0.2024 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.20244 to 0.19704, saving model to best.model\n",
      "0s - loss: 0.4043 - acc: 0.8539 - val_loss: 0.1970 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19704 to 0.19180, saving model to best.model\n",
      "0s - loss: 0.4029 - acc: 0.8315 - val_loss: 0.1918 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.19180 to 0.18678, saving model to best.model\n",
      "0s - loss: 0.3747 - acc: 0.8427 - val_loss: 0.1868 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.18678 to 0.18176, saving model to best.model\n",
      "0s - loss: 0.3656 - acc: 0.8989 - val_loss: 0.1818 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.18176 to 0.17685, saving model to best.model\n",
      "0s - loss: 0.3736 - acc: 0.8652 - val_loss: 0.1768 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.17685 to 0.17194, saving model to best.model\n",
      "0s - loss: 0.3891 - acc: 0.8764 - val_loss: 0.1719 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.17194 to 0.16724, saving model to best.model\n",
      "0s - loss: 0.3926 - acc: 0.8315 - val_loss: 0.1672 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.16724 to 0.16280, saving model to best.model\n",
      "0s - loss: 0.3815 - acc: 0.8427 - val_loss: 0.1628 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.16280 to 0.15850, saving model to best.model\n",
      "0s - loss: 0.2766 - acc: 0.9326 - val_loss: 0.1585 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.15850 to 0.15449, saving model to best.model\n",
      "0s - loss: 0.3619 - acc: 0.8764 - val_loss: 0.1545 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.15449 to 0.15078, saving model to best.model\n",
      "0s - loss: 0.3718 - acc: 0.8876 - val_loss: 0.1508 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.15078 to 0.14728, saving model to best.model\n",
      "0s - loss: 0.2760 - acc: 0.9213 - val_loss: 0.1473 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.14728 to 0.14388, saving model to best.model\n",
      "0s - loss: 0.3160 - acc: 0.8764 - val_loss: 0.1439 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.14388 to 0.14066, saving model to best.model\n",
      "0s - loss: 0.4024 - acc: 0.8764 - val_loss: 0.1407 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.14066 to 0.13731, saving model to best.model\n",
      "0s - loss: 0.3753 - acc: 0.8539 - val_loss: 0.1373 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.13731 to 0.13391, saving model to best.model\n",
      "0s - loss: 0.3954 - acc: 0.8315 - val_loss: 0.1339 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.13391 to 0.13053, saving model to best.model\n",
      "0s - loss: 0.3136 - acc: 0.8764 - val_loss: 0.1305 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.13053 to 0.12712, saving model to best.model\n",
      "0s - loss: 0.2858 - acc: 0.8876 - val_loss: 0.1271 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.12712 to 0.12355, saving model to best.model\n",
      "0s - loss: 0.2632 - acc: 0.9101 - val_loss: 0.1235 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.12355 to 0.11998, saving model to best.model\n",
      "0s - loss: 0.3653 - acc: 0.8202 - val_loss: 0.1200 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.11998 to 0.11637, saving model to best.model\n",
      "0s - loss: 0.2699 - acc: 0.8652 - val_loss: 0.1164 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.11637 to 0.11274, saving model to best.model\n",
      "0s - loss: 0.2502 - acc: 0.9438 - val_loss: 0.1127 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.11274 to 0.10940, saving model to best.model\n",
      "0s - loss: 0.2777 - acc: 0.9213 - val_loss: 0.1094 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.10940 to 0.10617, saving model to best.model\n",
      "0s - loss: 0.2698 - acc: 0.8989 - val_loss: 0.1062 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.10617 to 0.10308, saving model to best.model\n",
      "0s - loss: 0.2889 - acc: 0.8764 - val_loss: 0.1031 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.10308 to 0.10013, saving model to best.model\n",
      "0s - loss: 0.2802 - acc: 0.9438 - val_loss: 0.1001 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.10013 to 0.09735, saving model to best.model\n",
      "0s - loss: 0.2939 - acc: 0.9213 - val_loss: 0.0973 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.09735 to 0.09452, saving model to best.model\n",
      "0s - loss: 0.3203 - acc: 0.8652 - val_loss: 0.0945 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.09452 to 0.09178, saving model to best.model\n",
      "0s - loss: 0.2657 - acc: 0.9101 - val_loss: 0.0918 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.09178 to 0.08920, saving model to best.model\n",
      "0s - loss: 0.2469 - acc: 0.8876 - val_loss: 0.0892 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.08920 to 0.08686, saving model to best.model\n",
      "0s - loss: 0.2199 - acc: 0.9326 - val_loss: 0.0869 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.08686 to 0.08465, saving model to best.model\n",
      "0s - loss: 0.2226 - acc: 0.9326 - val_loss: 0.0846 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.08465 to 0.08253, saving model to best.model\n",
      "0s - loss: 0.2257 - acc: 0.9326 - val_loss: 0.0825 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.08253 to 0.08052, saving model to best.model\n",
      "0s - loss: 0.2704 - acc: 0.9101 - val_loss: 0.0805 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.08052 to 0.07852, saving model to best.model\n",
      "0s - loss: 0.2101 - acc: 0.8989 - val_loss: 0.0785 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.07852 to 0.07679, saving model to best.model\n",
      "0s - loss: 0.2981 - acc: 0.8764 - val_loss: 0.0768 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.07679 to 0.07530, saving model to best.model\n",
      "0s - loss: 0.2486 - acc: 0.9213 - val_loss: 0.0753 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.07530 to 0.07420, saving model to best.model\n",
      "0s - loss: 0.3062 - acc: 0.8876 - val_loss: 0.0742 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.07420 to 0.07346, saving model to best.model\n",
      "0s - loss: 0.3499 - acc: 0.8652 - val_loss: 0.0735 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.07346 to 0.07264, saving model to best.model\n",
      "0s - loss: 0.2588 - acc: 0.9101 - val_loss: 0.0726 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.07264 to 0.07218, saving model to best.model\n",
      "0s - loss: 0.2550 - acc: 0.8764 - val_loss: 0.0722 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.07218 to 0.07179, saving model to best.model\n",
      "0s - loss: 0.2555 - acc: 0.9101 - val_loss: 0.0718 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.07179 to 0.07146, saving model to best.model\n",
      "0s - loss: 0.2019 - acc: 0.9551 - val_loss: 0.0715 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.07146 to 0.07123, saving model to best.model\n",
      "0s - loss: 0.2058 - acc: 0.9438 - val_loss: 0.0712 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.07123 to 0.07094, saving model to best.model\n",
      "0s - loss: 0.2451 - acc: 0.8989 - val_loss: 0.0709 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.07094 to 0.07001, saving model to best.model\n",
      "0s - loss: 0.2776 - acc: 0.8764 - val_loss: 0.0700 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.07001 to 0.06831, saving model to best.model\n",
      "0s - loss: 0.2389 - acc: 0.9101 - val_loss: 0.0683 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.06831 to 0.06632, saving model to best.model\n",
      "0s - loss: 0.2088 - acc: 0.8989 - val_loss: 0.0663 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.06632 to 0.06419, saving model to best.model\n",
      "0s - loss: 0.2269 - acc: 0.9101 - val_loss: 0.0642 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.06419 to 0.06233, saving model to best.model\n",
      "0s - loss: 0.1752 - acc: 0.9551 - val_loss: 0.0623 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.06233 to 0.06063, saving model to best.model\n",
      "0s - loss: 0.2146 - acc: 0.8989 - val_loss: 0.0606 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.06063 to 0.05908, saving model to best.model\n",
      "0s - loss: 0.2400 - acc: 0.9101 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.05908 to 0.05774, saving model to best.model\n",
      "0s - loss: 0.2856 - acc: 0.8989 - val_loss: 0.0577 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.05774 to 0.05639, saving model to best.model\n",
      "0s - loss: 0.1917 - acc: 0.9213 - val_loss: 0.0564 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.05639 to 0.05492, saving model to best.model\n",
      "0s - loss: 0.2046 - acc: 0.9101 - val_loss: 0.0549 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.05492 to 0.05327, saving model to best.model\n",
      "0s - loss: 0.1822 - acc: 0.9213 - val_loss: 0.0533 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.05327 to 0.05197, saving model to best.model\n",
      "0s - loss: 0.2184 - acc: 0.9438 - val_loss: 0.0520 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.13658, saving model to best.model\n",
      "0s - loss: 1.2437 - acc: 0.3371 - val_loss: 1.1366 - val_acc: 0.5217\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.2032 - acc: 0.3371 - val_loss: 1.1497 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.1757 - acc: 0.4270 - val_loss: 1.1625 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1926 - acc: 0.3596 - val_loss: 1.1747 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2501 - acc: 0.3933 - val_loss: 1.1852 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1897 - acc: 0.4157 - val_loss: 1.1888 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1602 - acc: 0.4045 - val_loss: 1.1872 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2427 - acc: 0.3933 - val_loss: 1.1802 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2010 - acc: 0.3820 - val_loss: 1.1692 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2005 - acc: 0.3820 - val_loss: 1.1560 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1441 - acc: 0.4719 - val_loss: 1.1451 - val_acc: 0.3913\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.13658 to 1.13526, saving model to best.model\n",
      "0s - loss: 1.2668 - acc: 0.4045 - val_loss: 1.1353 - val_acc: 0.3913\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.13526 to 1.12696, saving model to best.model\n",
      "0s - loss: 1.1495 - acc: 0.4157 - val_loss: 1.1270 - val_acc: 0.3913\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.12696 to 1.12111, saving model to best.model\n",
      "0s - loss: 1.2254 - acc: 0.2809 - val_loss: 1.1211 - val_acc: 0.3913\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.12111 to 1.11664, saving model to best.model\n",
      "0s - loss: 1.2267 - acc: 0.3933 - val_loss: 1.1166 - val_acc: 0.3913\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.11664 to 1.11369, saving model to best.model\n",
      "0s - loss: 1.1531 - acc: 0.4494 - val_loss: 1.1137 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.11369 to 1.10988, saving model to best.model\n",
      "0s - loss: 1.2182 - acc: 0.3820 - val_loss: 1.1099 - val_acc: 0.3913\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.10988 to 1.10723, saving model to best.model\n",
      "0s - loss: 1.1166 - acc: 0.4270 - val_loss: 1.1072 - val_acc: 0.3913\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.10723 to 1.10559, saving model to best.model\n",
      "0s - loss: 1.1386 - acc: 0.4157 - val_loss: 1.1056 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.10559 to 1.10406, saving model to best.model\n",
      "0s - loss: 1.1504 - acc: 0.4382 - val_loss: 1.1041 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.10406 to 1.10272, saving model to best.model\n",
      "0s - loss: 1.1213 - acc: 0.4045 - val_loss: 1.1027 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.10272 to 1.10187, saving model to best.model\n",
      "0s - loss: 1.0420 - acc: 0.4270 - val_loss: 1.1019 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.10187 to 1.09927, saving model to best.model\n",
      "0s - loss: 1.1388 - acc: 0.4382 - val_loss: 1.0993 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.09927 to 1.09722, saving model to best.model\n",
      "0s - loss: 1.1218 - acc: 0.3820 - val_loss: 1.0972 - val_acc: 0.4348\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.09722 to 1.09518, saving model to best.model\n",
      "0s - loss: 1.1174 - acc: 0.4382 - val_loss: 1.0952 - val_acc: 0.4348\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.09518 to 1.09249, saving model to best.model\n",
      "0s - loss: 1.0759 - acc: 0.4607 - val_loss: 1.0925 - val_acc: 0.4783\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.09249 to 1.09135, saving model to best.model\n",
      "0s - loss: 1.1980 - acc: 0.3258 - val_loss: 1.0914 - val_acc: 0.5217\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.09135 to 1.09109, saving model to best.model\n",
      "0s - loss: 1.1387 - acc: 0.3596 - val_loss: 1.0911 - val_acc: 0.5217\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.0778 - acc: 0.4157 - val_loss: 1.0915 - val_acc: 0.5652\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.09109 to 1.09098, saving model to best.model\n",
      "0s - loss: 1.1884 - acc: 0.4270 - val_loss: 1.0910 - val_acc: 0.5652\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.09098 to 1.08964, saving model to best.model\n",
      "0s - loss: 1.1425 - acc: 0.4382 - val_loss: 1.0896 - val_acc: 0.5652\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.08964 to 1.08800, saving model to best.model\n",
      "0s - loss: 1.1646 - acc: 0.3820 - val_loss: 1.0880 - val_acc: 0.5217\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.08800 to 1.08625, saving model to best.model\n",
      "0s - loss: 1.0872 - acc: 0.4382 - val_loss: 1.0862 - val_acc: 0.4783\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.08625 to 1.08354, saving model to best.model\n",
      "0s - loss: 1.0821 - acc: 0.3820 - val_loss: 1.0835 - val_acc: 0.4783\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.08354 to 1.08112, saving model to best.model\n",
      "0s - loss: 1.1600 - acc: 0.4831 - val_loss: 1.0811 - val_acc: 0.4348\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.08112 to 1.07861, saving model to best.model\n",
      "0s - loss: 1.0541 - acc: 0.4831 - val_loss: 1.0786 - val_acc: 0.4348\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.07861 to 1.07513, saving model to best.model\n",
      "0s - loss: 1.0165 - acc: 0.4944 - val_loss: 1.0751 - val_acc: 0.4348\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.07513 to 1.07096, saving model to best.model\n",
      "0s - loss: 1.0469 - acc: 0.4270 - val_loss: 1.0710 - val_acc: 0.4348\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.07096 to 1.06648, saving model to best.model\n",
      "0s - loss: 1.0663 - acc: 0.4494 - val_loss: 1.0665 - val_acc: 0.4348\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.06648 to 1.06153, saving model to best.model\n",
      "0s - loss: 1.0349 - acc: 0.4719 - val_loss: 1.0615 - val_acc: 0.4348\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.06153 to 1.05587, saving model to best.model\n",
      "0s - loss: 1.0262 - acc: 0.5506 - val_loss: 1.0559 - val_acc: 0.4348\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.05587 to 1.05105, saving model to best.model\n",
      "0s - loss: 1.0619 - acc: 0.4944 - val_loss: 1.0510 - val_acc: 0.4348\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.05105 to 1.04660, saving model to best.model\n",
      "0s - loss: 1.1051 - acc: 0.4045 - val_loss: 1.0466 - val_acc: 0.4348\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.04660 to 1.04180, saving model to best.model\n",
      "0s - loss: 1.0940 - acc: 0.4494 - val_loss: 1.0418 - val_acc: 0.4348\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.04180 to 1.03677, saving model to best.model\n",
      "0s - loss: 0.9706 - acc: 0.5056 - val_loss: 1.0368 - val_acc: 0.4348\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.03677 to 1.03220, saving model to best.model\n",
      "0s - loss: 1.0301 - acc: 0.5056 - val_loss: 1.0322 - val_acc: 0.4783\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.03220 to 1.02720, saving model to best.model\n",
      "0s - loss: 1.0807 - acc: 0.4045 - val_loss: 1.0272 - val_acc: 0.4783\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.02720 to 1.02160, saving model to best.model\n",
      "0s - loss: 1.0389 - acc: 0.4719 - val_loss: 1.0216 - val_acc: 0.4783\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.02160 to 1.01578, saving model to best.model\n",
      "0s - loss: 1.0064 - acc: 0.5281 - val_loss: 1.0158 - val_acc: 0.5217\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.01578 to 1.00990, saving model to best.model\n",
      "0s - loss: 0.9602 - acc: 0.5618 - val_loss: 1.0099 - val_acc: 0.5217\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.00990 to 1.00314, saving model to best.model\n",
      "0s - loss: 0.9725 - acc: 0.5506 - val_loss: 1.0031 - val_acc: 0.5652\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.00314 to 0.99798, saving model to best.model\n",
      "0s - loss: 0.9680 - acc: 0.4944 - val_loss: 0.9980 - val_acc: 0.5652\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.99798 to 0.99304, saving model to best.model\n",
      "0s - loss: 0.9627 - acc: 0.5618 - val_loss: 0.9930 - val_acc: 0.5652\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.99304 to 0.98790, saving model to best.model\n",
      "0s - loss: 1.0024 - acc: 0.4831 - val_loss: 0.9879 - val_acc: 0.5652\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.98790 to 0.98306, saving model to best.model\n",
      "0s - loss: 0.9435 - acc: 0.4944 - val_loss: 0.9831 - val_acc: 0.5652\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.98306 to 0.97735, saving model to best.model\n",
      "0s - loss: 0.9186 - acc: 0.5843 - val_loss: 0.9773 - val_acc: 0.5652\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.97735 to 0.97161, saving model to best.model\n",
      "0s - loss: 0.9710 - acc: 0.4382 - val_loss: 0.9716 - val_acc: 0.5652\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.97161 to 0.96603, saving model to best.model\n",
      "0s - loss: 0.9884 - acc: 0.4831 - val_loss: 0.9660 - val_acc: 0.5652\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.96603 to 0.96008, saving model to best.model\n",
      "0s - loss: 0.9305 - acc: 0.6067 - val_loss: 0.9601 - val_acc: 0.5652\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.96008 to 0.95368, saving model to best.model\n",
      "0s - loss: 0.9693 - acc: 0.4382 - val_loss: 0.9537 - val_acc: 0.5652\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.95368 to 0.94702, saving model to best.model\n",
      "0s - loss: 0.9523 - acc: 0.5393 - val_loss: 0.9470 - val_acc: 0.5652\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.94702 to 0.94007, saving model to best.model\n",
      "0s - loss: 0.9135 - acc: 0.5843 - val_loss: 0.9401 - val_acc: 0.5652\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.94007 to 0.93266, saving model to best.model\n",
      "0s - loss: 0.9547 - acc: 0.5506 - val_loss: 0.9327 - val_acc: 0.5652\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.93266 to 0.92442, saving model to best.model\n",
      "0s - loss: 0.9494 - acc: 0.5281 - val_loss: 0.9244 - val_acc: 0.5217\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.92442 to 0.91672, saving model to best.model\n",
      "0s - loss: 0.8827 - acc: 0.5618 - val_loss: 0.9167 - val_acc: 0.5217\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.91672 to 0.90892, saving model to best.model\n",
      "0s - loss: 0.8961 - acc: 0.5618 - val_loss: 0.9089 - val_acc: 0.5217\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.90892 to 0.90092, saving model to best.model\n",
      "0s - loss: 0.8354 - acc: 0.6067 - val_loss: 0.9009 - val_acc: 0.5217\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.90092 to 0.89262, saving model to best.model\n",
      "0s - loss: 0.8653 - acc: 0.5506 - val_loss: 0.8926 - val_acc: 0.5217\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.89262 to 0.88339, saving model to best.model\n",
      "0s - loss: 0.9516 - acc: 0.5618 - val_loss: 0.8834 - val_acc: 0.5217\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.88339 to 0.87438, saving model to best.model\n",
      "0s - loss: 0.8549 - acc: 0.5506 - val_loss: 0.8744 - val_acc: 0.5652\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.87438 to 0.86578, saving model to best.model\n",
      "0s - loss: 0.8301 - acc: 0.5955 - val_loss: 0.8658 - val_acc: 0.5652\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.86578 to 0.85586, saving model to best.model\n",
      "0s - loss: 0.8780 - acc: 0.5618 - val_loss: 0.8559 - val_acc: 0.5652\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.85586 to 0.84562, saving model to best.model\n",
      "0s - loss: 0.8473 - acc: 0.6292 - val_loss: 0.8456 - val_acc: 0.5652\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.84562 to 0.83540, saving model to best.model\n",
      "0s - loss: 0.7653 - acc: 0.6292 - val_loss: 0.8354 - val_acc: 0.6087\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.83540 to 0.82518, saving model to best.model\n",
      "0s - loss: 0.8728 - acc: 0.5730 - val_loss: 0.8252 - val_acc: 0.6087\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.82518 to 0.81586, saving model to best.model\n",
      "0s - loss: 0.9019 - acc: 0.5843 - val_loss: 0.8159 - val_acc: 0.6087\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.81586 to 0.80610, saving model to best.model\n",
      "0s - loss: 0.8040 - acc: 0.6067 - val_loss: 0.8061 - val_acc: 0.6087\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.80610 to 0.79598, saving model to best.model\n",
      "0s - loss: 0.8455 - acc: 0.5955 - val_loss: 0.7960 - val_acc: 0.6087\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.79598 to 0.78605, saving model to best.model\n",
      "0s - loss: 0.8019 - acc: 0.6404 - val_loss: 0.7860 - val_acc: 0.6087\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.78605 to 0.77667, saving model to best.model\n",
      "0s - loss: 0.8804 - acc: 0.5393 - val_loss: 0.7767 - val_acc: 0.6087\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.77667 to 0.76673, saving model to best.model\n",
      "0s - loss: 0.7971 - acc: 0.6292 - val_loss: 0.7667 - val_acc: 0.6087\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.76673 to 0.75674, saving model to best.model\n",
      "0s - loss: 0.8633 - acc: 0.5281 - val_loss: 0.7567 - val_acc: 0.6087\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.75674 to 0.74620, saving model to best.model\n",
      "0s - loss: 0.8169 - acc: 0.6629 - val_loss: 0.7462 - val_acc: 0.6087\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.74620 to 0.73550, saving model to best.model\n",
      "0s - loss: 0.7592 - acc: 0.6629 - val_loss: 0.7355 - val_acc: 0.6087\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.73550 to 0.72473, saving model to best.model\n",
      "0s - loss: 0.8410 - acc: 0.6180 - val_loss: 0.7247 - val_acc: 0.6087\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.72473 to 0.71374, saving model to best.model\n",
      "0s - loss: 0.7735 - acc: 0.6292 - val_loss: 0.7137 - val_acc: 0.6087\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.71374 to 0.70210, saving model to best.model\n",
      "0s - loss: 0.6770 - acc: 0.7079 - val_loss: 0.7021 - val_acc: 0.6087\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.70210 to 0.69091, saving model to best.model\n",
      "0s - loss: 0.7084 - acc: 0.6742 - val_loss: 0.6909 - val_acc: 0.6087\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.69091 to 0.68001, saving model to best.model\n",
      "0s - loss: 0.7318 - acc: 0.6966 - val_loss: 0.6800 - val_acc: 0.6087\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68001 to 0.67040, saving model to best.model\n",
      "0s - loss: 0.7359 - acc: 0.6966 - val_loss: 0.6704 - val_acc: 0.6087\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67040 to 0.66014, saving model to best.model\n",
      "0s - loss: 0.6720 - acc: 0.7303 - val_loss: 0.6601 - val_acc: 0.6087\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66014 to 0.64812, saving model to best.model\n",
      "0s - loss: 0.6555 - acc: 0.7191 - val_loss: 0.6481 - val_acc: 0.6087\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.64812 to 0.63625, saving model to best.model\n",
      "0s - loss: 0.6375 - acc: 0.7865 - val_loss: 0.6363 - val_acc: 0.6087\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.63625 to 0.62460, saving model to best.model\n",
      "0s - loss: 0.7177 - acc: 0.7303 - val_loss: 0.6246 - val_acc: 0.6087\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.62460 to 0.61306, saving model to best.model\n",
      "0s - loss: 0.6471 - acc: 0.7079 - val_loss: 0.6131 - val_acc: 0.6087\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.61306 to 0.60197, saving model to best.model\n",
      "0s - loss: 0.6703 - acc: 0.6966 - val_loss: 0.6020 - val_acc: 0.6522\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.60197 to 0.59138, saving model to best.model\n",
      "0s - loss: 0.6439 - acc: 0.7303 - val_loss: 0.5914 - val_acc: 0.6957\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.59138 to 0.58079, saving model to best.model\n",
      "0s - loss: 0.6647 - acc: 0.7303 - val_loss: 0.5808 - val_acc: 0.6957\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.58079 to 0.57134, saving model to best.model\n",
      "0s - loss: 0.6323 - acc: 0.6966 - val_loss: 0.5713 - val_acc: 0.6957\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.57134 to 0.56202, saving model to best.model\n",
      "0s - loss: 0.6423 - acc: 0.7416 - val_loss: 0.5620 - val_acc: 0.6957\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.56202 to 0.55410, saving model to best.model\n",
      "0s - loss: 0.5769 - acc: 0.8202 - val_loss: 0.5541 - val_acc: 0.6957\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.55410 to 0.54636, saving model to best.model\n",
      "0s - loss: 0.6411 - acc: 0.7303 - val_loss: 0.5464 - val_acc: 0.6957\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.54636 to 0.53891, saving model to best.model\n",
      "0s - loss: 0.5679 - acc: 0.7753 - val_loss: 0.5389 - val_acc: 0.6957\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.53891 to 0.53229, saving model to best.model\n",
      "0s - loss: 0.6134 - acc: 0.7528 - val_loss: 0.5323 - val_acc: 0.6957\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.53229 to 0.52659, saving model to best.model\n",
      "0s - loss: 0.5359 - acc: 0.8090 - val_loss: 0.5266 - val_acc: 0.6957\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.52659 to 0.52115, saving model to best.model\n",
      "0s - loss: 0.5716 - acc: 0.7640 - val_loss: 0.5212 - val_acc: 0.6957\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.52115 to 0.51578, saving model to best.model\n",
      "0s - loss: 0.5266 - acc: 0.7753 - val_loss: 0.5158 - val_acc: 0.6957\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.51578 to 0.50959, saving model to best.model\n",
      "0s - loss: 0.5904 - acc: 0.7191 - val_loss: 0.5096 - val_acc: 0.6957\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.50959 to 0.50202, saving model to best.model\n",
      "0s - loss: 0.4984 - acc: 0.7640 - val_loss: 0.5020 - val_acc: 0.6957\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.50202 to 0.49394, saving model to best.model\n",
      "0s - loss: 0.5109 - acc: 0.8090 - val_loss: 0.4939 - val_acc: 0.7391\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.49394 to 0.48566, saving model to best.model\n",
      "0s - loss: 0.5318 - acc: 0.7640 - val_loss: 0.4857 - val_acc: 0.7391\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.48566 to 0.47616, saving model to best.model\n",
      "0s - loss: 0.5426 - acc: 0.7753 - val_loss: 0.4762 - val_acc: 0.8261\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.47616 to 0.46739, saving model to best.model\n",
      "0s - loss: 0.4976 - acc: 0.8090 - val_loss: 0.4674 - val_acc: 0.8696\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.46739 to 0.45702, saving model to best.model\n",
      "0s - loss: 0.4812 - acc: 0.8202 - val_loss: 0.4570 - val_acc: 0.8696\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.45702 to 0.44784, saving model to best.model\n",
      "0s - loss: 0.5337 - acc: 0.7753 - val_loss: 0.4478 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.44784 to 0.43856, saving model to best.model\n",
      "0s - loss: 0.4615 - acc: 0.8315 - val_loss: 0.4386 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.43856 to 0.42973, saving model to best.model\n",
      "0s - loss: 0.4669 - acc: 0.8090 - val_loss: 0.4297 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.42973 to 0.42090, saving model to best.model\n",
      "0s - loss: 0.4550 - acc: 0.8427 - val_loss: 0.4209 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.42090 to 0.41358, saving model to best.model\n",
      "0s - loss: 0.4460 - acc: 0.8427 - val_loss: 0.4136 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.41358 to 0.40647, saving model to best.model\n",
      "0s - loss: 0.4988 - acc: 0.7978 - val_loss: 0.4065 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.40647 to 0.40069, saving model to best.model\n",
      "0s - loss: 0.4528 - acc: 0.7978 - val_loss: 0.4007 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.40069 to 0.39591, saving model to best.model\n",
      "0s - loss: 0.4731 - acc: 0.7978 - val_loss: 0.3959 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.39591 to 0.39187, saving model to best.model\n",
      "0s - loss: 0.4846 - acc: 0.7865 - val_loss: 0.3919 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.39187 to 0.38817, saving model to best.model\n",
      "0s - loss: 0.4800 - acc: 0.7753 - val_loss: 0.3882 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.38817 to 0.38294, saving model to best.model\n",
      "0s - loss: 0.4754 - acc: 0.8090 - val_loss: 0.3829 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.38294 to 0.37723, saving model to best.model\n",
      "0s - loss: 0.4423 - acc: 0.8315 - val_loss: 0.3772 - val_acc: 0.9565\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.37723 to 0.37237, saving model to best.model\n",
      "0s - loss: 0.3956 - acc: 0.8315 - val_loss: 0.3724 - val_acc: 0.9565\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.37237 to 0.36815, saving model to best.model\n",
      "0s - loss: 0.4617 - acc: 0.8539 - val_loss: 0.3681 - val_acc: 0.9565\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.36815 to 0.36427, saving model to best.model\n",
      "0s - loss: 0.3535 - acc: 0.8876 - val_loss: 0.3643 - val_acc: 0.9565\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.36427 to 0.36083, saving model to best.model\n",
      "0s - loss: 0.5163 - acc: 0.7079 - val_loss: 0.3608 - val_acc: 0.9565\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.36083 to 0.35757, saving model to best.model\n",
      "0s - loss: 0.4481 - acc: 0.8652 - val_loss: 0.3576 - val_acc: 0.9565\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.35757 to 0.35331, saving model to best.model\n",
      "0s - loss: 0.4225 - acc: 0.8315 - val_loss: 0.3533 - val_acc: 0.9565\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.35331 to 0.34864, saving model to best.model\n",
      "0s - loss: 0.4083 - acc: 0.8764 - val_loss: 0.3486 - val_acc: 0.9565\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.34864 to 0.34244, saving model to best.model\n",
      "0s - loss: 0.3515 - acc: 0.8427 - val_loss: 0.3424 - val_acc: 0.9565\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.34244 to 0.33425, saving model to best.model\n",
      "0s - loss: 0.3975 - acc: 0.8876 - val_loss: 0.3342 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.33425 to 0.32567, saving model to best.model\n",
      "0s - loss: 0.4278 - acc: 0.8090 - val_loss: 0.3257 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.32567 to 0.31721, saving model to best.model\n",
      "0s - loss: 0.3871 - acc: 0.8876 - val_loss: 0.3172 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.31721 to 0.30961, saving model to best.model\n",
      "0s - loss: 0.2800 - acc: 0.9438 - val_loss: 0.3096 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.30961 to 0.30270, saving model to best.model\n",
      "0s - loss: 0.3383 - acc: 0.9438 - val_loss: 0.3027 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.30270 to 0.29597, saving model to best.model\n",
      "0s - loss: 0.3851 - acc: 0.8315 - val_loss: 0.2960 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.29597 to 0.28999, saving model to best.model\n",
      "0s - loss: 0.2690 - acc: 0.9438 - val_loss: 0.2900 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.28999 to 0.28480, saving model to best.model\n",
      "0s - loss: 0.3029 - acc: 0.8876 - val_loss: 0.2848 - val_acc: 0.9565\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.28480 to 0.27954, saving model to best.model\n",
      "0s - loss: 0.2445 - acc: 0.9551 - val_loss: 0.2795 - val_acc: 0.9565\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.27954 to 0.27433, saving model to best.model\n",
      "0s - loss: 0.3200 - acc: 0.8652 - val_loss: 0.2743 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.27433 to 0.26942, saving model to best.model\n",
      "0s - loss: 0.2784 - acc: 0.8989 - val_loss: 0.2694 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.26942 to 0.26432, saving model to best.model\n",
      "0s - loss: 0.2716 - acc: 0.8876 - val_loss: 0.2643 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.26432 to 0.25960, saving model to best.model\n",
      "0s - loss: 0.2893 - acc: 0.9101 - val_loss: 0.2596 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.25960 to 0.25543, saving model to best.model\n",
      "0s - loss: 0.3491 - acc: 0.8764 - val_loss: 0.2554 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.25543 to 0.25094, saving model to best.model\n",
      "0s - loss: 0.2355 - acc: 0.9326 - val_loss: 0.2509 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.25094 to 0.24581, saving model to best.model\n",
      "0s - loss: 0.2840 - acc: 0.8989 - val_loss: 0.2458 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.24581 to 0.24164, saving model to best.model\n",
      "0s - loss: 0.2798 - acc: 0.8876 - val_loss: 0.2416 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.24164 to 0.23744, saving model to best.model\n",
      "0s - loss: 0.2718 - acc: 0.9213 - val_loss: 0.2374 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.23744 to 0.23297, saving model to best.model\n",
      "0s - loss: 0.3212 - acc: 0.8652 - val_loss: 0.2330 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.23297 to 0.22895, saving model to best.model\n",
      "0s - loss: 0.3099 - acc: 0.8652 - val_loss: 0.2289 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.22895 to 0.22463, saving model to best.model\n",
      "0s - loss: 0.2632 - acc: 0.9101 - val_loss: 0.2246 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.22463 to 0.22059, saving model to best.model\n",
      "0s - loss: 0.2822 - acc: 0.8764 - val_loss: 0.2206 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.22059 to 0.21728, saving model to best.model\n",
      "0s - loss: 0.2974 - acc: 0.8764 - val_loss: 0.2173 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.21728 to 0.21329, saving model to best.model\n",
      "0s - loss: 0.2745 - acc: 0.8989 - val_loss: 0.2133 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.21329 to 0.20950, saving model to best.model\n",
      "0s - loss: 0.3166 - acc: 0.9326 - val_loss: 0.2095 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.20950 to 0.20527, saving model to best.model\n",
      "0s - loss: 0.2411 - acc: 0.9213 - val_loss: 0.2053 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.20527 to 0.20082, saving model to best.model\n",
      "0s - loss: 0.2699 - acc: 0.8989 - val_loss: 0.2008 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.20082 to 0.19648, saving model to best.model\n",
      "0s - loss: 0.2437 - acc: 0.9213 - val_loss: 0.1965 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.19648 to 0.19250, saving model to best.model\n",
      "0s - loss: 0.2454 - acc: 0.9326 - val_loss: 0.1925 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.19250 to 0.18893, saving model to best.model\n",
      "0s - loss: 0.1985 - acc: 0.9551 - val_loss: 0.1889 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.18893 to 0.18539, saving model to best.model\n",
      "0s - loss: 0.3077 - acc: 0.9101 - val_loss: 0.1854 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.18539 to 0.18111, saving model to best.model\n",
      "0s - loss: 0.2202 - acc: 0.9101 - val_loss: 0.1811 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.18111 to 0.17749, saving model to best.model\n",
      "0s - loss: 0.2069 - acc: 0.9213 - val_loss: 0.1775 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.17749 to 0.17310, saving model to best.model\n",
      "0s - loss: 0.2224 - acc: 0.9213 - val_loss: 0.1731 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.17310 to 0.16882, saving model to best.model\n",
      "0s - loss: 0.1881 - acc: 0.9663 - val_loss: 0.1688 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.16882 to 0.16526, saving model to best.model\n",
      "0s - loss: 0.2015 - acc: 0.9438 - val_loss: 0.1653 - val_acc: 0.9565\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.16526 to 0.16133, saving model to best.model\n",
      "0s - loss: 0.2147 - acc: 0.9438 - val_loss: 0.1613 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.16133 to 0.15778, saving model to best.model\n",
      "0s - loss: 0.2346 - acc: 0.9326 - val_loss: 0.1578 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.15778 to 0.15364, saving model to best.model\n",
      "0s - loss: 0.2416 - acc: 0.9101 - val_loss: 0.1536 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.15364 to 0.14885, saving model to best.model\n",
      "0s - loss: 0.2249 - acc: 0.9213 - val_loss: 0.1489 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.14885 to 0.14428, saving model to best.model\n",
      "0s - loss: 0.1899 - acc: 0.9438 - val_loss: 0.1443 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.14428 to 0.14005, saving model to best.model\n",
      "0s - loss: 0.1872 - acc: 0.9438 - val_loss: 0.1401 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.14005 to 0.13667, saving model to best.model\n",
      "0s - loss: 0.1909 - acc: 0.9663 - val_loss: 0.1367 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.13667 to 0.13392, saving model to best.model\n",
      "0s - loss: 0.1776 - acc: 0.9438 - val_loss: 0.1339 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.13392 to 0.13052, saving model to best.model\n",
      "0s - loss: 0.2102 - acc: 0.9438 - val_loss: 0.1305 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.13052 to 0.12735, saving model to best.model\n",
      "0s - loss: 0.2114 - acc: 0.9326 - val_loss: 0.1274 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.12735 to 0.12511, saving model to best.model\n",
      "0s - loss: 0.1940 - acc: 0.9438 - val_loss: 0.1251 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.12511 to 0.12300, saving model to best.model\n",
      "0s - loss: 0.2038 - acc: 0.9101 - val_loss: 0.1230 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.12300 to 0.12114, saving model to best.model\n",
      "0s - loss: 0.2564 - acc: 0.8764 - val_loss: 0.1211 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.12114 to 0.11971, saving model to best.model\n",
      "0s - loss: 0.1277 - acc: 0.9551 - val_loss: 0.1197 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.11971 to 0.11770, saving model to best.model\n",
      "0s - loss: 0.1864 - acc: 0.9438 - val_loss: 0.1177 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.11770 to 0.11627, saving model to best.model\n",
      "0s - loss: 0.1643 - acc: 0.9438 - val_loss: 0.1163 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.11627 to 0.11443, saving model to best.model\n",
      "0s - loss: 0.1728 - acc: 0.9551 - val_loss: 0.1144 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.11443 to 0.11315, saving model to best.model\n",
      "0s - loss: 0.1562 - acc: 0.9775 - val_loss: 0.1132 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.11315 to 0.11183, saving model to best.model\n",
      "0s - loss: 0.1705 - acc: 0.9101 - val_loss: 0.1118 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.11183 to 0.11019, saving model to best.model\n",
      "0s - loss: 0.2256 - acc: 0.9101 - val_loss: 0.1102 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.11019 to 0.10861, saving model to best.model\n",
      "0s - loss: 0.1361 - acc: 0.9663 - val_loss: 0.1086 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.10861 to 0.10689, saving model to best.model\n",
      "0s - loss: 0.1471 - acc: 0.9663 - val_loss: 0.1069 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.10689 to 0.10447, saving model to best.model\n",
      "0s - loss: 0.2124 - acc: 0.9326 - val_loss: 0.1045 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.10447 to 0.10240, saving model to best.model\n",
      "0s - loss: 0.1734 - acc: 0.9326 - val_loss: 0.1024 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.10240 to 0.10101, saving model to best.model\n",
      "0s - loss: 0.1697 - acc: 0.9775 - val_loss: 0.1010 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.10101 to 0.10091, saving model to best.model\n",
      "0s - loss: 0.1187 - acc: 0.9663 - val_loss: 0.1009 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.10091 to 0.10064, saving model to best.model\n",
      "0s - loss: 0.1060 - acc: 0.9663 - val_loss: 0.1006 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.10064 to 0.09996, saving model to best.model\n",
      "0s - loss: 0.1469 - acc: 0.9663 - val_loss: 0.1000 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.09996 to 0.09854, saving model to best.model\n",
      "0s - loss: 0.1389 - acc: 0.9551 - val_loss: 0.0985 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.09854 to 0.09762, saving model to best.model\n",
      "0s - loss: 0.1448 - acc: 0.9775 - val_loss: 0.0976 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.18830, saving model to best.model\n",
      "0s - loss: 1.5593 - acc: 0.2921 - val_loss: 1.1883 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.18830 to 1.13496, saving model to best.model\n",
      "0s - loss: 1.4971 - acc: 0.3371 - val_loss: 1.1350 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.13496 to 1.10706, saving model to best.model\n",
      "0s - loss: 1.3945 - acc: 0.3371 - val_loss: 1.1071 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.10706 to 1.10101, saving model to best.model\n",
      "0s - loss: 1.3195 - acc: 0.3596 - val_loss: 1.1010 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2809 - acc: 0.3596 - val_loss: 1.1067 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2712 - acc: 0.3258 - val_loss: 1.1239 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2380 - acc: 0.3596 - val_loss: 1.1496 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2198 - acc: 0.3933 - val_loss: 1.1800 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1722 - acc: 0.4270 - val_loss: 1.2059 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1566 - acc: 0.3371 - val_loss: 1.2256 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.3504 - acc: 0.3034 - val_loss: 1.2362 - val_acc: 0.2609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.3343 - acc: 0.3596 - val_loss: 1.2340 - val_acc: 0.2609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2182 - acc: 0.4157 - val_loss: 1.2229 - val_acc: 0.2609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2459 - acc: 0.3820 - val_loss: 1.2055 - val_acc: 0.2609\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1732 - acc: 0.4494 - val_loss: 1.1835 - val_acc: 0.2609\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1231 - acc: 0.4045 - val_loss: 1.1608 - val_acc: 0.2609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2432 - acc: 0.3371 - val_loss: 1.1379 - val_acc: 0.2609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1364 - acc: 0.4045 - val_loss: 1.1181 - val_acc: 0.4348\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2813 - acc: 0.3258 - val_loss: 1.1021 - val_acc: 0.4348\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.10101 to 1.08870, saving model to best.model\n",
      "0s - loss: 1.2525 - acc: 0.3708 - val_loss: 1.0887 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.08870 to 1.07915, saving model to best.model\n",
      "0s - loss: 1.1664 - acc: 0.3146 - val_loss: 1.0791 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.07915 to 1.07126, saving model to best.model\n",
      "0s - loss: 1.1837 - acc: 0.4045 - val_loss: 1.0713 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.07126 to 1.06586, saving model to best.model\n",
      "0s - loss: 1.2302 - acc: 0.3708 - val_loss: 1.0659 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.06586 to 1.06148, saving model to best.model\n",
      "0s - loss: 1.1979 - acc: 0.3596 - val_loss: 1.0615 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.06148 to 1.05756, saving model to best.model\n",
      "0s - loss: 1.1163 - acc: 0.3933 - val_loss: 1.0576 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.05756 to 1.05387, saving model to best.model\n",
      "0s - loss: 1.1708 - acc: 0.3933 - val_loss: 1.0539 - val_acc: 0.3913\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.05387 to 1.05052, saving model to best.model\n",
      "0s - loss: 1.1838 - acc: 0.3146 - val_loss: 1.0505 - val_acc: 0.3913\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.05052 to 1.04805, saving model to best.model\n",
      "0s - loss: 1.2630 - acc: 0.3371 - val_loss: 1.0481 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.04805 to 1.04630, saving model to best.model\n",
      "0s - loss: 1.2466 - acc: 0.2921 - val_loss: 1.0463 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.04630 to 1.04488, saving model to best.model\n",
      "0s - loss: 1.1485 - acc: 0.3708 - val_loss: 1.0449 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.04488 to 1.04405, saving model to best.model\n",
      "0s - loss: 1.1681 - acc: 0.3034 - val_loss: 1.0440 - val_acc: 0.3913\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.04405 to 1.04395, saving model to best.model\n",
      "0s - loss: 1.1800 - acc: 0.3596 - val_loss: 1.0439 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.2039 - acc: 0.3820 - val_loss: 1.0441 - val_acc: 0.3913\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1552 - acc: 0.3933 - val_loss: 1.0449 - val_acc: 0.4348\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.2459 - acc: 0.3483 - val_loss: 1.0464 - val_acc: 0.5652\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.1544 - acc: 0.3371 - val_loss: 1.0481 - val_acc: 0.5652\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 1.1880 - acc: 0.3146 - val_loss: 1.0503 - val_acc: 0.5652\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 1.1562 - acc: 0.4157 - val_loss: 1.0517 - val_acc: 0.6087\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 1.1226 - acc: 0.4045 - val_loss: 1.0525 - val_acc: 0.6087\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 1.0752 - acc: 0.4831 - val_loss: 1.0523 - val_acc: 0.6087\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 1.1100 - acc: 0.3933 - val_loss: 1.0509 - val_acc: 0.6087\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 1.1141 - acc: 0.3820 - val_loss: 1.0489 - val_acc: 0.6087\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 1.1276 - acc: 0.3820 - val_loss: 1.0458 - val_acc: 0.6087\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.04395 to 1.04223, saving model to best.model\n",
      "0s - loss: 1.1100 - acc: 0.3820 - val_loss: 1.0422 - val_acc: 0.6087\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.04223 to 1.03850, saving model to best.model\n",
      "0s - loss: 1.1581 - acc: 0.2697 - val_loss: 1.0385 - val_acc: 0.5652\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.03850 to 1.03423, saving model to best.model\n",
      "0s - loss: 1.0413 - acc: 0.4944 - val_loss: 1.0342 - val_acc: 0.5652\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.03423 to 1.02963, saving model to best.model\n",
      "0s - loss: 1.1718 - acc: 0.4045 - val_loss: 1.0296 - val_acc: 0.5652\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.02963 to 1.02419, saving model to best.model\n",
      "0s - loss: 1.0397 - acc: 0.4607 - val_loss: 1.0242 - val_acc: 0.5652\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.02419 to 1.01894, saving model to best.model\n",
      "0s - loss: 1.0620 - acc: 0.4831 - val_loss: 1.0189 - val_acc: 0.6087\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.01894 to 1.01351, saving model to best.model\n",
      "0s - loss: 1.0578 - acc: 0.4157 - val_loss: 1.0135 - val_acc: 0.5652\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.01351 to 1.00771, saving model to best.model\n",
      "0s - loss: 0.9891 - acc: 0.4831 - val_loss: 1.0077 - val_acc: 0.5652\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.00771 to 1.00150, saving model to best.model\n",
      "0s - loss: 0.9655 - acc: 0.5281 - val_loss: 1.0015 - val_acc: 0.5652\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.00150 to 0.99516, saving model to best.model\n",
      "0s - loss: 1.1525 - acc: 0.3596 - val_loss: 0.9952 - val_acc: 0.5652\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.99516 to 0.98871, saving model to best.model\n",
      "0s - loss: 1.0759 - acc: 0.4831 - val_loss: 0.9887 - val_acc: 0.5652\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.98871 to 0.98222, saving model to best.model\n",
      "0s - loss: 0.9777 - acc: 0.5281 - val_loss: 0.9822 - val_acc: 0.5652\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.98222 to 0.97623, saving model to best.model\n",
      "0s - loss: 1.0941 - acc: 0.4382 - val_loss: 0.9762 - val_acc: 0.5652\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.97623 to 0.97061, saving model to best.model\n",
      "0s - loss: 1.0972 - acc: 0.3933 - val_loss: 0.9706 - val_acc: 0.5652\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.97061 to 0.96567, saving model to best.model\n",
      "0s - loss: 1.0093 - acc: 0.4944 - val_loss: 0.9657 - val_acc: 0.5652\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.96567 to 0.96093, saving model to best.model\n",
      "0s - loss: 0.9284 - acc: 0.5506 - val_loss: 0.9609 - val_acc: 0.6087\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.96093 to 0.95603, saving model to best.model\n",
      "0s - loss: 1.1168 - acc: 0.3933 - val_loss: 0.9560 - val_acc: 0.6087\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.95603 to 0.95134, saving model to best.model\n",
      "0s - loss: 1.0191 - acc: 0.4494 - val_loss: 0.9513 - val_acc: 0.6087\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.95134 to 0.94656, saving model to best.model\n",
      "0s - loss: 1.0686 - acc: 0.4831 - val_loss: 0.9466 - val_acc: 0.6087\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.94656 to 0.94182, saving model to best.model\n",
      "0s - loss: 1.0006 - acc: 0.5281 - val_loss: 0.9418 - val_acc: 0.6087\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.94182 to 0.93670, saving model to best.model\n",
      "0s - loss: 1.1270 - acc: 0.3371 - val_loss: 0.9367 - val_acc: 0.6087\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.93670 to 0.93264, saving model to best.model\n",
      "0s - loss: 0.9807 - acc: 0.4944 - val_loss: 0.9326 - val_acc: 0.6087\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.93264 to 0.92759, saving model to best.model\n",
      "0s - loss: 1.0755 - acc: 0.4382 - val_loss: 0.9276 - val_acc: 0.6087\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.92759 to 0.92195, saving model to best.model\n",
      "0s - loss: 0.9801 - acc: 0.5056 - val_loss: 0.9219 - val_acc: 0.6087\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.92195 to 0.91631, saving model to best.model\n",
      "0s - loss: 0.9905 - acc: 0.4607 - val_loss: 0.9163 - val_acc: 0.6087\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.91631 to 0.91016, saving model to best.model\n",
      "0s - loss: 0.9750 - acc: 0.4494 - val_loss: 0.9102 - val_acc: 0.6087\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.91016 to 0.90364, saving model to best.model\n",
      "0s - loss: 1.0386 - acc: 0.4607 - val_loss: 0.9036 - val_acc: 0.6087\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.90364 to 0.89659, saving model to best.model\n",
      "0s - loss: 1.0034 - acc: 0.4719 - val_loss: 0.8966 - val_acc: 0.6087\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.89659 to 0.88905, saving model to best.model\n",
      "0s - loss: 0.9621 - acc: 0.5056 - val_loss: 0.8891 - val_acc: 0.6087\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.88905 to 0.88047, saving model to best.model\n",
      "0s - loss: 0.9419 - acc: 0.4944 - val_loss: 0.8805 - val_acc: 0.6087\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.88047 to 0.87188, saving model to best.model\n",
      "0s - loss: 0.9018 - acc: 0.6966 - val_loss: 0.8719 - val_acc: 0.6087\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.87188 to 0.86300, saving model to best.model\n",
      "0s - loss: 1.0177 - acc: 0.4045 - val_loss: 0.8630 - val_acc: 0.6522\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.86300 to 0.85401, saving model to best.model\n",
      "0s - loss: 0.8852 - acc: 0.6067 - val_loss: 0.8540 - val_acc: 0.6957\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.85401 to 0.84484, saving model to best.model\n",
      "0s - loss: 0.9217 - acc: 0.5393 - val_loss: 0.8448 - val_acc: 0.7391\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.84484 to 0.83551, saving model to best.model\n",
      "0s - loss: 0.8819 - acc: 0.5843 - val_loss: 0.8355 - val_acc: 0.7391\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.83551 to 0.82596, saving model to best.model\n",
      "0s - loss: 0.9248 - acc: 0.5393 - val_loss: 0.8260 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.82596 to 0.81622, saving model to best.model\n",
      "0s - loss: 0.9056 - acc: 0.5169 - val_loss: 0.8162 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.81622 to 0.80524, saving model to best.model\n",
      "0s - loss: 0.9587 - acc: 0.5506 - val_loss: 0.8052 - val_acc: 0.8261\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.80524 to 0.79413, saving model to best.model\n",
      "0s - loss: 0.8785 - acc: 0.5393 - val_loss: 0.7941 - val_acc: 0.8261\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.79413 to 0.78336, saving model to best.model\n",
      "0s - loss: 0.8837 - acc: 0.5730 - val_loss: 0.7834 - val_acc: 0.8261\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.78336 to 0.77252, saving model to best.model\n",
      "0s - loss: 0.9019 - acc: 0.6067 - val_loss: 0.7725 - val_acc: 0.8696\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.77252 to 0.76141, saving model to best.model\n",
      "0s - loss: 0.7667 - acc: 0.6854 - val_loss: 0.7614 - val_acc: 0.8696\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.76141 to 0.75069, saving model to best.model\n",
      "0s - loss: 0.9032 - acc: 0.5618 - val_loss: 0.7507 - val_acc: 0.8696\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.75069 to 0.73978, saving model to best.model\n",
      "0s - loss: 0.8222 - acc: 0.5730 - val_loss: 0.7398 - val_acc: 0.9565\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.73978 to 0.72866, saving model to best.model\n",
      "0s - loss: 0.8059 - acc: 0.6854 - val_loss: 0.7287 - val_acc: 0.9565\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.72866 to 0.71799, saving model to best.model\n",
      "0s - loss: 0.8735 - acc: 0.5730 - val_loss: 0.7180 - val_acc: 0.9565\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.71799 to 0.70709, saving model to best.model\n",
      "0s - loss: 0.7743 - acc: 0.6966 - val_loss: 0.7071 - val_acc: 0.9565\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.70709 to 0.69633, saving model to best.model\n",
      "0s - loss: 0.7849 - acc: 0.7753 - val_loss: 0.6963 - val_acc: 0.9565\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.69633 to 0.68579, saving model to best.model\n",
      "0s - loss: 0.8172 - acc: 0.6629 - val_loss: 0.6858 - val_acc: 0.9565\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68579 to 0.67540, saving model to best.model\n",
      "0s - loss: 0.7720 - acc: 0.6742 - val_loss: 0.6754 - val_acc: 0.9565\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67540 to 0.66533, saving model to best.model\n",
      "0s - loss: 0.7917 - acc: 0.6854 - val_loss: 0.6653 - val_acc: 0.9565\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66533 to 0.65494, saving model to best.model\n",
      "0s - loss: 0.7343 - acc: 0.7191 - val_loss: 0.6549 - val_acc: 0.9565\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.65494 to 0.64423, saving model to best.model\n",
      "0s - loss: 0.7237 - acc: 0.7191 - val_loss: 0.6442 - val_acc: 0.9565\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.64423 to 0.63336, saving model to best.model\n",
      "0s - loss: 0.6969 - acc: 0.7191 - val_loss: 0.6334 - val_acc: 0.9565\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.63336 to 0.62244, saving model to best.model\n",
      "0s - loss: 0.6697 - acc: 0.7416 - val_loss: 0.6224 - val_acc: 0.9565\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.62244 to 0.61165, saving model to best.model\n",
      "0s - loss: 0.7449 - acc: 0.6067 - val_loss: 0.6117 - val_acc: 0.9565\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.61165 to 0.60075, saving model to best.model\n",
      "0s - loss: 0.6905 - acc: 0.7865 - val_loss: 0.6008 - val_acc: 0.9565\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.60075 to 0.58990, saving model to best.model\n",
      "0s - loss: 0.6330 - acc: 0.7528 - val_loss: 0.5899 - val_acc: 0.9565\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.58990 to 0.57976, saving model to best.model\n",
      "0s - loss: 0.6678 - acc: 0.7640 - val_loss: 0.5798 - val_acc: 0.9565\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.57976 to 0.57000, saving model to best.model\n",
      "0s - loss: 0.5966 - acc: 0.7978 - val_loss: 0.5700 - val_acc: 0.9565\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.57000 to 0.56035, saving model to best.model\n",
      "0s - loss: 0.6542 - acc: 0.7978 - val_loss: 0.5604 - val_acc: 0.9565\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.56035 to 0.55091, saving model to best.model\n",
      "0s - loss: 0.6995 - acc: 0.7416 - val_loss: 0.5509 - val_acc: 0.9565\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.55091 to 0.54070, saving model to best.model\n",
      "0s - loss: 0.6281 - acc: 0.7753 - val_loss: 0.5407 - val_acc: 0.9565\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.54070 to 0.53022, saving model to best.model\n",
      "0s - loss: 0.6108 - acc: 0.7191 - val_loss: 0.5302 - val_acc: 0.9565\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.53022 to 0.51983, saving model to best.model\n",
      "0s - loss: 0.6384 - acc: 0.7640 - val_loss: 0.5198 - val_acc: 0.9565\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.51983 to 0.50906, saving model to best.model\n",
      "0s - loss: 0.6170 - acc: 0.7303 - val_loss: 0.5091 - val_acc: 0.9565\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.50906 to 0.49863, saving model to best.model\n",
      "0s - loss: 0.6246 - acc: 0.7528 - val_loss: 0.4986 - val_acc: 0.9565\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.49863 to 0.48863, saving model to best.model\n",
      "0s - loss: 0.5494 - acc: 0.8090 - val_loss: 0.4886 - val_acc: 0.9565\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.48863 to 0.47929, saving model to best.model\n",
      "0s - loss: 0.6266 - acc: 0.7416 - val_loss: 0.4793 - val_acc: 0.9565\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.47929 to 0.46997, saving model to best.model\n",
      "0s - loss: 0.5936 - acc: 0.7640 - val_loss: 0.4700 - val_acc: 0.9565\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.46997 to 0.46079, saving model to best.model\n",
      "0s - loss: 0.5370 - acc: 0.8202 - val_loss: 0.4608 - val_acc: 0.9565\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.46079 to 0.45160, saving model to best.model\n",
      "0s - loss: 0.5592 - acc: 0.8090 - val_loss: 0.4516 - val_acc: 0.9565\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.45160 to 0.44252, saving model to best.model\n",
      "0s - loss: 0.6009 - acc: 0.7753 - val_loss: 0.4425 - val_acc: 0.9565\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.44252 to 0.43312, saving model to best.model\n",
      "0s - loss: 0.5262 - acc: 0.7865 - val_loss: 0.4331 - val_acc: 0.9565\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.43312 to 0.42369, saving model to best.model\n",
      "0s - loss: 0.5199 - acc: 0.8427 - val_loss: 0.4237 - val_acc: 0.9565\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.42369 to 0.41502, saving model to best.model\n",
      "0s - loss: 0.5032 - acc: 0.8539 - val_loss: 0.4150 - val_acc: 0.9565\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.41502 to 0.40723, saving model to best.model\n",
      "0s - loss: 0.5300 - acc: 0.8090 - val_loss: 0.4072 - val_acc: 0.9565\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.40723 to 0.40011, saving model to best.model\n",
      "0s - loss: 0.4560 - acc: 0.8652 - val_loss: 0.4001 - val_acc: 0.9565\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.40011 to 0.39347, saving model to best.model\n",
      "0s - loss: 0.5138 - acc: 0.8764 - val_loss: 0.3935 - val_acc: 0.9565\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.39347 to 0.38709, saving model to best.model\n",
      "0s - loss: 0.4788 - acc: 0.8652 - val_loss: 0.3871 - val_acc: 0.9565\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.38709 to 0.38132, saving model to best.model\n",
      "0s - loss: 0.4673 - acc: 0.8202 - val_loss: 0.3813 - val_acc: 0.9565\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.38132 to 0.37499, saving model to best.model\n",
      "0s - loss: 0.5422 - acc: 0.7978 - val_loss: 0.3750 - val_acc: 0.9565\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.37499 to 0.36889, saving model to best.model\n",
      "0s - loss: 0.4990 - acc: 0.7978 - val_loss: 0.3689 - val_acc: 0.9565\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.36889 to 0.36206, saving model to best.model\n",
      "0s - loss: 0.4182 - acc: 0.8652 - val_loss: 0.3621 - val_acc: 0.9565\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.36206 to 0.35576, saving model to best.model\n",
      "0s - loss: 0.4064 - acc: 0.8652 - val_loss: 0.3558 - val_acc: 0.9565\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.35576 to 0.35000, saving model to best.model\n",
      "0s - loss: 0.4378 - acc: 0.8427 - val_loss: 0.3500 - val_acc: 0.9565\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.35000 to 0.34392, saving model to best.model\n",
      "0s - loss: 0.4334 - acc: 0.8764 - val_loss: 0.3439 - val_acc: 0.9565\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.34392 to 0.33773, saving model to best.model\n",
      "0s - loss: 0.3750 - acc: 0.8989 - val_loss: 0.3377 - val_acc: 0.9565\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.33773 to 0.33085, saving model to best.model\n",
      "0s - loss: 0.4431 - acc: 0.8427 - val_loss: 0.3309 - val_acc: 0.9565\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.33085 to 0.32432, saving model to best.model\n",
      "0s - loss: 0.4323 - acc: 0.8315 - val_loss: 0.3243 - val_acc: 0.9565\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.32432 to 0.31759, saving model to best.model\n",
      "0s - loss: 0.4369 - acc: 0.8427 - val_loss: 0.3176 - val_acc: 0.9565\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.31759 to 0.31149, saving model to best.model\n",
      "0s - loss: 0.4129 - acc: 0.9101 - val_loss: 0.3115 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.31149 to 0.30585, saving model to best.model\n",
      "0s - loss: 0.3290 - acc: 0.9213 - val_loss: 0.3059 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.30585 to 0.30092, saving model to best.model\n",
      "0s - loss: 0.4797 - acc: 0.7753 - val_loss: 0.3009 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.30092 to 0.29538, saving model to best.model\n",
      "0s - loss: 0.3229 - acc: 0.9326 - val_loss: 0.2954 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.29538 to 0.29001, saving model to best.model\n",
      "0s - loss: 0.3447 - acc: 0.8876 - val_loss: 0.2900 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.29001 to 0.28448, saving model to best.model\n",
      "0s - loss: 0.4295 - acc: 0.8427 - val_loss: 0.2845 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.28448 to 0.27844, saving model to best.model\n",
      "0s - loss: 0.3462 - acc: 0.8539 - val_loss: 0.2784 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.27844 to 0.27226, saving model to best.model\n",
      "0s - loss: 0.4121 - acc: 0.8539 - val_loss: 0.2723 - val_acc: 0.9565\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.27226 to 0.26581, saving model to best.model\n",
      "0s - loss: 0.3527 - acc: 0.8989 - val_loss: 0.2658 - val_acc: 0.9565\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.26581 to 0.25889, saving model to best.model\n",
      "0s - loss: 0.3240 - acc: 0.8876 - val_loss: 0.2589 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.25889 to 0.25275, saving model to best.model\n",
      "0s - loss: 0.3902 - acc: 0.8764 - val_loss: 0.2527 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.25275 to 0.24741, saving model to best.model\n",
      "0s - loss: 0.3079 - acc: 0.9438 - val_loss: 0.2474 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.24741 to 0.24239, saving model to best.model\n",
      "0s - loss: 0.3295 - acc: 0.8989 - val_loss: 0.2424 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.24239 to 0.23774, saving model to best.model\n",
      "0s - loss: 0.2716 - acc: 0.9326 - val_loss: 0.2377 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.23774 to 0.23264, saving model to best.model\n",
      "0s - loss: 0.3485 - acc: 0.9101 - val_loss: 0.2326 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.23264 to 0.22823, saving model to best.model\n",
      "0s - loss: 0.3188 - acc: 0.8315 - val_loss: 0.2282 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.22823 to 0.22440, saving model to best.model\n",
      "0s - loss: 0.3228 - acc: 0.8315 - val_loss: 0.2244 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.22440 to 0.22038, saving model to best.model\n",
      "0s - loss: 0.2347 - acc: 0.9326 - val_loss: 0.2204 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.22038 to 0.21648, saving model to best.model\n",
      "0s - loss: 0.3314 - acc: 0.8539 - val_loss: 0.2165 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.21648 to 0.21326, saving model to best.model\n",
      "0s - loss: 0.2837 - acc: 0.9326 - val_loss: 0.2133 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.21326 to 0.21083, saving model to best.model\n",
      "0s - loss: 0.3047 - acc: 0.8989 - val_loss: 0.2108 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.21083 to 0.20907, saving model to best.model\n",
      "0s - loss: 0.2643 - acc: 0.9213 - val_loss: 0.2091 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.20907 to 0.20621, saving model to best.model\n",
      "0s - loss: 0.2296 - acc: 0.9438 - val_loss: 0.2062 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.20621 to 0.20213, saving model to best.model\n",
      "0s - loss: 0.2481 - acc: 0.9326 - val_loss: 0.2021 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.20213 to 0.19712, saving model to best.model\n",
      "0s - loss: 0.2692 - acc: 0.9213 - val_loss: 0.1971 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.19712 to 0.19218, saving model to best.model\n",
      "0s - loss: 0.2749 - acc: 0.9213 - val_loss: 0.1922 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.19218 to 0.18786, saving model to best.model\n",
      "0s - loss: 0.2749 - acc: 0.9101 - val_loss: 0.1879 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.18786 to 0.18404, saving model to best.model\n",
      "0s - loss: 0.2003 - acc: 0.9438 - val_loss: 0.1840 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.18404 to 0.18037, saving model to best.model\n",
      "0s - loss: 0.2557 - acc: 0.9213 - val_loss: 0.1804 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.18037 to 0.17739, saving model to best.model\n",
      "0s - loss: 0.2404 - acc: 0.9551 - val_loss: 0.1774 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.17739 to 0.17421, saving model to best.model\n",
      "0s - loss: 0.2655 - acc: 0.8989 - val_loss: 0.1742 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.17421 to 0.17167, saving model to best.model\n",
      "0s - loss: 0.2125 - acc: 0.9551 - val_loss: 0.1717 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.17167 to 0.16970, saving model to best.model\n",
      "0s - loss: 0.2938 - acc: 0.8989 - val_loss: 0.1697 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.16970 to 0.16834, saving model to best.model\n",
      "0s - loss: 0.1992 - acc: 0.9438 - val_loss: 0.1683 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.16834 to 0.16650, saving model to best.model\n",
      "0s - loss: 0.2269 - acc: 0.8989 - val_loss: 0.1665 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.16650 to 0.16390, saving model to best.model\n",
      "0s - loss: 0.1918 - acc: 0.9551 - val_loss: 0.1639 - val_acc: 0.9565\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.16390 to 0.16126, saving model to best.model\n",
      "0s - loss: 0.1871 - acc: 0.9551 - val_loss: 0.1613 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.16126 to 0.15808, saving model to best.model\n",
      "0s - loss: 0.1871 - acc: 0.9775 - val_loss: 0.1581 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.15808 to 0.15536, saving model to best.model\n",
      "0s - loss: 0.1646 - acc: 0.9551 - val_loss: 0.1554 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.15536 to 0.15288, saving model to best.model\n",
      "0s - loss: 0.2008 - acc: 0.9551 - val_loss: 0.1529 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.15288 to 0.15077, saving model to best.model\n",
      "0s - loss: 0.1932 - acc: 0.9438 - val_loss: 0.1508 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.15077 to 0.14854, saving model to best.model\n",
      "0s - loss: 0.2007 - acc: 0.9438 - val_loss: 0.1485 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.14854 to 0.14827, saving model to best.model\n",
      "0s - loss: 0.2464 - acc: 0.8989 - val_loss: 0.1483 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.14827 to 0.14701, saving model to best.model\n",
      "0s - loss: 0.2066 - acc: 0.8989 - val_loss: 0.1470 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.14701 to 0.14608, saving model to best.model\n",
      "0s - loss: 0.1894 - acc: 0.9438 - val_loss: 0.1461 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.14608 to 0.14467, saving model to best.model\n",
      "0s - loss: 0.1812 - acc: 0.9775 - val_loss: 0.1447 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.14467 to 0.14263, saving model to best.model\n",
      "0s - loss: 0.1808 - acc: 0.9326 - val_loss: 0.1426 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.14263 to 0.13975, saving model to best.model\n",
      "0s - loss: 0.1272 - acc: 0.9551 - val_loss: 0.1397 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.13975 to 0.13678, saving model to best.model\n",
      "0s - loss: 0.1528 - acc: 0.9663 - val_loss: 0.1368 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.13678 to 0.13325, saving model to best.model\n",
      "0s - loss: 0.1263 - acc: 0.9775 - val_loss: 0.1333 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.13325 to 0.12972, saving model to best.model\n",
      "0s - loss: 0.1674 - acc: 0.9551 - val_loss: 0.1297 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.12972 to 0.12746, saving model to best.model\n",
      "0s - loss: 0.1565 - acc: 0.9663 - val_loss: 0.1275 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.12746 to 0.12530, saving model to best.model\n",
      "0s - loss: 0.1523 - acc: 0.9888 - val_loss: 0.1253 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.12530 to 0.12280, saving model to best.model\n",
      "0s - loss: 0.1222 - acc: 0.9775 - val_loss: 0.1228 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.12280 to 0.12041, saving model to best.model\n",
      "0s - loss: 0.1504 - acc: 0.9663 - val_loss: 0.1204 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.12041 to 0.11703, saving model to best.model\n",
      "0s - loss: 0.1248 - acc: 0.9551 - val_loss: 0.1170 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.11703 to 0.11380, saving model to best.model\n",
      "0s - loss: 0.1134 - acc: 0.9775 - val_loss: 0.1138 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.11380 to 0.10967, saving model to best.model\n",
      "0s - loss: 0.1875 - acc: 0.9326 - val_loss: 0.1097 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.10967 to 0.10514, saving model to best.model\n",
      "0s - loss: 0.1535 - acc: 0.9438 - val_loss: 0.1051 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.10514 to 0.10028, saving model to best.model\n",
      "0s - loss: 0.1247 - acc: 0.9551 - val_loss: 0.1003 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.10028 to 0.09644, saving model to best.model\n",
      "0s - loss: 0.1788 - acc: 0.8989 - val_loss: 0.0964 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.09644 to 0.09317, saving model to best.model\n",
      "0s - loss: 0.1475 - acc: 0.9775 - val_loss: 0.0932 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.09317 to 0.09132, saving model to best.model\n",
      "0s - loss: 0.1623 - acc: 0.9551 - val_loss: 0.0913 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.09132 to 0.09007, saving model to best.model\n",
      "0s - loss: 0.2078 - acc: 0.9213 - val_loss: 0.0901 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.09007 to 0.08984, saving model to best.model\n",
      "0s - loss: 0.0959 - acc: 1.0000 - val_loss: 0.0898 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.2004 - acc: 0.9326 - val_loss: 0.0913 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.12661, saving model to best.model\n",
      "0s - loss: 1.4674 - acc: 0.2584 - val_loss: 1.1266 - val_acc: 0.3913\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.12661 to 1.09746, saving model to best.model\n",
      "0s - loss: 1.2355 - acc: 0.3596 - val_loss: 1.0975 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.3524 - acc: 0.2360 - val_loss: 1.0999 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.3947 - acc: 0.2360 - val_loss: 1.1036 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2540 - acc: 0.3034 - val_loss: 1.1125 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2557 - acc: 0.3034 - val_loss: 1.1228 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1721 - acc: 0.4045 - val_loss: 1.1295 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2233 - acc: 0.3708 - val_loss: 1.1321 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1877 - acc: 0.3371 - val_loss: 1.1356 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2558 - acc: 0.3371 - val_loss: 1.1389 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2253 - acc: 0.3483 - val_loss: 1.1412 - val_acc: 0.3913\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2673 - acc: 0.3596 - val_loss: 1.1442 - val_acc: 0.4348\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1619 - acc: 0.3708 - val_loss: 1.1441 - val_acc: 0.4348\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.1925 - acc: 0.3483 - val_loss: 1.1384 - val_acc: 0.4348\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2456 - acc: 0.3483 - val_loss: 1.1304 - val_acc: 0.4348\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1565 - acc: 0.3933 - val_loss: 1.1223 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.0755 - acc: 0.4382 - val_loss: 1.1130 - val_acc: 0.3913\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1612 - acc: 0.3708 - val_loss: 1.1064 - val_acc: 0.3913\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1283 - acc: 0.3708 - val_loss: 1.1019 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1939 - acc: 0.3820 - val_loss: 1.0986 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.09746 to 1.09536, saving model to best.model\n",
      "0s - loss: 1.2317 - acc: 0.3258 - val_loss: 1.0954 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.09536 to 1.09155, saving model to best.model\n",
      "0s - loss: 1.1617 - acc: 0.3820 - val_loss: 1.0916 - val_acc: 0.4348\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.09155 to 1.08816, saving model to best.model\n",
      "0s - loss: 1.2697 - acc: 0.3708 - val_loss: 1.0882 - val_acc: 0.4348\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.08816 to 1.08541, saving model to best.model\n",
      "0s - loss: 1.1191 - acc: 0.4157 - val_loss: 1.0854 - val_acc: 0.4348\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.08541 to 1.08126, saving model to best.model\n",
      "0s - loss: 1.1801 - acc: 0.3146 - val_loss: 1.0813 - val_acc: 0.4348\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.08126 to 1.08087, saving model to best.model\n",
      "0s - loss: 1.1701 - acc: 0.3596 - val_loss: 1.0809 - val_acc: 0.4348\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.08087 to 1.08047, saving model to best.model\n",
      "0s - loss: 1.1970 - acc: 0.3596 - val_loss: 1.0805 - val_acc: 0.4348\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1164 - acc: 0.3933 - val_loss: 1.0826 - val_acc: 0.4783\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.2209 - acc: 0.2921 - val_loss: 1.0852 - val_acc: 0.4783\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.0992 - acc: 0.4831 - val_loss: 1.0874 - val_acc: 0.4783\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.0891 - acc: 0.4045 - val_loss: 1.0885 - val_acc: 0.4783\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.2431 - acc: 0.3371 - val_loss: 1.0896 - val_acc: 0.4783\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.1696 - acc: 0.3146 - val_loss: 1.0913 - val_acc: 0.4783\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.0856 - acc: 0.4270 - val_loss: 1.0919 - val_acc: 0.4783\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1920 - acc: 0.3146 - val_loss: 1.0947 - val_acc: 0.4783\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.1114 - acc: 0.4270 - val_loss: 1.0962 - val_acc: 0.4783\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 1.1478 - acc: 0.3596 - val_loss: 1.0935 - val_acc: 0.4783\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 1.0957 - acc: 0.4382 - val_loss: 1.0895 - val_acc: 0.4783\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 1.1051 - acc: 0.4607 - val_loss: 1.0839 - val_acc: 0.4783\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.08047 to 1.07611, saving model to best.model\n",
      "0s - loss: 1.0770 - acc: 0.4045 - val_loss: 1.0761 - val_acc: 0.4783\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.07611 to 1.06856, saving model to best.model\n",
      "0s - loss: 1.1105 - acc: 0.3933 - val_loss: 1.0686 - val_acc: 0.4783\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.06856 to 1.05889, saving model to best.model\n",
      "0s - loss: 1.0873 - acc: 0.4494 - val_loss: 1.0589 - val_acc: 0.4783\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.05889 to 1.05029, saving model to best.model\n",
      "0s - loss: 1.1273 - acc: 0.4045 - val_loss: 1.0503 - val_acc: 0.4783\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.05029 to 1.04268, saving model to best.model\n",
      "0s - loss: 1.0430 - acc: 0.5056 - val_loss: 1.0427 - val_acc: 0.4783\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.04268 to 1.03590, saving model to best.model\n",
      "0s - loss: 1.0125 - acc: 0.4719 - val_loss: 1.0359 - val_acc: 0.4783\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.03590 to 1.03129, saving model to best.model\n",
      "0s - loss: 1.0679 - acc: 0.4607 - val_loss: 1.0313 - val_acc: 0.4783\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.03129 to 1.02882, saving model to best.model\n",
      "0s - loss: 1.0285 - acc: 0.5056 - val_loss: 1.0288 - val_acc: 0.4783\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.02882 to 1.02868, saving model to best.model\n",
      "0s - loss: 1.0626 - acc: 0.4157 - val_loss: 1.0287 - val_acc: 0.4783\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 1.0017 - acc: 0.4831 - val_loss: 1.0297 - val_acc: 0.4783\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.9995 - acc: 0.5281 - val_loss: 1.0299 - val_acc: 0.4783\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 1.0314 - acc: 0.4831 - val_loss: 1.0297 - val_acc: 0.4783\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.9884 - acc: 0.5506 - val_loss: 1.0302 - val_acc: 0.4783\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.02868 to 1.02797, saving model to best.model\n",
      "0s - loss: 1.0676 - acc: 0.4494 - val_loss: 1.0280 - val_acc: 0.4783\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.02797 to 1.02769, saving model to best.model\n",
      "0s - loss: 1.0309 - acc: 0.4494 - val_loss: 1.0277 - val_acc: 0.4783\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 1.02769 to 1.02578, saving model to best.model\n",
      "0s - loss: 1.0119 - acc: 0.4831 - val_loss: 1.0258 - val_acc: 0.4783\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 1.02578 to 1.02239, saving model to best.model\n",
      "0s - loss: 1.0713 - acc: 0.4607 - val_loss: 1.0224 - val_acc: 0.4783\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 1.02239 to 1.01904, saving model to best.model\n",
      "0s - loss: 1.0235 - acc: 0.5056 - val_loss: 1.0190 - val_acc: 0.4783\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 1.01904 to 1.01334, saving model to best.model\n",
      "0s - loss: 1.0240 - acc: 0.5281 - val_loss: 1.0133 - val_acc: 0.4783\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 1.01334 to 1.00653, saving model to best.model\n",
      "0s - loss: 0.9788 - acc: 0.5730 - val_loss: 1.0065 - val_acc: 0.4783\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 1.00653 to 0.99681, saving model to best.model\n",
      "0s - loss: 1.0039 - acc: 0.5618 - val_loss: 0.9968 - val_acc: 0.4783\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.99681 to 0.98547, saving model to best.model\n",
      "0s - loss: 0.9548 - acc: 0.5393 - val_loss: 0.9855 - val_acc: 0.4783\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.98547 to 0.97630, saving model to best.model\n",
      "0s - loss: 0.9664 - acc: 0.5955 - val_loss: 0.9763 - val_acc: 0.4783\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.97630 to 0.96617, saving model to best.model\n",
      "0s - loss: 0.9551 - acc: 0.5506 - val_loss: 0.9662 - val_acc: 0.4783\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.96617 to 0.95472, saving model to best.model\n",
      "0s - loss: 0.9204 - acc: 0.5955 - val_loss: 0.9547 - val_acc: 0.4783\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.95472 to 0.94164, saving model to best.model\n",
      "0s - loss: 0.9446 - acc: 0.5393 - val_loss: 0.9416 - val_acc: 0.4783\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.94164 to 0.92989, saving model to best.model\n",
      "0s - loss: 0.9146 - acc: 0.5393 - val_loss: 0.9299 - val_acc: 0.4783\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.92989 to 0.91722, saving model to best.model\n",
      "0s - loss: 0.9619 - acc: 0.5056 - val_loss: 0.9172 - val_acc: 0.5652\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.91722 to 0.90692, saving model to best.model\n",
      "0s - loss: 0.9524 - acc: 0.5843 - val_loss: 0.9069 - val_acc: 0.5652\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.90692 to 0.89807, saving model to best.model\n",
      "0s - loss: 0.8998 - acc: 0.5843 - val_loss: 0.8981 - val_acc: 0.5652\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.89807 to 0.88987, saving model to best.model\n",
      "0s - loss: 0.9218 - acc: 0.5393 - val_loss: 0.8899 - val_acc: 0.5652\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.88987 to 0.88128, saving model to best.model\n",
      "0s - loss: 0.9017 - acc: 0.6067 - val_loss: 0.8813 - val_acc: 0.5652\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.88128 to 0.87401, saving model to best.model\n",
      "0s - loss: 0.8652 - acc: 0.6292 - val_loss: 0.8740 - val_acc: 0.5652\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.87401 to 0.86814, saving model to best.model\n",
      "0s - loss: 0.8110 - acc: 0.6742 - val_loss: 0.8681 - val_acc: 0.5652\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.86814 to 0.86313, saving model to best.model\n",
      "0s - loss: 0.7612 - acc: 0.7865 - val_loss: 0.8631 - val_acc: 0.5652\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.86313 to 0.85602, saving model to best.model\n",
      "0s - loss: 0.8356 - acc: 0.6517 - val_loss: 0.8560 - val_acc: 0.5652\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.85602 to 0.84724, saving model to best.model\n",
      "0s - loss: 0.8282 - acc: 0.6742 - val_loss: 0.8472 - val_acc: 0.5652\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.84724 to 0.83736, saving model to best.model\n",
      "0s - loss: 0.7844 - acc: 0.6404 - val_loss: 0.8374 - val_acc: 0.5652\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.83736 to 0.82804, saving model to best.model\n",
      "0s - loss: 0.7928 - acc: 0.6854 - val_loss: 0.8280 - val_acc: 0.6522\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.82804 to 0.81681, saving model to best.model\n",
      "0s - loss: 0.8323 - acc: 0.6629 - val_loss: 0.8168 - val_acc: 0.6957\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.81681 to 0.80405, saving model to best.model\n",
      "0s - loss: 0.7433 - acc: 0.7528 - val_loss: 0.8041 - val_acc: 0.6957\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.80405 to 0.79042, saving model to best.model\n",
      "0s - loss: 0.7455 - acc: 0.6966 - val_loss: 0.7904 - val_acc: 0.6957\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.79042 to 0.77860, saving model to best.model\n",
      "0s - loss: 0.7448 - acc: 0.6854 - val_loss: 0.7786 - val_acc: 0.7826\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.77860 to 0.76586, saving model to best.model\n",
      "0s - loss: 0.7493 - acc: 0.6966 - val_loss: 0.7659 - val_acc: 0.7826\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.76586 to 0.75295, saving model to best.model\n",
      "0s - loss: 0.6962 - acc: 0.7303 - val_loss: 0.7530 - val_acc: 0.7826\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.75295 to 0.73964, saving model to best.model\n",
      "0s - loss: 0.7311 - acc: 0.6966 - val_loss: 0.7396 - val_acc: 0.7826\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.73964 to 0.72669, saving model to best.model\n",
      "0s - loss: 0.7030 - acc: 0.7416 - val_loss: 0.7267 - val_acc: 0.8261\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.72669 to 0.71492, saving model to best.model\n",
      "0s - loss: 0.6920 - acc: 0.7528 - val_loss: 0.7149 - val_acc: 0.8261\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.71492 to 0.70267, saving model to best.model\n",
      "0s - loss: 0.6204 - acc: 0.7303 - val_loss: 0.7027 - val_acc: 0.9130\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.70267 to 0.69003, saving model to best.model\n",
      "0s - loss: 0.6406 - acc: 0.7303 - val_loss: 0.6900 - val_acc: 0.9130\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69003 to 0.67742, saving model to best.model\n",
      "0s - loss: 0.6356 - acc: 0.7528 - val_loss: 0.6774 - val_acc: 0.9130\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67742 to 0.66519, saving model to best.model\n",
      "0s - loss: 0.6541 - acc: 0.7528 - val_loss: 0.6652 - val_acc: 0.9130\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66519 to 0.65313, saving model to best.model\n",
      "0s - loss: 0.6747 - acc: 0.7416 - val_loss: 0.6531 - val_acc: 0.9130\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.65313 to 0.64235, saving model to best.model\n",
      "0s - loss: 0.5758 - acc: 0.7753 - val_loss: 0.6424 - val_acc: 0.9130\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.64235 to 0.63271, saving model to best.model\n",
      "0s - loss: 0.5664 - acc: 0.7753 - val_loss: 0.6327 - val_acc: 0.9130\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.63271 to 0.62071, saving model to best.model\n",
      "0s - loss: 0.5972 - acc: 0.7416 - val_loss: 0.6207 - val_acc: 0.9130\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.62071 to 0.60786, saving model to best.model\n",
      "0s - loss: 0.5781 - acc: 0.7865 - val_loss: 0.6079 - val_acc: 0.9130\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.60786 to 0.59478, saving model to best.model\n",
      "0s - loss: 0.5376 - acc: 0.8202 - val_loss: 0.5948 - val_acc: 0.9130\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.59478 to 0.58224, saving model to best.model\n",
      "0s - loss: 0.5411 - acc: 0.7978 - val_loss: 0.5822 - val_acc: 0.9565\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.58224 to 0.57000, saving model to best.model\n",
      "0s - loss: 0.5410 - acc: 0.8315 - val_loss: 0.5700 - val_acc: 0.9565\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.57000 to 0.55717, saving model to best.model\n",
      "0s - loss: 0.5268 - acc: 0.8202 - val_loss: 0.5572 - val_acc: 0.9565\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.55717 to 0.54496, saving model to best.model\n",
      "0s - loss: 0.4725 - acc: 0.8539 - val_loss: 0.5450 - val_acc: 0.9565\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.54496 to 0.53222, saving model to best.model\n",
      "0s - loss: 0.5650 - acc: 0.7640 - val_loss: 0.5322 - val_acc: 0.9565\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.53222 to 0.52190, saving model to best.model\n",
      "0s - loss: 0.4865 - acc: 0.8539 - val_loss: 0.5219 - val_acc: 0.9565\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.52190 to 0.51181, saving model to best.model\n",
      "0s - loss: 0.5446 - acc: 0.7753 - val_loss: 0.5118 - val_acc: 0.9565\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.51181 to 0.50155, saving model to best.model\n",
      "0s - loss: 0.4733 - acc: 0.8202 - val_loss: 0.5015 - val_acc: 0.9565\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.50155 to 0.49177, saving model to best.model\n",
      "0s - loss: 0.4994 - acc: 0.8427 - val_loss: 0.4918 - val_acc: 0.9565\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.49177 to 0.48162, saving model to best.model\n",
      "0s - loss: 0.4758 - acc: 0.8315 - val_loss: 0.4816 - val_acc: 0.9565\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.48162 to 0.47115, saving model to best.model\n",
      "0s - loss: 0.5026 - acc: 0.7978 - val_loss: 0.4711 - val_acc: 0.9565\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.47115 to 0.46164, saving model to best.model\n",
      "0s - loss: 0.4645 - acc: 0.8090 - val_loss: 0.4616 - val_acc: 0.9565\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.46164 to 0.45238, saving model to best.model\n",
      "0s - loss: 0.4699 - acc: 0.8202 - val_loss: 0.4524 - val_acc: 0.9565\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.45238 to 0.44395, saving model to best.model\n",
      "0s - loss: 0.4902 - acc: 0.8202 - val_loss: 0.4439 - val_acc: 0.9565\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.44395 to 0.43624, saving model to best.model\n",
      "0s - loss: 0.4446 - acc: 0.8652 - val_loss: 0.4362 - val_acc: 0.9565\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.43624 to 0.42969, saving model to best.model\n",
      "0s - loss: 0.4834 - acc: 0.7978 - val_loss: 0.4297 - val_acc: 0.9565\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.42969 to 0.42205, saving model to best.model\n",
      "0s - loss: 0.4010 - acc: 0.8539 - val_loss: 0.4220 - val_acc: 0.9565\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.42205 to 0.41352, saving model to best.model\n",
      "0s - loss: 0.4245 - acc: 0.8315 - val_loss: 0.4135 - val_acc: 0.9565\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.41352 to 0.40572, saving model to best.model\n",
      "0s - loss: 0.4681 - acc: 0.8202 - val_loss: 0.4057 - val_acc: 0.9565\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.40572 to 0.39667, saving model to best.model\n",
      "0s - loss: 0.4036 - acc: 0.8989 - val_loss: 0.3967 - val_acc: 0.9565\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.39667 to 0.38718, saving model to best.model\n",
      "0s - loss: 0.4630 - acc: 0.8539 - val_loss: 0.3872 - val_acc: 0.9565\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.38718 to 0.37782, saving model to best.model\n",
      "0s - loss: 0.4296 - acc: 0.7865 - val_loss: 0.3778 - val_acc: 0.9565\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.37782 to 0.36824, saving model to best.model\n",
      "0s - loss: 0.4373 - acc: 0.8764 - val_loss: 0.3682 - val_acc: 0.9565\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.36824 to 0.35789, saving model to best.model\n",
      "0s - loss: 0.3788 - acc: 0.8764 - val_loss: 0.3579 - val_acc: 0.9565\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.35789 to 0.34890, saving model to best.model\n",
      "0s - loss: 0.3963 - acc: 0.8876 - val_loss: 0.3489 - val_acc: 0.9565\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.34890 to 0.33898, saving model to best.model\n",
      "0s - loss: 0.3890 - acc: 0.8315 - val_loss: 0.3390 - val_acc: 0.9565\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.33898 to 0.32939, saving model to best.model\n",
      "0s - loss: 0.3782 - acc: 0.8764 - val_loss: 0.3294 - val_acc: 0.9565\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.32939 to 0.32168, saving model to best.model\n",
      "0s - loss: 0.3989 - acc: 0.8764 - val_loss: 0.3217 - val_acc: 0.9565\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.32168 to 0.31426, saving model to best.model\n",
      "0s - loss: 0.3654 - acc: 0.8876 - val_loss: 0.3143 - val_acc: 0.9565\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.31426 to 0.30670, saving model to best.model\n",
      "0s - loss: 0.3065 - acc: 0.8989 - val_loss: 0.3067 - val_acc: 0.9565\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.30670 to 0.30185, saving model to best.model\n",
      "0s - loss: 0.3545 - acc: 0.8764 - val_loss: 0.3019 - val_acc: 0.9565\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.30185 to 0.29705, saving model to best.model\n",
      "0s - loss: 0.3455 - acc: 0.8539 - val_loss: 0.2970 - val_acc: 0.9565\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.29705 to 0.29395, saving model to best.model\n",
      "0s - loss: 0.3538 - acc: 0.8764 - val_loss: 0.2940 - val_acc: 0.9565\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.29395 to 0.29191, saving model to best.model\n",
      "0s - loss: 0.3172 - acc: 0.9101 - val_loss: 0.2919 - val_acc: 0.9565\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.29191 to 0.29074, saving model to best.model\n",
      "0s - loss: 0.3327 - acc: 0.8876 - val_loss: 0.2907 - val_acc: 0.9565\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.29074 to 0.28880, saving model to best.model\n",
      "0s - loss: 0.3387 - acc: 0.8764 - val_loss: 0.2888 - val_acc: 0.9565\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.28880 to 0.28610, saving model to best.model\n",
      "0s - loss: 0.3507 - acc: 0.8764 - val_loss: 0.2861 - val_acc: 0.9565\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.28610 to 0.28294, saving model to best.model\n",
      "0s - loss: 0.3449 - acc: 0.8764 - val_loss: 0.2829 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.28294 to 0.27942, saving model to best.model\n",
      "0s - loss: 0.2725 - acc: 0.9326 - val_loss: 0.2794 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.27942 to 0.27493, saving model to best.model\n",
      "0s - loss: 0.3063 - acc: 0.8876 - val_loss: 0.2749 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.27493 to 0.27097, saving model to best.model\n",
      "0s - loss: 0.2914 - acc: 0.8989 - val_loss: 0.2710 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.27097 to 0.26691, saving model to best.model\n",
      "0s - loss: 0.3274 - acc: 0.8876 - val_loss: 0.2669 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.26691 to 0.26313, saving model to best.model\n",
      "0s - loss: 0.2406 - acc: 0.9326 - val_loss: 0.2631 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.26313 to 0.25992, saving model to best.model\n",
      "0s - loss: 0.2856 - acc: 0.8876 - val_loss: 0.2599 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.25992 to 0.25689, saving model to best.model\n",
      "0s - loss: 0.2930 - acc: 0.9213 - val_loss: 0.2569 - val_acc: 0.9565\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.25689 to 0.25360, saving model to best.model\n",
      "0s - loss: 0.2854 - acc: 0.8876 - val_loss: 0.2536 - val_acc: 0.9565\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.25360 to 0.25051, saving model to best.model\n",
      "0s - loss: 0.3007 - acc: 0.9213 - val_loss: 0.2505 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.25051 to 0.24683, saving model to best.model\n",
      "0s - loss: 0.2802 - acc: 0.9101 - val_loss: 0.2468 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.24683 to 0.24262, saving model to best.model\n",
      "0s - loss: 0.2977 - acc: 0.9326 - val_loss: 0.2426 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.24262 to 0.23872, saving model to best.model\n",
      "0s - loss: 0.2788 - acc: 0.8764 - val_loss: 0.2387 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.23872 to 0.23439, saving model to best.model\n",
      "0s - loss: 0.2868 - acc: 0.9213 - val_loss: 0.2344 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.23439 to 0.23059, saving model to best.model\n",
      "0s - loss: 0.2502 - acc: 0.9326 - val_loss: 0.2306 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.23059 to 0.22633, saving model to best.model\n",
      "0s - loss: 0.2433 - acc: 0.9213 - val_loss: 0.2263 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.22633 to 0.22247, saving model to best.model\n",
      "0s - loss: 0.2795 - acc: 0.9326 - val_loss: 0.2225 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.22247 to 0.21895, saving model to best.model\n",
      "0s - loss: 0.2258 - acc: 0.9213 - val_loss: 0.2189 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.21895 to 0.21541, saving model to best.model\n",
      "0s - loss: 0.2472 - acc: 0.9551 - val_loss: 0.2154 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.21541 to 0.21142, saving model to best.model\n",
      "0s - loss: 0.1895 - acc: 0.9438 - val_loss: 0.2114 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.21142 to 0.20821, saving model to best.model\n",
      "0s - loss: 0.2791 - acc: 0.9213 - val_loss: 0.2082 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.20821 to 0.20592, saving model to best.model\n",
      "0s - loss: 0.1777 - acc: 0.9438 - val_loss: 0.2059 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.20592 to 0.20381, saving model to best.model\n",
      "0s - loss: 0.2201 - acc: 0.9101 - val_loss: 0.2038 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.20381 to 0.20165, saving model to best.model\n",
      "0s - loss: 0.2304 - acc: 0.9213 - val_loss: 0.2016 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.20165 to 0.19951, saving model to best.model\n",
      "0s - loss: 0.2576 - acc: 0.8876 - val_loss: 0.1995 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.19951 to 0.19792, saving model to best.model\n",
      "0s - loss: 0.2233 - acc: 0.9438 - val_loss: 0.1979 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.19792 to 0.19621, saving model to best.model\n",
      "0s - loss: 0.2454 - acc: 0.9101 - val_loss: 0.1962 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.19621 to 0.19440, saving model to best.model\n",
      "0s - loss: 0.1951 - acc: 0.9326 - val_loss: 0.1944 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.19440 to 0.19256, saving model to best.model\n",
      "0s - loss: 0.2056 - acc: 0.9663 - val_loss: 0.1926 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.19256 to 0.19027, saving model to best.model\n",
      "0s - loss: 0.1910 - acc: 0.9438 - val_loss: 0.1903 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.19027 to 0.18762, saving model to best.model\n",
      "0s - loss: 0.2061 - acc: 0.9663 - val_loss: 0.1876 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.18762 to 0.18485, saving model to best.model\n",
      "0s - loss: 0.1661 - acc: 0.9551 - val_loss: 0.1848 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.18485 to 0.18154, saving model to best.model\n",
      "0s - loss: 0.2055 - acc: 0.9326 - val_loss: 0.1815 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.18154 to 0.17915, saving model to best.model\n",
      "0s - loss: 0.2110 - acc: 0.9101 - val_loss: 0.1792 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.17915 to 0.17691, saving model to best.model\n",
      "0s - loss: 0.1930 - acc: 0.9551 - val_loss: 0.1769 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.17691 to 0.17477, saving model to best.model\n",
      "0s - loss: 0.1708 - acc: 0.9663 - val_loss: 0.1748 - val_acc: 0.9565\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.17477 to 0.17351, saving model to best.model\n",
      "0s - loss: 0.2352 - acc: 0.9326 - val_loss: 0.1735 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.17351 to 0.17246, saving model to best.model\n",
      "0s - loss: 0.1882 - acc: 0.9551 - val_loss: 0.1725 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.17246 to 0.17094, saving model to best.model\n",
      "0s - loss: 0.1693 - acc: 0.9551 - val_loss: 0.1709 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.17094 to 0.16936, saving model to best.model\n",
      "0s - loss: 0.2402 - acc: 0.9213 - val_loss: 0.1694 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.16936 to 0.16790, saving model to best.model\n",
      "0s - loss: 0.2138 - acc: 0.9438 - val_loss: 0.1679 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.16790 to 0.16718, saving model to best.model\n",
      "0s - loss: 0.1741 - acc: 0.9551 - val_loss: 0.1672 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.16718 to 0.16619, saving model to best.model\n",
      "0s - loss: 0.1746 - acc: 0.9326 - val_loss: 0.1662 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.16619 to 0.16552, saving model to best.model\n",
      "0s - loss: 0.1668 - acc: 0.9663 - val_loss: 0.1655 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.16552 to 0.16476, saving model to best.model\n",
      "0s - loss: 0.1590 - acc: 0.9663 - val_loss: 0.1648 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.16476 to 0.16468, saving model to best.model\n",
      "0s - loss: 0.1664 - acc: 0.9775 - val_loss: 0.1647 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.16468 to 0.16431, saving model to best.model\n",
      "0s - loss: 0.1509 - acc: 0.9663 - val_loss: 0.1643 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.16431 to 0.16380, saving model to best.model\n",
      "0s - loss: 0.1077 - acc: 0.9775 - val_loss: 0.1638 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.16380 to 0.16317, saving model to best.model\n",
      "0s - loss: 0.1773 - acc: 0.9326 - val_loss: 0.1632 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.16317 to 0.16207, saving model to best.model\n",
      "0s - loss: 0.2196 - acc: 0.9213 - val_loss: 0.1621 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.16207 to 0.16107, saving model to best.model\n",
      "0s - loss: 0.2241 - acc: 0.9101 - val_loss: 0.1611 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.16107 to 0.16038, saving model to best.model\n",
      "0s - loss: 0.1444 - acc: 0.9663 - val_loss: 0.1604 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.16038 to 0.15968, saving model to best.model\n",
      "0s - loss: 0.1792 - acc: 0.9326 - val_loss: 0.1597 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.15968 to 0.15963, saving model to best.model\n",
      "0s - loss: 0.1512 - acc: 0.9438 - val_loss: 0.1596 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.1661 - acc: 0.9663 - val_loss: 0.1598 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.1322 - acc: 0.9775 - val_loss: 0.1598 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.15963 to 0.15950, saving model to best.model\n",
      "0s - loss: 0.1968 - acc: 0.9438 - val_loss: 0.1595 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.15950 to 0.15924, saving model to best.model\n",
      "0s - loss: 0.1223 - acc: 0.9438 - val_loss: 0.1592 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.15924 to 0.15879, saving model to best.model\n",
      "0s - loss: 0.1128 - acc: 0.9775 - val_loss: 0.1588 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.15879 to 0.15789, saving model to best.model\n",
      "0s - loss: 0.1344 - acc: 0.9663 - val_loss: 0.1579 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.15789 to 0.15751, saving model to best.model\n",
      "0s - loss: 0.1237 - acc: 0.9663 - val_loss: 0.1575 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.15751 to 0.15651, saving model to best.model\n",
      "0s - loss: 0.1240 - acc: 0.9775 - val_loss: 0.1565 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.15651 to 0.15509, saving model to best.model\n",
      "0s - loss: 0.0921 - acc: 0.9888 - val_loss: 0.1551 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.15509 to 0.15432, saving model to best.model\n",
      "0s - loss: 0.0988 - acc: 0.9775 - val_loss: 0.1543 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.15432 to 0.15341, saving model to best.model\n",
      "0s - loss: 0.1416 - acc: 0.9438 - val_loss: 0.1534 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.15341 to 0.15264, saving model to best.model\n",
      "0s - loss: 0.1190 - acc: 0.9663 - val_loss: 0.1526 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.11564, saving model to best.model\n",
      "0s - loss: 1.3916 - acc: 0.3146 - val_loss: 1.1156 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.2861 - acc: 0.3258 - val_loss: 1.1200 - val_acc: 0.3478\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.2159 - acc: 0.4270 - val_loss: 1.1284 - val_acc: 0.3478\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2171 - acc: 0.3146 - val_loss: 1.1412 - val_acc: 0.3478\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2884 - acc: 0.2809 - val_loss: 1.1573 - val_acc: 0.3478\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1366 - acc: 0.4157 - val_loss: 1.1684 - val_acc: 0.3478\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2780 - acc: 0.4157 - val_loss: 1.1708 - val_acc: 0.3478\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2791 - acc: 0.3820 - val_loss: 1.1679 - val_acc: 0.3478\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2212 - acc: 0.4045 - val_loss: 1.1623 - val_acc: 0.3478\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2446 - acc: 0.3483 - val_loss: 1.1532 - val_acc: 0.3478\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1987 - acc: 0.3820 - val_loss: 1.1419 - val_acc: 0.3478\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1440 - acc: 0.3820 - val_loss: 1.1311 - val_acc: 0.3478\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1272 - acc: 0.4831 - val_loss: 1.1201 - val_acc: 0.3478\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.11564 to 1.11086, saving model to best.model\n",
      "0s - loss: 1.1979 - acc: 0.4157 - val_loss: 1.1109 - val_acc: 0.3478\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.11086 to 1.10441, saving model to best.model\n",
      "0s - loss: 1.2469 - acc: 0.3146 - val_loss: 1.1044 - val_acc: 0.3478\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.10441 to 1.09878, saving model to best.model\n",
      "0s - loss: 1.2574 - acc: 0.3708 - val_loss: 1.0988 - val_acc: 0.3478\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.09878 to 1.09432, saving model to best.model\n",
      "0s - loss: 1.1349 - acc: 0.4270 - val_loss: 1.0943 - val_acc: 0.3478\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.09432 to 1.09007, saving model to best.model\n",
      "0s - loss: 1.1174 - acc: 0.4831 - val_loss: 1.0901 - val_acc: 0.3478\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.09007 to 1.08588, saving model to best.model\n",
      "0s - loss: 1.1390 - acc: 0.4382 - val_loss: 1.0859 - val_acc: 0.3478\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.08588 to 1.08212, saving model to best.model\n",
      "0s - loss: 1.2127 - acc: 0.3371 - val_loss: 1.0821 - val_acc: 0.3478\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.08212 to 1.07931, saving model to best.model\n",
      "0s - loss: 1.1452 - acc: 0.3708 - val_loss: 1.0793 - val_acc: 0.3478\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.07931 to 1.07745, saving model to best.model\n",
      "0s - loss: 1.1164 - acc: 0.3933 - val_loss: 1.0775 - val_acc: 0.3478\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.07745 to 1.07620, saving model to best.model\n",
      "0s - loss: 1.2188 - acc: 0.3371 - val_loss: 1.0762 - val_acc: 0.3478\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.07620 to 1.07424, saving model to best.model\n",
      "0s - loss: 1.1525 - acc: 0.3820 - val_loss: 1.0742 - val_acc: 0.3478\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.07424 to 1.07229, saving model to best.model\n",
      "0s - loss: 1.1629 - acc: 0.3933 - val_loss: 1.0723 - val_acc: 0.3478\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.07229 to 1.07023, saving model to best.model\n",
      "0s - loss: 1.2250 - acc: 0.3483 - val_loss: 1.0702 - val_acc: 0.3478\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.07023 to 1.06915, saving model to best.model\n",
      "0s - loss: 1.2076 - acc: 0.3820 - val_loss: 1.0692 - val_acc: 0.3478\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1055 - acc: 0.4494 - val_loss: 1.0692 - val_acc: 0.3478\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.9694 - acc: 0.5393 - val_loss: 1.0693 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.06915 to 1.06901, saving model to best.model\n",
      "0s - loss: 1.0321 - acc: 0.4270 - val_loss: 1.0690 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.06901 to 1.06843, saving model to best.model\n",
      "0s - loss: 1.0728 - acc: 0.5056 - val_loss: 1.0684 - val_acc: 0.3913\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.06843 to 1.06709, saving model to best.model\n",
      "0s - loss: 1.1196 - acc: 0.5056 - val_loss: 1.0671 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.06709 to 1.06498, saving model to best.model\n",
      "0s - loss: 1.1074 - acc: 0.4382 - val_loss: 1.0650 - val_acc: 0.3913\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.06498 to 1.06275, saving model to best.model\n",
      "0s - loss: 1.1691 - acc: 0.3034 - val_loss: 1.0627 - val_acc: 0.3478\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.06275 to 1.06060, saving model to best.model\n",
      "0s - loss: 1.1529 - acc: 0.3820 - val_loss: 1.0606 - val_acc: 0.3478\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.06060 to 1.05838, saving model to best.model\n",
      "0s - loss: 1.1640 - acc: 0.4045 - val_loss: 1.0584 - val_acc: 0.3478\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.05838 to 1.05460, saving model to best.model\n",
      "0s - loss: 1.0329 - acc: 0.5056 - val_loss: 1.0546 - val_acc: 0.3478\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.05460 to 1.04988, saving model to best.model\n",
      "0s - loss: 1.0875 - acc: 0.4045 - val_loss: 1.0499 - val_acc: 0.3478\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.04988 to 1.04523, saving model to best.model\n",
      "0s - loss: 1.0820 - acc: 0.4607 - val_loss: 1.0452 - val_acc: 0.3478\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.04523 to 1.04069, saving model to best.model\n",
      "0s - loss: 1.1308 - acc: 0.3708 - val_loss: 1.0407 - val_acc: 0.3478\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.04069 to 1.03596, saving model to best.model\n",
      "0s - loss: 1.0415 - acc: 0.4831 - val_loss: 1.0360 - val_acc: 0.3478\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.03596 to 1.03219, saving model to best.model\n",
      "0s - loss: 1.0615 - acc: 0.4382 - val_loss: 1.0322 - val_acc: 0.3478\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.03219 to 1.02861, saving model to best.model\n",
      "0s - loss: 0.9551 - acc: 0.5730 - val_loss: 1.0286 - val_acc: 0.3478\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.02861 to 1.02365, saving model to best.model\n",
      "0s - loss: 1.0227 - acc: 0.5281 - val_loss: 1.0237 - val_acc: 0.3478\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.02365 to 1.01716, saving model to best.model\n",
      "0s - loss: 1.0637 - acc: 0.4382 - val_loss: 1.0172 - val_acc: 0.3478\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.01716 to 1.01063, saving model to best.model\n",
      "0s - loss: 1.1038 - acc: 0.4270 - val_loss: 1.0106 - val_acc: 0.3478\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.01063 to 1.00461, saving model to best.model\n",
      "0s - loss: 0.9943 - acc: 0.5730 - val_loss: 1.0046 - val_acc: 0.3478\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.00461 to 0.99851, saving model to best.model\n",
      "0s - loss: 1.0093 - acc: 0.4944 - val_loss: 0.9985 - val_acc: 0.3913\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.99851 to 0.99266, saving model to best.model\n",
      "0s - loss: 1.0610 - acc: 0.4494 - val_loss: 0.9927 - val_acc: 0.4348\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.99266 to 0.98701, saving model to best.model\n",
      "0s - loss: 1.0351 - acc: 0.4831 - val_loss: 0.9870 - val_acc: 0.5217\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.98701 to 0.98076, saving model to best.model\n",
      "0s - loss: 1.0313 - acc: 0.4944 - val_loss: 0.9808 - val_acc: 0.5217\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.98076 to 0.97441, saving model to best.model\n",
      "0s - loss: 1.0198 - acc: 0.4270 - val_loss: 0.9744 - val_acc: 0.5217\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.97441 to 0.96835, saving model to best.model\n",
      "0s - loss: 1.0804 - acc: 0.4719 - val_loss: 0.9684 - val_acc: 0.5217\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.96835 to 0.96252, saving model to best.model\n",
      "0s - loss: 1.0375 - acc: 0.4157 - val_loss: 0.9625 - val_acc: 0.5217\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.96252 to 0.95733, saving model to best.model\n",
      "0s - loss: 1.0214 - acc: 0.4944 - val_loss: 0.9573 - val_acc: 0.5217\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.95733 to 0.95170, saving model to best.model\n",
      "0s - loss: 1.0152 - acc: 0.4382 - val_loss: 0.9517 - val_acc: 0.5217\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.95170 to 0.94570, saving model to best.model\n",
      "0s - loss: 0.9557 - acc: 0.5955 - val_loss: 0.9457 - val_acc: 0.5217\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.94570 to 0.93956, saving model to best.model\n",
      "0s - loss: 1.0378 - acc: 0.4494 - val_loss: 0.9396 - val_acc: 0.5217\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.93956 to 0.93278, saving model to best.model\n",
      "0s - loss: 1.0085 - acc: 0.4494 - val_loss: 0.9328 - val_acc: 0.5217\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.93278 to 0.92493, saving model to best.model\n",
      "0s - loss: 1.0055 - acc: 0.5169 - val_loss: 0.9249 - val_acc: 0.5217\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.92493 to 0.91700, saving model to best.model\n",
      "0s - loss: 0.8651 - acc: 0.5169 - val_loss: 0.9170 - val_acc: 0.5217\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.91700 to 0.90864, saving model to best.model\n",
      "0s - loss: 0.9556 - acc: 0.5169 - val_loss: 0.9086 - val_acc: 0.5217\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.90864 to 0.89908, saving model to best.model\n",
      "0s - loss: 0.9727 - acc: 0.4719 - val_loss: 0.8991 - val_acc: 0.5217\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.89908 to 0.88996, saving model to best.model\n",
      "0s - loss: 0.9031 - acc: 0.5506 - val_loss: 0.8900 - val_acc: 0.5217\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.88996 to 0.88078, saving model to best.model\n",
      "0s - loss: 0.9487 - acc: 0.4831 - val_loss: 0.8808 - val_acc: 0.5652\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.88078 to 0.87054, saving model to best.model\n",
      "0s - loss: 0.9119 - acc: 0.5506 - val_loss: 0.8705 - val_acc: 0.5652\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.87054 to 0.85989, saving model to best.model\n",
      "0s - loss: 0.9232 - acc: 0.5843 - val_loss: 0.8599 - val_acc: 0.5652\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.85989 to 0.84974, saving model to best.model\n",
      "0s - loss: 0.8840 - acc: 0.5618 - val_loss: 0.8497 - val_acc: 0.5652\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.84974 to 0.83928, saving model to best.model\n",
      "0s - loss: 0.9178 - acc: 0.5393 - val_loss: 0.8393 - val_acc: 0.6957\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.83928 to 0.82821, saving model to best.model\n",
      "0s - loss: 0.8444 - acc: 0.6067 - val_loss: 0.8282 - val_acc: 0.7391\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.82821 to 0.81601, saving model to best.model\n",
      "0s - loss: 0.8715 - acc: 0.6067 - val_loss: 0.8160 - val_acc: 0.7826\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.81601 to 0.80380, saving model to best.model\n",
      "0s - loss: 0.8150 - acc: 0.6067 - val_loss: 0.8038 - val_acc: 0.7826\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.80380 to 0.79129, saving model to best.model\n",
      "0s - loss: 0.8860 - acc: 0.5618 - val_loss: 0.7913 - val_acc: 0.7826\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.79129 to 0.77917, saving model to best.model\n",
      "0s - loss: 0.8131 - acc: 0.6742 - val_loss: 0.7792 - val_acc: 0.7826\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.77917 to 0.76702, saving model to best.model\n",
      "0s - loss: 0.7698 - acc: 0.6966 - val_loss: 0.7670 - val_acc: 0.7826\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.76702 to 0.75506, saving model to best.model\n",
      "0s - loss: 0.7924 - acc: 0.6854 - val_loss: 0.7551 - val_acc: 0.8261\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.75506 to 0.74274, saving model to best.model\n",
      "0s - loss: 0.7953 - acc: 0.6966 - val_loss: 0.7427 - val_acc: 0.8261\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.74274 to 0.73041, saving model to best.model\n",
      "0s - loss: 0.7687 - acc: 0.6629 - val_loss: 0.7304 - val_acc: 0.8261\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.73041 to 0.71806, saving model to best.model\n",
      "0s - loss: 0.7323 - acc: 0.7191 - val_loss: 0.7181 - val_acc: 0.8696\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.71806 to 0.70608, saving model to best.model\n",
      "0s - loss: 0.7162 - acc: 0.7079 - val_loss: 0.7061 - val_acc: 0.8696\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70608 to 0.69399, saving model to best.model\n",
      "0s - loss: 0.7283 - acc: 0.7416 - val_loss: 0.6940 - val_acc: 0.9130\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69399 to 0.68163, saving model to best.model\n",
      "0s - loss: 0.7158 - acc: 0.6854 - val_loss: 0.6816 - val_acc: 0.9130\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68163 to 0.66839, saving model to best.model\n",
      "0s - loss: 0.6718 - acc: 0.7303 - val_loss: 0.6684 - val_acc: 0.9130\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.66839 to 0.65464, saving model to best.model\n",
      "0s - loss: 0.7042 - acc: 0.7191 - val_loss: 0.6546 - val_acc: 0.9130\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.65464 to 0.64118, saving model to best.model\n",
      "0s - loss: 0.7392 - acc: 0.6966 - val_loss: 0.6412 - val_acc: 0.9130\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.64118 to 0.62855, saving model to best.model\n",
      "0s - loss: 0.7174 - acc: 0.7079 - val_loss: 0.6286 - val_acc: 0.9130\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.62855 to 0.61602, saving model to best.model\n",
      "0s - loss: 0.6986 - acc: 0.7416 - val_loss: 0.6160 - val_acc: 0.9130\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.61602 to 0.60329, saving model to best.model\n",
      "0s - loss: 0.7562 - acc: 0.6292 - val_loss: 0.6033 - val_acc: 0.9130\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.60329 to 0.59087, saving model to best.model\n",
      "0s - loss: 0.5935 - acc: 0.7753 - val_loss: 0.5909 - val_acc: 0.9130\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.59087 to 0.57820, saving model to best.model\n",
      "0s - loss: 0.6303 - acc: 0.7865 - val_loss: 0.5782 - val_acc: 0.9130\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.57820 to 0.56471, saving model to best.model\n",
      "0s - loss: 0.6757 - acc: 0.7079 - val_loss: 0.5647 - val_acc: 0.9130\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.56471 to 0.55117, saving model to best.model\n",
      "0s - loss: 0.7139 - acc: 0.7528 - val_loss: 0.5512 - val_acc: 0.9130\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.55117 to 0.53845, saving model to best.model\n",
      "0s - loss: 0.5616 - acc: 0.7865 - val_loss: 0.5384 - val_acc: 0.9130\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.53845 to 0.52565, saving model to best.model\n",
      "0s - loss: 0.5695 - acc: 0.7865 - val_loss: 0.5257 - val_acc: 0.9130\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.52565 to 0.51338, saving model to best.model\n",
      "0s - loss: 0.6032 - acc: 0.7528 - val_loss: 0.5134 - val_acc: 0.9130\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.51338 to 0.50151, saving model to best.model\n",
      "0s - loss: 0.6818 - acc: 0.6742 - val_loss: 0.5015 - val_acc: 0.9130\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.50151 to 0.49026, saving model to best.model\n",
      "0s - loss: 0.5735 - acc: 0.8202 - val_loss: 0.4903 - val_acc: 0.9130\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.49026 to 0.47961, saving model to best.model\n",
      "0s - loss: 0.4948 - acc: 0.8539 - val_loss: 0.4796 - val_acc: 0.9130\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.47961 to 0.46921, saving model to best.model\n",
      "0s - loss: 0.5527 - acc: 0.7978 - val_loss: 0.4692 - val_acc: 0.9130\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.46921 to 0.45889, saving model to best.model\n",
      "0s - loss: 0.6858 - acc: 0.7303 - val_loss: 0.4589 - val_acc: 0.9130\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.45889 to 0.44843, saving model to best.model\n",
      "0s - loss: 0.5713 - acc: 0.7640 - val_loss: 0.4484 - val_acc: 0.9130\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.44843 to 0.43806, saving model to best.model\n",
      "0s - loss: 0.5457 - acc: 0.8090 - val_loss: 0.4381 - val_acc: 0.9130\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.43806 to 0.42816, saving model to best.model\n",
      "0s - loss: 0.4582 - acc: 0.8427 - val_loss: 0.4282 - val_acc: 0.9130\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.42816 to 0.41901, saving model to best.model\n",
      "0s - loss: 0.4699 - acc: 0.8539 - val_loss: 0.4190 - val_acc: 0.9130\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.41901 to 0.41053, saving model to best.model\n",
      "0s - loss: 0.4824 - acc: 0.8539 - val_loss: 0.4105 - val_acc: 0.9130\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.41053 to 0.40222, saving model to best.model\n",
      "0s - loss: 0.4403 - acc: 0.8539 - val_loss: 0.4022 - val_acc: 0.9130\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.40222 to 0.39437, saving model to best.model\n",
      "0s - loss: 0.5169 - acc: 0.8202 - val_loss: 0.3944 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.39437 to 0.38690, saving model to best.model\n",
      "0s - loss: 0.4501 - acc: 0.8539 - val_loss: 0.3869 - val_acc: 0.9130\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.38690 to 0.38009, saving model to best.model\n",
      "0s - loss: 0.4608 - acc: 0.8315 - val_loss: 0.3801 - val_acc: 0.9130\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.38009 to 0.37309, saving model to best.model\n",
      "0s - loss: 0.4115 - acc: 0.8764 - val_loss: 0.3731 - val_acc: 0.9130\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.37309 to 0.36562, saving model to best.model\n",
      "0s - loss: 0.4553 - acc: 0.8090 - val_loss: 0.3656 - val_acc: 0.9130\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.36562 to 0.35789, saving model to best.model\n",
      "0s - loss: 0.4187 - acc: 0.8315 - val_loss: 0.3579 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.35789 to 0.35038, saving model to best.model\n",
      "0s - loss: 0.3867 - acc: 0.8539 - val_loss: 0.3504 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.35038 to 0.34263, saving model to best.model\n",
      "0s - loss: 0.3788 - acc: 0.8427 - val_loss: 0.3426 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.34263 to 0.33485, saving model to best.model\n",
      "0s - loss: 0.4661 - acc: 0.8202 - val_loss: 0.3348 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.33485 to 0.32790, saving model to best.model\n",
      "0s - loss: 0.4110 - acc: 0.8539 - val_loss: 0.3279 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.32790 to 0.32124, saving model to best.model\n",
      "0s - loss: 0.4321 - acc: 0.8090 - val_loss: 0.3212 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.32124 to 0.31502, saving model to best.model\n",
      "0s - loss: 0.3852 - acc: 0.8539 - val_loss: 0.3150 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.31502 to 0.30847, saving model to best.model\n",
      "0s - loss: 0.4234 - acc: 0.8090 - val_loss: 0.3085 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.30847 to 0.30193, saving model to best.model\n",
      "0s - loss: 0.3404 - acc: 0.9213 - val_loss: 0.3019 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.30193 to 0.29583, saving model to best.model\n",
      "0s - loss: 0.4208 - acc: 0.8989 - val_loss: 0.2958 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.29583 to 0.28991, saving model to best.model\n",
      "0s - loss: 0.3757 - acc: 0.8427 - val_loss: 0.2899 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.28991 to 0.28364, saving model to best.model\n",
      "0s - loss: 0.3776 - acc: 0.8090 - val_loss: 0.2836 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.28364 to 0.27799, saving model to best.model\n",
      "0s - loss: 0.3413 - acc: 0.8876 - val_loss: 0.2780 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.27799 to 0.27221, saving model to best.model\n",
      "0s - loss: 0.3940 - acc: 0.8202 - val_loss: 0.2722 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.27221 to 0.26546, saving model to best.model\n",
      "0s - loss: 0.3901 - acc: 0.8539 - val_loss: 0.2655 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.26546 to 0.25822, saving model to best.model\n",
      "0s - loss: 0.3745 - acc: 0.8876 - val_loss: 0.2582 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.25822 to 0.25131, saving model to best.model\n",
      "0s - loss: 0.3369 - acc: 0.8652 - val_loss: 0.2513 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.25131 to 0.24494, saving model to best.model\n",
      "0s - loss: 0.3157 - acc: 0.9326 - val_loss: 0.2449 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.24494 to 0.23888, saving model to best.model\n",
      "0s - loss: 0.3380 - acc: 0.8876 - val_loss: 0.2389 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.23888 to 0.23300, saving model to best.model\n",
      "0s - loss: 0.2981 - acc: 0.8989 - val_loss: 0.2330 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.23300 to 0.22732, saving model to best.model\n",
      "0s - loss: 0.3226 - acc: 0.8876 - val_loss: 0.2273 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.22732 to 0.22197, saving model to best.model\n",
      "0s - loss: 0.3145 - acc: 0.9101 - val_loss: 0.2220 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.22197 to 0.21671, saving model to best.model\n",
      "0s - loss: 0.3145 - acc: 0.8989 - val_loss: 0.2167 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.21671 to 0.21162, saving model to best.model\n",
      "0s - loss: 0.3243 - acc: 0.8652 - val_loss: 0.2116 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.21162 to 0.20691, saving model to best.model\n",
      "0s - loss: 0.3171 - acc: 0.8652 - val_loss: 0.2069 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.20691 to 0.20224, saving model to best.model\n",
      "0s - loss: 0.3225 - acc: 0.8989 - val_loss: 0.2022 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.20224 to 0.19749, saving model to best.model\n",
      "0s - loss: 0.3144 - acc: 0.9213 - val_loss: 0.1975 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.19749 to 0.19283, saving model to best.model\n",
      "0s - loss: 0.3518 - acc: 0.8652 - val_loss: 0.1928 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.19283 to 0.18838, saving model to best.model\n",
      "0s - loss: 0.2685 - acc: 0.8989 - val_loss: 0.1884 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.18838 to 0.18405, saving model to best.model\n",
      "0s - loss: 0.2927 - acc: 0.9213 - val_loss: 0.1840 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.18405 to 0.17985, saving model to best.model\n",
      "0s - loss: 0.2914 - acc: 0.8876 - val_loss: 0.1799 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.17985 to 0.17569, saving model to best.model\n",
      "0s - loss: 0.3021 - acc: 0.8989 - val_loss: 0.1757 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.17569 to 0.17166, saving model to best.model\n",
      "0s - loss: 0.2522 - acc: 0.9438 - val_loss: 0.1717 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.17166 to 0.16789, saving model to best.model\n",
      "0s - loss: 0.2326 - acc: 0.9663 - val_loss: 0.1679 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.16789 to 0.16422, saving model to best.model\n",
      "0s - loss: 0.2750 - acc: 0.9101 - val_loss: 0.1642 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.16422 to 0.16057, saving model to best.model\n",
      "0s - loss: 0.2573 - acc: 0.9438 - val_loss: 0.1606 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.16057 to 0.15711, saving model to best.model\n",
      "0s - loss: 0.2609 - acc: 0.9101 - val_loss: 0.1571 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.15711 to 0.15371, saving model to best.model\n",
      "0s - loss: 0.2399 - acc: 0.8876 - val_loss: 0.1537 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.15371 to 0.15041, saving model to best.model\n",
      "0s - loss: 0.2604 - acc: 0.9213 - val_loss: 0.1504 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.15041 to 0.14709, saving model to best.model\n",
      "0s - loss: 0.2602 - acc: 0.9101 - val_loss: 0.1471 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.14709 to 0.14364, saving model to best.model\n",
      "0s - loss: 0.3002 - acc: 0.8764 - val_loss: 0.1436 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.14364 to 0.14029, saving model to best.model\n",
      "0s - loss: 0.2256 - acc: 0.9438 - val_loss: 0.1403 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.14029 to 0.13709, saving model to best.model\n",
      "0s - loss: 0.1718 - acc: 0.9326 - val_loss: 0.1371 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.13709 to 0.13382, saving model to best.model\n",
      "0s - loss: 0.2210 - acc: 0.9101 - val_loss: 0.1338 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.13382 to 0.13051, saving model to best.model\n",
      "0s - loss: 0.2000 - acc: 0.9213 - val_loss: 0.1305 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.13051 to 0.12738, saving model to best.model\n",
      "0s - loss: 0.1720 - acc: 0.9551 - val_loss: 0.1274 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.12738 to 0.12444, saving model to best.model\n",
      "0s - loss: 0.2504 - acc: 0.9101 - val_loss: 0.1244 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.12444 to 0.12156, saving model to best.model\n",
      "0s - loss: 0.1749 - acc: 0.9663 - val_loss: 0.1216 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.12156 to 0.11889, saving model to best.model\n",
      "0s - loss: 0.1837 - acc: 0.9551 - val_loss: 0.1189 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.11889 to 0.11619, saving model to best.model\n",
      "0s - loss: 0.1850 - acc: 0.9438 - val_loss: 0.1162 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.11619 to 0.11349, saving model to best.model\n",
      "0s - loss: 0.1991 - acc: 0.9438 - val_loss: 0.1135 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.11349 to 0.11081, saving model to best.model\n",
      "0s - loss: 0.1945 - acc: 0.9438 - val_loss: 0.1108 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.11081 to 0.10849, saving model to best.model\n",
      "0s - loss: 0.1433 - acc: 0.9438 - val_loss: 0.1085 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.10849 to 0.10643, saving model to best.model\n",
      "0s - loss: 0.2572 - acc: 0.8876 - val_loss: 0.1064 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.10643 to 0.10450, saving model to best.model\n",
      "0s - loss: 0.2079 - acc: 0.9438 - val_loss: 0.1045 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.10450 to 0.10271, saving model to best.model\n",
      "0s - loss: 0.1692 - acc: 0.9326 - val_loss: 0.1027 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.10271 to 0.10088, saving model to best.model\n",
      "0s - loss: 0.1746 - acc: 0.9438 - val_loss: 0.1009 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.10088 to 0.09919, saving model to best.model\n",
      "0s - loss: 0.2018 - acc: 0.9438 - val_loss: 0.0992 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.09919 to 0.09803, saving model to best.model\n",
      "0s - loss: 0.1964 - acc: 0.9438 - val_loss: 0.0980 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.09803 to 0.09683, saving model to best.model\n",
      "0s - loss: 0.1381 - acc: 0.9551 - val_loss: 0.0968 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.09683 to 0.09537, saving model to best.model\n",
      "0s - loss: 0.2105 - acc: 0.9101 - val_loss: 0.0954 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.09537 to 0.09364, saving model to best.model\n",
      "0s - loss: 0.1760 - acc: 0.9326 - val_loss: 0.0936 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.09364 to 0.09134, saving model to best.model\n",
      "0s - loss: 0.1626 - acc: 0.9551 - val_loss: 0.0913 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.09134 to 0.08936, saving model to best.model\n",
      "0s - loss: 0.1650 - acc: 0.9551 - val_loss: 0.0894 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.08936 to 0.08727, saving model to best.model\n",
      "0s - loss: 0.1478 - acc: 0.9438 - val_loss: 0.0873 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.08727 to 0.08492, saving model to best.model\n",
      "0s - loss: 0.1558 - acc: 0.9326 - val_loss: 0.0849 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.08492 to 0.08274, saving model to best.model\n",
      "0s - loss: 0.1564 - acc: 0.9663 - val_loss: 0.0827 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.08274 to 0.08087, saving model to best.model\n",
      "0s - loss: 0.1532 - acc: 0.9438 - val_loss: 0.0809 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.08087 to 0.07914, saving model to best.model\n",
      "0s - loss: 0.1454 - acc: 0.9551 - val_loss: 0.0791 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.07914 to 0.07768, saving model to best.model\n",
      "0s - loss: 0.1404 - acc: 0.9551 - val_loss: 0.0777 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.07768 to 0.07624, saving model to best.model\n",
      "0s - loss: 0.1322 - acc: 0.9775 - val_loss: 0.0762 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.07624 to 0.07493, saving model to best.model\n",
      "0s - loss: 0.1448 - acc: 0.9551 - val_loss: 0.0749 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.07493 to 0.07390, saving model to best.model\n",
      "0s - loss: 0.1221 - acc: 0.9888 - val_loss: 0.0739 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.07390 to 0.07285, saving model to best.model\n",
      "0s - loss: 0.1041 - acc: 0.9775 - val_loss: 0.0728 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.07285 to 0.07181, saving model to best.model\n",
      "0s - loss: 0.0948 - acc: 0.9888 - val_loss: 0.0718 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.07181 to 0.07103, saving model to best.model\n",
      "0s - loss: 0.1762 - acc: 0.9551 - val_loss: 0.0710 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.07103 to 0.07033, saving model to best.model\n",
      "0s - loss: 0.1763 - acc: 0.9326 - val_loss: 0.0703 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.07033 to 0.06948, saving model to best.model\n",
      "0s - loss: 0.1269 - acc: 0.9438 - val_loss: 0.0695 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.06948 to 0.06868, saving model to best.model\n",
      "0s - loss: 0.1103 - acc: 0.9663 - val_loss: 0.0687 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.06868 to 0.06797, saving model to best.model\n",
      "0s - loss: 0.1522 - acc: 0.9551 - val_loss: 0.0680 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.06797 to 0.06737, saving model to best.model\n",
      "0s - loss: 0.1374 - acc: 0.9775 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.06737 to 0.06689, saving model to best.model\n",
      "0s - loss: 0.1287 - acc: 0.9663 - val_loss: 0.0669 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.06689 to 0.06626, saving model to best.model\n",
      "0s - loss: 0.1442 - acc: 0.9551 - val_loss: 0.0663 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.06626 to 0.06542, saving model to best.model\n",
      "0s - loss: 0.1187 - acc: 0.9663 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.06542 to 0.06446, saving model to best.model\n",
      "0s - loss: 0.1574 - acc: 0.9438 - val_loss: 0.0645 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.06446 to 0.06350, saving model to best.model\n",
      "0s - loss: 0.0925 - acc: 0.9888 - val_loss: 0.0635 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.06350 to 0.06281, saving model to best.model\n",
      "0s - loss: 0.1325 - acc: 0.9326 - val_loss: 0.0628 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.06281 to 0.06233, saving model to best.model\n",
      "0s - loss: 0.1325 - acc: 0.9438 - val_loss: 0.0623 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.06233 to 0.06175, saving model to best.model\n",
      "0s - loss: 0.1415 - acc: 0.9326 - val_loss: 0.0617 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.11151, saving model to best.model\n",
      "0s - loss: 1.3699 - acc: 0.2809 - val_loss: 1.1115 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.11151 to 1.08880, saving model to best.model\n",
      "0s - loss: 1.2826 - acc: 0.3596 - val_loss: 1.0888 - val_acc: 0.4783\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.08880 to 1.08539, saving model to best.model\n",
      "0s - loss: 1.2841 - acc: 0.3483 - val_loss: 1.0854 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2009 - acc: 0.3596 - val_loss: 1.0934 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2919 - acc: 0.4045 - val_loss: 1.1029 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1823 - acc: 0.4494 - val_loss: 1.1068 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1808 - acc: 0.3708 - val_loss: 1.1085 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.3401 - acc: 0.3596 - val_loss: 1.1076 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1457 - acc: 0.4607 - val_loss: 1.1050 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2513 - acc: 0.2247 - val_loss: 1.1015 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2539 - acc: 0.3371 - val_loss: 1.0972 - val_acc: 0.3913\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1507 - acc: 0.4157 - val_loss: 1.0924 - val_acc: 0.6957\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2486 - acc: 0.3596 - val_loss: 1.0869 - val_acc: 0.7391\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.08539 to 1.08240, saving model to best.model\n",
      "0s - loss: 1.2193 - acc: 0.3483 - val_loss: 1.0824 - val_acc: 0.6957\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.08240 to 1.07796, saving model to best.model\n",
      "0s - loss: 1.3069 - acc: 0.3483 - val_loss: 1.0780 - val_acc: 0.6957\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.07796 to 1.07360, saving model to best.model\n",
      "0s - loss: 1.1758 - acc: 0.3371 - val_loss: 1.0736 - val_acc: 0.7391\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.07360 to 1.06937, saving model to best.model\n",
      "0s - loss: 1.1832 - acc: 0.3820 - val_loss: 1.0694 - val_acc: 0.6957\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.06937 to 1.06569, saving model to best.model\n",
      "0s - loss: 1.2000 - acc: 0.3820 - val_loss: 1.0657 - val_acc: 0.6957\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.06569 to 1.06247, saving model to best.model\n",
      "0s - loss: 1.0958 - acc: 0.4157 - val_loss: 1.0625 - val_acc: 0.7391\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.06247 to 1.05934, saving model to best.model\n",
      "0s - loss: 1.1861 - acc: 0.3483 - val_loss: 1.0593 - val_acc: 0.6957\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.05934 to 1.05657, saving model to best.model\n",
      "0s - loss: 1.2021 - acc: 0.3596 - val_loss: 1.0566 - val_acc: 0.6087\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.05657 to 1.05426, saving model to best.model\n",
      "0s - loss: 1.0924 - acc: 0.4157 - val_loss: 1.0543 - val_acc: 0.4348\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.05426 to 1.05249, saving model to best.model\n",
      "0s - loss: 1.0830 - acc: 0.4270 - val_loss: 1.0525 - val_acc: 0.4348\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.05249 to 1.05101, saving model to best.model\n",
      "0s - loss: 1.1192 - acc: 0.4045 - val_loss: 1.0510 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.05101 to 1.04961, saving model to best.model\n",
      "0s - loss: 1.1867 - acc: 0.3596 - val_loss: 1.0496 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.04961 to 1.04831, saving model to best.model\n",
      "0s - loss: 1.1785 - acc: 0.4157 - val_loss: 1.0483 - val_acc: 0.3913\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.04831 to 1.04718, saving model to best.model\n",
      "0s - loss: 1.1267 - acc: 0.4045 - val_loss: 1.0472 - val_acc: 0.3913\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.04718 to 1.04607, saving model to best.model\n",
      "0s - loss: 1.1952 - acc: 0.3483 - val_loss: 1.0461 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.04607 to 1.04495, saving model to best.model\n",
      "0s - loss: 1.0094 - acc: 0.5281 - val_loss: 1.0450 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.04495 to 1.04394, saving model to best.model\n",
      "0s - loss: 1.1068 - acc: 0.4045 - val_loss: 1.0439 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.04394 to 1.04253, saving model to best.model\n",
      "0s - loss: 1.1962 - acc: 0.3596 - val_loss: 1.0425 - val_acc: 0.3913\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.04253 to 1.04100, saving model to best.model\n",
      "0s - loss: 1.1894 - acc: 0.3596 - val_loss: 1.0410 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.04100 to 1.03933, saving model to best.model\n",
      "0s - loss: 1.1442 - acc: 0.4719 - val_loss: 1.0393 - val_acc: 0.3913\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.03933 to 1.03739, saving model to best.model\n",
      "0s - loss: 1.1551 - acc: 0.4157 - val_loss: 1.0374 - val_acc: 0.3913\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.03739 to 1.03552, saving model to best.model\n",
      "0s - loss: 1.1798 - acc: 0.4045 - val_loss: 1.0355 - val_acc: 0.3913\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.03552 to 1.03356, saving model to best.model\n",
      "0s - loss: 1.2132 - acc: 0.2921 - val_loss: 1.0336 - val_acc: 0.3913\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.03356 to 1.03110, saving model to best.model\n",
      "0s - loss: 1.1428 - acc: 0.4045 - val_loss: 1.0311 - val_acc: 0.3913\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.03110 to 1.02846, saving model to best.model\n",
      "0s - loss: 1.0190 - acc: 0.4719 - val_loss: 1.0285 - val_acc: 0.3913\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.02846 to 1.02566, saving model to best.model\n",
      "0s - loss: 1.1065 - acc: 0.3933 - val_loss: 1.0257 - val_acc: 0.3913\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.02566 to 1.02252, saving model to best.model\n",
      "0s - loss: 1.0807 - acc: 0.4270 - val_loss: 1.0225 - val_acc: 0.4348\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.02252 to 1.01965, saving model to best.model\n",
      "0s - loss: 1.0498 - acc: 0.4831 - val_loss: 1.0197 - val_acc: 0.4348\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.01965 to 1.01676, saving model to best.model\n",
      "0s - loss: 1.1073 - acc: 0.4157 - val_loss: 1.0168 - val_acc: 0.4783\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.01676 to 1.01402, saving model to best.model\n",
      "0s - loss: 1.0176 - acc: 0.5169 - val_loss: 1.0140 - val_acc: 0.5217\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.01402 to 1.01112, saving model to best.model\n",
      "0s - loss: 1.1173 - acc: 0.4157 - val_loss: 1.0111 - val_acc: 0.6087\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.01112 to 1.00817, saving model to best.model\n",
      "0s - loss: 1.1643 - acc: 0.4270 - val_loss: 1.0082 - val_acc: 0.6522\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.00817 to 1.00496, saving model to best.model\n",
      "0s - loss: 1.0685 - acc: 0.3596 - val_loss: 1.0050 - val_acc: 0.6522\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.00496 to 1.00199, saving model to best.model\n",
      "0s - loss: 1.1494 - acc: 0.3371 - val_loss: 1.0020 - val_acc: 0.6957\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.00199 to 0.99884, saving model to best.model\n",
      "0s - loss: 1.0937 - acc: 0.4382 - val_loss: 0.9988 - val_acc: 0.6957\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.99884 to 0.99534, saving model to best.model\n",
      "0s - loss: 1.0557 - acc: 0.3933 - val_loss: 0.9953 - val_acc: 0.6957\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.99534 to 0.99153, saving model to best.model\n",
      "0s - loss: 1.0628 - acc: 0.4607 - val_loss: 0.9915 - val_acc: 0.6957\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.99153 to 0.98773, saving model to best.model\n",
      "0s - loss: 0.9840 - acc: 0.5281 - val_loss: 0.9877 - val_acc: 0.6957\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.98773 to 0.98387, saving model to best.model\n",
      "0s - loss: 1.0841 - acc: 0.4944 - val_loss: 0.9839 - val_acc: 0.6957\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.98387 to 0.97975, saving model to best.model\n",
      "0s - loss: 1.0643 - acc: 0.4607 - val_loss: 0.9798 - val_acc: 0.6957\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.97975 to 0.97552, saving model to best.model\n",
      "0s - loss: 1.0423 - acc: 0.3933 - val_loss: 0.9755 - val_acc: 0.6957\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.97552 to 0.97099, saving model to best.model\n",
      "0s - loss: 1.0221 - acc: 0.4607 - val_loss: 0.9710 - val_acc: 0.6957\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.97099 to 0.96642, saving model to best.model\n",
      "0s - loss: 1.1353 - acc: 0.3596 - val_loss: 0.9664 - val_acc: 0.6957\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.96642 to 0.96188, saving model to best.model\n",
      "0s - loss: 1.0177 - acc: 0.4607 - val_loss: 0.9619 - val_acc: 0.6957\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.96188 to 0.95735, saving model to best.model\n",
      "0s - loss: 1.0146 - acc: 0.5056 - val_loss: 0.9574 - val_acc: 0.6957\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.95735 to 0.95265, saving model to best.model\n",
      "0s - loss: 1.0276 - acc: 0.4944 - val_loss: 0.9526 - val_acc: 0.6957\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.95265 to 0.94763, saving model to best.model\n",
      "0s - loss: 1.0359 - acc: 0.4157 - val_loss: 0.9476 - val_acc: 0.6957\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.94763 to 0.94234, saving model to best.model\n",
      "0s - loss: 0.9874 - acc: 0.4944 - val_loss: 0.9423 - val_acc: 0.6957\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.94234 to 0.93682, saving model to best.model\n",
      "0s - loss: 1.0546 - acc: 0.4045 - val_loss: 0.9368 - val_acc: 0.6957\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.93682 to 0.93103, saving model to best.model\n",
      "0s - loss: 0.9945 - acc: 0.5281 - val_loss: 0.9310 - val_acc: 0.6957\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.93103 to 0.92505, saving model to best.model\n",
      "0s - loss: 1.0303 - acc: 0.4719 - val_loss: 0.9251 - val_acc: 0.6957\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.92505 to 0.91917, saving model to best.model\n",
      "0s - loss: 1.0280 - acc: 0.5169 - val_loss: 0.9192 - val_acc: 0.6957\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.91917 to 0.91307, saving model to best.model\n",
      "0s - loss: 1.0150 - acc: 0.4719 - val_loss: 0.9131 - val_acc: 0.6957\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.91307 to 0.90668, saving model to best.model\n",
      "0s - loss: 0.9675 - acc: 0.5506 - val_loss: 0.9067 - val_acc: 0.6957\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.90668 to 0.89992, saving model to best.model\n",
      "0s - loss: 0.9000 - acc: 0.6067 - val_loss: 0.8999 - val_acc: 0.6957\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.89992 to 0.89284, saving model to best.model\n",
      "0s - loss: 1.0092 - acc: 0.4831 - val_loss: 0.8928 - val_acc: 0.6957\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.89284 to 0.88542, saving model to best.model\n",
      "0s - loss: 0.9320 - acc: 0.6180 - val_loss: 0.8854 - val_acc: 0.6957\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.88542 to 0.87766, saving model to best.model\n",
      "0s - loss: 0.9679 - acc: 0.6067 - val_loss: 0.8777 - val_acc: 0.6957\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.87766 to 0.86972, saving model to best.model\n",
      "0s - loss: 0.9466 - acc: 0.6067 - val_loss: 0.8697 - val_acc: 0.6957\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.86972 to 0.86150, saving model to best.model\n",
      "0s - loss: 0.9659 - acc: 0.5618 - val_loss: 0.8615 - val_acc: 0.6957\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.86150 to 0.85290, saving model to best.model\n",
      "0s - loss: 0.9807 - acc: 0.4944 - val_loss: 0.8529 - val_acc: 0.6957\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.85290 to 0.84406, saving model to best.model\n",
      "0s - loss: 0.9497 - acc: 0.5955 - val_loss: 0.8441 - val_acc: 0.6957\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.84406 to 0.83501, saving model to best.model\n",
      "0s - loss: 0.9053 - acc: 0.6292 - val_loss: 0.8350 - val_acc: 0.6957\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.83501 to 0.82551, saving model to best.model\n",
      "0s - loss: 0.9512 - acc: 0.5393 - val_loss: 0.8255 - val_acc: 0.6957\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.82551 to 0.81577, saving model to best.model\n",
      "0s - loss: 0.8574 - acc: 0.6517 - val_loss: 0.8158 - val_acc: 0.6957\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.81577 to 0.80594, saving model to best.model\n",
      "0s - loss: 0.9205 - acc: 0.5506 - val_loss: 0.8059 - val_acc: 0.6957\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.80594 to 0.79554, saving model to best.model\n",
      "0s - loss: 0.8791 - acc: 0.6067 - val_loss: 0.7955 - val_acc: 0.6957\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.79554 to 0.78503, saving model to best.model\n",
      "0s - loss: 0.8966 - acc: 0.5843 - val_loss: 0.7850 - val_acc: 0.6957\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.78503 to 0.77422, saving model to best.model\n",
      "0s - loss: 0.8536 - acc: 0.6404 - val_loss: 0.7742 - val_acc: 0.6957\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.77422 to 0.76312, saving model to best.model\n",
      "0s - loss: 0.8109 - acc: 0.6629 - val_loss: 0.7631 - val_acc: 0.6957\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.76312 to 0.75185, saving model to best.model\n",
      "0s - loss: 0.8462 - acc: 0.6404 - val_loss: 0.7519 - val_acc: 0.6957\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.75185 to 0.74048, saving model to best.model\n",
      "0s - loss: 0.8348 - acc: 0.6180 - val_loss: 0.7405 - val_acc: 0.6957\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.74048 to 0.72917, saving model to best.model\n",
      "0s - loss: 0.8232 - acc: 0.6067 - val_loss: 0.7292 - val_acc: 0.6957\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.72917 to 0.71750, saving model to best.model\n",
      "0s - loss: 0.8088 - acc: 0.6742 - val_loss: 0.7175 - val_acc: 0.6957\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.71750 to 0.70624, saving model to best.model\n",
      "0s - loss: 0.7939 - acc: 0.6629 - val_loss: 0.7062 - val_acc: 0.6957\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.70624 to 0.69472, saving model to best.model\n",
      "0s - loss: 0.7660 - acc: 0.6854 - val_loss: 0.6947 - val_acc: 0.6957\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69472 to 0.68330, saving model to best.model\n",
      "0s - loss: 0.7726 - acc: 0.6966 - val_loss: 0.6833 - val_acc: 0.6957\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68330 to 0.67180, saving model to best.model\n",
      "0s - loss: 0.7669 - acc: 0.6966 - val_loss: 0.6718 - val_acc: 0.6957\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67180 to 0.66014, saving model to best.model\n",
      "0s - loss: 0.7484 - acc: 0.6517 - val_loss: 0.6601 - val_acc: 0.6957\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66014 to 0.64851, saving model to best.model\n",
      "0s - loss: 0.7356 - acc: 0.7416 - val_loss: 0.6485 - val_acc: 0.6957\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.64851 to 0.63693, saving model to best.model\n",
      "0s - loss: 0.6522 - acc: 0.7753 - val_loss: 0.6369 - val_acc: 0.6957\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.63693 to 0.62538, saving model to best.model\n",
      "0s - loss: 0.6896 - acc: 0.6966 - val_loss: 0.6254 - val_acc: 0.6957\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.62538 to 0.61402, saving model to best.model\n",
      "0s - loss: 0.7207 - acc: 0.7191 - val_loss: 0.6140 - val_acc: 0.6957\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.61402 to 0.60244, saving model to best.model\n",
      "0s - loss: 0.6355 - acc: 0.8090 - val_loss: 0.6024 - val_acc: 0.6957\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.60244 to 0.59109, saving model to best.model\n",
      "0s - loss: 0.7070 - acc: 0.7416 - val_loss: 0.5911 - val_acc: 0.6957\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.59109 to 0.57952, saving model to best.model\n",
      "0s - loss: 0.6988 - acc: 0.7191 - val_loss: 0.5795 - val_acc: 0.6957\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.57952 to 0.56785, saving model to best.model\n",
      "0s - loss: 0.6967 - acc: 0.7528 - val_loss: 0.5678 - val_acc: 0.7826\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.56785 to 0.55663, saving model to best.model\n",
      "0s - loss: 0.6353 - acc: 0.7416 - val_loss: 0.5566 - val_acc: 0.7826\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.55663 to 0.54526, saving model to best.model\n",
      "0s - loss: 0.6591 - acc: 0.7416 - val_loss: 0.5453 - val_acc: 0.7826\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.54526 to 0.53417, saving model to best.model\n",
      "0s - loss: 0.7044 - acc: 0.7079 - val_loss: 0.5342 - val_acc: 0.7826\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.53417 to 0.52332, saving model to best.model\n",
      "0s - loss: 0.7149 - acc: 0.6854 - val_loss: 0.5233 - val_acc: 0.7826\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.52332 to 0.51258, saving model to best.model\n",
      "0s - loss: 0.6239 - acc: 0.7640 - val_loss: 0.5126 - val_acc: 0.8696\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.51258 to 0.50193, saving model to best.model\n",
      "0s - loss: 0.6742 - acc: 0.6966 - val_loss: 0.5019 - val_acc: 0.8696\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.50193 to 0.49144, saving model to best.model\n",
      "0s - loss: 0.6449 - acc: 0.7416 - val_loss: 0.4914 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.49144 to 0.48134, saving model to best.model\n",
      "0s - loss: 0.6863 - acc: 0.7191 - val_loss: 0.4813 - val_acc: 0.9130\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.48134 to 0.47179, saving model to best.model\n",
      "0s - loss: 0.6431 - acc: 0.7303 - val_loss: 0.4718 - val_acc: 0.9130\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.47179 to 0.46254, saving model to best.model\n",
      "0s - loss: 0.5543 - acc: 0.7978 - val_loss: 0.4625 - val_acc: 0.9565\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.46254 to 0.45353, saving model to best.model\n",
      "0s - loss: 0.6335 - acc: 0.6854 - val_loss: 0.4535 - val_acc: 0.9565\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.45353 to 0.44365, saving model to best.model\n",
      "0s - loss: 0.5578 - acc: 0.7865 - val_loss: 0.4437 - val_acc: 0.9565\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.44365 to 0.43393, saving model to best.model\n",
      "0s - loss: 0.5929 - acc: 0.7865 - val_loss: 0.4339 - val_acc: 0.9565\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.43393 to 0.42418, saving model to best.model\n",
      "0s - loss: 0.5510 - acc: 0.7978 - val_loss: 0.4242 - val_acc: 0.9565\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.42418 to 0.41472, saving model to best.model\n",
      "0s - loss: 0.5202 - acc: 0.8427 - val_loss: 0.4147 - val_acc: 0.9565\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.41472 to 0.40595, saving model to best.model\n",
      "0s - loss: 0.4814 - acc: 0.8539 - val_loss: 0.4060 - val_acc: 0.9565\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.40595 to 0.39732, saving model to best.model\n",
      "0s - loss: 0.4892 - acc: 0.8652 - val_loss: 0.3973 - val_acc: 0.9565\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.39732 to 0.38886, saving model to best.model\n",
      "0s - loss: 0.4680 - acc: 0.8202 - val_loss: 0.3889 - val_acc: 0.9565\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.38886 to 0.38055, saving model to best.model\n",
      "0s - loss: 0.5078 - acc: 0.8427 - val_loss: 0.3805 - val_acc: 0.9565\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.38055 to 0.37248, saving model to best.model\n",
      "0s - loss: 0.5174 - acc: 0.8202 - val_loss: 0.3725 - val_acc: 0.9565\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.37248 to 0.36441, saving model to best.model\n",
      "0s - loss: 0.4759 - acc: 0.8315 - val_loss: 0.3644 - val_acc: 0.9565\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.36441 to 0.35635, saving model to best.model\n",
      "0s - loss: 0.4633 - acc: 0.8090 - val_loss: 0.3563 - val_acc: 0.9565\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.35635 to 0.34840, saving model to best.model\n",
      "0s - loss: 0.4350 - acc: 0.8539 - val_loss: 0.3484 - val_acc: 0.9565\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.34840 to 0.34049, saving model to best.model\n",
      "0s - loss: 0.4219 - acc: 0.8539 - val_loss: 0.3405 - val_acc: 0.9565\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.34049 to 0.33290, saving model to best.model\n",
      "0s - loss: 0.4244 - acc: 0.8764 - val_loss: 0.3329 - val_acc: 0.9565\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.33290 to 0.32521, saving model to best.model\n",
      "0s - loss: 0.4668 - acc: 0.8202 - val_loss: 0.3252 - val_acc: 0.9565\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.32521 to 0.31756, saving model to best.model\n",
      "0s - loss: 0.5078 - acc: 0.7753 - val_loss: 0.3176 - val_acc: 0.9565\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.31756 to 0.31023, saving model to best.model\n",
      "0s - loss: 0.4382 - acc: 0.8315 - val_loss: 0.3102 - val_acc: 0.9565\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.31023 to 0.30321, saving model to best.model\n",
      "0s - loss: 0.4529 - acc: 0.8427 - val_loss: 0.3032 - val_acc: 0.9565\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.30321 to 0.29641, saving model to best.model\n",
      "0s - loss: 0.4124 - acc: 0.8764 - val_loss: 0.2964 - val_acc: 0.9565\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.29641 to 0.29006, saving model to best.model\n",
      "0s - loss: 0.4415 - acc: 0.7865 - val_loss: 0.2901 - val_acc: 0.9565\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.29006 to 0.28366, saving model to best.model\n",
      "0s - loss: 0.4174 - acc: 0.8652 - val_loss: 0.2837 - val_acc: 0.9565\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.28366 to 0.27712, saving model to best.model\n",
      "0s - loss: 0.4579 - acc: 0.8202 - val_loss: 0.2771 - val_acc: 0.9565\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.27712 to 0.27103, saving model to best.model\n",
      "0s - loss: 0.3724 - acc: 0.8652 - val_loss: 0.2710 - val_acc: 0.9565\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.27103 to 0.26518, saving model to best.model\n",
      "0s - loss: 0.3526 - acc: 0.9101 - val_loss: 0.2652 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.26518 to 0.25931, saving model to best.model\n",
      "0s - loss: 0.4806 - acc: 0.8202 - val_loss: 0.2593 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.25931 to 0.25289, saving model to best.model\n",
      "0s - loss: 0.3796 - acc: 0.8764 - val_loss: 0.2529 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.25289 to 0.24640, saving model to best.model\n",
      "0s - loss: 0.4455 - acc: 0.7978 - val_loss: 0.2464 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.24640 to 0.24013, saving model to best.model\n",
      "0s - loss: 0.3340 - acc: 0.9213 - val_loss: 0.2401 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.24013 to 0.23352, saving model to best.model\n",
      "0s - loss: 0.3916 - acc: 0.8315 - val_loss: 0.2335 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.23352 to 0.22685, saving model to best.model\n",
      "0s - loss: 0.3423 - acc: 0.8989 - val_loss: 0.2269 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.22685 to 0.22011, saving model to best.model\n",
      "0s - loss: 0.3594 - acc: 0.8764 - val_loss: 0.2201 - val_acc: 0.9565\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.22011 to 0.21335, saving model to best.model\n",
      "0s - loss: 0.3718 - acc: 0.8764 - val_loss: 0.2134 - val_acc: 0.9565\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.21335 to 0.20714, saving model to best.model\n",
      "0s - loss: 0.3884 - acc: 0.8652 - val_loss: 0.2071 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.20714 to 0.20144, saving model to best.model\n",
      "0s - loss: 0.2831 - acc: 0.9213 - val_loss: 0.2014 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.20144 to 0.19562, saving model to best.model\n",
      "0s - loss: 0.3094 - acc: 0.9101 - val_loss: 0.1956 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.19562 to 0.18993, saving model to best.model\n",
      "0s - loss: 0.2638 - acc: 0.9101 - val_loss: 0.1899 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.18993 to 0.18461, saving model to best.model\n",
      "0s - loss: 0.3220 - acc: 0.8876 - val_loss: 0.1846 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.18461 to 0.17964, saving model to best.model\n",
      "0s - loss: 0.2272 - acc: 0.9775 - val_loss: 0.1796 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.17964 to 0.17459, saving model to best.model\n",
      "0s - loss: 0.2746 - acc: 0.9213 - val_loss: 0.1746 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.17459 to 0.16912, saving model to best.model\n",
      "0s - loss: 0.2892 - acc: 0.9101 - val_loss: 0.1691 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.16912 to 0.16431, saving model to best.model\n",
      "0s - loss: 0.2189 - acc: 0.9438 - val_loss: 0.1643 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.16431 to 0.15930, saving model to best.model\n",
      "0s - loss: 0.2452 - acc: 0.9326 - val_loss: 0.1593 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.15930 to 0.15538, saving model to best.model\n",
      "0s - loss: 0.2453 - acc: 0.9551 - val_loss: 0.1554 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.15538 to 0.15187, saving model to best.model\n",
      "0s - loss: 0.2276 - acc: 0.9101 - val_loss: 0.1519 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.15187 to 0.14837, saving model to best.model\n",
      "0s - loss: 0.2800 - acc: 0.9213 - val_loss: 0.1484 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.14837 to 0.14524, saving model to best.model\n",
      "0s - loss: 0.2272 - acc: 0.9551 - val_loss: 0.1452 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.14524 to 0.14200, saving model to best.model\n",
      "0s - loss: 0.2561 - acc: 0.8764 - val_loss: 0.1420 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.14200 to 0.13883, saving model to best.model\n",
      "0s - loss: 0.2778 - acc: 0.8764 - val_loss: 0.1388 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.13883 to 0.13604, saving model to best.model\n",
      "0s - loss: 0.2376 - acc: 0.9326 - val_loss: 0.1360 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.13604 to 0.13361, saving model to best.model\n",
      "0s - loss: 0.2361 - acc: 0.9326 - val_loss: 0.1336 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.13361 to 0.13088, saving model to best.model\n",
      "0s - loss: 0.2863 - acc: 0.8764 - val_loss: 0.1309 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.13088 to 0.12821, saving model to best.model\n",
      "0s - loss: 0.2219 - acc: 0.9326 - val_loss: 0.1282 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.12821 to 0.12609, saving model to best.model\n",
      "0s - loss: 0.2416 - acc: 0.9326 - val_loss: 0.1261 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.12609 to 0.12401, saving model to best.model\n",
      "0s - loss: 0.2219 - acc: 0.9101 - val_loss: 0.1240 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.12401 to 0.12196, saving model to best.model\n",
      "0s - loss: 0.2528 - acc: 0.8989 - val_loss: 0.1220 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.12196 to 0.11985, saving model to best.model\n",
      "0s - loss: 0.1736 - acc: 0.9551 - val_loss: 0.1198 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.11985 to 0.11752, saving model to best.model\n",
      "0s - loss: 0.2222 - acc: 0.9213 - val_loss: 0.1175 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.11752 to 0.11503, saving model to best.model\n",
      "0s - loss: 0.2235 - acc: 0.9326 - val_loss: 0.1150 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.11503 to 0.11183, saving model to best.model\n",
      "0s - loss: 0.2328 - acc: 0.9326 - val_loss: 0.1118 - val_acc: 0.9565\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.11183 to 0.10898, saving model to best.model\n",
      "0s - loss: 0.1691 - acc: 0.9663 - val_loss: 0.1090 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.10898 to 0.10654, saving model to best.model\n",
      "0s - loss: 0.1900 - acc: 0.9438 - val_loss: 0.1065 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.10654 to 0.10364, saving model to best.model\n",
      "0s - loss: 0.1903 - acc: 0.9326 - val_loss: 0.1036 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.10364 to 0.10089, saving model to best.model\n",
      "0s - loss: 0.1470 - acc: 0.9775 - val_loss: 0.1009 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.10089 to 0.09814, saving model to best.model\n",
      "0s - loss: 0.2703 - acc: 0.8989 - val_loss: 0.0981 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.09814 to 0.09563, saving model to best.model\n",
      "0s - loss: 0.2074 - acc: 0.9438 - val_loss: 0.0956 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.09563 to 0.09311, saving model to best.model\n",
      "0s - loss: 0.1451 - acc: 0.9775 - val_loss: 0.0931 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.09311 to 0.09080, saving model to best.model\n",
      "0s - loss: 0.1808 - acc: 0.9551 - val_loss: 0.0908 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.09080 to 0.08845, saving model to best.model\n",
      "0s - loss: 0.1749 - acc: 0.9663 - val_loss: 0.0884 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.08845 to 0.08653, saving model to best.model\n",
      "0s - loss: 0.2151 - acc: 0.9101 - val_loss: 0.0865 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.08653 to 0.08547, saving model to best.model\n",
      "0s - loss: 0.1579 - acc: 0.9326 - val_loss: 0.0855 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.08547 to 0.08440, saving model to best.model\n",
      "0s - loss: 0.1731 - acc: 0.9438 - val_loss: 0.0844 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.08440 to 0.08398, saving model to best.model\n",
      "0s - loss: 0.1245 - acc: 0.9663 - val_loss: 0.0840 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.08398 to 0.08253, saving model to best.model\n",
      "0s - loss: 0.1857 - acc: 0.9438 - val_loss: 0.0825 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.08253 to 0.08128, saving model to best.model\n",
      "0s - loss: 0.1744 - acc: 0.9438 - val_loss: 0.0813 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.08128 to 0.08002, saving model to best.model\n",
      "0s - loss: 0.1903 - acc: 0.9438 - val_loss: 0.0800 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.08002 to 0.07841, saving model to best.model\n",
      "0s - loss: 0.1695 - acc: 0.9438 - val_loss: 0.0784 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.07841 to 0.07759, saving model to best.model\n",
      "0s - loss: 0.1491 - acc: 0.9663 - val_loss: 0.0776 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.07759 to 0.07651, saving model to best.model\n",
      "0s - loss: 0.2254 - acc: 0.9101 - val_loss: 0.0765 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.07651 to 0.07511, saving model to best.model\n",
      "0s - loss: 0.1537 - acc: 0.9326 - val_loss: 0.0751 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.07511 to 0.07243, saving model to best.model\n",
      "0s - loss: 0.2627 - acc: 0.8764 - val_loss: 0.0724 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.07243 to 0.07016, saving model to best.model\n",
      "0s - loss: 0.1434 - acc: 0.9663 - val_loss: 0.0702 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.07016 to 0.06844, saving model to best.model\n",
      "0s - loss: 0.1497 - acc: 0.9326 - val_loss: 0.0684 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.06844 to 0.06619, saving model to best.model\n",
      "0s - loss: 0.1452 - acc: 0.9551 - val_loss: 0.0662 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.06619 to 0.06373, saving model to best.model\n",
      "0s - loss: 0.2404 - acc: 0.8876 - val_loss: 0.0637 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.06373 to 0.06198, saving model to best.model\n",
      "0s - loss: 0.1643 - acc: 0.9551 - val_loss: 0.0620 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.06198 to 0.06050, saving model to best.model\n",
      "0s - loss: 0.1309 - acc: 0.9663 - val_loss: 0.0605 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.06050 to 0.05929, saving model to best.model\n",
      "0s - loss: 0.1511 - acc: 0.9326 - val_loss: 0.0593 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.05929 to 0.05837, saving model to best.model\n",
      "0s - loss: 0.1598 - acc: 0.9326 - val_loss: 0.0584 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.05837 to 0.05712, saving model to best.model\n",
      "0s - loss: 0.1538 - acc: 0.9551 - val_loss: 0.0571 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.08150, saving model to best.model\n",
      "0s - loss: 1.3225 - acc: 0.4157 - val_loss: 1.0815 - val_acc: 0.4348\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.08150 to 1.07905, saving model to best.model\n",
      "0s - loss: 1.3258 - acc: 0.2584 - val_loss: 1.0791 - val_acc: 0.4348\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.2729 - acc: 0.2809 - val_loss: 1.0821 - val_acc: 0.4348\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1870 - acc: 0.4270 - val_loss: 1.0904 - val_acc: 0.3478\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2104 - acc: 0.3371 - val_loss: 1.0950 - val_acc: 0.5217\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1473 - acc: 0.4045 - val_loss: 1.0976 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2194 - acc: 0.3820 - val_loss: 1.0938 - val_acc: 0.3478\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1175 - acc: 0.3596 - val_loss: 1.0898 - val_acc: 0.3043\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1325 - acc: 0.3820 - val_loss: 1.0845 - val_acc: 0.4348\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.07905 to 1.07776, saving model to best.model\n",
      "0s - loss: 1.2204 - acc: 0.3371 - val_loss: 1.0778 - val_acc: 0.4348\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.07776 to 1.07185, saving model to best.model\n",
      "0s - loss: 1.1256 - acc: 0.4270 - val_loss: 1.0719 - val_acc: 0.4348\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.07185 to 1.06725, saving model to best.model\n",
      "0s - loss: 1.2865 - acc: 0.3034 - val_loss: 1.0673 - val_acc: 0.4348\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.06725 to 1.06493, saving model to best.model\n",
      "0s - loss: 1.2874 - acc: 0.2472 - val_loss: 1.0649 - val_acc: 0.4348\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.06493 to 1.06405, saving model to best.model\n",
      "0s - loss: 1.2299 - acc: 0.3258 - val_loss: 1.0640 - val_acc: 0.4348\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.06405 to 1.06192, saving model to best.model\n",
      "0s - loss: 1.1823 - acc: 0.3933 - val_loss: 1.0619 - val_acc: 0.4348\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.06192 to 1.05961, saving model to best.model\n",
      "0s - loss: 1.2740 - acc: 0.3034 - val_loss: 1.0596 - val_acc: 0.4348\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.05961 to 1.05724, saving model to best.model\n",
      "0s - loss: 1.2935 - acc: 0.2584 - val_loss: 1.0572 - val_acc: 0.4348\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.05724 to 1.05554, saving model to best.model\n",
      "0s - loss: 1.1852 - acc: 0.3034 - val_loss: 1.0555 - val_acc: 0.4348\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.05554 to 1.05356, saving model to best.model\n",
      "0s - loss: 1.2137 - acc: 0.2472 - val_loss: 1.0536 - val_acc: 0.4348\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.05356 to 1.05222, saving model to best.model\n",
      "0s - loss: 1.1747 - acc: 0.3483 - val_loss: 1.0522 - val_acc: 0.4348\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.05222 to 1.05143, saving model to best.model\n",
      "0s - loss: 1.1467 - acc: 0.4157 - val_loss: 1.0514 - val_acc: 0.4348\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.05143 to 1.05054, saving model to best.model\n",
      "0s - loss: 1.1214 - acc: 0.3596 - val_loss: 1.0505 - val_acc: 0.4348\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.05054 to 1.04930, saving model to best.model\n",
      "0s - loss: 1.1917 - acc: 0.3596 - val_loss: 1.0493 - val_acc: 0.4348\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.04930 to 1.04867, saving model to best.model\n",
      "0s - loss: 1.2059 - acc: 0.4157 - val_loss: 1.0487 - val_acc: 0.4348\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.04867 to 1.04840, saving model to best.model\n",
      "0s - loss: 1.2391 - acc: 0.3146 - val_loss: 1.0484 - val_acc: 0.4348\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1927 - acc: 0.3596 - val_loss: 1.0492 - val_acc: 0.4348\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.2062 - acc: 0.3258 - val_loss: 1.0508 - val_acc: 0.4783\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.2154 - acc: 0.3258 - val_loss: 1.0512 - val_acc: 0.5652\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1411 - acc: 0.3596 - val_loss: 1.0500 - val_acc: 0.5652\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.04840 to 1.04826, saving model to best.model\n",
      "0s - loss: 1.2088 - acc: 0.3483 - val_loss: 1.0483 - val_acc: 0.5652\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.04826 to 1.04592, saving model to best.model\n",
      "0s - loss: 1.1684 - acc: 0.3596 - val_loss: 1.0459 - val_acc: 0.5652\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.04592 to 1.04369, saving model to best.model\n",
      "0s - loss: 1.1462 - acc: 0.3483 - val_loss: 1.0437 - val_acc: 0.5217\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.04369 to 1.04064, saving model to best.model\n",
      "0s - loss: 1.1617 - acc: 0.3146 - val_loss: 1.0406 - val_acc: 0.4783\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.04064 to 1.03725, saving model to best.model\n",
      "0s - loss: 1.1063 - acc: 0.3820 - val_loss: 1.0372 - val_acc: 0.4783\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.03725 to 1.03378, saving model to best.model\n",
      "0s - loss: 1.0718 - acc: 0.3933 - val_loss: 1.0338 - val_acc: 0.4348\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.03378 to 1.03076, saving model to best.model\n",
      "0s - loss: 1.1689 - acc: 0.3933 - val_loss: 1.0308 - val_acc: 0.4348\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.03076 to 1.02800, saving model to best.model\n",
      "0s - loss: 1.0490 - acc: 0.4831 - val_loss: 1.0280 - val_acc: 0.4348\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.02800 to 1.02547, saving model to best.model\n",
      "0s - loss: 1.1388 - acc: 0.3933 - val_loss: 1.0255 - val_acc: 0.4348\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.02547 to 1.02355, saving model to best.model\n",
      "0s - loss: 1.1969 - acc: 0.3933 - val_loss: 1.0235 - val_acc: 0.4783\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.02355 to 1.02208, saving model to best.model\n",
      "0s - loss: 1.1241 - acc: 0.3708 - val_loss: 1.0221 - val_acc: 0.4783\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.02208 to 1.02055, saving model to best.model\n",
      "0s - loss: 1.0758 - acc: 0.4607 - val_loss: 1.0206 - val_acc: 0.5652\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.02055 to 1.01902, saving model to best.model\n",
      "0s - loss: 1.0824 - acc: 0.4719 - val_loss: 1.0190 - val_acc: 0.5652\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.01902 to 1.01715, saving model to best.model\n",
      "0s - loss: 1.1644 - acc: 0.3596 - val_loss: 1.0171 - val_acc: 0.5652\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.01715 to 1.01503, saving model to best.model\n",
      "0s - loss: 1.1016 - acc: 0.4831 - val_loss: 1.0150 - val_acc: 0.6957\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.01503 to 1.01141, saving model to best.model\n",
      "0s - loss: 1.1461 - acc: 0.3933 - val_loss: 1.0114 - val_acc: 0.6957\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.01141 to 1.00702, saving model to best.model\n",
      "0s - loss: 1.0686 - acc: 0.4157 - val_loss: 1.0070 - val_acc: 0.7826\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.00702 to 1.00292, saving model to best.model\n",
      "0s - loss: 1.1017 - acc: 0.4045 - val_loss: 1.0029 - val_acc: 0.7826\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.00292 to 0.99858, saving model to best.model\n",
      "0s - loss: 1.1093 - acc: 0.3933 - val_loss: 0.9986 - val_acc: 0.7826\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.99858 to 0.99379, saving model to best.model\n",
      "0s - loss: 1.1414 - acc: 0.3820 - val_loss: 0.9938 - val_acc: 0.7826\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.99379 to 0.98862, saving model to best.model\n",
      "0s - loss: 1.0613 - acc: 0.4944 - val_loss: 0.9886 - val_acc: 0.7826\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.98862 to 0.98342, saving model to best.model\n",
      "0s - loss: 1.0047 - acc: 0.5618 - val_loss: 0.9834 - val_acc: 0.6957\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.98342 to 0.97829, saving model to best.model\n",
      "0s - loss: 1.1059 - acc: 0.3933 - val_loss: 0.9783 - val_acc: 0.6522\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.97829 to 0.97379, saving model to best.model\n",
      "0s - loss: 1.0916 - acc: 0.4382 - val_loss: 0.9738 - val_acc: 0.7391\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.97379 to 0.96923, saving model to best.model\n",
      "0s - loss: 1.0899 - acc: 0.4270 - val_loss: 0.9692 - val_acc: 0.7391\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.96923 to 0.96519, saving model to best.model\n",
      "0s - loss: 1.0294 - acc: 0.4607 - val_loss: 0.9652 - val_acc: 0.7826\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.96519 to 0.96109, saving model to best.model\n",
      "0s - loss: 1.0503 - acc: 0.4382 - val_loss: 0.9611 - val_acc: 0.8261\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.96109 to 0.95678, saving model to best.model\n",
      "0s - loss: 1.0219 - acc: 0.4719 - val_loss: 0.9568 - val_acc: 0.8696\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.95678 to 0.95240, saving model to best.model\n",
      "0s - loss: 1.0734 - acc: 0.4607 - val_loss: 0.9524 - val_acc: 0.8696\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.95240 to 0.94803, saving model to best.model\n",
      "0s - loss: 1.0952 - acc: 0.3820 - val_loss: 0.9480 - val_acc: 0.8696\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.94803 to 0.94300, saving model to best.model\n",
      "0s - loss: 0.9927 - acc: 0.5281 - val_loss: 0.9430 - val_acc: 0.8696\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.94300 to 0.93813, saving model to best.model\n",
      "0s - loss: 1.0489 - acc: 0.4831 - val_loss: 0.9381 - val_acc: 0.8696\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.93813 to 0.93361, saving model to best.model\n",
      "0s - loss: 1.0595 - acc: 0.4157 - val_loss: 0.9336 - val_acc: 0.8696\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.93361 to 0.92850, saving model to best.model\n",
      "0s - loss: 1.0360 - acc: 0.4157 - val_loss: 0.9285 - val_acc: 0.8696\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.92850 to 0.92294, saving model to best.model\n",
      "0s - loss: 1.0450 - acc: 0.4045 - val_loss: 0.9229 - val_acc: 0.8696\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.92294 to 0.91694, saving model to best.model\n",
      "0s - loss: 0.9991 - acc: 0.4494 - val_loss: 0.9169 - val_acc: 0.9130\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.91694 to 0.91050, saving model to best.model\n",
      "0s - loss: 1.0030 - acc: 0.5169 - val_loss: 0.9105 - val_acc: 0.9130\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.91050 to 0.90379, saving model to best.model\n",
      "0s - loss: 1.0071 - acc: 0.5169 - val_loss: 0.9038 - val_acc: 0.9130\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.90379 to 0.89668, saving model to best.model\n",
      "0s - loss: 0.9478 - acc: 0.5730 - val_loss: 0.8967 - val_acc: 0.9130\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.89668 to 0.88912, saving model to best.model\n",
      "0s - loss: 0.9673 - acc: 0.5506 - val_loss: 0.8891 - val_acc: 0.9130\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.88912 to 0.88127, saving model to best.model\n",
      "0s - loss: 0.9283 - acc: 0.5393 - val_loss: 0.8813 - val_acc: 0.8696\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.88127 to 0.87270, saving model to best.model\n",
      "0s - loss: 0.9996 - acc: 0.4719 - val_loss: 0.8727 - val_acc: 0.8696\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.87270 to 0.86356, saving model to best.model\n",
      "0s - loss: 0.9500 - acc: 0.5843 - val_loss: 0.8636 - val_acc: 0.8696\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.86356 to 0.85425, saving model to best.model\n",
      "0s - loss: 0.9488 - acc: 0.5169 - val_loss: 0.8543 - val_acc: 0.8696\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.85425 to 0.84480, saving model to best.model\n",
      "0s - loss: 0.8667 - acc: 0.6292 - val_loss: 0.8448 - val_acc: 0.8696\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.84480 to 0.83491, saving model to best.model\n",
      "0s - loss: 0.9231 - acc: 0.6180 - val_loss: 0.8349 - val_acc: 0.8696\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.83491 to 0.82458, saving model to best.model\n",
      "0s - loss: 0.9469 - acc: 0.5281 - val_loss: 0.8246 - val_acc: 0.8696\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.82458 to 0.81386, saving model to best.model\n",
      "0s - loss: 0.8279 - acc: 0.6629 - val_loss: 0.8139 - val_acc: 0.8696\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.81386 to 0.80337, saving model to best.model\n",
      "0s - loss: 0.8977 - acc: 0.5955 - val_loss: 0.8034 - val_acc: 0.8696\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.80337 to 0.79327, saving model to best.model\n",
      "0s - loss: 0.8883 - acc: 0.6292 - val_loss: 0.7933 - val_acc: 0.8696\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.79327 to 0.78349, saving model to best.model\n",
      "0s - loss: 0.8873 - acc: 0.5618 - val_loss: 0.7835 - val_acc: 0.8696\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.78349 to 0.77381, saving model to best.model\n",
      "0s - loss: 0.8802 - acc: 0.5506 - val_loss: 0.7738 - val_acc: 0.8696\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.77381 to 0.76413, saving model to best.model\n",
      "0s - loss: 0.8534 - acc: 0.6180 - val_loss: 0.7641 - val_acc: 0.8261\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.76413 to 0.75466, saving model to best.model\n",
      "0s - loss: 0.9113 - acc: 0.5169 - val_loss: 0.7547 - val_acc: 0.8261\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.75466 to 0.74489, saving model to best.model\n",
      "0s - loss: 0.8088 - acc: 0.6067 - val_loss: 0.7449 - val_acc: 0.8696\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.74489 to 0.73527, saving model to best.model\n",
      "0s - loss: 0.8402 - acc: 0.6067 - val_loss: 0.7353 - val_acc: 0.8696\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.73527 to 0.72562, saving model to best.model\n",
      "0s - loss: 0.8438 - acc: 0.6629 - val_loss: 0.7256 - val_acc: 0.8696\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.72562 to 0.71586, saving model to best.model\n",
      "0s - loss: 0.8036 - acc: 0.6292 - val_loss: 0.7159 - val_acc: 0.8696\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.71586 to 0.70577, saving model to best.model\n",
      "0s - loss: 0.8655 - acc: 0.5955 - val_loss: 0.7058 - val_acc: 0.8696\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.70577 to 0.69547, saving model to best.model\n",
      "0s - loss: 0.8891 - acc: 0.5955 - val_loss: 0.6955 - val_acc: 0.8261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69547 to 0.68468, saving model to best.model\n",
      "0s - loss: 0.7374 - acc: 0.6854 - val_loss: 0.6847 - val_acc: 0.8696\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68468 to 0.67270, saving model to best.model\n",
      "0s - loss: 0.7221 - acc: 0.7191 - val_loss: 0.6727 - val_acc: 0.8696\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67270 to 0.66083, saving model to best.model\n",
      "0s - loss: 0.7136 - acc: 0.6854 - val_loss: 0.6608 - val_acc: 0.8696\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66083 to 0.64872, saving model to best.model\n",
      "0s - loss: 0.7259 - acc: 0.7303 - val_loss: 0.6487 - val_acc: 0.8696\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.64872 to 0.63681, saving model to best.model\n",
      "0s - loss: 0.7240 - acc: 0.7640 - val_loss: 0.6368 - val_acc: 0.8696\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.63681 to 0.62484, saving model to best.model\n",
      "0s - loss: 0.6710 - acc: 0.7528 - val_loss: 0.6248 - val_acc: 0.8696\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.62484 to 0.61323, saving model to best.model\n",
      "0s - loss: 0.6730 - acc: 0.7865 - val_loss: 0.6132 - val_acc: 0.8696\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.61323 to 0.60185, saving model to best.model\n",
      "0s - loss: 0.7040 - acc: 0.7416 - val_loss: 0.6018 - val_acc: 0.8696\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.60185 to 0.59062, saving model to best.model\n",
      "0s - loss: 0.7368 - acc: 0.6966 - val_loss: 0.5906 - val_acc: 0.8696\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.59062 to 0.57956, saving model to best.model\n",
      "0s - loss: 0.6777 - acc: 0.7528 - val_loss: 0.5796 - val_acc: 0.8696\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.57956 to 0.56868, saving model to best.model\n",
      "0s - loss: 0.6492 - acc: 0.7416 - val_loss: 0.5687 - val_acc: 0.8696\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.56868 to 0.55825, saving model to best.model\n",
      "0s - loss: 0.7183 - acc: 0.6742 - val_loss: 0.5582 - val_acc: 0.9130\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.55825 to 0.54813, saving model to best.model\n",
      "0s - loss: 0.6258 - acc: 0.7753 - val_loss: 0.5481 - val_acc: 0.9130\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.54813 to 0.53816, saving model to best.model\n",
      "0s - loss: 0.6290 - acc: 0.7528 - val_loss: 0.5382 - val_acc: 0.9130\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.53816 to 0.52838, saving model to best.model\n",
      "0s - loss: 0.6430 - acc: 0.7865 - val_loss: 0.5284 - val_acc: 0.9130\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.52838 to 0.51885, saving model to best.model\n",
      "0s - loss: 0.6468 - acc: 0.7191 - val_loss: 0.5189 - val_acc: 0.9130\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.51885 to 0.50949, saving model to best.model\n",
      "0s - loss: 0.6141 - acc: 0.7303 - val_loss: 0.5095 - val_acc: 0.9130\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.50949 to 0.50034, saving model to best.model\n",
      "0s - loss: 0.5627 - acc: 0.8202 - val_loss: 0.5003 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.50034 to 0.49130, saving model to best.model\n",
      "0s - loss: 0.5917 - acc: 0.7640 - val_loss: 0.4913 - val_acc: 0.8696\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.49130 to 0.48246, saving model to best.model\n",
      "0s - loss: 0.5842 - acc: 0.7753 - val_loss: 0.4825 - val_acc: 0.8696\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.48246 to 0.47384, saving model to best.model\n",
      "0s - loss: 0.5428 - acc: 0.7978 - val_loss: 0.4738 - val_acc: 0.8696\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.47384 to 0.46541, saving model to best.model\n",
      "0s - loss: 0.5680 - acc: 0.8090 - val_loss: 0.4654 - val_acc: 0.8696\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.46541 to 0.45738, saving model to best.model\n",
      "0s - loss: 0.5861 - acc: 0.7978 - val_loss: 0.4574 - val_acc: 0.8696\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.45738 to 0.44977, saving model to best.model\n",
      "0s - loss: 0.5494 - acc: 0.8315 - val_loss: 0.4498 - val_acc: 0.8696\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.44977 to 0.44218, saving model to best.model\n",
      "0s - loss: 0.5396 - acc: 0.8315 - val_loss: 0.4422 - val_acc: 0.8696\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.44218 to 0.43514, saving model to best.model\n",
      "0s - loss: 0.5260 - acc: 0.7640 - val_loss: 0.4351 - val_acc: 0.8696\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.43514 to 0.42823, saving model to best.model\n",
      "0s - loss: 0.4819 - acc: 0.8427 - val_loss: 0.4282 - val_acc: 0.8696\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.42823 to 0.42159, saving model to best.model\n",
      "0s - loss: 0.5142 - acc: 0.8315 - val_loss: 0.4216 - val_acc: 0.8696\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.42159 to 0.41465, saving model to best.model\n",
      "0s - loss: 0.5608 - acc: 0.7978 - val_loss: 0.4147 - val_acc: 0.8696\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.41465 to 0.40772, saving model to best.model\n",
      "0s - loss: 0.4749 - acc: 0.8427 - val_loss: 0.4077 - val_acc: 0.8696\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.40772 to 0.40081, saving model to best.model\n",
      "0s - loss: 0.4814 - acc: 0.8427 - val_loss: 0.4008 - val_acc: 0.8696\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.40081 to 0.39373, saving model to best.model\n",
      "0s - loss: 0.4193 - acc: 0.8427 - val_loss: 0.3937 - val_acc: 0.8696\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.39373 to 0.38679, saving model to best.model\n",
      "0s - loss: 0.5015 - acc: 0.7865 - val_loss: 0.3868 - val_acc: 0.8696\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.38679 to 0.38008, saving model to best.model\n",
      "0s - loss: 0.4711 - acc: 0.8652 - val_loss: 0.3801 - val_acc: 0.8696\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.38008 to 0.37373, saving model to best.model\n",
      "0s - loss: 0.4594 - acc: 0.8315 - val_loss: 0.3737 - val_acc: 0.8696\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.37373 to 0.36750, saving model to best.model\n",
      "0s - loss: 0.5200 - acc: 0.7978 - val_loss: 0.3675 - val_acc: 0.8696\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.36750 to 0.36177, saving model to best.model\n",
      "0s - loss: 0.4630 - acc: 0.8202 - val_loss: 0.3618 - val_acc: 0.8696\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.36177 to 0.35647, saving model to best.model\n",
      "0s - loss: 0.4611 - acc: 0.8090 - val_loss: 0.3565 - val_acc: 0.8696\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.35647 to 0.35132, saving model to best.model\n",
      "0s - loss: 0.3906 - acc: 0.8539 - val_loss: 0.3513 - val_acc: 0.8696\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.35132 to 0.34644, saving model to best.model\n",
      "0s - loss: 0.4279 - acc: 0.8427 - val_loss: 0.3464 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.34644 to 0.34127, saving model to best.model\n",
      "0s - loss: 0.3122 - acc: 0.9326 - val_loss: 0.3413 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.34127 to 0.33608, saving model to best.model\n",
      "0s - loss: 0.4155 - acc: 0.8539 - val_loss: 0.3361 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.33608 to 0.33098, saving model to best.model\n",
      "0s - loss: 0.3795 - acc: 0.8652 - val_loss: 0.3310 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.33098 to 0.32576, saving model to best.model\n",
      "0s - loss: 0.4249 - acc: 0.8427 - val_loss: 0.3258 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.32576 to 0.32091, saving model to best.model\n",
      "0s - loss: 0.4059 - acc: 0.8539 - val_loss: 0.3209 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.32091 to 0.31616, saving model to best.model\n",
      "0s - loss: 0.4219 - acc: 0.8539 - val_loss: 0.3162 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.31616 to 0.31100, saving model to best.model\n",
      "0s - loss: 0.4070 - acc: 0.8764 - val_loss: 0.3110 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.31100 to 0.30521, saving model to best.model\n",
      "0s - loss: 0.3951 - acc: 0.8764 - val_loss: 0.3052 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.30521 to 0.29958, saving model to best.model\n",
      "0s - loss: 0.3991 - acc: 0.8764 - val_loss: 0.2996 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.29958 to 0.29418, saving model to best.model\n",
      "0s - loss: 0.3572 - acc: 0.8427 - val_loss: 0.2942 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.29418 to 0.28949, saving model to best.model\n",
      "0s - loss: 0.3646 - acc: 0.8652 - val_loss: 0.2895 - val_acc: 0.8696\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.28949 to 0.28484, saving model to best.model\n",
      "0s - loss: 0.3730 - acc: 0.8652 - val_loss: 0.2848 - val_acc: 0.8696\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.28484 to 0.28044, saving model to best.model\n",
      "0s - loss: 0.3186 - acc: 0.8989 - val_loss: 0.2804 - val_acc: 0.8696\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.28044 to 0.27601, saving model to best.model\n",
      "0s - loss: 0.3488 - acc: 0.8315 - val_loss: 0.2760 - val_acc: 0.8696\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.27601 to 0.27159, saving model to best.model\n",
      "0s - loss: 0.3542 - acc: 0.8764 - val_loss: 0.2716 - val_acc: 0.8696\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.27159 to 0.26800, saving model to best.model\n",
      "0s - loss: 0.3149 - acc: 0.8876 - val_loss: 0.2680 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.26800 to 0.26467, saving model to best.model\n",
      "0s - loss: 0.3029 - acc: 0.8989 - val_loss: 0.2647 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.26467 to 0.26202, saving model to best.model\n",
      "0s - loss: 0.3399 - acc: 0.8876 - val_loss: 0.2620 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.26202 to 0.25969, saving model to best.model\n",
      "0s - loss: 0.2784 - acc: 0.9213 - val_loss: 0.2597 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.25969 to 0.25736, saving model to best.model\n",
      "0s - loss: 0.3471 - acc: 0.8427 - val_loss: 0.2574 - val_acc: 0.8696\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.25736 to 0.25486, saving model to best.model\n",
      "0s - loss: 0.2848 - acc: 0.9326 - val_loss: 0.2549 - val_acc: 0.8696\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.25486 to 0.25211, saving model to best.model\n",
      "0s - loss: 0.2922 - acc: 0.8876 - val_loss: 0.2521 - val_acc: 0.8696\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.25211 to 0.24984, saving model to best.model\n",
      "0s - loss: 0.2796 - acc: 0.9213 - val_loss: 0.2498 - val_acc: 0.8696\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.24984 to 0.24661, saving model to best.model\n",
      "0s - loss: 0.2474 - acc: 0.9326 - val_loss: 0.2466 - val_acc: 0.8696\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.24661 to 0.24280, saving model to best.model\n",
      "0s - loss: 0.2600 - acc: 0.9326 - val_loss: 0.2428 - val_acc: 0.8696\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.24280 to 0.23926, saving model to best.model\n",
      "0s - loss: 0.2278 - acc: 0.9551 - val_loss: 0.2393 - val_acc: 0.8696\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.23926 to 0.23538, saving model to best.model\n",
      "0s - loss: 0.2516 - acc: 0.8989 - val_loss: 0.2354 - val_acc: 0.8696\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.23538 to 0.23150, saving model to best.model\n",
      "0s - loss: 0.2331 - acc: 0.9326 - val_loss: 0.2315 - val_acc: 0.8696\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.23150 to 0.22770, saving model to best.model\n",
      "0s - loss: 0.2768 - acc: 0.9213 - val_loss: 0.2277 - val_acc: 0.8696\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.22770 to 0.22520, saving model to best.model\n",
      "0s - loss: 0.3030 - acc: 0.8989 - val_loss: 0.2252 - val_acc: 0.8696\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.22520 to 0.22306, saving model to best.model\n",
      "0s - loss: 0.2467 - acc: 0.9438 - val_loss: 0.2231 - val_acc: 0.8696\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.22306 to 0.22056, saving model to best.model\n",
      "0s - loss: 0.2549 - acc: 0.9326 - val_loss: 0.2206 - val_acc: 0.8696\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.22056 to 0.21776, saving model to best.model\n",
      "0s - loss: 0.2384 - acc: 0.9213 - val_loss: 0.2178 - val_acc: 0.8696\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.21776 to 0.21492, saving model to best.model\n",
      "0s - loss: 0.3219 - acc: 0.8764 - val_loss: 0.2149 - val_acc: 0.8696\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.21492 to 0.21229, saving model to best.model\n",
      "0s - loss: 0.1965 - acc: 0.9663 - val_loss: 0.2123 - val_acc: 0.8696\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.21229 to 0.21057, saving model to best.model\n",
      "0s - loss: 0.2727 - acc: 0.9213 - val_loss: 0.2106 - val_acc: 0.8696\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.21057 to 0.20861, saving model to best.model\n",
      "0s - loss: 0.2041 - acc: 0.9213 - val_loss: 0.2086 - val_acc: 0.8696\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.20861 to 0.20668, saving model to best.model\n",
      "0s - loss: 0.2497 - acc: 0.8876 - val_loss: 0.2067 - val_acc: 0.8696\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.20668 to 0.20395, saving model to best.model\n",
      "0s - loss: 0.1879 - acc: 0.9663 - val_loss: 0.2040 - val_acc: 0.8696\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.20395 to 0.20023, saving model to best.model\n",
      "0s - loss: 0.2643 - acc: 0.9101 - val_loss: 0.2002 - val_acc: 0.8696\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.20023 to 0.19709, saving model to best.model\n",
      "0s - loss: 0.2006 - acc: 0.9551 - val_loss: 0.1971 - val_acc: 0.8696\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.19709 to 0.19397, saving model to best.model\n",
      "0s - loss: 0.2317 - acc: 0.9213 - val_loss: 0.1940 - val_acc: 0.8696\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.19397 to 0.19195, saving model to best.model\n",
      "0s - loss: 0.2306 - acc: 0.8876 - val_loss: 0.1919 - val_acc: 0.8696\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.19195 to 0.19082, saving model to best.model\n",
      "0s - loss: 0.1856 - acc: 0.9326 - val_loss: 0.1908 - val_acc: 0.8696\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.19082 to 0.19008, saving model to best.model\n",
      "0s - loss: 0.1669 - acc: 0.9551 - val_loss: 0.1901 - val_acc: 0.8696\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.19008 to 0.18887, saving model to best.model\n",
      "0s - loss: 0.1951 - acc: 0.9213 - val_loss: 0.1889 - val_acc: 0.8696\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.18887 to 0.18826, saving model to best.model\n",
      "0s - loss: 0.1914 - acc: 0.9663 - val_loss: 0.1883 - val_acc: 0.8696\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.1562 - acc: 0.9438 - val_loss: 0.1884 - val_acc: 0.8696\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.18826 to 0.18749, saving model to best.model\n",
      "0s - loss: 0.2332 - acc: 0.9101 - val_loss: 0.1875 - val_acc: 0.8696\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.18749 to 0.18502, saving model to best.model\n",
      "0s - loss: 0.1855 - acc: 0.9326 - val_loss: 0.1850 - val_acc: 0.8696\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.18502 to 0.18157, saving model to best.model\n",
      "0s - loss: 0.1587 - acc: 0.9888 - val_loss: 0.1816 - val_acc: 0.9130\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.18157 to 0.17779, saving model to best.model\n",
      "0s - loss: 0.1893 - acc: 0.9101 - val_loss: 0.1778 - val_acc: 0.9130\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.17779 to 0.17397, saving model to best.model\n",
      "0s - loss: 0.1654 - acc: 0.9551 - val_loss: 0.1740 - val_acc: 0.9130\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.17397 to 0.17045, saving model to best.model\n",
      "0s - loss: 0.2152 - acc: 0.9326 - val_loss: 0.1705 - val_acc: 0.9130\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.17045 to 0.16747, saving model to best.model\n",
      "0s - loss: 0.1703 - acc: 0.9551 - val_loss: 0.1675 - val_acc: 0.9130\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.16747 to 0.16567, saving model to best.model\n",
      "0s - loss: 0.2187 - acc: 0.9438 - val_loss: 0.1657 - val_acc: 0.9130\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.16567 to 0.16520, saving model to best.model\n",
      "0s - loss: 0.1876 - acc: 0.9438 - val_loss: 0.1652 - val_acc: 0.9130\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.1882 - acc: 0.9213 - val_loss: 0.1653 - val_acc: 0.9130\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.1352 - acc: 0.9663 - val_loss: 0.1661 - val_acc: 0.9130\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.1590 - acc: 0.9551 - val_loss: 0.1684 - val_acc: 0.9130\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.2161 - acc: 0.9213 - val_loss: 0.1725 - val_acc: 0.9130\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.1103 - acc: 0.9888 - val_loss: 0.1764 - val_acc: 0.9130\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.1122 - acc: 0.9775 - val_loss: 0.1783 - val_acc: 0.9130\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.1371 - acc: 0.9888 - val_loss: 0.1788 - val_acc: 0.9130\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.0951 - acc: 1.0000 - val_loss: 0.1794 - val_acc: 0.9130\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.1327 - acc: 0.9551 - val_loss: 0.1798 - val_acc: 0.9130\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.1473 - acc: 0.9326 - val_loss: 0.1790 - val_acc: 0.9130\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.1065 - acc: 0.9663 - val_loss: 0.1774 - val_acc: 0.9130\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.1180 - acc: 0.9775 - val_loss: 0.1759 - val_acc: 0.9130\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.1436 - acc: 0.9663 - val_loss: 0.1728 - val_acc: 0.9130\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.0661 - acc: 1.0000 - val_loss: 0.1697 - val_acc: 0.9130\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.12059, saving model to best.model\n",
      "0s - loss: 1.4293 - acc: 0.3708 - val_loss: 1.1206 - val_acc: 0.3043\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.12059 to 1.08998, saving model to best.model\n",
      "0s - loss: 1.3217 - acc: 0.3371 - val_loss: 1.0900 - val_acc: 0.3043\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.08998 to 1.08538, saving model to best.model\n",
      "0s - loss: 1.2703 - acc: 0.3483 - val_loss: 1.0854 - val_acc: 0.3043\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.3226 - acc: 0.3034 - val_loss: 1.0973 - val_acc: 0.2609\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2946 - acc: 0.3146 - val_loss: 1.1186 - val_acc: 0.2609\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2771 - acc: 0.3258 - val_loss: 1.1390 - val_acc: 0.2609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.3133 - acc: 0.2584 - val_loss: 1.1594 - val_acc: 0.2609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2734 - acc: 0.3483 - val_loss: 1.1726 - val_acc: 0.2609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.4124 - acc: 0.2921 - val_loss: 1.1747 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1666 - acc: 0.4382 - val_loss: 1.1673 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2284 - acc: 0.3933 - val_loss: 1.1571 - val_acc: 0.2609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.3718 - acc: 0.3034 - val_loss: 1.1427 - val_acc: 0.2609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2314 - acc: 0.3034 - val_loss: 1.1274 - val_acc: 0.2609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.1372 - acc: 0.4270 - val_loss: 1.1121 - val_acc: 0.2609\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1668 - acc: 0.3933 - val_loss: 1.1015 - val_acc: 0.2609\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1345 - acc: 0.3483 - val_loss: 1.0940 - val_acc: 0.2609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1804 - acc: 0.3483 - val_loss: 1.0874 - val_acc: 0.2609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.08538 to 1.08174, saving model to best.model\n",
      "0s - loss: 1.1321 - acc: 0.3371 - val_loss: 1.0817 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.08174 to 1.07721, saving model to best.model\n",
      "0s - loss: 1.1508 - acc: 0.3258 - val_loss: 1.0772 - val_acc: 0.5217\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.07721 to 1.07292, saving model to best.model\n",
      "0s - loss: 1.2900 - acc: 0.3371 - val_loss: 1.0729 - val_acc: 0.3043\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.07292 to 1.06909, saving model to best.model\n",
      "0s - loss: 1.2368 - acc: 0.2921 - val_loss: 1.0691 - val_acc: 0.3043\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.06909 to 1.06703, saving model to best.model\n",
      "0s - loss: 1.2082 - acc: 0.3596 - val_loss: 1.0670 - val_acc: 0.3043\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.06703 to 1.06394, saving model to best.model\n",
      "0s - loss: 1.1712 - acc: 0.3708 - val_loss: 1.0639 - val_acc: 0.3043\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.06394 to 1.06069, saving model to best.model\n",
      "0s - loss: 1.2925 - acc: 0.2584 - val_loss: 1.0607 - val_acc: 0.3043\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.06069 to 1.05730, saving model to best.model\n",
      "0s - loss: 1.2460 - acc: 0.3034 - val_loss: 1.0573 - val_acc: 0.4348\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.05730 to 1.05342, saving model to best.model\n",
      "0s - loss: 1.0937 - acc: 0.3933 - val_loss: 1.0534 - val_acc: 0.7391\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.05342 to 1.05167, saving model to best.model\n",
      "0s - loss: 1.1463 - acc: 0.3933 - val_loss: 1.0517 - val_acc: 0.7391\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.05167 to 1.05066, saving model to best.model\n",
      "0s - loss: 1.2240 - acc: 0.3708 - val_loss: 1.0507 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.05066 to 1.04975, saving model to best.model\n",
      "0s - loss: 1.1529 - acc: 0.3820 - val_loss: 1.0497 - val_acc: 0.3043\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.04975 to 1.04898, saving model to best.model\n",
      "0s - loss: 1.1305 - acc: 0.3933 - val_loss: 1.0490 - val_acc: 0.3043\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.04898 to 1.04853, saving model to best.model\n",
      "0s - loss: 1.1261 - acc: 0.4831 - val_loss: 1.0485 - val_acc: 0.2609\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.04853 to 1.04815, saving model to best.model\n",
      "0s - loss: 1.1954 - acc: 0.3258 - val_loss: 1.0481 - val_acc: 0.2609\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.1698 - acc: 0.4157 - val_loss: 1.0482 - val_acc: 0.2609\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.04815 to 1.04653, saving model to best.model\n",
      "0s - loss: 1.1115 - acc: 0.4045 - val_loss: 1.0465 - val_acc: 0.2609\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.04653 to 1.04317, saving model to best.model\n",
      "0s - loss: 1.1223 - acc: 0.4157 - val_loss: 1.0432 - val_acc: 0.2609\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.04317 to 1.03954, saving model to best.model\n",
      "0s - loss: 1.1475 - acc: 0.3708 - val_loss: 1.0395 - val_acc: 0.2609\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.03954 to 1.03588, saving model to best.model\n",
      "0s - loss: 1.0959 - acc: 0.3596 - val_loss: 1.0359 - val_acc: 0.2609\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.03588 to 1.03233, saving model to best.model\n",
      "0s - loss: 1.1238 - acc: 0.3933 - val_loss: 1.0323 - val_acc: 0.3043\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.03233 to 1.02697, saving model to best.model\n",
      "0s - loss: 1.1535 - acc: 0.3596 - val_loss: 1.0270 - val_acc: 0.3043\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.02697 to 1.02106, saving model to best.model\n",
      "0s - loss: 1.0364 - acc: 0.4719 - val_loss: 1.0211 - val_acc: 0.3913\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.02106 to 1.01532, saving model to best.model\n",
      "0s - loss: 1.1201 - acc: 0.4270 - val_loss: 1.0153 - val_acc: 0.4783\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.01532 to 1.01067, saving model to best.model\n",
      "0s - loss: 1.0962 - acc: 0.4157 - val_loss: 1.0107 - val_acc: 0.5217\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.01067 to 1.00564, saving model to best.model\n",
      "0s - loss: 1.0609 - acc: 0.4944 - val_loss: 1.0056 - val_acc: 0.5652\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.00564 to 1.00073, saving model to best.model\n",
      "0s - loss: 1.0973 - acc: 0.4607 - val_loss: 1.0007 - val_acc: 0.5652\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.00073 to 0.99504, saving model to best.model\n",
      "0s - loss: 1.0646 - acc: 0.4719 - val_loss: 0.9950 - val_acc: 0.6087\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.99504 to 0.98845, saving model to best.model\n",
      "0s - loss: 1.0519 - acc: 0.4607 - val_loss: 0.9885 - val_acc: 0.6522\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.98845 to 0.98187, saving model to best.model\n",
      "0s - loss: 1.0576 - acc: 0.4157 - val_loss: 0.9819 - val_acc: 0.6957\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.98187 to 0.97532, saving model to best.model\n",
      "0s - loss: 1.0507 - acc: 0.4494 - val_loss: 0.9753 - val_acc: 0.7391\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.97532 to 0.96877, saving model to best.model\n",
      "0s - loss: 1.0719 - acc: 0.3371 - val_loss: 0.9688 - val_acc: 0.7391\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.96877 to 0.96163, saving model to best.model\n",
      "0s - loss: 0.9909 - acc: 0.5056 - val_loss: 0.9616 - val_acc: 0.7391\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.96163 to 0.95366, saving model to best.model\n",
      "0s - loss: 1.0144 - acc: 0.4719 - val_loss: 0.9537 - val_acc: 0.7391\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.95366 to 0.94645, saving model to best.model\n",
      "0s - loss: 1.0132 - acc: 0.5169 - val_loss: 0.9464 - val_acc: 0.7391\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.94645 to 0.93921, saving model to best.model\n",
      "0s - loss: 1.0990 - acc: 0.4494 - val_loss: 0.9392 - val_acc: 0.7391\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.93921 to 0.93247, saving model to best.model\n",
      "0s - loss: 1.0311 - acc: 0.4831 - val_loss: 0.9325 - val_acc: 0.7391\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.93247 to 0.92572, saving model to best.model\n",
      "0s - loss: 0.9988 - acc: 0.4831 - val_loss: 0.9257 - val_acc: 0.7391\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.92572 to 0.91812, saving model to best.model\n",
      "0s - loss: 1.0011 - acc: 0.5056 - val_loss: 0.9181 - val_acc: 0.7391\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.91812 to 0.91091, saving model to best.model\n",
      "0s - loss: 1.0343 - acc: 0.4382 - val_loss: 0.9109 - val_acc: 0.7391\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.91091 to 0.90314, saving model to best.model\n",
      "0s - loss: 1.0825 - acc: 0.4494 - val_loss: 0.9031 - val_acc: 0.7391\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.90314 to 0.89573, saving model to best.model\n",
      "0s - loss: 1.0165 - acc: 0.5056 - val_loss: 0.8957 - val_acc: 0.7391\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.89573 to 0.88785, saving model to best.model\n",
      "0s - loss: 0.9541 - acc: 0.5843 - val_loss: 0.8878 - val_acc: 0.7391\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.88785 to 0.87977, saving model to best.model\n",
      "0s - loss: 1.0397 - acc: 0.4494 - val_loss: 0.8798 - val_acc: 0.7391\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.87977 to 0.87158, saving model to best.model\n",
      "0s - loss: 0.9435 - acc: 0.4944 - val_loss: 0.8716 - val_acc: 0.7391\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.87158 to 0.86326, saving model to best.model\n",
      "0s - loss: 0.8964 - acc: 0.5618 - val_loss: 0.8633 - val_acc: 0.7826\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.86326 to 0.85399, saving model to best.model\n",
      "0s - loss: 1.0429 - acc: 0.4270 - val_loss: 0.8540 - val_acc: 0.7826\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.85399 to 0.84409, saving model to best.model\n",
      "0s - loss: 0.9485 - acc: 0.4944 - val_loss: 0.8441 - val_acc: 0.7826\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.84409 to 0.83342, saving model to best.model\n",
      "0s - loss: 0.9258 - acc: 0.5730 - val_loss: 0.8334 - val_acc: 0.8261\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.83342 to 0.82202, saving model to best.model\n",
      "0s - loss: 1.0149 - acc: 0.5056 - val_loss: 0.8220 - val_acc: 0.8261\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.82202 to 0.81055, saving model to best.model\n",
      "0s - loss: 0.9580 - acc: 0.5056 - val_loss: 0.8106 - val_acc: 0.9130\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.81055 to 0.79912, saving model to best.model\n",
      "0s - loss: 0.9479 - acc: 0.5506 - val_loss: 0.7991 - val_acc: 0.9130\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.79912 to 0.78740, saving model to best.model\n",
      "0s - loss: 0.9050 - acc: 0.5730 - val_loss: 0.7874 - val_acc: 0.9130\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.78740 to 0.77581, saving model to best.model\n",
      "0s - loss: 0.8635 - acc: 0.6404 - val_loss: 0.7758 - val_acc: 0.9565\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.77581 to 0.76404, saving model to best.model\n",
      "0s - loss: 0.8738 - acc: 0.5843 - val_loss: 0.7640 - val_acc: 0.9565\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.76404 to 0.75169, saving model to best.model\n",
      "0s - loss: 0.8803 - acc: 0.6067 - val_loss: 0.7517 - val_acc: 0.9565\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.75169 to 0.73973, saving model to best.model\n",
      "0s - loss: 0.9085 - acc: 0.5843 - val_loss: 0.7397 - val_acc: 0.9565\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.73973 to 0.72760, saving model to best.model\n",
      "0s - loss: 0.8678 - acc: 0.5955 - val_loss: 0.7276 - val_acc: 0.9565\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.72760 to 0.71564, saving model to best.model\n",
      "0s - loss: 0.8317 - acc: 0.6180 - val_loss: 0.7156 - val_acc: 0.9565\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.71564 to 0.70362, saving model to best.model\n",
      "0s - loss: 0.9013 - acc: 0.5281 - val_loss: 0.7036 - val_acc: 0.9565\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70362 to 0.69170, saving model to best.model\n",
      "0s - loss: 0.8964 - acc: 0.5506 - val_loss: 0.6917 - val_acc: 0.9565\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69170 to 0.67937, saving model to best.model\n",
      "0s - loss: 0.8159 - acc: 0.6067 - val_loss: 0.6794 - val_acc: 0.9565\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.67937 to 0.66713, saving model to best.model\n",
      "0s - loss: 0.8675 - acc: 0.5730 - val_loss: 0.6671 - val_acc: 0.9565\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.66713 to 0.65469, saving model to best.model\n",
      "0s - loss: 0.8470 - acc: 0.5843 - val_loss: 0.6547 - val_acc: 0.9565\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.65469 to 0.64292, saving model to best.model\n",
      "0s - loss: 0.8088 - acc: 0.5955 - val_loss: 0.6429 - val_acc: 0.9565\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.64292 to 0.63152, saving model to best.model\n",
      "0s - loss: 0.7750 - acc: 0.6629 - val_loss: 0.6315 - val_acc: 0.9565\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.63152 to 0.62013, saving model to best.model\n",
      "0s - loss: 0.7847 - acc: 0.6180 - val_loss: 0.6201 - val_acc: 0.9565\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.62013 to 0.60869, saving model to best.model\n",
      "0s - loss: 0.7576 - acc: 0.6966 - val_loss: 0.6087 - val_acc: 0.9565\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.60869 to 0.59709, saving model to best.model\n",
      "0s - loss: 0.7492 - acc: 0.7191 - val_loss: 0.5971 - val_acc: 0.9565\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.59709 to 0.58569, saving model to best.model\n",
      "0s - loss: 0.8077 - acc: 0.6292 - val_loss: 0.5857 - val_acc: 0.9565\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.58569 to 0.57447, saving model to best.model\n",
      "0s - loss: 0.7368 - acc: 0.6629 - val_loss: 0.5745 - val_acc: 0.9565\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.57447 to 0.56339, saving model to best.model\n",
      "0s - loss: 0.6494 - acc: 0.7079 - val_loss: 0.5634 - val_acc: 0.9565\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.56339 to 0.55219, saving model to best.model\n",
      "0s - loss: 0.6778 - acc: 0.7416 - val_loss: 0.5522 - val_acc: 0.9565\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.55219 to 0.54120, saving model to best.model\n",
      "0s - loss: 0.6437 - acc: 0.7640 - val_loss: 0.5412 - val_acc: 0.9565\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.54120 to 0.53057, saving model to best.model\n",
      "0s - loss: 0.6642 - acc: 0.7640 - val_loss: 0.5306 - val_acc: 0.9565\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.53057 to 0.51986, saving model to best.model\n",
      "0s - loss: 0.6480 - acc: 0.7079 - val_loss: 0.5199 - val_acc: 1.0000\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.51986 to 0.50924, saving model to best.model\n",
      "0s - loss: 0.7197 - acc: 0.6854 - val_loss: 0.5092 - val_acc: 1.0000\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.50924 to 0.49907, saving model to best.model\n",
      "0s - loss: 0.6193 - acc: 0.7528 - val_loss: 0.4991 - val_acc: 1.0000\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.49907 to 0.48930, saving model to best.model\n",
      "0s - loss: 0.6080 - acc: 0.8090 - val_loss: 0.4893 - val_acc: 1.0000\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.48930 to 0.47996, saving model to best.model\n",
      "0s - loss: 0.5712 - acc: 0.8090 - val_loss: 0.4800 - val_acc: 1.0000\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.47996 to 0.47086, saving model to best.model\n",
      "0s - loss: 0.6805 - acc: 0.6854 - val_loss: 0.4709 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.47086 to 0.46238, saving model to best.model\n",
      "0s - loss: 0.6115 - acc: 0.7079 - val_loss: 0.4624 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.46238 to 0.45426, saving model to best.model\n",
      "0s - loss: 0.5618 - acc: 0.7528 - val_loss: 0.4543 - val_acc: 1.0000\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.45426 to 0.44602, saving model to best.model\n",
      "0s - loss: 0.6569 - acc: 0.6629 - val_loss: 0.4460 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.44602 to 0.43793, saving model to best.model\n",
      "0s - loss: 0.6210 - acc: 0.6966 - val_loss: 0.4379 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.43793 to 0.42982, saving model to best.model\n",
      "0s - loss: 0.5512 - acc: 0.8202 - val_loss: 0.4298 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.42982 to 0.42189, saving model to best.model\n",
      "0s - loss: 0.5643 - acc: 0.7753 - val_loss: 0.4219 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.42189 to 0.41381, saving model to best.model\n",
      "0s - loss: 0.5157 - acc: 0.7416 - val_loss: 0.4138 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.41381 to 0.40577, saving model to best.model\n",
      "0s - loss: 0.5747 - acc: 0.7640 - val_loss: 0.4058 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.40577 to 0.39759, saving model to best.model\n",
      "0s - loss: 0.6405 - acc: 0.6966 - val_loss: 0.3976 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.39759 to 0.38959, saving model to best.model\n",
      "0s - loss: 0.5491 - acc: 0.7978 - val_loss: 0.3896 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.38959 to 0.38225, saving model to best.model\n",
      "0s - loss: 0.5114 - acc: 0.7865 - val_loss: 0.3822 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.38225 to 0.37492, saving model to best.model\n",
      "0s - loss: 0.5807 - acc: 0.7865 - val_loss: 0.3749 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.37492 to 0.36773, saving model to best.model\n",
      "0s - loss: 0.5080 - acc: 0.7978 - val_loss: 0.3677 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.36773 to 0.36106, saving model to best.model\n",
      "0s - loss: 0.5522 - acc: 0.8202 - val_loss: 0.3611 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.36106 to 0.35458, saving model to best.model\n",
      "0s - loss: 0.5303 - acc: 0.7416 - val_loss: 0.3546 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.35458 to 0.34815, saving model to best.model\n",
      "0s - loss: 0.5368 - acc: 0.7753 - val_loss: 0.3481 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.34815 to 0.34181, saving model to best.model\n",
      "0s - loss: 0.5223 - acc: 0.7865 - val_loss: 0.3418 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.34181 to 0.33549, saving model to best.model\n",
      "0s - loss: 0.5328 - acc: 0.7640 - val_loss: 0.3355 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.33549 to 0.32899, saving model to best.model\n",
      "0s - loss: 0.5234 - acc: 0.7528 - val_loss: 0.3290 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.32899 to 0.32262, saving model to best.model\n",
      "0s - loss: 0.5185 - acc: 0.7753 - val_loss: 0.3226 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.32262 to 0.31650, saving model to best.model\n",
      "0s - loss: 0.4784 - acc: 0.8090 - val_loss: 0.3165 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.31650 to 0.31067, saving model to best.model\n",
      "0s - loss: 0.5004 - acc: 0.7303 - val_loss: 0.3107 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.31067 to 0.30525, saving model to best.model\n",
      "0s - loss: 0.4402 - acc: 0.8315 - val_loss: 0.3053 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.30525 to 0.30006, saving model to best.model\n",
      "0s - loss: 0.4377 - acc: 0.8539 - val_loss: 0.3001 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.30006 to 0.29512, saving model to best.model\n",
      "0s - loss: 0.4892 - acc: 0.8090 - val_loss: 0.2951 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.29512 to 0.29041, saving model to best.model\n",
      "0s - loss: 0.5084 - acc: 0.7978 - val_loss: 0.2904 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.29041 to 0.28552, saving model to best.model\n",
      "0s - loss: 0.4355 - acc: 0.8090 - val_loss: 0.2855 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.28552 to 0.28089, saving model to best.model\n",
      "0s - loss: 0.4044 - acc: 0.8652 - val_loss: 0.2809 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.28089 to 0.27609, saving model to best.model\n",
      "0s - loss: 0.4376 - acc: 0.8539 - val_loss: 0.2761 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.27609 to 0.27152, saving model to best.model\n",
      "0s - loss: 0.3901 - acc: 0.8652 - val_loss: 0.2715 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.27152 to 0.26726, saving model to best.model\n",
      "0s - loss: 0.4009 - acc: 0.8427 - val_loss: 0.2673 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.26726 to 0.26283, saving model to best.model\n",
      "0s - loss: 0.3949 - acc: 0.8539 - val_loss: 0.2628 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.26283 to 0.25818, saving model to best.model\n",
      "0s - loss: 0.4369 - acc: 0.8202 - val_loss: 0.2582 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.25818 to 0.25292, saving model to best.model\n",
      "0s - loss: 0.3807 - acc: 0.8539 - val_loss: 0.2529 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.25292 to 0.24731, saving model to best.model\n",
      "0s - loss: 0.3538 - acc: 0.8652 - val_loss: 0.2473 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.24731 to 0.24160, saving model to best.model\n",
      "0s - loss: 0.3957 - acc: 0.8652 - val_loss: 0.2416 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.24160 to 0.23583, saving model to best.model\n",
      "0s - loss: 0.4150 - acc: 0.8202 - val_loss: 0.2358 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.23583 to 0.23025, saving model to best.model\n",
      "0s - loss: 0.4170 - acc: 0.8315 - val_loss: 0.2303 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.23025 to 0.22471, saving model to best.model\n",
      "0s - loss: 0.4288 - acc: 0.8202 - val_loss: 0.2247 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.22471 to 0.21949, saving model to best.model\n",
      "0s - loss: 0.3394 - acc: 0.9101 - val_loss: 0.2195 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.21949 to 0.21507, saving model to best.model\n",
      "0s - loss: 0.4081 - acc: 0.8539 - val_loss: 0.2151 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.21507 to 0.21088, saving model to best.model\n",
      "0s - loss: 0.4379 - acc: 0.8090 - val_loss: 0.2109 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.21088 to 0.20691, saving model to best.model\n",
      "0s - loss: 0.3132 - acc: 0.9326 - val_loss: 0.2069 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.20691 to 0.20300, saving model to best.model\n",
      "0s - loss: 0.3511 - acc: 0.8876 - val_loss: 0.2030 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.20300 to 0.19951, saving model to best.model\n",
      "0s - loss: 0.3652 - acc: 0.8427 - val_loss: 0.1995 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.19951 to 0.19609, saving model to best.model\n",
      "0s - loss: 0.3733 - acc: 0.8539 - val_loss: 0.1961 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19609 to 0.19309, saving model to best.model\n",
      "0s - loss: 0.4409 - acc: 0.8427 - val_loss: 0.1931 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.19309 to 0.19024, saving model to best.model\n",
      "0s - loss: 0.3194 - acc: 0.8764 - val_loss: 0.1902 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.19024 to 0.18758, saving model to best.model\n",
      "0s - loss: 0.3509 - acc: 0.8989 - val_loss: 0.1876 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.18758 to 0.18414, saving model to best.model\n",
      "0s - loss: 0.3324 - acc: 0.8764 - val_loss: 0.1841 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.18414 to 0.18046, saving model to best.model\n",
      "0s - loss: 0.3481 - acc: 0.8764 - val_loss: 0.1805 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.18046 to 0.17679, saving model to best.model\n",
      "0s - loss: 0.2730 - acc: 0.9101 - val_loss: 0.1768 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.17679 to 0.17286, saving model to best.model\n",
      "0s - loss: 0.3294 - acc: 0.8764 - val_loss: 0.1729 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.17286 to 0.16865, saving model to best.model\n",
      "0s - loss: 0.2577 - acc: 0.9438 - val_loss: 0.1687 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.16865 to 0.16446, saving model to best.model\n",
      "0s - loss: 0.3376 - acc: 0.8427 - val_loss: 0.1645 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.16446 to 0.16021, saving model to best.model\n",
      "0s - loss: 0.2894 - acc: 0.9101 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.16021 to 0.15600, saving model to best.model\n",
      "0s - loss: 0.3092 - acc: 0.9101 - val_loss: 0.1560 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.15600 to 0.15220, saving model to best.model\n",
      "0s - loss: 0.3422 - acc: 0.8427 - val_loss: 0.1522 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.15220 to 0.14818, saving model to best.model\n",
      "0s - loss: 0.2636 - acc: 0.9213 - val_loss: 0.1482 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.14818 to 0.14457, saving model to best.model\n",
      "0s - loss: 0.2539 - acc: 0.9438 - val_loss: 0.1446 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.14457 to 0.14092, saving model to best.model\n",
      "0s - loss: 0.2320 - acc: 0.9213 - val_loss: 0.1409 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.14092 to 0.13757, saving model to best.model\n",
      "0s - loss: 0.2810 - acc: 0.9326 - val_loss: 0.1376 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.13757 to 0.13449, saving model to best.model\n",
      "0s - loss: 0.3267 - acc: 0.8315 - val_loss: 0.1345 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.13449 to 0.13172, saving model to best.model\n",
      "0s - loss: 0.2426 - acc: 0.8989 - val_loss: 0.1317 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.13172 to 0.12906, saving model to best.model\n",
      "0s - loss: 0.2940 - acc: 0.8989 - val_loss: 0.1291 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.12906 to 0.12658, saving model to best.model\n",
      "0s - loss: 0.2168 - acc: 0.9551 - val_loss: 0.1266 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.12658 to 0.12421, saving model to best.model\n",
      "0s - loss: 0.2409 - acc: 0.9438 - val_loss: 0.1242 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.12421 to 0.12182, saving model to best.model\n",
      "0s - loss: 0.2946 - acc: 0.8764 - val_loss: 0.1218 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.12182 to 0.11955, saving model to best.model\n",
      "0s - loss: 0.2638 - acc: 0.8989 - val_loss: 0.1195 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.11955 to 0.11736, saving model to best.model\n",
      "0s - loss: 0.2054 - acc: 0.9663 - val_loss: 0.1174 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.11736 to 0.11548, saving model to best.model\n",
      "0s - loss: 0.2417 - acc: 0.9213 - val_loss: 0.1155 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.11548 to 0.11371, saving model to best.model\n",
      "0s - loss: 0.1922 - acc: 0.9551 - val_loss: 0.1137 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.11371 to 0.11198, saving model to best.model\n",
      "0s - loss: 0.2518 - acc: 0.9213 - val_loss: 0.1120 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.11198 to 0.11041, saving model to best.model\n",
      "0s - loss: 0.2172 - acc: 0.9101 - val_loss: 0.1104 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.11041 to 0.10888, saving model to best.model\n",
      "0s - loss: 0.2266 - acc: 0.9213 - val_loss: 0.1089 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.10888 to 0.10738, saving model to best.model\n",
      "0s - loss: 0.2388 - acc: 0.9101 - val_loss: 0.1074 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.10738 to 0.10553, saving model to best.model\n",
      "0s - loss: 0.2232 - acc: 0.9213 - val_loss: 0.1055 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.10553 to 0.10368, saving model to best.model\n",
      "0s - loss: 0.2354 - acc: 0.9213 - val_loss: 0.1037 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.10368 to 0.10173, saving model to best.model\n",
      "0s - loss: 0.1987 - acc: 0.9438 - val_loss: 0.1017 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.10173 to 0.09996, saving model to best.model\n",
      "0s - loss: 0.1650 - acc: 0.9663 - val_loss: 0.1000 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.09996 to 0.09817, saving model to best.model\n",
      "0s - loss: 0.1846 - acc: 0.9438 - val_loss: 0.0982 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.09817 to 0.09640, saving model to best.model\n",
      "0s - loss: 0.1794 - acc: 0.9438 - val_loss: 0.0964 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.09640 to 0.09469, saving model to best.model\n",
      "0s - loss: 0.1569 - acc: 0.9775 - val_loss: 0.0947 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.09469 to 0.09301, saving model to best.model\n",
      "0s - loss: 0.2118 - acc: 0.9326 - val_loss: 0.0930 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.09301 to 0.09135, saving model to best.model\n",
      "0s - loss: 0.2316 - acc: 0.9213 - val_loss: 0.0913 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.09135 to 0.08978, saving model to best.model\n",
      "0s - loss: 0.1564 - acc: 0.9551 - val_loss: 0.0898 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.08978 to 0.08821, saving model to best.model\n",
      "0s - loss: 0.1799 - acc: 0.9663 - val_loss: 0.0882 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.08821 to 0.08677, saving model to best.model\n",
      "0s - loss: 0.1939 - acc: 0.9326 - val_loss: 0.0868 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.08677 to 0.08545, saving model to best.model\n",
      "0s - loss: 0.1511 - acc: 0.9551 - val_loss: 0.0854 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.08545 to 0.08411, saving model to best.model\n",
      "0s - loss: 0.1584 - acc: 0.9775 - val_loss: 0.0841 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.08411 to 0.08278, saving model to best.model\n",
      "0s - loss: 0.1583 - acc: 0.9551 - val_loss: 0.0828 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.08278 to 0.08150, saving model to best.model\n",
      "0s - loss: 0.1589 - acc: 0.9438 - val_loss: 0.0815 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.08150 to 0.08040, saving model to best.model\n",
      "0s - loss: 0.1727 - acc: 0.9326 - val_loss: 0.0804 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.08040 to 0.07918, saving model to best.model\n",
      "0s - loss: 0.1625 - acc: 0.9663 - val_loss: 0.0792 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.07918 to 0.07793, saving model to best.model\n",
      "0s - loss: 0.2020 - acc: 0.9213 - val_loss: 0.0779 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.07793 to 0.07688, saving model to best.model\n",
      "0s - loss: 0.1573 - acc: 0.9775 - val_loss: 0.0769 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.07688 to 0.07587, saving model to best.model\n",
      "0s - loss: 0.1468 - acc: 0.9438 - val_loss: 0.0759 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.07587 to 0.07495, saving model to best.model\n",
      "0s - loss: 0.1398 - acc: 0.9663 - val_loss: 0.0750 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.07495 to 0.07408, saving model to best.model\n",
      "0s - loss: 0.1176 - acc: 0.9663 - val_loss: 0.0741 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.07408 to 0.07337, saving model to best.model\n",
      "0s - loss: 0.1911 - acc: 0.9326 - val_loss: 0.0734 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.07337 to 0.07275, saving model to best.model\n",
      "0s - loss: 0.1944 - acc: 0.8989 - val_loss: 0.0727 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.07275 to 0.07222, saving model to best.model\n",
      "0s - loss: 0.0949 - acc: 0.9888 - val_loss: 0.0722 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.85798, saving model to best.model\n",
      "0s - loss: 1.8623 - acc: 0.3596 - val_loss: 1.8580 - val_acc: 0.2174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.85798 to 1.60654, saving model to best.model\n",
      "0s - loss: 1.7479 - acc: 0.3596 - val_loss: 1.6065 - val_acc: 0.2174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.60654 to 1.40108, saving model to best.model\n",
      "0s - loss: 1.5738 - acc: 0.3596 - val_loss: 1.4011 - val_acc: 0.2174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.40108 to 1.24661, saving model to best.model\n",
      "0s - loss: 1.4474 - acc: 0.3034 - val_loss: 1.2466 - val_acc: 0.2174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.24661 to 1.14016, saving model to best.model\n",
      "0s - loss: 1.2859 - acc: 0.3596 - val_loss: 1.1402 - val_acc: 0.2174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.14016 to 1.07522, saving model to best.model\n",
      "0s - loss: 1.2799 - acc: 0.3483 - val_loss: 1.0752 - val_acc: 0.5217\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.07522 to 1.04005, saving model to best.model\n",
      "0s - loss: 1.2383 - acc: 0.3371 - val_loss: 1.0400 - val_acc: 0.5217\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.04005 to 1.02428, saving model to best.model\n",
      "0s - loss: 1.2630 - acc: 0.3034 - val_loss: 1.0243 - val_acc: 0.5217\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.02428 to 1.01943, saving model to best.model\n",
      "0s - loss: 1.3362 - acc: 0.3933 - val_loss: 1.0194 - val_acc: 0.5217\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2046 - acc: 0.3820 - val_loss: 1.0199 - val_acc: 0.5217\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2305 - acc: 0.4045 - val_loss: 1.0219 - val_acc: 0.5217\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2174 - acc: 0.4382 - val_loss: 1.0240 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.3626 - acc: 0.3820 - val_loss: 1.0251 - val_acc: 0.5217\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.3779 - acc: 0.2921 - val_loss: 1.0245 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2720 - acc: 0.3708 - val_loss: 1.0227 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2250 - acc: 0.4270 - val_loss: 1.0208 - val_acc: 0.5217\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1897 - acc: 0.3933 - val_loss: 1.0199 - val_acc: 0.5217\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.3160 - acc: 0.3258 - val_loss: 1.0206 - val_acc: 0.5217\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2557 - acc: 0.3596 - val_loss: 1.0248 - val_acc: 0.5217\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2370 - acc: 0.3371 - val_loss: 1.0318 - val_acc: 0.5217\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.2307 - acc: 0.3483 - val_loss: 1.0414 - val_acc: 0.5217\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1490 - acc: 0.3371 - val_loss: 1.0530 - val_acc: 0.6522\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2238 - acc: 0.3146 - val_loss: 1.0640 - val_acc: 0.6522\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1690 - acc: 0.3708 - val_loss: 1.0752 - val_acc: 0.2609\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.2704 - acc: 0.3034 - val_loss: 1.0847 - val_acc: 0.2174\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1880 - acc: 0.3820 - val_loss: 1.0920 - val_acc: 0.2174\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1535 - acc: 0.3146 - val_loss: 1.0956 - val_acc: 0.2174\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.2097 - acc: 0.3596 - val_loss: 1.0970 - val_acc: 0.2174\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1984 - acc: 0.3933 - val_loss: 1.0942 - val_acc: 0.2174\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.2076 - acc: 0.3933 - val_loss: 1.0885 - val_acc: 0.2174\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.2333 - acc: 0.2921 - val_loss: 1.0815 - val_acc: 0.2174\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1769 - acc: 0.4157 - val_loss: 1.0721 - val_acc: 0.2174\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.2588 - acc: 0.3371 - val_loss: 1.0623 - val_acc: 0.2609\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1802 - acc: 0.3371 - val_loss: 1.0511 - val_acc: 0.5652\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1036 - acc: 0.4831 - val_loss: 1.0406 - val_acc: 0.6957\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.13132, saving model to best.model\n",
      "0s - loss: 1.2103 - acc: 0.3483 - val_loss: 1.1313 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.2876 - acc: 0.2697 - val_loss: 1.1330 - val_acc: 0.3478\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.2457 - acc: 0.2921 - val_loss: 1.1451 - val_acc: 0.3478\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1309 - acc: 0.4045 - val_loss: 1.1602 - val_acc: 0.3043\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2894 - acc: 0.3483 - val_loss: 1.1754 - val_acc: 0.3043\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2205 - acc: 0.3371 - val_loss: 1.1875 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2940 - acc: 0.3034 - val_loss: 1.1905 - val_acc: 0.3043\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2160 - acc: 0.4157 - val_loss: 1.1872 - val_acc: 0.3043\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1950 - acc: 0.3820 - val_loss: 1.1822 - val_acc: 0.3043\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1413 - acc: 0.3933 - val_loss: 1.1733 - val_acc: 0.3043\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1779 - acc: 0.3708 - val_loss: 1.1641 - val_acc: 0.3043\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.3262 - acc: 0.3034 - val_loss: 1.1536 - val_acc: 0.3043\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1974 - acc: 0.4157 - val_loss: 1.1429 - val_acc: 0.5217\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.1495 - acc: 0.4270 - val_loss: 1.1329 - val_acc: 0.4348\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.13132 to 1.12425, saving model to best.model\n",
      "0s - loss: 1.1337 - acc: 0.3933 - val_loss: 1.1242 - val_acc: 0.3478\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.12425 to 1.11590, saving model to best.model\n",
      "0s - loss: 1.1324 - acc: 0.4270 - val_loss: 1.1159 - val_acc: 0.3478\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.11590 to 1.11022, saving model to best.model\n",
      "0s - loss: 1.1788 - acc: 0.3708 - val_loss: 1.1102 - val_acc: 0.3478\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.11022 to 1.10473, saving model to best.model\n",
      "0s - loss: 1.1940 - acc: 0.3483 - val_loss: 1.1047 - val_acc: 0.3478\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.10473 to 1.10053, saving model to best.model\n",
      "0s - loss: 1.1042 - acc: 0.4045 - val_loss: 1.1005 - val_acc: 0.3478\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.10053 to 1.09746, saving model to best.model\n",
      "0s - loss: 1.1952 - acc: 0.3483 - val_loss: 1.0975 - val_acc: 0.3478\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.09746 to 1.09594, saving model to best.model\n",
      "0s - loss: 1.0725 - acc: 0.4270 - val_loss: 1.0959 - val_acc: 0.3478\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.09594 to 1.09406, saving model to best.model\n",
      "0s - loss: 1.1608 - acc: 0.3708 - val_loss: 1.0941 - val_acc: 0.3478\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.09406 to 1.09220, saving model to best.model\n",
      "0s - loss: 1.2189 - acc: 0.3708 - val_loss: 1.0922 - val_acc: 0.4348\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.09220 to 1.09199, saving model to best.model\n",
      "0s - loss: 1.0800 - acc: 0.4607 - val_loss: 1.0920 - val_acc: 0.6087\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.0712 - acc: 0.4157 - val_loss: 1.0924 - val_acc: 0.6522\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1425 - acc: 0.4494 - val_loss: 1.0929 - val_acc: 0.6087\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1320 - acc: 0.3371 - val_loss: 1.0936 - val_acc: 0.6087\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1305 - acc: 0.4045 - val_loss: 1.0940 - val_acc: 0.5652\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.0702 - acc: 0.4494 - val_loss: 1.0942 - val_acc: 0.5652\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1654 - acc: 0.3146 - val_loss: 1.0933 - val_acc: 0.5652\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.09199 to 1.09197, saving model to best.model\n",
      "0s - loss: 1.1744 - acc: 0.3371 - val_loss: 1.0920 - val_acc: 0.5652\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.09197 to 1.08947, saving model to best.model\n",
      "0s - loss: 1.0668 - acc: 0.4607 - val_loss: 1.0895 - val_acc: 0.5652\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.08947 to 1.08673, saving model to best.model\n",
      "0s - loss: 1.1623 - acc: 0.3820 - val_loss: 1.0867 - val_acc: 0.5652\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.08673 to 1.08403, saving model to best.model\n",
      "0s - loss: 1.0492 - acc: 0.3933 - val_loss: 1.0840 - val_acc: 0.5652\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.08403 to 1.08171, saving model to best.model\n",
      "0s - loss: 1.0880 - acc: 0.4045 - val_loss: 1.0817 - val_acc: 0.5652\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.08171 to 1.07967, saving model to best.model\n",
      "0s - loss: 1.0650 - acc: 0.4382 - val_loss: 1.0797 - val_acc: 0.5652\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.07967 to 1.07772, saving model to best.model\n",
      "0s - loss: 1.0726 - acc: 0.4270 - val_loss: 1.0777 - val_acc: 0.5652\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.07772 to 1.07500, saving model to best.model\n",
      "0s - loss: 1.0516 - acc: 0.4607 - val_loss: 1.0750 - val_acc: 0.5652\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.07500 to 1.07284, saving model to best.model\n",
      "0s - loss: 1.0423 - acc: 0.4719 - val_loss: 1.0728 - val_acc: 0.6087\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.07284 to 1.06937, saving model to best.model\n",
      "0s - loss: 1.0486 - acc: 0.4944 - val_loss: 1.0694 - val_acc: 0.6087\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.06937 to 1.06621, saving model to best.model\n",
      "0s - loss: 1.1365 - acc: 0.4045 - val_loss: 1.0662 - val_acc: 0.6087\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.06621 to 1.06259, saving model to best.model\n",
      "0s - loss: 1.1065 - acc: 0.3258 - val_loss: 1.0626 - val_acc: 0.6087\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.06259 to 1.05937, saving model to best.model\n",
      "0s - loss: 1.0814 - acc: 0.4157 - val_loss: 1.0594 - val_acc: 0.6522\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.05937 to 1.05578, saving model to best.model\n",
      "0s - loss: 1.0977 - acc: 0.4719 - val_loss: 1.0558 - val_acc: 0.6522\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.05578 to 1.05289, saving model to best.model\n",
      "0s - loss: 1.0840 - acc: 0.4045 - val_loss: 1.0529 - val_acc: 0.6522\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.05289 to 1.04985, saving model to best.model\n",
      "0s - loss: 1.0836 - acc: 0.3933 - val_loss: 1.0498 - val_acc: 0.6522\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.04985 to 1.04712, saving model to best.model\n",
      "0s - loss: 1.0163 - acc: 0.4494 - val_loss: 1.0471 - val_acc: 0.6522\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.04712 to 1.04350, saving model to best.model\n",
      "0s - loss: 1.0906 - acc: 0.3708 - val_loss: 1.0435 - val_acc: 0.6522\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.04350 to 1.03936, saving model to best.model\n",
      "0s - loss: 1.0344 - acc: 0.4607 - val_loss: 1.0394 - val_acc: 0.6522\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.03936 to 1.03493, saving model to best.model\n",
      "0s - loss: 1.0435 - acc: 0.4494 - val_loss: 1.0349 - val_acc: 0.6522\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.03493 to 1.02963, saving model to best.model\n",
      "0s - loss: 1.0745 - acc: 0.3933 - val_loss: 1.0296 - val_acc: 0.6522\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.02963 to 1.02363, saving model to best.model\n",
      "0s - loss: 0.9539 - acc: 0.5169 - val_loss: 1.0236 - val_acc: 0.6522\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.02363 to 1.01815, saving model to best.model\n",
      "0s - loss: 0.9946 - acc: 0.5056 - val_loss: 1.0181 - val_acc: 0.6522\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.01815 to 1.01201, saving model to best.model\n",
      "0s - loss: 1.0481 - acc: 0.3933 - val_loss: 1.0120 - val_acc: 0.6522\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 1.01201 to 1.00597, saving model to best.model\n",
      "0s - loss: 0.9900 - acc: 0.4494 - val_loss: 1.0060 - val_acc: 0.6522\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 1.00597 to 0.99974, saving model to best.model\n",
      "0s - loss: 0.9346 - acc: 0.5281 - val_loss: 0.9997 - val_acc: 0.6522\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.99974 to 0.99337, saving model to best.model\n",
      "0s - loss: 0.9892 - acc: 0.4944 - val_loss: 0.9934 - val_acc: 0.6522\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.99337 to 0.98622, saving model to best.model\n",
      "0s - loss: 0.9913 - acc: 0.4382 - val_loss: 0.9862 - val_acc: 0.6522\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.98622 to 0.97813, saving model to best.model\n",
      "0s - loss: 1.0024 - acc: 0.4607 - val_loss: 0.9781 - val_acc: 0.6087\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.97813 to 0.97000, saving model to best.model\n",
      "0s - loss: 1.0438 - acc: 0.4494 - val_loss: 0.9700 - val_acc: 0.6087\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.97000 to 0.96157, saving model to best.model\n",
      "0s - loss: 1.0087 - acc: 0.4607 - val_loss: 0.9616 - val_acc: 0.6087\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.96157 to 0.95280, saving model to best.model\n",
      "0s - loss: 0.9722 - acc: 0.5730 - val_loss: 0.9528 - val_acc: 0.6087\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.95280 to 0.94497, saving model to best.model\n",
      "0s - loss: 1.0029 - acc: 0.4944 - val_loss: 0.9450 - val_acc: 0.6087\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.94497 to 0.93775, saving model to best.model\n",
      "0s - loss: 0.9892 - acc: 0.5169 - val_loss: 0.9378 - val_acc: 0.6087\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.93775 to 0.93086, saving model to best.model\n",
      "0s - loss: 0.9578 - acc: 0.4719 - val_loss: 0.9309 - val_acc: 0.6087\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.93086 to 0.92406, saving model to best.model\n",
      "0s - loss: 0.9869 - acc: 0.5506 - val_loss: 0.9241 - val_acc: 0.6087\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.92406 to 0.91731, saving model to best.model\n",
      "0s - loss: 0.9554 - acc: 0.5393 - val_loss: 0.9173 - val_acc: 0.6087\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.91731 to 0.91009, saving model to best.model\n",
      "0s - loss: 0.9686 - acc: 0.4494 - val_loss: 0.9101 - val_acc: 0.6087\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.91009 to 0.90264, saving model to best.model\n",
      "0s - loss: 0.9540 - acc: 0.5281 - val_loss: 0.9026 - val_acc: 0.6087\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.90264 to 0.89420, saving model to best.model\n",
      "0s - loss: 0.9369 - acc: 0.5169 - val_loss: 0.8942 - val_acc: 0.6087\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.89420 to 0.88487, saving model to best.model\n",
      "0s - loss: 0.9552 - acc: 0.5506 - val_loss: 0.8849 - val_acc: 0.6087\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.88487 to 0.87497, saving model to best.model\n",
      "0s - loss: 0.9162 - acc: 0.4831 - val_loss: 0.8750 - val_acc: 0.6087\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.87497 to 0.86469, saving model to best.model\n",
      "0s - loss: 0.8795 - acc: 0.5843 - val_loss: 0.8647 - val_acc: 0.6087\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.86469 to 0.85381, saving model to best.model\n",
      "0s - loss: 0.9842 - acc: 0.5506 - val_loss: 0.8538 - val_acc: 0.6087\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.85381 to 0.84189, saving model to best.model\n",
      "0s - loss: 0.8500 - acc: 0.6067 - val_loss: 0.8419 - val_acc: 0.6087\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.84189 to 0.82931, saving model to best.model\n",
      "0s - loss: 0.9124 - acc: 0.6067 - val_loss: 0.8293 - val_acc: 0.6087\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.82931 to 0.81696, saving model to best.model\n",
      "0s - loss: 0.8920 - acc: 0.5843 - val_loss: 0.8170 - val_acc: 0.6087\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.81696 to 0.80482, saving model to best.model\n",
      "0s - loss: 0.8329 - acc: 0.6742 - val_loss: 0.8048 - val_acc: 0.6087\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.80482 to 0.79235, saving model to best.model\n",
      "0s - loss: 0.8115 - acc: 0.6404 - val_loss: 0.7924 - val_acc: 0.6087\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.79235 to 0.78018, saving model to best.model\n",
      "0s - loss: 0.8482 - acc: 0.5843 - val_loss: 0.7802 - val_acc: 0.6087\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.78018 to 0.76759, saving model to best.model\n",
      "0s - loss: 0.8632 - acc: 0.5506 - val_loss: 0.7676 - val_acc: 0.6522\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.76759 to 0.75598, saving model to best.model\n",
      "0s - loss: 0.7981 - acc: 0.6742 - val_loss: 0.7560 - val_acc: 0.6522\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.75598 to 0.74360, saving model to best.model\n",
      "0s - loss: 0.7846 - acc: 0.6966 - val_loss: 0.7436 - val_acc: 0.6522\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.74360 to 0.73203, saving model to best.model\n",
      "0s - loss: 0.8152 - acc: 0.6404 - val_loss: 0.7320 - val_acc: 0.6957\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.73203 to 0.72038, saving model to best.model\n",
      "0s - loss: 0.7998 - acc: 0.6180 - val_loss: 0.7204 - val_acc: 0.6957\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.72038 to 0.70912, saving model to best.model\n",
      "0s - loss: 0.8585 - acc: 0.5506 - val_loss: 0.7091 - val_acc: 0.6957\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.70912 to 0.69753, saving model to best.model\n",
      "0s - loss: 0.8196 - acc: 0.6180 - val_loss: 0.6975 - val_acc: 0.7391\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.69753 to 0.68577, saving model to best.model\n",
      "0s - loss: 0.7577 - acc: 0.7191 - val_loss: 0.6858 - val_acc: 0.7391\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68577 to 0.67290, saving model to best.model\n",
      "0s - loss: 0.7425 - acc: 0.7191 - val_loss: 0.6729 - val_acc: 0.7826\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67290 to 0.66029, saving model to best.model\n",
      "0s - loss: 0.7557 - acc: 0.6517 - val_loss: 0.6603 - val_acc: 0.7826\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.66029 to 0.64801, saving model to best.model\n",
      "0s - loss: 0.7469 - acc: 0.6404 - val_loss: 0.6480 - val_acc: 0.7826\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.64801 to 0.63546, saving model to best.model\n",
      "0s - loss: 0.7017 - acc: 0.7079 - val_loss: 0.6355 - val_acc: 0.7826\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.63546 to 0.62331, saving model to best.model\n",
      "0s - loss: 0.7018 - acc: 0.7191 - val_loss: 0.6233 - val_acc: 0.8261\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.62331 to 0.61174, saving model to best.model\n",
      "0s - loss: 0.7047 - acc: 0.7079 - val_loss: 0.6117 - val_acc: 0.8261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.61174 to 0.60036, saving model to best.model\n",
      "0s - loss: 0.6832 - acc: 0.7191 - val_loss: 0.6004 - val_acc: 0.8261\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.60036 to 0.58887, saving model to best.model\n",
      "0s - loss: 0.7070 - acc: 0.6854 - val_loss: 0.5889 - val_acc: 0.8261\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.58887 to 0.57714, saving model to best.model\n",
      "0s - loss: 0.7069 - acc: 0.6966 - val_loss: 0.5771 - val_acc: 0.8261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.57714 to 0.56575, saving model to best.model\n",
      "0s - loss: 0.6317 - acc: 0.7865 - val_loss: 0.5658 - val_acc: 0.8261\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.56575 to 0.55429, saving model to best.model\n",
      "0s - loss: 0.6612 - acc: 0.7528 - val_loss: 0.5543 - val_acc: 0.8261\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.55429 to 0.54348, saving model to best.model\n",
      "0s - loss: 0.6810 - acc: 0.7079 - val_loss: 0.5435 - val_acc: 0.8261\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.54348 to 0.53399, saving model to best.model\n",
      "0s - loss: 0.6473 - acc: 0.7416 - val_loss: 0.5340 - val_acc: 0.8261\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.53399 to 0.52519, saving model to best.model\n",
      "0s - loss: 0.5390 - acc: 0.8090 - val_loss: 0.5252 - val_acc: 0.8261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.52519 to 0.51600, saving model to best.model\n",
      "0s - loss: 0.5971 - acc: 0.7865 - val_loss: 0.5160 - val_acc: 0.8261\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.51600 to 0.50722, saving model to best.model\n",
      "0s - loss: 0.6543 - acc: 0.7079 - val_loss: 0.5072 - val_acc: 0.8261\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.50722 to 0.49847, saving model to best.model\n",
      "0s - loss: 0.5882 - acc: 0.7528 - val_loss: 0.4985 - val_acc: 0.8261\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.49847 to 0.48941, saving model to best.model\n",
      "0s - loss: 0.5806 - acc: 0.7416 - val_loss: 0.4894 - val_acc: 0.8696\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.48941 to 0.48025, saving model to best.model\n",
      "0s - loss: 0.6508 - acc: 0.7640 - val_loss: 0.4802 - val_acc: 0.8696\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.48025 to 0.47057, saving model to best.model\n",
      "0s - loss: 0.5514 - acc: 0.7640 - val_loss: 0.4706 - val_acc: 0.8261\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.47057 to 0.46074, saving model to best.model\n",
      "0s - loss: 0.5503 - acc: 0.8202 - val_loss: 0.4607 - val_acc: 0.8696\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.46074 to 0.45077, saving model to best.model\n",
      "0s - loss: 0.5286 - acc: 0.7865 - val_loss: 0.4508 - val_acc: 0.8696\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.45077 to 0.44093, saving model to best.model\n",
      "0s - loss: 0.5567 - acc: 0.7753 - val_loss: 0.4409 - val_acc: 0.8696\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.44093 to 0.43157, saving model to best.model\n",
      "0s - loss: 0.5717 - acc: 0.7978 - val_loss: 0.4316 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.43157 to 0.42293, saving model to best.model\n",
      "0s - loss: 0.5218 - acc: 0.8090 - val_loss: 0.4229 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.42293 to 0.41484, saving model to best.model\n",
      "0s - loss: 0.4659 - acc: 0.8539 - val_loss: 0.4148 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.41484 to 0.40783, saving model to best.model\n",
      "0s - loss: 0.4986 - acc: 0.7753 - val_loss: 0.4078 - val_acc: 0.8696\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.40783 to 0.40104, saving model to best.model\n",
      "0s - loss: 0.4685 - acc: 0.7978 - val_loss: 0.4010 - val_acc: 0.8696\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.40104 to 0.39478, saving model to best.model\n",
      "0s - loss: 0.4400 - acc: 0.8989 - val_loss: 0.3948 - val_acc: 0.8696\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.39478 to 0.38872, saving model to best.model\n",
      "0s - loss: 0.5066 - acc: 0.8090 - val_loss: 0.3887 - val_acc: 0.8696\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.38872 to 0.38301, saving model to best.model\n",
      "0s - loss: 0.4915 - acc: 0.7640 - val_loss: 0.3830 - val_acc: 0.8696\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.38301 to 0.37801, saving model to best.model\n",
      "0s - loss: 0.4805 - acc: 0.8315 - val_loss: 0.3780 - val_acc: 0.8696\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.37801 to 0.37256, saving model to best.model\n",
      "0s - loss: 0.4956 - acc: 0.8315 - val_loss: 0.3726 - val_acc: 0.8696\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.37256 to 0.36695, saving model to best.model\n",
      "0s - loss: 0.3923 - acc: 0.8764 - val_loss: 0.3669 - val_acc: 0.8696\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.36695 to 0.36059, saving model to best.model\n",
      "0s - loss: 0.5351 - acc: 0.7753 - val_loss: 0.3606 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.36059 to 0.35460, saving model to best.model\n",
      "0s - loss: 0.4130 - acc: 0.8315 - val_loss: 0.3546 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.35460 to 0.34910, saving model to best.model\n",
      "0s - loss: 0.4082 - acc: 0.8764 - val_loss: 0.3491 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.34910 to 0.34415, saving model to best.model\n",
      "0s - loss: 0.4636 - acc: 0.8315 - val_loss: 0.3442 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.34415 to 0.33929, saving model to best.model\n",
      "0s - loss: 0.4239 - acc: 0.8539 - val_loss: 0.3393 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.33929 to 0.33357, saving model to best.model\n",
      "0s - loss: 0.4153 - acc: 0.8989 - val_loss: 0.3336 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.33357 to 0.32671, saving model to best.model\n",
      "0s - loss: 0.4664 - acc: 0.8090 - val_loss: 0.3267 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.32671 to 0.31988, saving model to best.model\n",
      "0s - loss: 0.4234 - acc: 0.8764 - val_loss: 0.3199 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.31988 to 0.31280, saving model to best.model\n",
      "0s - loss: 0.4206 - acc: 0.8427 - val_loss: 0.3128 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.31280 to 0.30672, saving model to best.model\n",
      "0s - loss: 0.4040 - acc: 0.8315 - val_loss: 0.3067 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.30672 to 0.30169, saving model to best.model\n",
      "0s - loss: 0.4603 - acc: 0.8539 - val_loss: 0.3017 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.30169 to 0.29695, saving model to best.model\n",
      "0s - loss: 0.4640 - acc: 0.8090 - val_loss: 0.2970 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.29695 to 0.29323, saving model to best.model\n",
      "0s - loss: 0.3254 - acc: 0.8989 - val_loss: 0.2932 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.29323 to 0.28958, saving model to best.model\n",
      "0s - loss: 0.4387 - acc: 0.8202 - val_loss: 0.2896 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.28958 to 0.28583, saving model to best.model\n",
      "0s - loss: 0.3830 - acc: 0.8652 - val_loss: 0.2858 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.28583 to 0.28274, saving model to best.model\n",
      "0s - loss: 0.4378 - acc: 0.8539 - val_loss: 0.2827 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.28274 to 0.28006, saving model to best.model\n",
      "0s - loss: 0.3687 - acc: 0.8427 - val_loss: 0.2801 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.28006 to 0.27789, saving model to best.model\n",
      "0s - loss: 0.3020 - acc: 0.9101 - val_loss: 0.2779 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.27789 to 0.27560, saving model to best.model\n",
      "0s - loss: 0.3388 - acc: 0.8764 - val_loss: 0.2756 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.27560 to 0.27295, saving model to best.model\n",
      "0s - loss: 0.3790 - acc: 0.8764 - val_loss: 0.2730 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.27295 to 0.27012, saving model to best.model\n",
      "0s - loss: 0.2702 - acc: 0.9101 - val_loss: 0.2701 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.27012 to 0.26649, saving model to best.model\n",
      "0s - loss: 0.3652 - acc: 0.8427 - val_loss: 0.2665 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.26649 to 0.26314, saving model to best.model\n",
      "0s - loss: 0.3474 - acc: 0.8539 - val_loss: 0.2631 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.26314 to 0.25928, saving model to best.model\n",
      "0s - loss: 0.3335 - acc: 0.9101 - val_loss: 0.2593 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.25928 to 0.25450, saving model to best.model\n",
      "0s - loss: 0.2736 - acc: 0.9213 - val_loss: 0.2545 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.25450 to 0.25000, saving model to best.model\n",
      "0s - loss: 0.3208 - acc: 0.8764 - val_loss: 0.2500 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.25000 to 0.24529, saving model to best.model\n",
      "0s - loss: 0.2893 - acc: 0.9213 - val_loss: 0.2453 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.24529 to 0.24130, saving model to best.model\n",
      "0s - loss: 0.2886 - acc: 0.8989 - val_loss: 0.2413 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.24130 to 0.23643, saving model to best.model\n",
      "0s - loss: 0.2841 - acc: 0.8652 - val_loss: 0.2364 - val_acc: 0.9130\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.23643 to 0.23138, saving model to best.model\n",
      "0s - loss: 0.3433 - acc: 0.8876 - val_loss: 0.2314 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.23138 to 0.22668, saving model to best.model\n",
      "0s - loss: 0.3737 - acc: 0.8652 - val_loss: 0.2267 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.22668 to 0.22231, saving model to best.model\n",
      "0s - loss: 0.2709 - acc: 0.8876 - val_loss: 0.2223 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.22231 to 0.21868, saving model to best.model\n",
      "0s - loss: 0.2883 - acc: 0.9101 - val_loss: 0.2187 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.21868 to 0.21548, saving model to best.model\n",
      "0s - loss: 0.3452 - acc: 0.8764 - val_loss: 0.2155 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.21548 to 0.21291, saving model to best.model\n",
      "0s - loss: 0.2831 - acc: 0.9101 - val_loss: 0.2129 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.21291 to 0.21053, saving model to best.model\n",
      "0s - loss: 0.2953 - acc: 0.9213 - val_loss: 0.2105 - val_acc: 0.9130\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.21053 to 0.20786, saving model to best.model\n",
      "0s - loss: 0.2969 - acc: 0.8989 - val_loss: 0.2079 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.20786 to 0.20580, saving model to best.model\n",
      "0s - loss: 0.2795 - acc: 0.8989 - val_loss: 0.2058 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.20580 to 0.20352, saving model to best.model\n",
      "0s - loss: 0.2483 - acc: 0.9213 - val_loss: 0.2035 - val_acc: 0.9130\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.20352 to 0.20106, saving model to best.model\n",
      "0s - loss: 0.3199 - acc: 0.8539 - val_loss: 0.2011 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.20106 to 0.19897, saving model to best.model\n",
      "0s - loss: 0.2350 - acc: 0.9101 - val_loss: 0.1990 - val_acc: 0.9130\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.19897 to 0.19686, saving model to best.model\n",
      "0s - loss: 0.2056 - acc: 0.9438 - val_loss: 0.1969 - val_acc: 0.9130\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.19686 to 0.19483, saving model to best.model\n",
      "0s - loss: 0.2460 - acc: 0.9213 - val_loss: 0.1948 - val_acc: 0.9130\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.19483 to 0.19304, saving model to best.model\n",
      "0s - loss: 0.2449 - acc: 0.9101 - val_loss: 0.1930 - val_acc: 0.9130\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.19304 to 0.19112, saving model to best.model\n",
      "0s - loss: 0.2567 - acc: 0.9213 - val_loss: 0.1911 - val_acc: 0.9130\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.19112 to 0.18905, saving model to best.model\n",
      "0s - loss: 0.3328 - acc: 0.8989 - val_loss: 0.1890 - val_acc: 0.9130\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.18905 to 0.18762, saving model to best.model\n",
      "0s - loss: 0.2439 - acc: 0.9101 - val_loss: 0.1876 - val_acc: 0.9130\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.18762 to 0.18574, saving model to best.model\n",
      "0s - loss: 0.2130 - acc: 0.9213 - val_loss: 0.1857 - val_acc: 0.9130\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.18574 to 0.18348, saving model to best.model\n",
      "0s - loss: 0.1967 - acc: 0.9101 - val_loss: 0.1835 - val_acc: 0.9130\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.18348 to 0.18202, saving model to best.model\n",
      "0s - loss: 0.2352 - acc: 0.9438 - val_loss: 0.1820 - val_acc: 0.9130\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.18202 to 0.17991, saving model to best.model\n",
      "0s - loss: 0.2170 - acc: 0.9213 - val_loss: 0.1799 - val_acc: 0.9130\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.17991 to 0.17806, saving model to best.model\n",
      "0s - loss: 0.2041 - acc: 0.9551 - val_loss: 0.1781 - val_acc: 0.9130\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.17806 to 0.17587, saving model to best.model\n",
      "0s - loss: 0.1911 - acc: 0.9663 - val_loss: 0.1759 - val_acc: 0.9130\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.17587 to 0.17415, saving model to best.model\n",
      "0s - loss: 0.1960 - acc: 0.9438 - val_loss: 0.1741 - val_acc: 0.9130\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.17415 to 0.17241, saving model to best.model\n",
      "0s - loss: 0.1973 - acc: 0.9326 - val_loss: 0.1724 - val_acc: 0.9130\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.17241 to 0.17054, saving model to best.model\n",
      "0s - loss: 0.2245 - acc: 0.9213 - val_loss: 0.1705 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.17054 to 0.16860, saving model to best.model\n",
      "0s - loss: 0.2187 - acc: 0.9101 - val_loss: 0.1686 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.16860 to 0.16682, saving model to best.model\n",
      "0s - loss: 0.2063 - acc: 0.9326 - val_loss: 0.1668 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.16682 to 0.16481, saving model to best.model\n",
      "0s - loss: 0.2086 - acc: 0.8876 - val_loss: 0.1648 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.16481 to 0.16298, saving model to best.model\n",
      "0s - loss: 0.2323 - acc: 0.8989 - val_loss: 0.1630 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.16298 to 0.16134, saving model to best.model\n",
      "0s - loss: 0.2073 - acc: 0.9326 - val_loss: 0.1613 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.16134 to 0.16030, saving model to best.model\n",
      "0s - loss: 0.2737 - acc: 0.9101 - val_loss: 0.1603 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.16030 to 0.15914, saving model to best.model\n",
      "0s - loss: 0.1768 - acc: 0.9438 - val_loss: 0.1591 - val_acc: 0.9130\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.15914 to 0.15774, saving model to best.model\n",
      "0s - loss: 0.1616 - acc: 0.9326 - val_loss: 0.1577 - val_acc: 0.9130\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.15774 to 0.15651, saving model to best.model\n",
      "0s - loss: 0.2350 - acc: 0.9213 - val_loss: 0.1565 - val_acc: 0.9130\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.15651 to 0.15581, saving model to best.model\n",
      "0s - loss: 0.1644 - acc: 0.9551 - val_loss: 0.1558 - val_acc: 0.9130\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.15581 to 0.15521, saving model to best.model\n",
      "0s - loss: 0.1648 - acc: 0.9663 - val_loss: 0.1552 - val_acc: 0.9130\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.15521 to 0.15420, saving model to best.model\n",
      "0s - loss: 0.1784 - acc: 0.9551 - val_loss: 0.1542 - val_acc: 0.9130\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.15420 to 0.15318, saving model to best.model\n",
      "0s - loss: 0.1453 - acc: 0.9663 - val_loss: 0.1532 - val_acc: 0.9130\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.15318 to 0.15137, saving model to best.model\n",
      "0s - loss: 0.1898 - acc: 0.9551 - val_loss: 0.1514 - val_acc: 0.9130\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.15137 to 0.14957, saving model to best.model\n",
      "0s - loss: 0.1562 - acc: 0.9663 - val_loss: 0.1496 - val_acc: 0.9130\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.14957 to 0.14771, saving model to best.model\n",
      "0s - loss: 0.1860 - acc: 0.9326 - val_loss: 0.1477 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.14771 to 0.14554, saving model to best.model\n",
      "0s - loss: 0.1276 - acc: 0.9775 - val_loss: 0.1455 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.14554 to 0.14343, saving model to best.model\n",
      "0s - loss: 0.2153 - acc: 0.9326 - val_loss: 0.1434 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.14343 to 0.14164, saving model to best.model\n",
      "0s - loss: 0.1615 - acc: 0.9438 - val_loss: 0.1416 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.14164 to 0.13951, saving model to best.model\n",
      "0s - loss: 0.1810 - acc: 0.9213 - val_loss: 0.1395 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.13951 to 0.13765, saving model to best.model\n",
      "0s - loss: 0.2123 - acc: 0.9213 - val_loss: 0.1376 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.13765 to 0.13627, saving model to best.model\n",
      "0s - loss: 0.1860 - acc: 0.9438 - val_loss: 0.1363 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.38310, saving model to best.model\n",
      "0s - loss: 1.5337 - acc: 0.4270 - val_loss: 1.3831 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.38310 to 1.24342, saving model to best.model\n",
      "0s - loss: 1.4284 - acc: 0.4157 - val_loss: 1.2434 - val_acc: 0.3478\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.24342 to 1.14176, saving model to best.model\n",
      "0s - loss: 1.3212 - acc: 0.4382 - val_loss: 1.1418 - val_acc: 0.3478\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.14176 to 1.07625, saving model to best.model\n",
      "0s - loss: 1.3452 - acc: 0.3820 - val_loss: 1.0763 - val_acc: 0.3478\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.07625 to 1.03743, saving model to best.model\n",
      "0s - loss: 1.3229 - acc: 0.3596 - val_loss: 1.0374 - val_acc: 0.3478\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.03743 to 1.01893, saving model to best.model\n",
      "0s - loss: 1.2343 - acc: 0.4157 - val_loss: 1.0189 - val_acc: 0.4783\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.01893 to 1.01478, saving model to best.model\n",
      "0s - loss: 1.1824 - acc: 0.3483 - val_loss: 1.0148 - val_acc: 0.4783\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1841 - acc: 0.3483 - val_loss: 1.0192 - val_acc: 0.4783\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2021 - acc: 0.3820 - val_loss: 1.0285 - val_acc: 0.4783\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1426 - acc: 0.3820 - val_loss: 1.0402 - val_acc: 0.4783\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1628 - acc: 0.3933 - val_loss: 1.0507 - val_acc: 0.4783\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2654 - acc: 0.2584 - val_loss: 1.0607 - val_acc: 0.4783\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.0888 - acc: 0.4494 - val_loss: 1.0684 - val_acc: 0.4783\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.1364 - acc: 0.4494 - val_loss: 1.0730 - val_acc: 0.4783\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1127 - acc: 0.4270 - val_loss: 1.0752 - val_acc: 0.8261\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2254 - acc: 0.3483 - val_loss: 1.0767 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2361 - acc: 0.3483 - val_loss: 1.0772 - val_acc: 0.3478\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1471 - acc: 0.4494 - val_loss: 1.0741 - val_acc: 0.3478\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1260 - acc: 0.4045 - val_loss: 1.0705 - val_acc: 0.3478\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1858 - acc: 0.4045 - val_loss: 1.0662 - val_acc: 0.3478\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1790 - acc: 0.3820 - val_loss: 1.0617 - val_acc: 0.3478\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1039 - acc: 0.4719 - val_loss: 1.0575 - val_acc: 0.3478\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.0730 - acc: 0.4157 - val_loss: 1.0537 - val_acc: 0.3478\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.2143 - acc: 0.3596 - val_loss: 1.0485 - val_acc: 0.3478\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1383 - acc: 0.4045 - val_loss: 1.0422 - val_acc: 0.3478\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1863 - acc: 0.3596 - val_loss: 1.0352 - val_acc: 0.3478\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.0877 - acc: 0.5169 - val_loss: 1.0269 - val_acc: 0.3478\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1420 - acc: 0.4045 - val_loss: 1.0191 - val_acc: 0.3478\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.01478 to 1.01181, saving model to best.model\n",
      "0s - loss: 1.1478 - acc: 0.3933 - val_loss: 1.0118 - val_acc: 0.3478\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.01181 to 1.00516, saving model to best.model\n",
      "0s - loss: 1.1518 - acc: 0.3483 - val_loss: 1.0052 - val_acc: 0.3478\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.00516 to 0.99949, saving model to best.model\n",
      "0s - loss: 1.1014 - acc: 0.4494 - val_loss: 0.9995 - val_acc: 0.5217\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.99949 to 0.99492, saving model to best.model\n",
      "0s - loss: 1.1427 - acc: 0.4045 - val_loss: 0.9949 - val_acc: 0.6087\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.99492 to 0.99194, saving model to best.model\n",
      "0s - loss: 1.1511 - acc: 0.3371 - val_loss: 0.9919 - val_acc: 0.6522\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.99194 to 0.98944, saving model to best.model\n",
      "0s - loss: 1.1620 - acc: 0.3933 - val_loss: 0.9894 - val_acc: 0.6522\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.98944 to 0.98785, saving model to best.model\n",
      "0s - loss: 1.1332 - acc: 0.4270 - val_loss: 0.9878 - val_acc: 0.6957\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.98785 to 0.98694, saving model to best.model\n",
      "0s - loss: 1.0308 - acc: 0.4719 - val_loss: 0.9869 - val_acc: 0.6957\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.98694 to 0.98665, saving model to best.model\n",
      "0s - loss: 1.1208 - acc: 0.4045 - val_loss: 0.9866 - val_acc: 0.6522\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 1.1380 - acc: 0.4607 - val_loss: 0.9870 - val_acc: 0.6522\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 1.1096 - acc: 0.3933 - val_loss: 0.9882 - val_acc: 0.6522\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 1.0574 - acc: 0.4607 - val_loss: 0.9901 - val_acc: 0.5652\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 1.0522 - acc: 0.3933 - val_loss: 0.9919 - val_acc: 0.5217\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 1.0331 - acc: 0.5056 - val_loss: 0.9928 - val_acc: 0.5217\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 1.1191 - acc: 0.4045 - val_loss: 0.9932 - val_acc: 0.4783\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 1.1088 - acc: 0.4157 - val_loss: 0.9936 - val_acc: 0.3913\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 1.1441 - acc: 0.3596 - val_loss: 0.9932 - val_acc: 0.3913\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 1.0356 - acc: 0.4719 - val_loss: 0.9922 - val_acc: 0.3913\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 1.1631 - acc: 0.3371 - val_loss: 0.9903 - val_acc: 0.3913\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 1.1068 - acc: 0.4045 - val_loss: 0.9875 - val_acc: 0.3913\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.98665 to 0.98329, saving model to best.model\n",
      "0s - loss: 1.0806 - acc: 0.4607 - val_loss: 0.9833 - val_acc: 0.3913\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.98329 to 0.97882, saving model to best.model\n",
      "0s - loss: 1.0194 - acc: 0.4607 - val_loss: 0.9788 - val_acc: 0.4783\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.97882 to 0.97370, saving model to best.model\n",
      "0s - loss: 1.0461 - acc: 0.4831 - val_loss: 0.9737 - val_acc: 0.5217\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.97370 to 0.96806, saving model to best.model\n",
      "0s - loss: 0.9438 - acc: 0.5169 - val_loss: 0.9681 - val_acc: 0.5217\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.96806 to 0.96236, saving model to best.model\n",
      "0s - loss: 1.0683 - acc: 0.3933 - val_loss: 0.9624 - val_acc: 0.5652\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.96236 to 0.95710, saving model to best.model\n",
      "0s - loss: 1.0131 - acc: 0.5281 - val_loss: 0.9571 - val_acc: 0.6087\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.95710 to 0.95147, saving model to best.model\n",
      "0s - loss: 1.0791 - acc: 0.4382 - val_loss: 0.9515 - val_acc: 0.6522\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.95147 to 0.94480, saving model to best.model\n",
      "0s - loss: 1.0596 - acc: 0.4719 - val_loss: 0.9448 - val_acc: 0.6522\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.94480 to 0.93850, saving model to best.model\n",
      "0s - loss: 1.0140 - acc: 0.4382 - val_loss: 0.9385 - val_acc: 0.6522\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.93850 to 0.93190, saving model to best.model\n",
      "0s - loss: 1.0086 - acc: 0.4494 - val_loss: 0.9319 - val_acc: 0.7391\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.93190 to 0.92494, saving model to best.model\n",
      "0s - loss: 1.0578 - acc: 0.4157 - val_loss: 0.9249 - val_acc: 0.8261\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.92494 to 0.91798, saving model to best.model\n",
      "0s - loss: 0.9835 - acc: 0.5281 - val_loss: 0.9180 - val_acc: 0.8261\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.91798 to 0.91165, saving model to best.model\n",
      "0s - loss: 1.0323 - acc: 0.5281 - val_loss: 0.9116 - val_acc: 0.8261\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.91165 to 0.90660, saving model to best.model\n",
      "0s - loss: 0.9403 - acc: 0.4944 - val_loss: 0.9066 - val_acc: 0.8261\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.90660 to 0.90230, saving model to best.model\n",
      "0s - loss: 0.9797 - acc: 0.5506 - val_loss: 0.9023 - val_acc: 0.8261\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.90230 to 0.89894, saving model to best.model\n",
      "0s - loss: 0.9363 - acc: 0.6067 - val_loss: 0.8989 - val_acc: 0.8261\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.89894 to 0.89602, saving model to best.model\n",
      "0s - loss: 0.9842 - acc: 0.5393 - val_loss: 0.8960 - val_acc: 0.8261\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.89602 to 0.89368, saving model to best.model\n",
      "0s - loss: 1.0152 - acc: 0.4270 - val_loss: 0.8937 - val_acc: 0.7826\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.89368 to 0.89046, saving model to best.model\n",
      "0s - loss: 0.9706 - acc: 0.5281 - val_loss: 0.8905 - val_acc: 0.7391\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.89046 to 0.88696, saving model to best.model\n",
      "0s - loss: 1.0558 - acc: 0.4607 - val_loss: 0.8870 - val_acc: 0.7391\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.88696 to 0.88297, saving model to best.model\n",
      "0s - loss: 0.9393 - acc: 0.5506 - val_loss: 0.8830 - val_acc: 0.7391\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.88297 to 0.87810, saving model to best.model\n",
      "0s - loss: 0.9752 - acc: 0.5056 - val_loss: 0.8781 - val_acc: 0.7391\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.87810 to 0.87227, saving model to best.model\n",
      "0s - loss: 0.8847 - acc: 0.5955 - val_loss: 0.8723 - val_acc: 0.7391\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.87227 to 0.86610, saving model to best.model\n",
      "0s - loss: 0.9843 - acc: 0.4831 - val_loss: 0.8661 - val_acc: 0.7391\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.86610 to 0.85904, saving model to best.model\n",
      "0s - loss: 0.9443 - acc: 0.5618 - val_loss: 0.8590 - val_acc: 0.7391\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.85904 to 0.85183, saving model to best.model\n",
      "0s - loss: 0.8894 - acc: 0.6404 - val_loss: 0.8518 - val_acc: 0.7391\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.85183 to 0.84452, saving model to best.model\n",
      "0s - loss: 0.9451 - acc: 0.5843 - val_loss: 0.8445 - val_acc: 0.7391\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.84452 to 0.83687, saving model to best.model\n",
      "0s - loss: 0.9642 - acc: 0.5169 - val_loss: 0.8369 - val_acc: 0.7391\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.83687 to 0.82899, saving model to best.model\n",
      "0s - loss: 0.9105 - acc: 0.5169 - val_loss: 0.8290 - val_acc: 0.7391\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.82899 to 0.82071, saving model to best.model\n",
      "0s - loss: 0.8747 - acc: 0.5955 - val_loss: 0.8207 - val_acc: 0.7391\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.82071 to 0.81158, saving model to best.model\n",
      "0s - loss: 0.9100 - acc: 0.5955 - val_loss: 0.8116 - val_acc: 0.7391\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.81158 to 0.80127, saving model to best.model\n",
      "0s - loss: 0.9031 - acc: 0.5843 - val_loss: 0.8013 - val_acc: 0.7391\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.80127 to 0.79029, saving model to best.model\n",
      "0s - loss: 0.8828 - acc: 0.5843 - val_loss: 0.7903 - val_acc: 0.7826\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.79029 to 0.77978, saving model to best.model\n",
      "0s - loss: 0.8899 - acc: 0.5955 - val_loss: 0.7798 - val_acc: 0.7826\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.77978 to 0.76815, saving model to best.model\n",
      "0s - loss: 0.8696 - acc: 0.6067 - val_loss: 0.7681 - val_acc: 0.7826\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.76815 to 0.75639, saving model to best.model\n",
      "0s - loss: 0.8149 - acc: 0.6629 - val_loss: 0.7564 - val_acc: 0.7826\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.75639 to 0.74508, saving model to best.model\n",
      "0s - loss: 0.8517 - acc: 0.5955 - val_loss: 0.7451 - val_acc: 0.8261\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.74508 to 0.73332, saving model to best.model\n",
      "0s - loss: 0.8619 - acc: 0.6742 - val_loss: 0.7333 - val_acc: 0.8261\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.73332 to 0.72225, saving model to best.model\n",
      "0s - loss: 0.8682 - acc: 0.6404 - val_loss: 0.7223 - val_acc: 0.8261\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.72225 to 0.71158, saving model to best.model\n",
      "0s - loss: 0.8809 - acc: 0.5730 - val_loss: 0.7116 - val_acc: 0.8261\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.71158 to 0.70013, saving model to best.model\n",
      "0s - loss: 0.7852 - acc: 0.6517 - val_loss: 0.7001 - val_acc: 0.8261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.70013 to 0.68801, saving model to best.model\n",
      "0s - loss: 0.7880 - acc: 0.7191 - val_loss: 0.6880 - val_acc: 0.8261\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68801 to 0.67603, saving model to best.model\n",
      "0s - loss: 0.8100 - acc: 0.6854 - val_loss: 0.6760 - val_acc: 0.8261\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67603 to 0.66362, saving model to best.model\n",
      "0s - loss: 0.7304 - acc: 0.7191 - val_loss: 0.6636 - val_acc: 0.8261\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66362 to 0.65130, saving model to best.model\n",
      "0s - loss: 0.7593 - acc: 0.6854 - val_loss: 0.6513 - val_acc: 0.8261\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.65130 to 0.63934, saving model to best.model\n",
      "0s - loss: 0.7630 - acc: 0.6742 - val_loss: 0.6393 - val_acc: 0.8261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.63934 to 0.62764, saving model to best.model\n",
      "0s - loss: 0.7860 - acc: 0.6742 - val_loss: 0.6276 - val_acc: 0.8261\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.62764 to 0.61641, saving model to best.model\n",
      "0s - loss: 0.7714 - acc: 0.6854 - val_loss: 0.6164 - val_acc: 0.8261\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.61641 to 0.60515, saving model to best.model\n",
      "0s - loss: 0.7181 - acc: 0.7191 - val_loss: 0.6052 - val_acc: 0.8261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.60515 to 0.59416, saving model to best.model\n",
      "0s - loss: 0.6967 - acc: 0.7416 - val_loss: 0.5942 - val_acc: 0.8261\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.59416 to 0.58329, saving model to best.model\n",
      "0s - loss: 0.6633 - acc: 0.7528 - val_loss: 0.5833 - val_acc: 0.8261\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.58329 to 0.57329, saving model to best.model\n",
      "0s - loss: 0.7084 - acc: 0.6629 - val_loss: 0.5733 - val_acc: 0.8261\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.57329 to 0.56219, saving model to best.model\n",
      "0s - loss: 0.7305 - acc: 0.7079 - val_loss: 0.5622 - val_acc: 0.8261\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.56219 to 0.55113, saving model to best.model\n",
      "0s - loss: 0.7267 - acc: 0.6629 - val_loss: 0.5511 - val_acc: 0.8261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.55113 to 0.54028, saving model to best.model\n",
      "0s - loss: 0.6619 - acc: 0.7416 - val_loss: 0.5403 - val_acc: 0.9130\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.54028 to 0.52905, saving model to best.model\n",
      "0s - loss: 0.6712 - acc: 0.7303 - val_loss: 0.5290 - val_acc: 0.9130\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.52905 to 0.51814, saving model to best.model\n",
      "0s - loss: 0.6463 - acc: 0.6966 - val_loss: 0.5181 - val_acc: 0.9130\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.51814 to 0.50755, saving model to best.model\n",
      "0s - loss: 0.7562 - acc: 0.6629 - val_loss: 0.5075 - val_acc: 0.9130\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.50755 to 0.49753, saving model to best.model\n",
      "0s - loss: 0.6930 - acc: 0.7303 - val_loss: 0.4975 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.49753 to 0.48743, saving model to best.model\n",
      "0s - loss: 0.6585 - acc: 0.7416 - val_loss: 0.4874 - val_acc: 0.9130\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.48743 to 0.47725, saving model to best.model\n",
      "0s - loss: 0.6478 - acc: 0.6966 - val_loss: 0.4773 - val_acc: 0.9130\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.47725 to 0.46688, saving model to best.model\n",
      "0s - loss: 0.5957 - acc: 0.7865 - val_loss: 0.4669 - val_acc: 0.9130\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.46688 to 0.45651, saving model to best.model\n",
      "0s - loss: 0.6506 - acc: 0.6966 - val_loss: 0.4565 - val_acc: 0.9130\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.45651 to 0.44586, saving model to best.model\n",
      "0s - loss: 0.6021 - acc: 0.7865 - val_loss: 0.4459 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.44586 to 0.43523, saving model to best.model\n",
      "0s - loss: 0.5511 - acc: 0.8427 - val_loss: 0.4352 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.43523 to 0.42485, saving model to best.model\n",
      "0s - loss: 0.5882 - acc: 0.7978 - val_loss: 0.4248 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.42485 to 0.41442, saving model to best.model\n",
      "0s - loss: 0.5122 - acc: 0.8090 - val_loss: 0.4144 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.41442 to 0.40430, saving model to best.model\n",
      "0s - loss: 0.5574 - acc: 0.7753 - val_loss: 0.4043 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.40430 to 0.39433, saving model to best.model\n",
      "0s - loss: 0.5700 - acc: 0.7978 - val_loss: 0.3943 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.39433 to 0.38480, saving model to best.model\n",
      "0s - loss: 0.5802 - acc: 0.7640 - val_loss: 0.3848 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.38480 to 0.37586, saving model to best.model\n",
      "0s - loss: 0.5644 - acc: 0.7865 - val_loss: 0.3759 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.37586 to 0.36707, saving model to best.model\n",
      "0s - loss: 0.6000 - acc: 0.7753 - val_loss: 0.3671 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.36707 to 0.35904, saving model to best.model\n",
      "0s - loss: 0.5146 - acc: 0.8090 - val_loss: 0.3590 - val_acc: 0.9565\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.35904 to 0.35143, saving model to best.model\n",
      "0s - loss: 0.5429 - acc: 0.7865 - val_loss: 0.3514 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.35143 to 0.34509, saving model to best.model\n",
      "0s - loss: 0.4548 - acc: 0.8539 - val_loss: 0.3451 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.34509 to 0.33850, saving model to best.model\n",
      "0s - loss: 0.5355 - acc: 0.7978 - val_loss: 0.3385 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.33850 to 0.33142, saving model to best.model\n",
      "0s - loss: 0.5185 - acc: 0.7416 - val_loss: 0.3314 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.33142 to 0.32482, saving model to best.model\n",
      "0s - loss: 0.4648 - acc: 0.8427 - val_loss: 0.3248 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.32482 to 0.31835, saving model to best.model\n",
      "0s - loss: 0.4500 - acc: 0.8652 - val_loss: 0.3183 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.31835 to 0.31237, saving model to best.model\n",
      "0s - loss: 0.5192 - acc: 0.8202 - val_loss: 0.3124 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.31237 to 0.30665, saving model to best.model\n",
      "0s - loss: 0.3980 - acc: 0.8427 - val_loss: 0.3067 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.30665 to 0.30081, saving model to best.model\n",
      "0s - loss: 0.3928 - acc: 0.8989 - val_loss: 0.3008 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.30081 to 0.29507, saving model to best.model\n",
      "0s - loss: 0.3886 - acc: 0.8315 - val_loss: 0.2951 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.29507 to 0.28839, saving model to best.model\n",
      "0s - loss: 0.3942 - acc: 0.8427 - val_loss: 0.2884 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.28839 to 0.28147, saving model to best.model\n",
      "0s - loss: 0.4185 - acc: 0.8427 - val_loss: 0.2815 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.28147 to 0.27451, saving model to best.model\n",
      "0s - loss: 0.4139 - acc: 0.8202 - val_loss: 0.2745 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.27451 to 0.26654, saving model to best.model\n",
      "0s - loss: 0.4263 - acc: 0.8539 - val_loss: 0.2665 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.26654 to 0.25943, saving model to best.model\n",
      "0s - loss: 0.4365 - acc: 0.8427 - val_loss: 0.2594 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.25943 to 0.25262, saving model to best.model\n",
      "0s - loss: 0.4290 - acc: 0.8315 - val_loss: 0.2526 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.25262 to 0.24571, saving model to best.model\n",
      "0s - loss: 0.3650 - acc: 0.8876 - val_loss: 0.2457 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.24571 to 0.23974, saving model to best.model\n",
      "0s - loss: 0.4089 - acc: 0.8764 - val_loss: 0.2397 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.23974 to 0.23432, saving model to best.model\n",
      "0s - loss: 0.3651 - acc: 0.8652 - val_loss: 0.2343 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.23432 to 0.22793, saving model to best.model\n",
      "0s - loss: 0.3540 - acc: 0.8989 - val_loss: 0.2279 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.22793 to 0.22084, saving model to best.model\n",
      "0s - loss: 0.3543 - acc: 0.8539 - val_loss: 0.2208 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.22084 to 0.21407, saving model to best.model\n",
      "0s - loss: 0.3853 - acc: 0.8539 - val_loss: 0.2141 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.21407 to 0.20701, saving model to best.model\n",
      "0s - loss: 0.3463 - acc: 0.8876 - val_loss: 0.2070 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.20701 to 0.20064, saving model to best.model\n",
      "0s - loss: 0.3544 - acc: 0.8764 - val_loss: 0.2006 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.20064 to 0.19481, saving model to best.model\n",
      "0s - loss: 0.3423 - acc: 0.8652 - val_loss: 0.1948 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.19481 to 0.18945, saving model to best.model\n",
      "0s - loss: 0.4149 - acc: 0.8427 - val_loss: 0.1894 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.18945 to 0.18506, saving model to best.model\n",
      "0s - loss: 0.3423 - acc: 0.8764 - val_loss: 0.1851 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.18506 to 0.18049, saving model to best.model\n",
      "0s - loss: 0.3454 - acc: 0.9101 - val_loss: 0.1805 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.18049 to 0.17589, saving model to best.model\n",
      "0s - loss: 0.3893 - acc: 0.8427 - val_loss: 0.1759 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.17589 to 0.17117, saving model to best.model\n",
      "0s - loss: 0.3168 - acc: 0.8764 - val_loss: 0.1712 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.17117 to 0.16630, saving model to best.model\n",
      "0s - loss: 0.3057 - acc: 0.8539 - val_loss: 0.1663 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.16630 to 0.16166, saving model to best.model\n",
      "0s - loss: 0.3176 - acc: 0.8876 - val_loss: 0.1617 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.16166 to 0.15780, saving model to best.model\n",
      "0s - loss: 0.2611 - acc: 0.9101 - val_loss: 0.1578 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.15780 to 0.15438, saving model to best.model\n",
      "0s - loss: 0.2493 - acc: 0.9213 - val_loss: 0.1544 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.15438 to 0.15109, saving model to best.model\n",
      "0s - loss: 0.2592 - acc: 0.9213 - val_loss: 0.1511 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.15109 to 0.14750, saving model to best.model\n",
      "0s - loss: 0.3405 - acc: 0.8652 - val_loss: 0.1475 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.14750 to 0.14390, saving model to best.model\n",
      "0s - loss: 0.3360 - acc: 0.8652 - val_loss: 0.1439 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.14390 to 0.14019, saving model to best.model\n",
      "0s - loss: 0.2499 - acc: 0.9326 - val_loss: 0.1402 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.14019 to 0.13728, saving model to best.model\n",
      "0s - loss: 0.3084 - acc: 0.9101 - val_loss: 0.1373 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.13728 to 0.13428, saving model to best.model\n",
      "0s - loss: 0.2484 - acc: 0.9101 - val_loss: 0.1343 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.13428 to 0.13103, saving model to best.model\n",
      "0s - loss: 0.2485 - acc: 0.9101 - val_loss: 0.1310 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.13103 to 0.12809, saving model to best.model\n",
      "0s - loss: 0.2090 - acc: 0.9551 - val_loss: 0.1281 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.12809 to 0.12505, saving model to best.model\n",
      "0s - loss: 0.2369 - acc: 0.9438 - val_loss: 0.1250 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.12505 to 0.12179, saving model to best.model\n",
      "0s - loss: 0.2449 - acc: 0.9213 - val_loss: 0.1218 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.12179 to 0.11828, saving model to best.model\n",
      "0s - loss: 0.2039 - acc: 0.9551 - val_loss: 0.1183 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.11828 to 0.11510, saving model to best.model\n",
      "0s - loss: 0.2207 - acc: 0.9438 - val_loss: 0.1151 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.11510 to 0.11249, saving model to best.model\n",
      "0s - loss: 0.2063 - acc: 0.9551 - val_loss: 0.1125 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.11249 to 0.11054, saving model to best.model\n",
      "0s - loss: 0.2637 - acc: 0.9326 - val_loss: 0.1105 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.11054 to 0.10834, saving model to best.model\n",
      "0s - loss: 0.2660 - acc: 0.8876 - val_loss: 0.1083 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.10834 to 0.10639, saving model to best.model\n",
      "0s - loss: 0.2604 - acc: 0.9326 - val_loss: 0.1064 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.10639 to 0.10463, saving model to best.model\n",
      "0s - loss: 0.2298 - acc: 0.9101 - val_loss: 0.1046 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.10463 to 0.10335, saving model to best.model\n",
      "0s - loss: 0.2005 - acc: 0.9213 - val_loss: 0.1033 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.10335 to 0.10204, saving model to best.model\n",
      "0s - loss: 0.2244 - acc: 0.9101 - val_loss: 0.1020 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.10204 to 0.10035, saving model to best.model\n",
      "0s - loss: 0.2812 - acc: 0.8876 - val_loss: 0.1004 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.10035 to 0.09904, saving model to best.model\n",
      "0s - loss: 0.2131 - acc: 0.9101 - val_loss: 0.0990 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.09904 to 0.09810, saving model to best.model\n",
      "0s - loss: 0.2235 - acc: 0.9326 - val_loss: 0.0981 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.09810 to 0.09795, saving model to best.model\n",
      "0s - loss: 0.1751 - acc: 0.9775 - val_loss: 0.0980 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.09795 to 0.09710, saving model to best.model\n",
      "0s - loss: 0.1892 - acc: 0.9326 - val_loss: 0.0971 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.09710 to 0.09680, saving model to best.model\n",
      "0s - loss: 0.1885 - acc: 0.9326 - val_loss: 0.0968 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.09680 to 0.09663, saving model to best.model\n",
      "0s - loss: 0.2129 - acc: 0.8989 - val_loss: 0.0966 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.2763 - acc: 0.8876 - val_loss: 0.0970 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.2024 - acc: 0.9326 - val_loss: 0.0974 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.1964 - acc: 0.9663 - val_loss: 0.0973 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.2193 - acc: 0.9213 - val_loss: 0.0976 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.1393 - acc: 0.9888 - val_loss: 0.0977 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.09663 to 0.09620, saving model to best.model\n",
      "0s - loss: 0.1660 - acc: 0.9438 - val_loss: 0.0962 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.09620 to 0.09478, saving model to best.model\n",
      "0s - loss: 0.2000 - acc: 0.9326 - val_loss: 0.0948 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.09478 to 0.09174, saving model to best.model\n",
      "0s - loss: 0.1845 - acc: 0.9438 - val_loss: 0.0917 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.09174 to 0.08801, saving model to best.model\n",
      "0s - loss: 0.2069 - acc: 0.9326 - val_loss: 0.0880 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.08801 to 0.08328, saving model to best.model\n",
      "0s - loss: 0.1909 - acc: 0.9438 - val_loss: 0.0833 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.08328 to 0.07859, saving model to best.model\n",
      "0s - loss: 0.1548 - acc: 0.9888 - val_loss: 0.0786 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.07859 to 0.07341, saving model to best.model\n",
      "0s - loss: 0.1851 - acc: 0.9551 - val_loss: 0.0734 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.07341 to 0.06877, saving model to best.model\n",
      "0s - loss: 0.1821 - acc: 0.9551 - val_loss: 0.0688 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.06877 to 0.06522, saving model to best.model\n",
      "0s - loss: 0.1751 - acc: 0.9775 - val_loss: 0.0652 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.06522 to 0.06218, saving model to best.model\n",
      "0s - loss: 0.1856 - acc: 0.9326 - val_loss: 0.0622 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.06218 to 0.05994, saving model to best.model\n",
      "0s - loss: 0.1428 - acc: 0.9775 - val_loss: 0.0599 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.05994 to 0.05809, saving model to best.model\n",
      "0s - loss: 0.1743 - acc: 0.9326 - val_loss: 0.0581 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.05809 to 0.05671, saving model to best.model\n",
      "0s - loss: 0.1387 - acc: 0.9775 - val_loss: 0.0567 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.05671 to 0.05508, saving model to best.model\n",
      "0s - loss: 0.1489 - acc: 0.9438 - val_loss: 0.0551 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.42903, saving model to best.model\n",
      "0s - loss: 1.4808 - acc: 0.3596 - val_loss: 1.4290 - val_acc: 0.2174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.42903 to 1.32305, saving model to best.model\n",
      "0s - loss: 1.2666 - acc: 0.3820 - val_loss: 1.3231 - val_acc: 0.3043\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.32305 to 1.24541, saving model to best.model\n",
      "0s - loss: 1.3960 - acc: 0.2697 - val_loss: 1.2454 - val_acc: 0.3043\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.24541 to 1.18405, saving model to best.model\n",
      "0s - loss: 1.2979 - acc: 0.3371 - val_loss: 1.1840 - val_acc: 0.3043\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.18405 to 1.13775, saving model to best.model\n",
      "0s - loss: 1.3402 - acc: 0.3146 - val_loss: 1.1378 - val_acc: 0.3043\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.13775 to 1.10458, saving model to best.model\n",
      "0s - loss: 1.2914 - acc: 0.3146 - val_loss: 1.1046 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.10458 to 1.08258, saving model to best.model\n",
      "0s - loss: 1.2349 - acc: 0.4045 - val_loss: 1.0826 - val_acc: 0.3043\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.08258 to 1.06936, saving model to best.model\n",
      "0s - loss: 1.3302 - acc: 0.3596 - val_loss: 1.0694 - val_acc: 0.3043\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.06936 to 1.06146, saving model to best.model\n",
      "0s - loss: 1.1930 - acc: 0.3708 - val_loss: 1.0615 - val_acc: 0.3043\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.06146 to 1.05599, saving model to best.model\n",
      "0s - loss: 1.2880 - acc: 0.2921 - val_loss: 1.0560 - val_acc: 0.6957\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.05599 to 1.05227, saving model to best.model\n",
      "0s - loss: 1.3207 - acc: 0.3034 - val_loss: 1.0523 - val_acc: 0.5217\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.05227 to 1.04993, saving model to best.model\n",
      "0s - loss: 1.1806 - acc: 0.3820 - val_loss: 1.0499 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.04993 to 1.04945, saving model to best.model\n",
      "0s - loss: 1.2187 - acc: 0.3596 - val_loss: 1.0495 - val_acc: 0.6087\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.3839 - acc: 0.3034 - val_loss: 1.0508 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.0637 - acc: 0.4382 - val_loss: 1.0546 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1546 - acc: 0.3708 - val_loss: 1.0597 - val_acc: 0.3043\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2025 - acc: 0.3371 - val_loss: 1.0663 - val_acc: 0.3043\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1123 - acc: 0.4607 - val_loss: 1.0745 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1518 - acc: 0.3933 - val_loss: 1.0849 - val_acc: 0.3043\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1448 - acc: 0.4045 - val_loss: 1.0947 - val_acc: 0.3043\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1063 - acc: 0.4270 - val_loss: 1.1021 - val_acc: 0.3043\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1816 - acc: 0.3708 - val_loss: 1.1065 - val_acc: 0.3043\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2577 - acc: 0.2921 - val_loss: 1.1107 - val_acc: 0.3043\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1496 - acc: 0.3708 - val_loss: 1.1120 - val_acc: 0.3043\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1025 - acc: 0.4382 - val_loss: 1.1117 - val_acc: 0.3043\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.2504 - acc: 0.3034 - val_loss: 1.1090 - val_acc: 0.3043\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1256 - acc: 0.3483 - val_loss: 1.1049 - val_acc: 0.3043\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1727 - acc: 0.3258 - val_loss: 1.0989 - val_acc: 0.3043\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1850 - acc: 0.3483 - val_loss: 1.0910 - val_acc: 0.3043\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.0984 - acc: 0.4719 - val_loss: 1.0818 - val_acc: 0.3043\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.2613 - acc: 0.3034 - val_loss: 1.0738 - val_acc: 0.3043\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1413 - acc: 0.3933 - val_loss: 1.0650 - val_acc: 0.3043\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.1818 - acc: 0.3708 - val_loss: 1.0576 - val_acc: 0.3043\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.2117 - acc: 0.3596 - val_loss: 1.0509 - val_acc: 0.3043\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.04945 to 1.04561, saving model to best.model\n",
      "0s - loss: 1.1098 - acc: 0.3820 - val_loss: 1.0456 - val_acc: 0.3043\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.04561 to 1.04051, saving model to best.model\n",
      "0s - loss: 1.2190 - acc: 0.3034 - val_loss: 1.0405 - val_acc: 0.3043\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.04051 to 1.03655, saving model to best.model\n",
      "0s - loss: 1.1962 - acc: 0.3483 - val_loss: 1.0365 - val_acc: 0.3043\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.03655 to 1.03304, saving model to best.model\n",
      "0s - loss: 1.0847 - acc: 0.4719 - val_loss: 1.0330 - val_acc: 0.3043\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.03304 to 1.03003, saving model to best.model\n",
      "0s - loss: 1.1814 - acc: 0.4045 - val_loss: 1.0300 - val_acc: 0.3043\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.03003 to 1.02726, saving model to best.model\n",
      "0s - loss: 1.1729 - acc: 0.3596 - val_loss: 1.0273 - val_acc: 0.3043\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.02726 to 1.02450, saving model to best.model\n",
      "0s - loss: 1.0392 - acc: 0.4607 - val_loss: 1.0245 - val_acc: 0.3043\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.02450 to 1.02288, saving model to best.model\n",
      "0s - loss: 1.1262 - acc: 0.3596 - val_loss: 1.0229 - val_acc: 0.3043\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.02288 to 1.02205, saving model to best.model\n",
      "0s - loss: 1.1542 - acc: 0.3258 - val_loss: 1.0220 - val_acc: 0.3043\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 1.0901 - acc: 0.4382 - val_loss: 1.0223 - val_acc: 0.3043\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 1.1524 - acc: 0.4157 - val_loss: 1.0229 - val_acc: 0.3043\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 1.0307 - acc: 0.4157 - val_loss: 1.0236 - val_acc: 0.3043\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 1.0807 - acc: 0.4382 - val_loss: 1.0234 - val_acc: 0.3043\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 1.0951 - acc: 0.3820 - val_loss: 1.0232 - val_acc: 0.3043\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 1.0359 - acc: 0.4382 - val_loss: 1.0231 - val_acc: 0.3043\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 1.0853 - acc: 0.4831 - val_loss: 1.0233 - val_acc: 0.3043\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.02205 to 1.02170, saving model to best.model\n",
      "0s - loss: 1.1273 - acc: 0.4270 - val_loss: 1.0217 - val_acc: 0.3043\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.02170 to 1.02004, saving model to best.model\n",
      "0s - loss: 1.0284 - acc: 0.4831 - val_loss: 1.0200 - val_acc: 0.3043\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.02004 to 1.01788, saving model to best.model\n",
      "0s - loss: 1.1034 - acc: 0.3483 - val_loss: 1.0179 - val_acc: 0.3043\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.01788 to 1.01618, saving model to best.model\n",
      "0s - loss: 1.0983 - acc: 0.3820 - val_loss: 1.0162 - val_acc: 0.3043\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 1.01618 to 1.01327, saving model to best.model\n",
      "0s - loss: 1.1587 - acc: 0.3258 - val_loss: 1.0133 - val_acc: 0.3043\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 1.01327 to 1.00833, saving model to best.model\n",
      "0s - loss: 1.0541 - acc: 0.5281 - val_loss: 1.0083 - val_acc: 0.3043\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 1.00833 to 1.00264, saving model to best.model\n",
      "0s - loss: 1.0965 - acc: 0.4382 - val_loss: 1.0026 - val_acc: 0.3043\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 1.00264 to 0.99808, saving model to best.model\n",
      "0s - loss: 1.0524 - acc: 0.4157 - val_loss: 0.9981 - val_acc: 0.3043\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.99808 to 0.99295, saving model to best.model\n",
      "0s - loss: 0.9883 - acc: 0.4719 - val_loss: 0.9930 - val_acc: 0.3043\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.99295 to 0.98790, saving model to best.model\n",
      "0s - loss: 1.0710 - acc: 0.4382 - val_loss: 0.9879 - val_acc: 0.3043\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.98790 to 0.98308, saving model to best.model\n",
      "0s - loss: 1.0061 - acc: 0.4944 - val_loss: 0.9831 - val_acc: 0.3043\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.98308 to 0.97864, saving model to best.model\n",
      "0s - loss: 0.9977 - acc: 0.4719 - val_loss: 0.9786 - val_acc: 0.3043\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.97864 to 0.97397, saving model to best.model\n",
      "0s - loss: 1.0473 - acc: 0.4607 - val_loss: 0.9740 - val_acc: 0.3043\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.97397 to 0.96932, saving model to best.model\n",
      "0s - loss: 1.0569 - acc: 0.4607 - val_loss: 0.9693 - val_acc: 0.3043\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.96932 to 0.96444, saving model to best.model\n",
      "0s - loss: 0.9982 - acc: 0.5169 - val_loss: 0.9644 - val_acc: 0.3043\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.96444 to 0.95900, saving model to best.model\n",
      "0s - loss: 0.9811 - acc: 0.4831 - val_loss: 0.9590 - val_acc: 0.3043\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.95900 to 0.95244, saving model to best.model\n",
      "0s - loss: 1.0706 - acc: 0.4719 - val_loss: 0.9524 - val_acc: 0.3043\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.95244 to 0.94678, saving model to best.model\n",
      "0s - loss: 0.9977 - acc: 0.5056 - val_loss: 0.9468 - val_acc: 0.3478\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.94678 to 0.94012, saving model to best.model\n",
      "0s - loss: 1.0528 - acc: 0.4382 - val_loss: 0.9401 - val_acc: 0.3913\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.94012 to 0.93233, saving model to best.model\n",
      "0s - loss: 1.0301 - acc: 0.4831 - val_loss: 0.9323 - val_acc: 0.4783\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.93233 to 0.92433, saving model to best.model\n",
      "0s - loss: 0.9656 - acc: 0.5281 - val_loss: 0.9243 - val_acc: 0.4783\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.92433 to 0.91607, saving model to best.model\n",
      "0s - loss: 1.0203 - acc: 0.4382 - val_loss: 0.9161 - val_acc: 0.5652\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.91607 to 0.90730, saving model to best.model\n",
      "0s - loss: 0.9692 - acc: 0.5393 - val_loss: 0.9073 - val_acc: 0.7826\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.90730 to 0.89833, saving model to best.model\n",
      "0s - loss: 1.0108 - acc: 0.4944 - val_loss: 0.8983 - val_acc: 0.8696\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.89833 to 0.88940, saving model to best.model\n",
      "0s - loss: 0.9224 - acc: 0.6067 - val_loss: 0.8894 - val_acc: 0.8696\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.88940 to 0.88157, saving model to best.model\n",
      "0s - loss: 0.9254 - acc: 0.4831 - val_loss: 0.8816 - val_acc: 0.8696\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.88157 to 0.87426, saving model to best.model\n",
      "0s - loss: 1.0141 - acc: 0.4607 - val_loss: 0.8743 - val_acc: 0.8696\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.87426 to 0.86729, saving model to best.model\n",
      "0s - loss: 0.9619 - acc: 0.5281 - val_loss: 0.8673 - val_acc: 0.8696\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.86729 to 0.86053, saving model to best.model\n",
      "0s - loss: 0.9313 - acc: 0.5843 - val_loss: 0.8605 - val_acc: 0.8696\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.86053 to 0.85342, saving model to best.model\n",
      "0s - loss: 0.9282 - acc: 0.5843 - val_loss: 0.8534 - val_acc: 0.8696\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.85342 to 0.84692, saving model to best.model\n",
      "0s - loss: 0.9304 - acc: 0.5393 - val_loss: 0.8469 - val_acc: 0.8696\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.84692 to 0.84053, saving model to best.model\n",
      "0s - loss: 0.9176 - acc: 0.5506 - val_loss: 0.8405 - val_acc: 0.8696\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.84053 to 0.83387, saving model to best.model\n",
      "0s - loss: 0.9261 - acc: 0.6067 - val_loss: 0.8339 - val_acc: 0.8696\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.83387 to 0.82661, saving model to best.model\n",
      "0s - loss: 0.9332 - acc: 0.5281 - val_loss: 0.8266 - val_acc: 0.8696\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.82661 to 0.81884, saving model to best.model\n",
      "0s - loss: 0.9367 - acc: 0.4719 - val_loss: 0.8188 - val_acc: 0.8696\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.81884 to 0.81077, saving model to best.model\n",
      "0s - loss: 0.9214 - acc: 0.5618 - val_loss: 0.8108 - val_acc: 0.8696\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.81077 to 0.80249, saving model to best.model\n",
      "0s - loss: 0.9638 - acc: 0.5393 - val_loss: 0.8025 - val_acc: 0.8696\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.80249 to 0.79329, saving model to best.model\n",
      "0s - loss: 0.8574 - acc: 0.5730 - val_loss: 0.7933 - val_acc: 0.8696\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.79329 to 0.78363, saving model to best.model\n",
      "0s - loss: 0.8183 - acc: 0.6629 - val_loss: 0.7836 - val_acc: 0.8696\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.78363 to 0.77407, saving model to best.model\n",
      "0s - loss: 0.8816 - acc: 0.6067 - val_loss: 0.7741 - val_acc: 0.8696\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.77407 to 0.76334, saving model to best.model\n",
      "0s - loss: 0.8371 - acc: 0.6292 - val_loss: 0.7633 - val_acc: 0.8696\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.76334 to 0.75241, saving model to best.model\n",
      "0s - loss: 0.8780 - acc: 0.6292 - val_loss: 0.7524 - val_acc: 0.9130\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.75241 to 0.74187, saving model to best.model\n",
      "0s - loss: 0.8270 - acc: 0.6629 - val_loss: 0.7419 - val_acc: 0.9130\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.74187 to 0.73080, saving model to best.model\n",
      "0s - loss: 0.8882 - acc: 0.5730 - val_loss: 0.7308 - val_acc: 0.9130\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.73080 to 0.72059, saving model to best.model\n",
      "0s - loss: 0.8622 - acc: 0.5955 - val_loss: 0.7206 - val_acc: 0.9130\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.72059 to 0.71084, saving model to best.model\n",
      "0s - loss: 0.8573 - acc: 0.5955 - val_loss: 0.7108 - val_acc: 0.9130\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.71084 to 0.70087, saving model to best.model\n",
      "0s - loss: 0.8405 - acc: 0.6292 - val_loss: 0.7009 - val_acc: 0.9130\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.70087 to 0.68967, saving model to best.model\n",
      "0s - loss: 0.8199 - acc: 0.5843 - val_loss: 0.6897 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.68967 to 0.67750, saving model to best.model\n",
      "0s - loss: 0.8763 - acc: 0.5056 - val_loss: 0.6775 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.67750 to 0.66548, saving model to best.model\n",
      "0s - loss: 0.7628 - acc: 0.6966 - val_loss: 0.6655 - val_acc: 0.9565\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66548 to 0.65303, saving model to best.model\n",
      "0s - loss: 0.7517 - acc: 0.6854 - val_loss: 0.6530 - val_acc: 0.9565\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.65303 to 0.64140, saving model to best.model\n",
      "0s - loss: 0.6942 - acc: 0.7191 - val_loss: 0.6414 - val_acc: 0.9565\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.64140 to 0.62978, saving model to best.model\n",
      "0s - loss: 0.7102 - acc: 0.7416 - val_loss: 0.6298 - val_acc: 0.9565\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.62978 to 0.61835, saving model to best.model\n",
      "0s - loss: 0.6871 - acc: 0.7528 - val_loss: 0.6184 - val_acc: 0.9565\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.61835 to 0.60649, saving model to best.model\n",
      "0s - loss: 0.7549 - acc: 0.7191 - val_loss: 0.6065 - val_acc: 0.9565\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.60649 to 0.59550, saving model to best.model\n",
      "0s - loss: 0.7693 - acc: 0.6742 - val_loss: 0.5955 - val_acc: 0.9565\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.59550 to 0.58399, saving model to best.model\n",
      "0s - loss: 0.7338 - acc: 0.7191 - val_loss: 0.5840 - val_acc: 0.9565\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.58399 to 0.57268, saving model to best.model\n",
      "0s - loss: 0.7439 - acc: 0.6629 - val_loss: 0.5727 - val_acc: 0.9565\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.57268 to 0.56175, saving model to best.model\n",
      "0s - loss: 0.6538 - acc: 0.7416 - val_loss: 0.5617 - val_acc: 0.9565\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.56175 to 0.55123, saving model to best.model\n",
      "0s - loss: 0.6684 - acc: 0.7191 - val_loss: 0.5512 - val_acc: 0.9565\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.55123 to 0.54132, saving model to best.model\n",
      "0s - loss: 0.6872 - acc: 0.6854 - val_loss: 0.5413 - val_acc: 0.9565\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.54132 to 0.53101, saving model to best.model\n",
      "0s - loss: 0.6020 - acc: 0.7865 - val_loss: 0.5310 - val_acc: 0.9565\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.53101 to 0.52047, saving model to best.model\n",
      "0s - loss: 0.6484 - acc: 0.6854 - val_loss: 0.5205 - val_acc: 0.9565\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.52047 to 0.50957, saving model to best.model\n",
      "0s - loss: 0.6514 - acc: 0.6854 - val_loss: 0.5096 - val_acc: 0.9565\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.50957 to 0.49963, saving model to best.model\n",
      "0s - loss: 0.5983 - acc: 0.7865 - val_loss: 0.4996 - val_acc: 0.9565\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.49963 to 0.48910, saving model to best.model\n",
      "0s - loss: 0.6368 - acc: 0.7528 - val_loss: 0.4891 - val_acc: 0.9565\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.48910 to 0.47902, saving model to best.model\n",
      "0s - loss: 0.5909 - acc: 0.8090 - val_loss: 0.4790 - val_acc: 0.9565\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.47902 to 0.46904, saving model to best.model\n",
      "0s - loss: 0.6025 - acc: 0.7640 - val_loss: 0.4690 - val_acc: 0.9565\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.46904 to 0.45919, saving model to best.model\n",
      "0s - loss: 0.5985 - acc: 0.7753 - val_loss: 0.4592 - val_acc: 0.9565\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.45919 to 0.44968, saving model to best.model\n",
      "0s - loss: 0.6188 - acc: 0.7865 - val_loss: 0.4497 - val_acc: 0.9565\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.44968 to 0.44047, saving model to best.model\n",
      "0s - loss: 0.6920 - acc: 0.7079 - val_loss: 0.4405 - val_acc: 0.9565\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.44047 to 0.43153, saving model to best.model\n",
      "0s - loss: 0.5677 - acc: 0.7865 - val_loss: 0.4315 - val_acc: 0.9565\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.43153 to 0.42212, saving model to best.model\n",
      "0s - loss: 0.4914 - acc: 0.8652 - val_loss: 0.4221 - val_acc: 0.9565\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.42212 to 0.41299, saving model to best.model\n",
      "0s - loss: 0.6406 - acc: 0.6966 - val_loss: 0.4130 - val_acc: 0.9565\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.41299 to 0.40420, saving model to best.model\n",
      "0s - loss: 0.5504 - acc: 0.8202 - val_loss: 0.4042 - val_acc: 0.9565\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.40420 to 0.39556, saving model to best.model\n",
      "0s - loss: 0.5527 - acc: 0.7640 - val_loss: 0.3956 - val_acc: 0.9565\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.39556 to 0.38712, saving model to best.model\n",
      "0s - loss: 0.5799 - acc: 0.7865 - val_loss: 0.3871 - val_acc: 0.9565\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.38712 to 0.37905, saving model to best.model\n",
      "0s - loss: 0.6071 - acc: 0.7753 - val_loss: 0.3790 - val_acc: 0.9565\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.37905 to 0.37109, saving model to best.model\n",
      "0s - loss: 0.5296 - acc: 0.8202 - val_loss: 0.3711 - val_acc: 0.9565\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.37109 to 0.36331, saving model to best.model\n",
      "0s - loss: 0.4963 - acc: 0.8427 - val_loss: 0.3633 - val_acc: 0.9565\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.36331 to 0.35567, saving model to best.model\n",
      "0s - loss: 0.5237 - acc: 0.7978 - val_loss: 0.3557 - val_acc: 0.9565\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.35567 to 0.34819, saving model to best.model\n",
      "0s - loss: 0.4454 - acc: 0.8539 - val_loss: 0.3482 - val_acc: 0.9565\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.34819 to 0.34098, saving model to best.model\n",
      "0s - loss: 0.5416 - acc: 0.7753 - val_loss: 0.3410 - val_acc: 0.9565\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.34098 to 0.33403, saving model to best.model\n",
      "0s - loss: 0.4195 - acc: 0.8764 - val_loss: 0.3340 - val_acc: 0.9565\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.33403 to 0.32711, saving model to best.model\n",
      "0s - loss: 0.4338 - acc: 0.8989 - val_loss: 0.3271 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.32711 to 0.32017, saving model to best.model\n",
      "0s - loss: 0.3878 - acc: 0.8764 - val_loss: 0.3202 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.32017 to 0.31328, saving model to best.model\n",
      "0s - loss: 0.3994 - acc: 0.8876 - val_loss: 0.3133 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.31328 to 0.30647, saving model to best.model\n",
      "0s - loss: 0.3925 - acc: 0.8652 - val_loss: 0.3065 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.30647 to 0.29937, saving model to best.model\n",
      "0s - loss: 0.4302 - acc: 0.8202 - val_loss: 0.2994 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.29937 to 0.29238, saving model to best.model\n",
      "0s - loss: 0.4652 - acc: 0.8090 - val_loss: 0.2924 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.29238 to 0.28555, saving model to best.model\n",
      "0s - loss: 0.4013 - acc: 0.8652 - val_loss: 0.2856 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.28555 to 0.27886, saving model to best.model\n",
      "0s - loss: 0.3953 - acc: 0.8427 - val_loss: 0.2789 - val_acc: 0.9565\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.27886 to 0.27243, saving model to best.model\n",
      "0s - loss: 0.3797 - acc: 0.8539 - val_loss: 0.2724 - val_acc: 0.9565\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.27243 to 0.26590, saving model to best.model\n",
      "0s - loss: 0.3693 - acc: 0.9101 - val_loss: 0.2659 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.26590 to 0.25935, saving model to best.model\n",
      "0s - loss: 0.3745 - acc: 0.8539 - val_loss: 0.2594 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.25935 to 0.25287, saving model to best.model\n",
      "0s - loss: 0.3280 - acc: 0.8876 - val_loss: 0.2529 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.25287 to 0.24667, saving model to best.model\n",
      "0s - loss: 0.3927 - acc: 0.8427 - val_loss: 0.2467 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.24667 to 0.24080, saving model to best.model\n",
      "0s - loss: 0.2928 - acc: 0.9213 - val_loss: 0.2408 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.24080 to 0.23513, saving model to best.model\n",
      "0s - loss: 0.3430 - acc: 0.9101 - val_loss: 0.2351 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.23513 to 0.23002, saving model to best.model\n",
      "0s - loss: 0.3736 - acc: 0.8427 - val_loss: 0.2300 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.23002 to 0.22494, saving model to best.model\n",
      "0s - loss: 0.4075 - acc: 0.8202 - val_loss: 0.2249 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.22494 to 0.22000, saving model to best.model\n",
      "0s - loss: 0.3009 - acc: 0.9213 - val_loss: 0.2200 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.22000 to 0.21549, saving model to best.model\n",
      "0s - loss: 0.2821 - acc: 0.9101 - val_loss: 0.2155 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.21549 to 0.21081, saving model to best.model\n",
      "0s - loss: 0.3469 - acc: 0.9326 - val_loss: 0.2108 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.21081 to 0.20598, saving model to best.model\n",
      "0s - loss: 0.3099 - acc: 0.8876 - val_loss: 0.2060 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.20598 to 0.20039, saving model to best.model\n",
      "0s - loss: 0.3325 - acc: 0.8989 - val_loss: 0.2004 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.20039 to 0.19487, saving model to best.model\n",
      "0s - loss: 0.3026 - acc: 0.8876 - val_loss: 0.1949 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.19487 to 0.18933, saving model to best.model\n",
      "0s - loss: 0.2985 - acc: 0.8764 - val_loss: 0.1893 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.18933 to 0.18368, saving model to best.model\n",
      "0s - loss: 0.3124 - acc: 0.8876 - val_loss: 0.1837 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.18368 to 0.17852, saving model to best.model\n",
      "0s - loss: 0.3247 - acc: 0.8989 - val_loss: 0.1785 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.17852 to 0.17345, saving model to best.model\n",
      "0s - loss: 0.3680 - acc: 0.8315 - val_loss: 0.1735 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.17345 to 0.16849, saving model to best.model\n",
      "0s - loss: 0.2918 - acc: 0.8876 - val_loss: 0.1685 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.16849 to 0.16360, saving model to best.model\n",
      "0s - loss: 0.3539 - acc: 0.8764 - val_loss: 0.1636 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.16360 to 0.15898, saving model to best.model\n",
      "0s - loss: 0.2648 - acc: 0.9101 - val_loss: 0.1590 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.15898 to 0.15481, saving model to best.model\n",
      "0s - loss: 0.3519 - acc: 0.8315 - val_loss: 0.1548 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.15481 to 0.15099, saving model to best.model\n",
      "0s - loss: 0.3275 - acc: 0.8764 - val_loss: 0.1510 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.15099 to 0.14714, saving model to best.model\n",
      "0s - loss: 0.2417 - acc: 0.9213 - val_loss: 0.1471 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.14714 to 0.14363, saving model to best.model\n",
      "0s - loss: 0.2049 - acc: 0.9213 - val_loss: 0.1436 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.14363 to 0.14017, saving model to best.model\n",
      "0s - loss: 0.2597 - acc: 0.9326 - val_loss: 0.1402 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.14017 to 0.13679, saving model to best.model\n",
      "0s - loss: 0.2374 - acc: 0.9213 - val_loss: 0.1368 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.13679 to 0.13360, saving model to best.model\n",
      "0s - loss: 0.2510 - acc: 0.9101 - val_loss: 0.1336 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.13360 to 0.13066, saving model to best.model\n",
      "0s - loss: 0.2563 - acc: 0.9101 - val_loss: 0.1307 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.13066 to 0.12807, saving model to best.model\n",
      "0s - loss: 0.2349 - acc: 0.9326 - val_loss: 0.1281 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.12807 to 0.12568, saving model to best.model\n",
      "0s - loss: 0.2534 - acc: 0.9101 - val_loss: 0.1257 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.12568 to 0.12348, saving model to best.model\n",
      "0s - loss: 0.1583 - acc: 0.9888 - val_loss: 0.1235 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.12348 to 0.12165, saving model to best.model\n",
      "0s - loss: 0.2275 - acc: 0.9213 - val_loss: 0.1216 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.12165 to 0.12002, saving model to best.model\n",
      "0s - loss: 0.2543 - acc: 0.8989 - val_loss: 0.1200 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.12002 to 0.11816, saving model to best.model\n",
      "0s - loss: 0.2267 - acc: 0.9213 - val_loss: 0.1182 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.11816 to 0.11615, saving model to best.model\n",
      "0s - loss: 0.2269 - acc: 0.9438 - val_loss: 0.1161 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.11615 to 0.11430, saving model to best.model\n",
      "0s - loss: 0.2576 - acc: 0.9326 - val_loss: 0.1143 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.11430 to 0.11266, saving model to best.model\n",
      "0s - loss: 0.2661 - acc: 0.8876 - val_loss: 0.1127 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.11266 to 0.11054, saving model to best.model\n",
      "0s - loss: 0.2032 - acc: 0.9551 - val_loss: 0.1105 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.11054 to 0.10818, saving model to best.model\n",
      "0s - loss: 0.2204 - acc: 0.9663 - val_loss: 0.1082 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.10818 to 0.10547, saving model to best.model\n",
      "0s - loss: 0.1661 - acc: 0.9551 - val_loss: 0.1055 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.10547 to 0.10224, saving model to best.model\n",
      "0s - loss: 0.1847 - acc: 0.9551 - val_loss: 0.1022 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.10224 to 0.09897, saving model to best.model\n",
      "0s - loss: 0.2243 - acc: 0.9438 - val_loss: 0.0990 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.09897 to 0.09617, saving model to best.model\n",
      "0s - loss: 0.1681 - acc: 0.9438 - val_loss: 0.0962 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.09617 to 0.09354, saving model to best.model\n",
      "0s - loss: 0.1924 - acc: 0.9551 - val_loss: 0.0935 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.09354 to 0.09139, saving model to best.model\n",
      "0s - loss: 0.2040 - acc: 0.9326 - val_loss: 0.0914 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.09139 to 0.08869, saving model to best.model\n",
      "0s - loss: 0.2190 - acc: 0.9551 - val_loss: 0.0887 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.08869 to 0.08630, saving model to best.model\n",
      "0s - loss: 0.2013 - acc: 0.9213 - val_loss: 0.0863 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.08630 to 0.08384, saving model to best.model\n",
      "0s - loss: 0.1674 - acc: 0.9551 - val_loss: 0.0838 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.08384 to 0.08150, saving model to best.model\n",
      "0s - loss: 0.1838 - acc: 0.9213 - val_loss: 0.0815 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.08150 to 0.07953, saving model to best.model\n",
      "0s - loss: 0.2272 - acc: 0.9101 - val_loss: 0.0795 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.07953 to 0.07755, saving model to best.model\n",
      "0s - loss: 0.1212 - acc: 0.9775 - val_loss: 0.0776 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.07755 to 0.07569, saving model to best.model\n",
      "0s - loss: 0.2191 - acc: 0.9213 - val_loss: 0.0757 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.07569 to 0.07406, saving model to best.model\n",
      "0s - loss: 0.1881 - acc: 0.9101 - val_loss: 0.0741 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.07406 to 0.07257, saving model to best.model\n",
      "0s - loss: 0.1548 - acc: 0.9438 - val_loss: 0.0726 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.07257 to 0.07137, saving model to best.model\n",
      "0s - loss: 0.1402 - acc: 0.9663 - val_loss: 0.0714 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.07137 to 0.07047, saving model to best.model\n",
      "0s - loss: 0.1783 - acc: 0.9326 - val_loss: 0.0705 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.05619, saving model to best.model\n",
      "0s - loss: 1.2426 - acc: 0.3708 - val_loss: 1.0562 - val_acc: 0.5217\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.05619 to 1.04599, saving model to best.model\n",
      "0s - loss: 1.2342 - acc: 0.4045 - val_loss: 1.0460 - val_acc: 0.5217\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.04599 to 1.04438, saving model to best.model\n",
      "0s - loss: 1.2719 - acc: 0.3258 - val_loss: 1.0444 - val_acc: 0.5217\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.04438 to 1.04333, saving model to best.model\n",
      "0s - loss: 1.2561 - acc: 0.3146 - val_loss: 1.0433 - val_acc: 0.5217\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.04333 to 1.03900, saving model to best.model\n",
      "0s - loss: 1.1961 - acc: 0.3933 - val_loss: 1.0390 - val_acc: 0.5217\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.03900 to 1.03399, saving model to best.model\n",
      "0s - loss: 1.1900 - acc: 0.3596 - val_loss: 1.0340 - val_acc: 0.5217\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.03399 to 1.03149, saving model to best.model\n",
      "0s - loss: 1.2052 - acc: 0.4045 - val_loss: 1.0315 - val_acc: 0.5217\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.03149 to 1.02974, saving model to best.model\n",
      "0s - loss: 1.2148 - acc: 0.3708 - val_loss: 1.0297 - val_acc: 0.5217\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1638 - acc: 0.4831 - val_loss: 1.0308 - val_acc: 0.5217\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2379 - acc: 0.3933 - val_loss: 1.0321 - val_acc: 0.5217\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2212 - acc: 0.3146 - val_loss: 1.0333 - val_acc: 0.5217\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1506 - acc: 0.3371 - val_loss: 1.0357 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2153 - acc: 0.3371 - val_loss: 1.0395 - val_acc: 0.5217\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2221 - acc: 0.4045 - val_loss: 1.0418 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2265 - acc: 0.4045 - val_loss: 1.0410 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2335 - acc: 0.3483 - val_loss: 1.0391 - val_acc: 0.5217\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1405 - acc: 0.3708 - val_loss: 1.0361 - val_acc: 0.5217\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.02974 to 1.02805, saving model to best.model\n",
      "0s - loss: 1.2543 - acc: 0.3596 - val_loss: 1.0280 - val_acc: 0.5217\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.02805 to 1.01935, saving model to best.model\n",
      "0s - loss: 1.1542 - acc: 0.3933 - val_loss: 1.0194 - val_acc: 0.5217\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.01935 to 1.01110, saving model to best.model\n",
      "0s - loss: 1.1375 - acc: 0.4270 - val_loss: 1.0111 - val_acc: 0.5217\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.01110 to 1.00364, saving model to best.model\n",
      "0s - loss: 1.1814 - acc: 0.4157 - val_loss: 1.0036 - val_acc: 0.5217\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.00364 to 0.99691, saving model to best.model\n",
      "0s - loss: 1.2332 - acc: 0.3258 - val_loss: 0.9969 - val_acc: 0.5217\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.99691 to 0.99104, saving model to best.model\n",
      "0s - loss: 1.0239 - acc: 0.4157 - val_loss: 0.9910 - val_acc: 0.5217\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.99104 to 0.98746, saving model to best.model\n",
      "0s - loss: 1.1081 - acc: 0.4494 - val_loss: 0.9875 - val_acc: 0.5217\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.98746 to 0.98680, saving model to best.model\n",
      "0s - loss: 1.2495 - acc: 0.3034 - val_loss: 0.9868 - val_acc: 0.5217\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.98680 to 0.98550, saving model to best.model\n",
      "0s - loss: 1.1215 - acc: 0.4157 - val_loss: 0.9855 - val_acc: 0.5217\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.98550 to 0.98415, saving model to best.model\n",
      "0s - loss: 1.1059 - acc: 0.4045 - val_loss: 0.9842 - val_acc: 0.5217\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.98415 to 0.98356, saving model to best.model\n",
      "0s - loss: 1.1160 - acc: 0.3820 - val_loss: 0.9836 - val_acc: 0.5217\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.98356 to 0.98324, saving model to best.model\n",
      "0s - loss: 1.2173 - acc: 0.2809 - val_loss: 0.9832 - val_acc: 0.5217\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.98324 to 0.98222, saving model to best.model\n",
      "0s - loss: 1.0289 - acc: 0.5056 - val_loss: 0.9822 - val_acc: 0.5217\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.98222 to 0.98118, saving model to best.model\n",
      "0s - loss: 1.1250 - acc: 0.3596 - val_loss: 0.9812 - val_acc: 0.5217\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.98118 to 0.97916, saving model to best.model\n",
      "0s - loss: 1.1834 - acc: 0.3708 - val_loss: 0.9792 - val_acc: 0.5217\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.97916 to 0.97808, saving model to best.model\n",
      "0s - loss: 1.1414 - acc: 0.4045 - val_loss: 0.9781 - val_acc: 0.5217\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.97808 to 0.97745, saving model to best.model\n",
      "0s - loss: 1.1014 - acc: 0.4270 - val_loss: 0.9774 - val_acc: 0.5217\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.97745 to 0.97565, saving model to best.model\n",
      "0s - loss: 1.1267 - acc: 0.4719 - val_loss: 0.9757 - val_acc: 0.5217\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.97565 to 0.97365, saving model to best.model\n",
      "0s - loss: 1.0907 - acc: 0.3596 - val_loss: 0.9737 - val_acc: 0.5217\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.97365 to 0.97118, saving model to best.model\n",
      "0s - loss: 1.1273 - acc: 0.4157 - val_loss: 0.9712 - val_acc: 0.5217\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.97118 to 0.96979, saving model to best.model\n",
      "0s - loss: 1.1539 - acc: 0.3596 - val_loss: 0.9698 - val_acc: 0.5217\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.96979 to 0.96823, saving model to best.model\n",
      "0s - loss: 1.0565 - acc: 0.4157 - val_loss: 0.9682 - val_acc: 0.5217\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.96823 to 0.96591, saving model to best.model\n",
      "0s - loss: 1.1366 - acc: 0.4045 - val_loss: 0.9659 - val_acc: 0.5217\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.96591 to 0.96331, saving model to best.model\n",
      "0s - loss: 1.0293 - acc: 0.4831 - val_loss: 0.9633 - val_acc: 0.5217\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.96331 to 0.96157, saving model to best.model\n",
      "0s - loss: 1.0639 - acc: 0.4157 - val_loss: 0.9616 - val_acc: 0.5217\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.96157 to 0.95848, saving model to best.model\n",
      "0s - loss: 1.0370 - acc: 0.4270 - val_loss: 0.9585 - val_acc: 0.5217\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.95848 to 0.95384, saving model to best.model\n",
      "0s - loss: 1.0682 - acc: 0.4382 - val_loss: 0.9538 - val_acc: 0.6522\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.95384 to 0.94814, saving model to best.model\n",
      "0s - loss: 0.9801 - acc: 0.5393 - val_loss: 0.9481 - val_acc: 0.6522\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.94814 to 0.94176, saving model to best.model\n",
      "0s - loss: 1.0232 - acc: 0.4382 - val_loss: 0.9418 - val_acc: 0.6957\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.94176 to 0.93497, saving model to best.model\n",
      "0s - loss: 1.1390 - acc: 0.3483 - val_loss: 0.9350 - val_acc: 0.6957\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.93497 to 0.92784, saving model to best.model\n",
      "0s - loss: 1.0221 - acc: 0.4831 - val_loss: 0.9278 - val_acc: 0.6957\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.92784 to 0.92051, saving model to best.model\n",
      "0s - loss: 1.0641 - acc: 0.4157 - val_loss: 0.9205 - val_acc: 0.6957\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.92051 to 0.91241, saving model to best.model\n",
      "0s - loss: 1.0477 - acc: 0.4270 - val_loss: 0.9124 - val_acc: 0.7391\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.91241 to 0.90552, saving model to best.model\n",
      "0s - loss: 1.0513 - acc: 0.4382 - val_loss: 0.9055 - val_acc: 0.7391\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.90552 to 0.89958, saving model to best.model\n",
      "0s - loss: 0.9982 - acc: 0.5393 - val_loss: 0.8996 - val_acc: 0.7391\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.89958 to 0.89382, saving model to best.model\n",
      "0s - loss: 1.0571 - acc: 0.4045 - val_loss: 0.8938 - val_acc: 0.7391\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.89382 to 0.88789, saving model to best.model\n",
      "0s - loss: 1.0341 - acc: 0.4607 - val_loss: 0.8879 - val_acc: 0.7391\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.88789 to 0.88154, saving model to best.model\n",
      "0s - loss: 0.9230 - acc: 0.6067 - val_loss: 0.8815 - val_acc: 0.7391\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.88154 to 0.87464, saving model to best.model\n",
      "0s - loss: 0.9518 - acc: 0.5281 - val_loss: 0.8746 - val_acc: 0.7391\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.87464 to 0.86717, saving model to best.model\n",
      "0s - loss: 1.0178 - acc: 0.4831 - val_loss: 0.8672 - val_acc: 0.8261\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.86717 to 0.85965, saving model to best.model\n",
      "0s - loss: 0.9778 - acc: 0.5169 - val_loss: 0.8596 - val_acc: 0.8696\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.85965 to 0.85279, saving model to best.model\n",
      "0s - loss: 1.0304 - acc: 0.4494 - val_loss: 0.8528 - val_acc: 0.9130\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.85279 to 0.84590, saving model to best.model\n",
      "0s - loss: 1.0337 - acc: 0.4270 - val_loss: 0.8459 - val_acc: 0.9565\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.84590 to 0.83797, saving model to best.model\n",
      "0s - loss: 0.9436 - acc: 0.5056 - val_loss: 0.8380 - val_acc: 0.9565\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.83797 to 0.83012, saving model to best.model\n",
      "0s - loss: 1.0196 - acc: 0.4270 - val_loss: 0.8301 - val_acc: 1.0000\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.83012 to 0.82209, saving model to best.model\n",
      "0s - loss: 0.9598 - acc: 0.5169 - val_loss: 0.8221 - val_acc: 1.0000\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.82209 to 0.81402, saving model to best.model\n",
      "0s - loss: 0.9115 - acc: 0.4944 - val_loss: 0.8140 - val_acc: 1.0000\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.81402 to 0.80601, saving model to best.model\n",
      "0s - loss: 1.0196 - acc: 0.4382 - val_loss: 0.8060 - val_acc: 1.0000\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.80601 to 0.79775, saving model to best.model\n",
      "0s - loss: 0.9458 - acc: 0.5730 - val_loss: 0.7977 - val_acc: 1.0000\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.79775 to 0.78903, saving model to best.model\n",
      "0s - loss: 0.8994 - acc: 0.5281 - val_loss: 0.7890 - val_acc: 1.0000\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.78903 to 0.78044, saving model to best.model\n",
      "0s - loss: 1.0027 - acc: 0.4382 - val_loss: 0.7804 - val_acc: 1.0000\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.78044 to 0.77229, saving model to best.model\n",
      "0s - loss: 0.9234 - acc: 0.5843 - val_loss: 0.7723 - val_acc: 1.0000\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.77229 to 0.76403, saving model to best.model\n",
      "0s - loss: 0.9386 - acc: 0.5393 - val_loss: 0.7640 - val_acc: 1.0000\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.76403 to 0.75492, saving model to best.model\n",
      "0s - loss: 1.0031 - acc: 0.4494 - val_loss: 0.7549 - val_acc: 1.0000\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.75492 to 0.74423, saving model to best.model\n",
      "0s - loss: 0.8572 - acc: 0.6742 - val_loss: 0.7442 - val_acc: 1.0000\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.74423 to 0.73329, saving model to best.model\n",
      "0s - loss: 0.8760 - acc: 0.5955 - val_loss: 0.7333 - val_acc: 1.0000\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.73329 to 0.72290, saving model to best.model\n",
      "0s - loss: 0.8889 - acc: 0.5056 - val_loss: 0.7229 - val_acc: 1.0000\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.72290 to 0.71218, saving model to best.model\n",
      "0s - loss: 0.8410 - acc: 0.5730 - val_loss: 0.7122 - val_acc: 1.0000\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71218 to 0.70036, saving model to best.model\n",
      "0s - loss: 0.8131 - acc: 0.6404 - val_loss: 0.7004 - val_acc: 1.0000\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70036 to 0.68895, saving model to best.model\n",
      "0s - loss: 0.7766 - acc: 0.6404 - val_loss: 0.6890 - val_acc: 1.0000\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.68895 to 0.67834, saving model to best.model\n",
      "0s - loss: 0.8047 - acc: 0.6517 - val_loss: 0.6783 - val_acc: 1.0000\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.67834 to 0.66830, saving model to best.model\n",
      "0s - loss: 0.7647 - acc: 0.6742 - val_loss: 0.6683 - val_acc: 1.0000\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.66830 to 0.65772, saving model to best.model\n",
      "0s - loss: 0.8048 - acc: 0.6404 - val_loss: 0.6577 - val_acc: 0.9565\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.65772 to 0.64720, saving model to best.model\n",
      "0s - loss: 0.8312 - acc: 0.6292 - val_loss: 0.6472 - val_acc: 0.9565\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.64720 to 0.63740, saving model to best.model\n",
      "0s - loss: 0.8385 - acc: 0.6517 - val_loss: 0.6374 - val_acc: 0.9565\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.63740 to 0.62712, saving model to best.model\n",
      "0s - loss: 0.7837 - acc: 0.6629 - val_loss: 0.6271 - val_acc: 0.9565\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.62712 to 0.61696, saving model to best.model\n",
      "0s - loss: 0.7958 - acc: 0.6067 - val_loss: 0.6170 - val_acc: 0.9565\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.61696 to 0.60609, saving model to best.model\n",
      "0s - loss: 0.7574 - acc: 0.6629 - val_loss: 0.6061 - val_acc: 0.9565\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.60609 to 0.59417, saving model to best.model\n",
      "0s - loss: 0.7124 - acc: 0.7528 - val_loss: 0.5942 - val_acc: 0.9565\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.59417 to 0.58195, saving model to best.model\n",
      "0s - loss: 0.7726 - acc: 0.6180 - val_loss: 0.5820 - val_acc: 0.9565\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.58195 to 0.57033, saving model to best.model\n",
      "0s - loss: 0.7154 - acc: 0.7191 - val_loss: 0.5703 - val_acc: 0.9565\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.57033 to 0.55816, saving model to best.model\n",
      "0s - loss: 0.6969 - acc: 0.7191 - val_loss: 0.5582 - val_acc: 0.9565\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.55816 to 0.54530, saving model to best.model\n",
      "0s - loss: 0.6286 - acc: 0.7865 - val_loss: 0.5453 - val_acc: 0.9565\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.54530 to 0.53385, saving model to best.model\n",
      "0s - loss: 0.7129 - acc: 0.7416 - val_loss: 0.5339 - val_acc: 1.0000\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.53385 to 0.52229, saving model to best.model\n",
      "0s - loss: 0.7244 - acc: 0.6517 - val_loss: 0.5223 - val_acc: 1.0000\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.52229 to 0.51083, saving model to best.model\n",
      "0s - loss: 0.7523 - acc: 0.6854 - val_loss: 0.5108 - val_acc: 0.9565\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.51083 to 0.49921, saving model to best.model\n",
      "0s - loss: 0.6536 - acc: 0.7416 - val_loss: 0.4992 - val_acc: 0.9565\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.49921 to 0.48682, saving model to best.model\n",
      "0s - loss: 0.6811 - acc: 0.7191 - val_loss: 0.4868 - val_acc: 0.9565\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.48682 to 0.47524, saving model to best.model\n",
      "0s - loss: 0.6497 - acc: 0.7416 - val_loss: 0.4752 - val_acc: 0.9565\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.47524 to 0.46397, saving model to best.model\n",
      "0s - loss: 0.6003 - acc: 0.7753 - val_loss: 0.4640 - val_acc: 0.9565\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.46397 to 0.45376, saving model to best.model\n",
      "0s - loss: 0.5950 - acc: 0.7528 - val_loss: 0.4538 - val_acc: 0.9565\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.45376 to 0.44421, saving model to best.model\n",
      "0s - loss: 0.6461 - acc: 0.7079 - val_loss: 0.4442 - val_acc: 0.9565\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.44421 to 0.43502, saving model to best.model\n",
      "0s - loss: 0.6039 - acc: 0.7191 - val_loss: 0.4350 - val_acc: 0.9565\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.43502 to 0.42734, saving model to best.model\n",
      "0s - loss: 0.6195 - acc: 0.7416 - val_loss: 0.4273 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.42734 to 0.42044, saving model to best.model\n",
      "0s - loss: 0.5872 - acc: 0.7753 - val_loss: 0.4204 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.42044 to 0.41279, saving model to best.model\n",
      "0s - loss: 0.5408 - acc: 0.8315 - val_loss: 0.4128 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.41279 to 0.40581, saving model to best.model\n",
      "0s - loss: 0.5655 - acc: 0.7865 - val_loss: 0.4058 - val_acc: 0.9565\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.40581 to 0.39930, saving model to best.model\n",
      "0s - loss: 0.5720 - acc: 0.7978 - val_loss: 0.3993 - val_acc: 0.9565\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.39930 to 0.39207, saving model to best.model\n",
      "0s - loss: 0.5336 - acc: 0.7640 - val_loss: 0.3921 - val_acc: 0.9565\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.39207 to 0.38525, saving model to best.model\n",
      "0s - loss: 0.5408 - acc: 0.7640 - val_loss: 0.3853 - val_acc: 0.9565\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.38525 to 0.37849, saving model to best.model\n",
      "0s - loss: 0.5088 - acc: 0.8315 - val_loss: 0.3785 - val_acc: 0.9565\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.37849 to 0.37093, saving model to best.model\n",
      "0s - loss: 0.5386 - acc: 0.7640 - val_loss: 0.3709 - val_acc: 0.9565\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.37093 to 0.36485, saving model to best.model\n",
      "0s - loss: 0.5558 - acc: 0.7865 - val_loss: 0.3649 - val_acc: 0.9565\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.36485 to 0.35872, saving model to best.model\n",
      "0s - loss: 0.5114 - acc: 0.8427 - val_loss: 0.3587 - val_acc: 0.9130\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.35872 to 0.35357, saving model to best.model\n",
      "0s - loss: 0.5129 - acc: 0.8202 - val_loss: 0.3536 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.35357 to 0.34800, saving model to best.model\n",
      "0s - loss: 0.4623 - acc: 0.8652 - val_loss: 0.3480 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.34800 to 0.34158, saving model to best.model\n",
      "0s - loss: 0.4482 - acc: 0.8652 - val_loss: 0.3416 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.34158 to 0.33507, saving model to best.model\n",
      "0s - loss: 0.4637 - acc: 0.8427 - val_loss: 0.3351 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.33507 to 0.32741, saving model to best.model\n",
      "0s - loss: 0.5200 - acc: 0.7753 - val_loss: 0.3274 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.32741 to 0.31928, saving model to best.model\n",
      "0s - loss: 0.4444 - acc: 0.7978 - val_loss: 0.3193 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.31928 to 0.31075, saving model to best.model\n",
      "0s - loss: 0.4615 - acc: 0.8202 - val_loss: 0.3107 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.31075 to 0.30245, saving model to best.model\n",
      "0s - loss: 0.4799 - acc: 0.7978 - val_loss: 0.3025 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.30245 to 0.29415, saving model to best.model\n",
      "0s - loss: 0.4477 - acc: 0.8427 - val_loss: 0.2941 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.29415 to 0.28623, saving model to best.model\n",
      "0s - loss: 0.4253 - acc: 0.8652 - val_loss: 0.2862 - val_acc: 0.9565\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.28623 to 0.27932, saving model to best.model\n",
      "0s - loss: 0.4372 - acc: 0.8202 - val_loss: 0.2793 - val_acc: 0.9565\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.27932 to 0.27305, saving model to best.model\n",
      "0s - loss: 0.4144 - acc: 0.8090 - val_loss: 0.2731 - val_acc: 0.9565\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.27305 to 0.26814, saving model to best.model\n",
      "0s - loss: 0.5143 - acc: 0.7753 - val_loss: 0.2681 - val_acc: 0.9565\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.26814 to 0.26393, saving model to best.model\n",
      "0s - loss: 0.4537 - acc: 0.8315 - val_loss: 0.2639 - val_acc: 0.9565\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.26393 to 0.26067, saving model to best.model\n",
      "0s - loss: 0.4397 - acc: 0.8427 - val_loss: 0.2607 - val_acc: 0.9565\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.26067 to 0.25761, saving model to best.model\n",
      "0s - loss: 0.4617 - acc: 0.7528 - val_loss: 0.2576 - val_acc: 0.9565\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.25761 to 0.25483, saving model to best.model\n",
      "0s - loss: 0.4083 - acc: 0.8989 - val_loss: 0.2548 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.25483 to 0.25188, saving model to best.model\n",
      "0s - loss: 0.4365 - acc: 0.8315 - val_loss: 0.2519 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.25188 to 0.24955, saving model to best.model\n",
      "0s - loss: 0.4344 - acc: 0.8090 - val_loss: 0.2496 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.24955 to 0.24751, saving model to best.model\n",
      "0s - loss: 0.4232 - acc: 0.8202 - val_loss: 0.2475 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.24751 to 0.24588, saving model to best.model\n",
      "0s - loss: 0.4274 - acc: 0.8539 - val_loss: 0.2459 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.24588 to 0.24436, saving model to best.model\n",
      "0s - loss: 0.3561 - acc: 0.8652 - val_loss: 0.2444 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.24436 to 0.24261, saving model to best.model\n",
      "0s - loss: 0.3647 - acc: 0.8652 - val_loss: 0.2426 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.24261 to 0.24068, saving model to best.model\n",
      "0s - loss: 0.3299 - acc: 0.9213 - val_loss: 0.2407 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.24068 to 0.23829, saving model to best.model\n",
      "0s - loss: 0.3521 - acc: 0.8539 - val_loss: 0.2383 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.23829 to 0.23625, saving model to best.model\n",
      "0s - loss: 0.3403 - acc: 0.8989 - val_loss: 0.2362 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.23625 to 0.23271, saving model to best.model\n",
      "0s - loss: 0.3558 - acc: 0.8427 - val_loss: 0.2327 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.23271 to 0.22778, saving model to best.model\n",
      "0s - loss: 0.3520 - acc: 0.8764 - val_loss: 0.2278 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.22778 to 0.22143, saving model to best.model\n",
      "0s - loss: 0.3791 - acc: 0.8876 - val_loss: 0.2214 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.22143 to 0.21467, saving model to best.model\n",
      "0s - loss: 0.3708 - acc: 0.8652 - val_loss: 0.2147 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.21467 to 0.20718, saving model to best.model\n",
      "0s - loss: 0.3415 - acc: 0.8876 - val_loss: 0.2072 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.20718 to 0.20104, saving model to best.model\n",
      "0s - loss: 0.3531 - acc: 0.8989 - val_loss: 0.2010 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.20104 to 0.19410, saving model to best.model\n",
      "0s - loss: 0.3328 - acc: 0.8876 - val_loss: 0.1941 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19410 to 0.18837, saving model to best.model\n",
      "0s - loss: 0.3141 - acc: 0.9101 - val_loss: 0.1884 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.18837 to 0.18228, saving model to best.model\n",
      "0s - loss: 0.3009 - acc: 0.9101 - val_loss: 0.1823 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.18228 to 0.17686, saving model to best.model\n",
      "0s - loss: 0.2511 - acc: 0.9775 - val_loss: 0.1769 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.17686 to 0.17096, saving model to best.model\n",
      "0s - loss: 0.2822 - acc: 0.9213 - val_loss: 0.1710 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.17096 to 0.16535, saving model to best.model\n",
      "0s - loss: 0.2574 - acc: 0.9326 - val_loss: 0.1653 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.16535 to 0.16009, saving model to best.model\n",
      "0s - loss: 0.3089 - acc: 0.8989 - val_loss: 0.1601 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.16009 to 0.15528, saving model to best.model\n",
      "0s - loss: 0.3463 - acc: 0.8876 - val_loss: 0.1553 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.15528 to 0.15123, saving model to best.model\n",
      "0s - loss: 0.2899 - acc: 0.8652 - val_loss: 0.1512 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.15123 to 0.14682, saving model to best.model\n",
      "0s - loss: 0.3166 - acc: 0.8652 - val_loss: 0.1468 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.14682 to 0.14268, saving model to best.model\n",
      "0s - loss: 0.2580 - acc: 0.9213 - val_loss: 0.1427 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.14268 to 0.13896, saving model to best.model\n",
      "0s - loss: 0.2758 - acc: 0.9213 - val_loss: 0.1390 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.13896 to 0.13583, saving model to best.model\n",
      "0s - loss: 0.2609 - acc: 0.9438 - val_loss: 0.1358 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.13583 to 0.13360, saving model to best.model\n",
      "0s - loss: 0.2863 - acc: 0.8876 - val_loss: 0.1336 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.13360 to 0.13196, saving model to best.model\n",
      "0s - loss: 0.2726 - acc: 0.9101 - val_loss: 0.1320 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.13196 to 0.12987, saving model to best.model\n",
      "0s - loss: 0.2541 - acc: 0.8989 - val_loss: 0.1299 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.12987 to 0.12806, saving model to best.model\n",
      "0s - loss: 0.2831 - acc: 0.9213 - val_loss: 0.1281 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.12806 to 0.12647, saving model to best.model\n",
      "0s - loss: 0.2383 - acc: 0.9326 - val_loss: 0.1265 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.12647 to 0.12408, saving model to best.model\n",
      "0s - loss: 0.2333 - acc: 0.9213 - val_loss: 0.1241 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.12408 to 0.12144, saving model to best.model\n",
      "0s - loss: 0.2107 - acc: 0.9326 - val_loss: 0.1214 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.12144 to 0.11970, saving model to best.model\n",
      "0s - loss: 0.2524 - acc: 0.9213 - val_loss: 0.1197 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.11970 to 0.11780, saving model to best.model\n",
      "0s - loss: 0.2448 - acc: 0.9326 - val_loss: 0.1178 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.11780 to 0.11614, saving model to best.model\n",
      "0s - loss: 0.2642 - acc: 0.8989 - val_loss: 0.1161 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.11614 to 0.11444, saving model to best.model\n",
      "0s - loss: 0.2324 - acc: 0.9438 - val_loss: 0.1144 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.11444 to 0.11277, saving model to best.model\n",
      "0s - loss: 0.3225 - acc: 0.8652 - val_loss: 0.1128 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.11277 to 0.11141, saving model to best.model\n",
      "0s - loss: 0.2320 - acc: 0.9326 - val_loss: 0.1114 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.11141 to 0.11043, saving model to best.model\n",
      "0s - loss: 0.2731 - acc: 0.9101 - val_loss: 0.1104 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.11043 to 0.10860, saving model to best.model\n",
      "0s - loss: 0.2142 - acc: 0.9213 - val_loss: 0.1086 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.10860 to 0.10729, saving model to best.model\n",
      "0s - loss: 0.1862 - acc: 0.9438 - val_loss: 0.1073 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.10729 to 0.10664, saving model to best.model\n",
      "0s - loss: 0.1977 - acc: 0.9326 - val_loss: 0.1066 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.10664 to 0.10590, saving model to best.model\n",
      "0s - loss: 0.2169 - acc: 0.9551 - val_loss: 0.1059 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.10590 to 0.10511, saving model to best.model\n",
      "0s - loss: 0.2234 - acc: 0.9213 - val_loss: 0.1051 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.10511 to 0.10404, saving model to best.model\n",
      "0s - loss: 0.2011 - acc: 0.9326 - val_loss: 0.1040 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.10404 to 0.10293, saving model to best.model\n",
      "0s - loss: 0.2372 - acc: 0.9101 - val_loss: 0.1029 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.10293 to 0.10140, saving model to best.model\n",
      "0s - loss: 0.2367 - acc: 0.9326 - val_loss: 0.1014 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.10140 to 0.09865, saving model to best.model\n",
      "0s - loss: 0.2178 - acc: 0.9101 - val_loss: 0.0986 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.09865 to 0.09464, saving model to best.model\n",
      "0s - loss: 0.2104 - acc: 0.9438 - val_loss: 0.0946 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.09464 to 0.09028, saving model to best.model\n",
      "0s - loss: 0.2423 - acc: 0.8876 - val_loss: 0.0903 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.09028 to 0.08688, saving model to best.model\n",
      "0s - loss: 0.1949 - acc: 0.9213 - val_loss: 0.0869 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.08688 to 0.08352, saving model to best.model\n",
      "0s - loss: 0.1569 - acc: 0.9551 - val_loss: 0.0835 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.08352 to 0.08010, saving model to best.model\n",
      "0s - loss: 0.1824 - acc: 0.9663 - val_loss: 0.0801 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.08010 to 0.07673, saving model to best.model\n",
      "0s - loss: 0.1734 - acc: 0.9438 - val_loss: 0.0767 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.07673 to 0.07361, saving model to best.model\n",
      "0s - loss: 0.1447 - acc: 0.9438 - val_loss: 0.0736 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.07361 to 0.07056, saving model to best.model\n",
      "0s - loss: 0.1545 - acc: 0.9326 - val_loss: 0.0706 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.07056 to 0.06770, saving model to best.model\n",
      "0s - loss: 0.1767 - acc: 0.9213 - val_loss: 0.0677 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.06770 to 0.06544, saving model to best.model\n",
      "0s - loss: 0.1865 - acc: 0.9438 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.06544 to 0.06387, saving model to best.model\n",
      "0s - loss: 0.1835 - acc: 0.9438 - val_loss: 0.0639 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.06387 to 0.06256, saving model to best.model\n",
      "0s - loss: 0.1716 - acc: 0.9663 - val_loss: 0.0626 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.06256 to 0.06164, saving model to best.model\n",
      "0s - loss: 0.1902 - acc: 0.9438 - val_loss: 0.0616 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.06164 to 0.06074, saving model to best.model\n",
      "0s - loss: 0.1415 - acc: 0.9438 - val_loss: 0.0607 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.06074 to 0.05987, saving model to best.model\n",
      "0s - loss: 0.1404 - acc: 0.9775 - val_loss: 0.0599 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.05987 to 0.05935, saving model to best.model\n",
      "0s - loss: 0.1410 - acc: 0.9551 - val_loss: 0.0594 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.05935 to 0.05931, saving model to best.model\n",
      "0s - loss: 0.1668 - acc: 0.9551 - val_loss: 0.0593 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.1227 - acc: 0.9888 - val_loss: 0.0595 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.05931 to 0.05908, saving model to best.model\n",
      "0s - loss: 0.1331 - acc: 0.9551 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.05908 to 0.05898, saving model to best.model\n",
      "0s - loss: 0.1617 - acc: 0.9438 - val_loss: 0.0590 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.05898 to 0.05850, saving model to best.model\n",
      "0s - loss: 0.1451 - acc: 0.9663 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.06992, saving model to best.model\n",
      "0s - loss: 1.1622 - acc: 0.3483 - val_loss: 1.0699 - val_acc: 0.3913\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.9986 - acc: 0.5506 - val_loss: 1.0720 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.0882 - acc: 0.4494 - val_loss: 1.0770 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.0010 - acc: 0.4944 - val_loss: 1.0816 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.0501 - acc: 0.4270 - val_loss: 1.0867 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.0250 - acc: 0.5506 - val_loss: 1.0914 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1485 - acc: 0.4494 - val_loss: 1.0960 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1715 - acc: 0.3933 - val_loss: 1.0984 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1011 - acc: 0.4494 - val_loss: 1.0974 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.0310 - acc: 0.5281 - val_loss: 1.0940 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1620 - acc: 0.4045 - val_loss: 1.0902 - val_acc: 0.3913\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1776 - acc: 0.4045 - val_loss: 1.0864 - val_acc: 0.3913\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.0594 - acc: 0.4382 - val_loss: 1.0814 - val_acc: 0.3913\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.0534 - acc: 0.4831 - val_loss: 1.0761 - val_acc: 0.3913\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.0160 - acc: 0.4494 - val_loss: 1.0718 - val_acc: 0.3913\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.06992 to 1.06803, saving model to best.model\n",
      "0s - loss: 1.0716 - acc: 0.5281 - val_loss: 1.0680 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.06803 to 1.06405, saving model to best.model\n",
      "0s - loss: 1.0056 - acc: 0.4607 - val_loss: 1.0641 - val_acc: 0.3913\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.06405 to 1.05974, saving model to best.model\n",
      "0s - loss: 1.0488 - acc: 0.4719 - val_loss: 1.0597 - val_acc: 0.3913\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.05974 to 1.05574, saving model to best.model\n",
      "0s - loss: 0.9673 - acc: 0.5506 - val_loss: 1.0557 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.05574 to 1.05028, saving model to best.model\n",
      "0s - loss: 1.0412 - acc: 0.5169 - val_loss: 1.0503 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.05028 to 1.04702, saving model to best.model\n",
      "0s - loss: 1.1155 - acc: 0.4494 - val_loss: 1.0470 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.04702 to 1.04412, saving model to best.model\n",
      "0s - loss: 1.0551 - acc: 0.5169 - val_loss: 1.0441 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.04412 to 1.04105, saving model to best.model\n",
      "0s - loss: 1.0292 - acc: 0.4607 - val_loss: 1.0410 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.04105 to 1.03786, saving model to best.model\n",
      "0s - loss: 1.0550 - acc: 0.4270 - val_loss: 1.0379 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.03786 to 1.03482, saving model to best.model\n",
      "0s - loss: 1.0578 - acc: 0.4719 - val_loss: 1.0348 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.03482 to 1.03227, saving model to best.model\n",
      "0s - loss: 1.0497 - acc: 0.4494 - val_loss: 1.0323 - val_acc: 0.3913\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.03227 to 1.03035, saving model to best.model\n",
      "0s - loss: 1.0896 - acc: 0.4382 - val_loss: 1.0303 - val_acc: 0.3913\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.03035 to 1.02805, saving model to best.model\n",
      "0s - loss: 1.0501 - acc: 0.4719 - val_loss: 1.0281 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.02805 to 1.02784, saving model to best.model\n",
      "0s - loss: 1.0776 - acc: 0.4944 - val_loss: 1.0278 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.02784 to 1.02711, saving model to best.model\n",
      "0s - loss: 0.9648 - acc: 0.5393 - val_loss: 1.0271 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.02711 to 1.02694, saving model to best.model\n",
      "0s - loss: 0.9974 - acc: 0.5056 - val_loss: 1.0269 - val_acc: 0.3913\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.02694 to 1.02645, saving model to best.model\n",
      "0s - loss: 1.0269 - acc: 0.5056 - val_loss: 1.0264 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.9535 - acc: 0.5506 - val_loss: 1.0267 - val_acc: 0.3913\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.0261 - acc: 0.5169 - val_loss: 1.0271 - val_acc: 0.3913\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.02645 to 1.02633, saving model to best.model\n",
      "0s - loss: 0.9919 - acc: 0.6180 - val_loss: 1.0263 - val_acc: 0.3913\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.02633 to 1.02608, saving model to best.model\n",
      "0s - loss: 0.9641 - acc: 0.4944 - val_loss: 1.0261 - val_acc: 0.3913\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 1.0071 - acc: 0.4719 - val_loss: 1.0265 - val_acc: 0.3913\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.02608 to 1.02597, saving model to best.model\n",
      "0s - loss: 0.9526 - acc: 0.5955 - val_loss: 1.0260 - val_acc: 0.3913\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.02597 to 1.02481, saving model to best.model\n",
      "0s - loss: 0.9682 - acc: 0.5281 - val_loss: 1.0248 - val_acc: 0.3913\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.02481 to 1.02335, saving model to best.model\n",
      "0s - loss: 1.0005 - acc: 0.5281 - val_loss: 1.0233 - val_acc: 0.3913\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.02335 to 1.02094, saving model to best.model\n",
      "0s - loss: 0.9777 - acc: 0.4831 - val_loss: 1.0209 - val_acc: 0.3913\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.02094 to 1.01839, saving model to best.model\n",
      "0s - loss: 0.9652 - acc: 0.5730 - val_loss: 1.0184 - val_acc: 0.3913\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.01839 to 1.01456, saving model to best.model\n",
      "0s - loss: 0.9910 - acc: 0.5281 - val_loss: 1.0146 - val_acc: 0.3913\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.01456 to 1.01070, saving model to best.model\n",
      "0s - loss: 0.8875 - acc: 0.5730 - val_loss: 1.0107 - val_acc: 0.3913\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.01070 to 1.00609, saving model to best.model\n",
      "0s - loss: 0.9816 - acc: 0.5169 - val_loss: 1.0061 - val_acc: 0.3913\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.00609 to 1.00003, saving model to best.model\n",
      "0s - loss: 1.0068 - acc: 0.5169 - val_loss: 1.0000 - val_acc: 0.3913\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.00003 to 0.99564, saving model to best.model\n",
      "0s - loss: 0.9992 - acc: 0.4494 - val_loss: 0.9956 - val_acc: 0.3913\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.99564 to 0.99161, saving model to best.model\n",
      "0s - loss: 0.9471 - acc: 0.5169 - val_loss: 0.9916 - val_acc: 0.3913\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.99161 to 0.98753, saving model to best.model\n",
      "0s - loss: 0.8785 - acc: 0.5730 - val_loss: 0.9875 - val_acc: 0.3913\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.98753 to 0.98396, saving model to best.model\n",
      "0s - loss: 0.8825 - acc: 0.6067 - val_loss: 0.9840 - val_acc: 0.3913\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.98396 to 0.97926, saving model to best.model\n",
      "0s - loss: 0.9181 - acc: 0.5506 - val_loss: 0.9793 - val_acc: 0.4348\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.97926 to 0.97455, saving model to best.model\n",
      "0s - loss: 0.9674 - acc: 0.5618 - val_loss: 0.9746 - val_acc: 0.4348\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.97455 to 0.96967, saving model to best.model\n",
      "0s - loss: 0.9641 - acc: 0.5506 - val_loss: 0.9697 - val_acc: 0.4348\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.96967 to 0.96432, saving model to best.model\n",
      "0s - loss: 0.9347 - acc: 0.5506 - val_loss: 0.9643 - val_acc: 0.4783\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.96432 to 0.96014, saving model to best.model\n",
      "0s - loss: 0.9046 - acc: 0.5393 - val_loss: 0.9601 - val_acc: 0.4783\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.96014 to 0.95738, saving model to best.model\n",
      "0s - loss: 0.9368 - acc: 0.5169 - val_loss: 0.9574 - val_acc: 0.4783\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.95738 to 0.95403, saving model to best.model\n",
      "0s - loss: 0.9036 - acc: 0.6629 - val_loss: 0.9540 - val_acc: 0.4783\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.95403 to 0.95134, saving model to best.model\n",
      "0s - loss: 0.8829 - acc: 0.5730 - val_loss: 0.9513 - val_acc: 0.4783\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.95134 to 0.94858, saving model to best.model\n",
      "0s - loss: 0.8833 - acc: 0.6517 - val_loss: 0.9486 - val_acc: 0.4783\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.94858 to 0.94568, saving model to best.model\n",
      "0s - loss: 0.9426 - acc: 0.5730 - val_loss: 0.9457 - val_acc: 0.4783\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.94568 to 0.94192, saving model to best.model\n",
      "0s - loss: 0.9234 - acc: 0.5056 - val_loss: 0.9419 - val_acc: 0.4783\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.94192 to 0.93779, saving model to best.model\n",
      "0s - loss: 0.9592 - acc: 0.5393 - val_loss: 0.9378 - val_acc: 0.5217\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.93779 to 0.93349, saving model to best.model\n",
      "0s - loss: 0.9147 - acc: 0.6404 - val_loss: 0.9335 - val_acc: 0.5217\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.93349 to 0.92900, saving model to best.model\n",
      "0s - loss: 0.9791 - acc: 0.5843 - val_loss: 0.9290 - val_acc: 0.5217\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.92900 to 0.92430, saving model to best.model\n",
      "0s - loss: 0.8171 - acc: 0.6067 - val_loss: 0.9243 - val_acc: 0.5217\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.92430 to 0.91935, saving model to best.model\n",
      "0s - loss: 0.8983 - acc: 0.5843 - val_loss: 0.9193 - val_acc: 0.5217\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.91935 to 0.91347, saving model to best.model\n",
      "0s - loss: 0.8643 - acc: 0.6067 - val_loss: 0.9135 - val_acc: 0.6087\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.91347 to 0.90703, saving model to best.model\n",
      "0s - loss: 0.8481 - acc: 0.6180 - val_loss: 0.9070 - val_acc: 0.6957\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.90703 to 0.89973, saving model to best.model\n",
      "0s - loss: 0.8710 - acc: 0.6180 - val_loss: 0.8997 - val_acc: 0.6957\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.89973 to 0.89200, saving model to best.model\n",
      "0s - loss: 0.9119 - acc: 0.6180 - val_loss: 0.8920 - val_acc: 0.6957\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.89200 to 0.88421, saving model to best.model\n",
      "0s - loss: 0.9260 - acc: 0.5730 - val_loss: 0.8842 - val_acc: 0.7391\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.88421 to 0.87712, saving model to best.model\n",
      "0s - loss: 0.8716 - acc: 0.6067 - val_loss: 0.8771 - val_acc: 0.7391\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.87712 to 0.87013, saving model to best.model\n",
      "0s - loss: 0.9311 - acc: 0.5169 - val_loss: 0.8701 - val_acc: 0.7391\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.87013 to 0.86226, saving model to best.model\n",
      "0s - loss: 0.8558 - acc: 0.6067 - val_loss: 0.8623 - val_acc: 0.7391\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.86226 to 0.85388, saving model to best.model\n",
      "0s - loss: 0.8873 - acc: 0.5730 - val_loss: 0.8539 - val_acc: 0.7826\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.85388 to 0.84555, saving model to best.model\n",
      "0s - loss: 0.8055 - acc: 0.6854 - val_loss: 0.8455 - val_acc: 0.7826\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.84555 to 0.83627, saving model to best.model\n",
      "0s - loss: 0.8641 - acc: 0.6067 - val_loss: 0.8363 - val_acc: 0.7826\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.83627 to 0.82709, saving model to best.model\n",
      "0s - loss: 0.8321 - acc: 0.6517 - val_loss: 0.8271 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.82709 to 0.81786, saving model to best.model\n",
      "0s - loss: 0.8741 - acc: 0.6292 - val_loss: 0.8179 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.81786 to 0.80837, saving model to best.model\n",
      "0s - loss: 0.7612 - acc: 0.6742 - val_loss: 0.8084 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.80837 to 0.79898, saving model to best.model\n",
      "0s - loss: 0.6937 - acc: 0.7191 - val_loss: 0.7990 - val_acc: 0.7826\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.79898 to 0.78906, saving model to best.model\n",
      "0s - loss: 0.8045 - acc: 0.6292 - val_loss: 0.7891 - val_acc: 0.7826\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.78906 to 0.77842, saving model to best.model\n",
      "0s - loss: 0.7439 - acc: 0.7079 - val_loss: 0.7784 - val_acc: 0.7826\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.77842 to 0.76743, saving model to best.model\n",
      "0s - loss: 0.7770 - acc: 0.6517 - val_loss: 0.7674 - val_acc: 0.7826\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.76743 to 0.75649, saving model to best.model\n",
      "0s - loss: 0.7836 - acc: 0.7191 - val_loss: 0.7565 - val_acc: 0.8261\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.75649 to 0.74545, saving model to best.model\n",
      "0s - loss: 0.7906 - acc: 0.7079 - val_loss: 0.7455 - val_acc: 0.8261\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.74545 to 0.73470, saving model to best.model\n",
      "0s - loss: 0.7201 - acc: 0.7303 - val_loss: 0.7347 - val_acc: 0.8261\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.73470 to 0.72442, saving model to best.model\n",
      "0s - loss: 0.7501 - acc: 0.7191 - val_loss: 0.7244 - val_acc: 0.8261\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.72442 to 0.71456, saving model to best.model\n",
      "0s - loss: 0.7654 - acc: 0.7079 - val_loss: 0.7146 - val_acc: 0.8261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.71456 to 0.70495, saving model to best.model\n",
      "0s - loss: 0.7535 - acc: 0.6854 - val_loss: 0.7050 - val_acc: 0.8261\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.70495 to 0.69491, saving model to best.model\n",
      "0s - loss: 0.7977 - acc: 0.6517 - val_loss: 0.6949 - val_acc: 0.8261\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.69491 to 0.68487, saving model to best.model\n",
      "0s - loss: 0.7540 - acc: 0.7303 - val_loss: 0.6849 - val_acc: 0.8261\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68487 to 0.67520, saving model to best.model\n",
      "0s - loss: 0.7028 - acc: 0.7416 - val_loss: 0.6752 - val_acc: 0.8261\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67520 to 0.66538, saving model to best.model\n",
      "0s - loss: 0.6842 - acc: 0.7528 - val_loss: 0.6654 - val_acc: 0.8261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66538 to 0.65521, saving model to best.model\n",
      "0s - loss: 0.5904 - acc: 0.8315 - val_loss: 0.6552 - val_acc: 0.8261\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.65521 to 0.64439, saving model to best.model\n",
      "0s - loss: 0.6647 - acc: 0.7528 - val_loss: 0.6444 - val_acc: 0.8261\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.64439 to 0.63287, saving model to best.model\n",
      "0s - loss: 0.7083 - acc: 0.7416 - val_loss: 0.6329 - val_acc: 0.8261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.63287 to 0.62110, saving model to best.model\n",
      "0s - loss: 0.7171 - acc: 0.7528 - val_loss: 0.6211 - val_acc: 0.8261\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.62110 to 0.60941, saving model to best.model\n",
      "0s - loss: 0.6801 - acc: 0.7978 - val_loss: 0.6094 - val_acc: 0.8261\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.60941 to 0.59743, saving model to best.model\n",
      "0s - loss: 0.6670 - acc: 0.7640 - val_loss: 0.5974 - val_acc: 0.8261\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.59743 to 0.58543, saving model to best.model\n",
      "0s - loss: 0.6416 - acc: 0.7303 - val_loss: 0.5854 - val_acc: 0.8261\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.58543 to 0.57306, saving model to best.model\n",
      "0s - loss: 0.5723 - acc: 0.8315 - val_loss: 0.5731 - val_acc: 0.8261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.57306 to 0.56076, saving model to best.model\n",
      "0s - loss: 0.6157 - acc: 0.7865 - val_loss: 0.5608 - val_acc: 0.8261\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.56076 to 0.54849, saving model to best.model\n",
      "0s - loss: 0.5640 - acc: 0.8090 - val_loss: 0.5485 - val_acc: 0.8261\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.54849 to 0.53680, saving model to best.model\n",
      "0s - loss: 0.5055 - acc: 0.8427 - val_loss: 0.5368 - val_acc: 0.8261\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.53680 to 0.52463, saving model to best.model\n",
      "0s - loss: 0.6099 - acc: 0.7978 - val_loss: 0.5246 - val_acc: 0.8261\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.52463 to 0.51213, saving model to best.model\n",
      "0s - loss: 0.5565 - acc: 0.8202 - val_loss: 0.5121 - val_acc: 0.8261\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.51213 to 0.50003, saving model to best.model\n",
      "0s - loss: 0.5603 - acc: 0.7978 - val_loss: 0.5000 - val_acc: 0.8261\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.50003 to 0.48815, saving model to best.model\n",
      "0s - loss: 0.5491 - acc: 0.7978 - val_loss: 0.4882 - val_acc: 0.8261\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.48815 to 0.47653, saving model to best.model\n",
      "0s - loss: 0.5435 - acc: 0.7865 - val_loss: 0.4765 - val_acc: 0.8261\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.47653 to 0.46468, saving model to best.model\n",
      "0s - loss: 0.5491 - acc: 0.8202 - val_loss: 0.4647 - val_acc: 0.8261\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.46468 to 0.45306, saving model to best.model\n",
      "0s - loss: 0.5473 - acc: 0.7753 - val_loss: 0.4531 - val_acc: 0.8261\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.45306 to 0.44187, saving model to best.model\n",
      "0s - loss: 0.5084 - acc: 0.8539 - val_loss: 0.4419 - val_acc: 0.8261\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.44187 to 0.43123, saving model to best.model\n",
      "0s - loss: 0.5349 - acc: 0.8090 - val_loss: 0.4312 - val_acc: 0.8261\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.43123 to 0.42021, saving model to best.model\n",
      "0s - loss: 0.5621 - acc: 0.7978 - val_loss: 0.4202 - val_acc: 0.8261\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.42021 to 0.40969, saving model to best.model\n",
      "0s - loss: 0.5059 - acc: 0.7865 - val_loss: 0.4097 - val_acc: 0.8261\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.40969 to 0.39985, saving model to best.model\n",
      "0s - loss: 0.4841 - acc: 0.8539 - val_loss: 0.3998 - val_acc: 0.8261\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.39985 to 0.39068, saving model to best.model\n",
      "0s - loss: 0.4718 - acc: 0.8427 - val_loss: 0.3907 - val_acc: 0.8261\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.39068 to 0.38186, saving model to best.model\n",
      "0s - loss: 0.4415 - acc: 0.8315 - val_loss: 0.3819 - val_acc: 0.8261\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.38186 to 0.37335, saving model to best.model\n",
      "0s - loss: 0.4283 - acc: 0.8876 - val_loss: 0.3734 - val_acc: 0.8261\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.37335 to 0.36489, saving model to best.model\n",
      "0s - loss: 0.4341 - acc: 0.8427 - val_loss: 0.3649 - val_acc: 0.8261\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.36489 to 0.35634, saving model to best.model\n",
      "0s - loss: 0.4295 - acc: 0.8764 - val_loss: 0.3563 - val_acc: 0.8261\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.35634 to 0.34820, saving model to best.model\n",
      "0s - loss: 0.4233 - acc: 0.8539 - val_loss: 0.3482 - val_acc: 0.8261\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.34820 to 0.33976, saving model to best.model\n",
      "0s - loss: 0.4053 - acc: 0.8989 - val_loss: 0.3398 - val_acc: 0.8261\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.33976 to 0.33154, saving model to best.model\n",
      "0s - loss: 0.4598 - acc: 0.8427 - val_loss: 0.3315 - val_acc: 0.8261\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.33154 to 0.32370, saving model to best.model\n",
      "0s - loss: 0.3923 - acc: 0.8764 - val_loss: 0.3237 - val_acc: 0.8261\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.32370 to 0.31637, saving model to best.model\n",
      "0s - loss: 0.4010 - acc: 0.8539 - val_loss: 0.3164 - val_acc: 0.8696\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.31637 to 0.30936, saving model to best.model\n",
      "0s - loss: 0.3736 - acc: 0.8652 - val_loss: 0.3094 - val_acc: 0.8696\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.30936 to 0.30264, saving model to best.model\n",
      "0s - loss: 0.4136 - acc: 0.8315 - val_loss: 0.3026 - val_acc: 0.8696\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.30264 to 0.29469, saving model to best.model\n",
      "0s - loss: 0.3651 - acc: 0.8427 - val_loss: 0.2947 - val_acc: 0.8696\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.29469 to 0.28679, saving model to best.model\n",
      "0s - loss: 0.3824 - acc: 0.8539 - val_loss: 0.2868 - val_acc: 0.8696\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.28679 to 0.27731, saving model to best.model\n",
      "0s - loss: 0.3888 - acc: 0.8427 - val_loss: 0.2773 - val_acc: 0.8696\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.27731 to 0.26792, saving model to best.model\n",
      "0s - loss: 0.3718 - acc: 0.8652 - val_loss: 0.2679 - val_acc: 0.8696\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.26792 to 0.25893, saving model to best.model\n",
      "0s - loss: 0.3703 - acc: 0.8427 - val_loss: 0.2589 - val_acc: 0.8696\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.25893 to 0.24994, saving model to best.model\n",
      "0s - loss: 0.2865 - acc: 0.9213 - val_loss: 0.2499 - val_acc: 0.8696\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.24994 to 0.24153, saving model to best.model\n",
      "0s - loss: 0.3270 - acc: 0.8764 - val_loss: 0.2415 - val_acc: 0.8696\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.24153 to 0.23359, saving model to best.model\n",
      "0s - loss: 0.3602 - acc: 0.8652 - val_loss: 0.2336 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.23359 to 0.22600, saving model to best.model\n",
      "0s - loss: 0.3599 - acc: 0.8427 - val_loss: 0.2260 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.22600 to 0.21899, saving model to best.model\n",
      "0s - loss: 0.3262 - acc: 0.8876 - val_loss: 0.2190 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.21899 to 0.21240, saving model to best.model\n",
      "0s - loss: 0.3461 - acc: 0.8876 - val_loss: 0.2124 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.21240 to 0.20591, saving model to best.model\n",
      "0s - loss: 0.2844 - acc: 0.8876 - val_loss: 0.2059 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.20591 to 0.20025, saving model to best.model\n",
      "0s - loss: 0.3108 - acc: 0.8876 - val_loss: 0.2002 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.20025 to 0.19510, saving model to best.model\n",
      "0s - loss: 0.3290 - acc: 0.8876 - val_loss: 0.1951 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.19510 to 0.19027, saving model to best.model\n",
      "0s - loss: 0.3611 - acc: 0.8427 - val_loss: 0.1903 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19027 to 0.18525, saving model to best.model\n",
      "0s - loss: 0.2697 - acc: 0.9101 - val_loss: 0.1853 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.18525 to 0.17992, saving model to best.model\n",
      "0s - loss: 0.2978 - acc: 0.8989 - val_loss: 0.1799 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.17992 to 0.17483, saving model to best.model\n",
      "0s - loss: 0.2867 - acc: 0.9326 - val_loss: 0.1748 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.17483 to 0.16978, saving model to best.model\n",
      "0s - loss: 0.2985 - acc: 0.9101 - val_loss: 0.1698 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.16978 to 0.16468, saving model to best.model\n",
      "0s - loss: 0.2282 - acc: 0.9438 - val_loss: 0.1647 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.16468 to 0.15981, saving model to best.model\n",
      "0s - loss: 0.2742 - acc: 0.9551 - val_loss: 0.1598 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.15981 to 0.15512, saving model to best.model\n",
      "0s - loss: 0.2533 - acc: 0.9438 - val_loss: 0.1551 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.15512 to 0.15044, saving model to best.model\n",
      "0s - loss: 0.2897 - acc: 0.9101 - val_loss: 0.1504 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.15044 to 0.14603, saving model to best.model\n",
      "0s - loss: 0.2073 - acc: 0.9438 - val_loss: 0.1460 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.14603 to 0.14190, saving model to best.model\n",
      "0s - loss: 0.2256 - acc: 0.9438 - val_loss: 0.1419 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.14190 to 0.13825, saving model to best.model\n",
      "0s - loss: 0.2241 - acc: 0.8989 - val_loss: 0.1382 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.13825 to 0.13424, saving model to best.model\n",
      "0s - loss: 0.2757 - acc: 0.9101 - val_loss: 0.1342 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.13424 to 0.13022, saving model to best.model\n",
      "0s - loss: 0.2527 - acc: 0.9438 - val_loss: 0.1302 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.13022 to 0.12631, saving model to best.model\n",
      "0s - loss: 0.2440 - acc: 0.9326 - val_loss: 0.1263 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.12631 to 0.12246, saving model to best.model\n",
      "0s - loss: 0.2203 - acc: 0.9551 - val_loss: 0.1225 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.12246 to 0.11873, saving model to best.model\n",
      "0s - loss: 0.2444 - acc: 0.9551 - val_loss: 0.1187 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.11873 to 0.11478, saving model to best.model\n",
      "0s - loss: 0.2331 - acc: 0.9101 - val_loss: 0.1148 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.11478 to 0.11096, saving model to best.model\n",
      "0s - loss: 0.2360 - acc: 0.9101 - val_loss: 0.1110 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.11096 to 0.10758, saving model to best.model\n",
      "0s - loss: 0.1494 - acc: 0.9888 - val_loss: 0.1076 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.10758 to 0.10420, saving model to best.model\n",
      "0s - loss: 0.2397 - acc: 0.8876 - val_loss: 0.1042 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.10420 to 0.10105, saving model to best.model\n",
      "0s - loss: 0.2369 - acc: 0.9213 - val_loss: 0.1011 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.10105 to 0.09817, saving model to best.model\n",
      "0s - loss: 0.2001 - acc: 0.9551 - val_loss: 0.0982 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.09817 to 0.09548, saving model to best.model\n",
      "0s - loss: 0.2247 - acc: 0.9438 - val_loss: 0.0955 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.09548 to 0.09300, saving model to best.model\n",
      "0s - loss: 0.1894 - acc: 0.9438 - val_loss: 0.0930 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.09300 to 0.09064, saving model to best.model\n",
      "0s - loss: 0.1678 - acc: 0.9663 - val_loss: 0.0906 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.09064 to 0.08852, saving model to best.model\n",
      "0s - loss: 0.1814 - acc: 0.9438 - val_loss: 0.0885 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.08852 to 0.08661, saving model to best.model\n",
      "0s - loss: 0.2496 - acc: 0.9438 - val_loss: 0.0866 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.08661 to 0.08487, saving model to best.model\n",
      "0s - loss: 0.2054 - acc: 0.9551 - val_loss: 0.0849 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.08487 to 0.08334, saving model to best.model\n",
      "0s - loss: 0.1856 - acc: 0.9438 - val_loss: 0.0833 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.08334 to 0.08196, saving model to best.model\n",
      "0s - loss: 0.1679 - acc: 0.9438 - val_loss: 0.0820 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.08196 to 0.08114, saving model to best.model\n",
      "0s - loss: 0.2233 - acc: 0.8989 - val_loss: 0.0811 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.08114 to 0.08021, saving model to best.model\n",
      "0s - loss: 0.1814 - acc: 0.9551 - val_loss: 0.0802 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.08021 to 0.07867, saving model to best.model\n",
      "0s - loss: 0.1940 - acc: 0.9326 - val_loss: 0.0787 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.07867 to 0.07691, saving model to best.model\n",
      "0s - loss: 0.1757 - acc: 0.9551 - val_loss: 0.0769 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.07691 to 0.07496, saving model to best.model\n",
      "0s - loss: 0.1877 - acc: 0.9438 - val_loss: 0.0750 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.07496 to 0.07309, saving model to best.model\n",
      "0s - loss: 0.2059 - acc: 0.9213 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.07309 to 0.07076, saving model to best.model\n",
      "0s - loss: 0.2293 - acc: 0.8876 - val_loss: 0.0708 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.07076 to 0.06855, saving model to best.model\n",
      "0s - loss: 0.1722 - acc: 0.9213 - val_loss: 0.0686 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.06855 to 0.06648, saving model to best.model\n",
      "0s - loss: 0.2136 - acc: 0.9438 - val_loss: 0.0665 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.06648 to 0.06472, saving model to best.model\n",
      "0s - loss: 0.1761 - acc: 0.9213 - val_loss: 0.0647 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.06472 to 0.06321, saving model to best.model\n",
      "0s - loss: 0.1561 - acc: 0.9663 - val_loss: 0.0632 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.06321 to 0.06182, saving model to best.model\n",
      "0s - loss: 0.1345 - acc: 0.9663 - val_loss: 0.0618 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.06182 to 0.06048, saving model to best.model\n",
      "0s - loss: 0.1519 - acc: 0.9663 - val_loss: 0.0605 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.06048 to 0.05919, saving model to best.model\n",
      "0s - loss: 0.1064 - acc: 0.9775 - val_loss: 0.0592 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.05919 to 0.05797, saving model to best.model\n",
      "0s - loss: 0.1426 - acc: 0.9551 - val_loss: 0.0580 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.05797 to 0.05679, saving model to best.model\n",
      "0s - loss: 0.1814 - acc: 0.9326 - val_loss: 0.0568 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.05679 to 0.05561, saving model to best.model\n",
      "0s - loss: 0.1171 - acc: 0.9775 - val_loss: 0.0556 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.05561 to 0.05446, saving model to best.model\n",
      "0s - loss: 0.1686 - acc: 0.9551 - val_loss: 0.0545 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.05446 to 0.05327, saving model to best.model\n",
      "0s - loss: 0.1525 - acc: 0.9438 - val_loss: 0.0533 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.05327 to 0.05215, saving model to best.model\n",
      "0s - loss: 0.1089 - acc: 0.9775 - val_loss: 0.0521 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.05215 to 0.05116, saving model to best.model\n",
      "0s - loss: 0.1812 - acc: 0.9213 - val_loss: 0.0512 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.05116 to 0.05027, saving model to best.model\n",
      "0s - loss: 0.1396 - acc: 0.9551 - val_loss: 0.0503 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.05027 to 0.04944, saving model to best.model\n",
      "0s - loss: 0.1286 - acc: 0.9663 - val_loss: 0.0494 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.04944 to 0.04876, saving model to best.model\n",
      "0s - loss: 0.1676 - acc: 0.9551 - val_loss: 0.0488 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.04876 to 0.04832, saving model to best.model\n",
      "0s - loss: 0.1464 - acc: 0.9551 - val_loss: 0.0483 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.04832 to 0.04805, saving model to best.model\n",
      "0s - loss: 0.1615 - acc: 0.9551 - val_loss: 0.0481 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.20279, saving model to best.model\n",
      "0s - loss: 1.7533 - acc: 0.2921 - val_loss: 1.2028 - val_acc: 0.3913\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.20279 to 1.14383, saving model to best.model\n",
      "0s - loss: 1.5296 - acc: 0.2697 - val_loss: 1.1438 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.14383 to 1.10366, saving model to best.model\n",
      "0s - loss: 1.4806 - acc: 0.3371 - val_loss: 1.1037 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.10366 to 1.08669, saving model to best.model\n",
      "0s - loss: 1.4732 - acc: 0.2921 - val_loss: 1.0867 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.4186 - acc: 0.3146 - val_loss: 1.0948 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1848 - acc: 0.3933 - val_loss: 1.1289 - val_acc: 0.2609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2448 - acc: 0.3933 - val_loss: 1.1832 - val_acc: 0.2609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.3034 - acc: 0.3483 - val_loss: 1.2470 - val_acc: 0.2609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2246 - acc: 0.4045 - val_loss: 1.3085 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2505 - acc: 0.4270 - val_loss: 1.3609 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1722 - acc: 0.4494 - val_loss: 1.4027 - val_acc: 0.2609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1466 - acc: 0.4270 - val_loss: 1.4287 - val_acc: 0.2609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.3071 - acc: 0.4831 - val_loss: 1.4360 - val_acc: 0.2609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.3028 - acc: 0.4157 - val_loss: 1.4245 - val_acc: 0.2609\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.4340 - acc: 0.4382 - val_loss: 1.3974 - val_acc: 0.2609\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2385 - acc: 0.4382 - val_loss: 1.3642 - val_acc: 0.2609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1957 - acc: 0.4157 - val_loss: 1.3278 - val_acc: 0.2609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1818 - acc: 0.4494 - val_loss: 1.2885 - val_acc: 0.2609\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2114 - acc: 0.4157 - val_loss: 1.2498 - val_acc: 0.2609\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1733 - acc: 0.4045 - val_loss: 1.2134 - val_acc: 0.2609\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.2211 - acc: 0.4045 - val_loss: 1.1826 - val_acc: 0.2609\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.0960 - acc: 0.4719 - val_loss: 1.1565 - val_acc: 0.2609\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2620 - acc: 0.3258 - val_loss: 1.1359 - val_acc: 0.2609\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.2301 - acc: 0.3596 - val_loss: 1.1198 - val_acc: 0.2609\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.2225 - acc: 0.3820 - val_loss: 1.1076 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1574 - acc: 0.4270 - val_loss: 1.0979 - val_acc: 0.5217\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1765 - acc: 0.3820 - val_loss: 1.0907 - val_acc: 0.6522\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.08669 to 1.08581, saving model to best.model\n",
      "0s - loss: 1.1561 - acc: 0.3596 - val_loss: 1.0858 - val_acc: 0.6522\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.08581 to 1.08234, saving model to best.model\n",
      "0s - loss: 1.1586 - acc: 0.2921 - val_loss: 1.0823 - val_acc: 0.6522\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.08234 to 1.08030, saving model to best.model\n",
      "0s - loss: 1.1810 - acc: 0.3483 - val_loss: 1.0803 - val_acc: 0.6087\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.08030 to 1.07989, saving model to best.model\n",
      "0s - loss: 1.0766 - acc: 0.4045 - val_loss: 1.0799 - val_acc: 0.4783\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.2560 - acc: 0.3708 - val_loss: 1.0808 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.1750 - acc: 0.3596 - val_loss: 1.0835 - val_acc: 0.2609\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1059 - acc: 0.4607 - val_loss: 1.0881 - val_acc: 0.2609\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1559 - acc: 0.3708 - val_loss: 1.0934 - val_acc: 0.2609\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 1.1303 - acc: 0.4494 - val_loss: 1.0999 - val_acc: 0.2609\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 1.1596 - acc: 0.3371 - val_loss: 1.1064 - val_acc: 0.2609\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 1.1571 - acc: 0.3708 - val_loss: 1.1140 - val_acc: 0.2609\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 1.2006 - acc: 0.3371 - val_loss: 1.1216 - val_acc: 0.2609\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 1.1284 - acc: 0.3820 - val_loss: 1.1279 - val_acc: 0.2609\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 1.1348 - acc: 0.4270 - val_loss: 1.1338 - val_acc: 0.2609\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 1.0869 - acc: 0.4157 - val_loss: 1.1358 - val_acc: 0.2609\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 1.1139 - acc: 0.3820 - val_loss: 1.1372 - val_acc: 0.2609\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 1.0870 - acc: 0.4607 - val_loss: 1.1382 - val_acc: 0.2609\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 1.1395 - acc: 0.4270 - val_loss: 1.1371 - val_acc: 0.2609\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 1.0479 - acc: 0.5393 - val_loss: 1.1330 - val_acc: 0.2609\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 1.0271 - acc: 0.4382 - val_loss: 1.1277 - val_acc: 0.2609\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 1.0821 - acc: 0.4382 - val_loss: 1.1214 - val_acc: 0.2609\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 1.0266 - acc: 0.4831 - val_loss: 1.1139 - val_acc: 0.2609\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 1.0264 - acc: 0.4494 - val_loss: 1.1071 - val_acc: 0.2609\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 1.0617 - acc: 0.4270 - val_loss: 1.0999 - val_acc: 0.2609\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 1.1127 - acc: 0.3820 - val_loss: 1.0931 - val_acc: 0.2609\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 1.0573 - acc: 0.4157 - val_loss: 1.0868 - val_acc: 0.2609\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.07989 to 1.07946, saving model to best.model\n",
      "0s - loss: 1.0738 - acc: 0.4382 - val_loss: 1.0795 - val_acc: 0.2609\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 1.07946 to 1.07201, saving model to best.model\n",
      "0s - loss: 1.0841 - acc: 0.4157 - val_loss: 1.0720 - val_acc: 0.2609\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 1.07201 to 1.06429, saving model to best.model\n",
      "0s - loss: 1.0529 - acc: 0.4719 - val_loss: 1.0643 - val_acc: 0.2609\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 1.06429 to 1.05700, saving model to best.model\n",
      "0s - loss: 1.0699 - acc: 0.4045 - val_loss: 1.0570 - val_acc: 0.3913\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 1.05700 to 1.05183, saving model to best.model\n",
      "0s - loss: 1.1167 - acc: 0.4719 - val_loss: 1.0518 - val_acc: 0.3913\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 1.05183 to 1.04686, saving model to best.model\n",
      "0s - loss: 1.0159 - acc: 0.4944 - val_loss: 1.0469 - val_acc: 0.3913\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 1.04686 to 1.04199, saving model to best.model\n",
      "0s - loss: 1.1011 - acc: 0.4045 - val_loss: 1.0420 - val_acc: 0.3913\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 1.04199 to 1.03799, saving model to best.model\n",
      "0s - loss: 0.9995 - acc: 0.4494 - val_loss: 1.0380 - val_acc: 0.4348\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 1.03799 to 1.03383, saving model to best.model\n",
      "0s - loss: 1.0837 - acc: 0.4607 - val_loss: 1.0338 - val_acc: 0.4348\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 1.03383 to 1.02920, saving model to best.model\n",
      "0s - loss: 1.0160 - acc: 0.4494 - val_loss: 1.0292 - val_acc: 0.4348\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 1.02920 to 1.02483, saving model to best.model\n",
      "0s - loss: 1.0780 - acc: 0.4719 - val_loss: 1.0248 - val_acc: 0.4348\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 1.02483 to 1.02001, saving model to best.model\n",
      "0s - loss: 0.9999 - acc: 0.4831 - val_loss: 1.0200 - val_acc: 0.4348\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 1.02001 to 1.01539, saving model to best.model\n",
      "0s - loss: 1.0042 - acc: 0.5056 - val_loss: 1.0154 - val_acc: 0.4348\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 1.01539 to 1.00928, saving model to best.model\n",
      "0s - loss: 0.9291 - acc: 0.5393 - val_loss: 1.0093 - val_acc: 0.4783\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 1.00928 to 1.00278, saving model to best.model\n",
      "0s - loss: 1.0689 - acc: 0.4157 - val_loss: 1.0028 - val_acc: 0.4783\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 1.00278 to 0.99646, saving model to best.model\n",
      "0s - loss: 1.0348 - acc: 0.4494 - val_loss: 0.9965 - val_acc: 0.4783\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.99646 to 0.98959, saving model to best.model\n",
      "0s - loss: 0.9984 - acc: 0.5506 - val_loss: 0.9896 - val_acc: 0.4783\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.98959 to 0.98173, saving model to best.model\n",
      "0s - loss: 0.9727 - acc: 0.5056 - val_loss: 0.9817 - val_acc: 0.4783\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.98173 to 0.97314, saving model to best.model\n",
      "0s - loss: 1.0687 - acc: 0.4270 - val_loss: 0.9731 - val_acc: 0.4783\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.97314 to 0.96490, saving model to best.model\n",
      "0s - loss: 1.0483 - acc: 0.4382 - val_loss: 0.9649 - val_acc: 0.4783\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.96490 to 0.95660, saving model to best.model\n",
      "0s - loss: 1.0607 - acc: 0.3933 - val_loss: 0.9566 - val_acc: 0.4783\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.95660 to 0.94820, saving model to best.model\n",
      "0s - loss: 0.9825 - acc: 0.5506 - val_loss: 0.9482 - val_acc: 0.4783\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.94820 to 0.93825, saving model to best.model\n",
      "0s - loss: 0.9380 - acc: 0.4831 - val_loss: 0.9383 - val_acc: 0.4783\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.93825 to 0.92828, saving model to best.model\n",
      "0s - loss: 0.9686 - acc: 0.5281 - val_loss: 0.9283 - val_acc: 0.5217\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.92828 to 0.91832, saving model to best.model\n",
      "0s - loss: 0.9498 - acc: 0.5281 - val_loss: 0.9183 - val_acc: 0.5652\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.91832 to 0.90801, saving model to best.model\n",
      "0s - loss: 0.9784 - acc: 0.5618 - val_loss: 0.9080 - val_acc: 0.6087\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.90801 to 0.89873, saving model to best.model\n",
      "0s - loss: 0.9234 - acc: 0.5618 - val_loss: 0.8987 - val_acc: 0.6087\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.89873 to 0.88956, saving model to best.model\n",
      "0s - loss: 0.9532 - acc: 0.4719 - val_loss: 0.8896 - val_acc: 0.6087\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.88956 to 0.88108, saving model to best.model\n",
      "0s - loss: 0.9230 - acc: 0.5955 - val_loss: 0.8811 - val_acc: 0.6087\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.88108 to 0.87292, saving model to best.model\n",
      "0s - loss: 0.9610 - acc: 0.5618 - val_loss: 0.8729 - val_acc: 0.6087\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.87292 to 0.86408, saving model to best.model\n",
      "0s - loss: 0.9275 - acc: 0.5618 - val_loss: 0.8641 - val_acc: 0.6087\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.86408 to 0.85509, saving model to best.model\n",
      "0s - loss: 0.8387 - acc: 0.6404 - val_loss: 0.8551 - val_acc: 0.6087\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.85509 to 0.84722, saving model to best.model\n",
      "0s - loss: 0.8786 - acc: 0.6067 - val_loss: 0.8472 - val_acc: 0.6087\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.84722 to 0.83899, saving model to best.model\n",
      "0s - loss: 0.9717 - acc: 0.5506 - val_loss: 0.8390 - val_acc: 0.6087\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.83899 to 0.83119, saving model to best.model\n",
      "0s - loss: 0.8301 - acc: 0.6180 - val_loss: 0.8312 - val_acc: 0.6087\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.83119 to 0.82300, saving model to best.model\n",
      "0s - loss: 0.8704 - acc: 0.5618 - val_loss: 0.8230 - val_acc: 0.6087\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.82300 to 0.81383, saving model to best.model\n",
      "0s - loss: 0.8840 - acc: 0.5169 - val_loss: 0.8138 - val_acc: 0.6087\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.81383 to 0.80443, saving model to best.model\n",
      "0s - loss: 0.8826 - acc: 0.5393 - val_loss: 0.8044 - val_acc: 0.6087\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.80443 to 0.79468, saving model to best.model\n",
      "0s - loss: 0.8303 - acc: 0.5955 - val_loss: 0.7947 - val_acc: 0.6087\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.79468 to 0.78399, saving model to best.model\n",
      "0s - loss: 0.8404 - acc: 0.6517 - val_loss: 0.7840 - val_acc: 0.6087\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.78399 to 0.77313, saving model to best.model\n",
      "0s - loss: 0.7523 - acc: 0.6742 - val_loss: 0.7731 - val_acc: 0.6087\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.77313 to 0.76077, saving model to best.model\n",
      "0s - loss: 0.8807 - acc: 0.5281 - val_loss: 0.7608 - val_acc: 0.6522\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.76077 to 0.74806, saving model to best.model\n",
      "0s - loss: 0.8232 - acc: 0.6180 - val_loss: 0.7481 - val_acc: 0.6522\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.74806 to 0.73523, saving model to best.model\n",
      "0s - loss: 0.8490 - acc: 0.5843 - val_loss: 0.7352 - val_acc: 0.6522\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.73523 to 0.72147, saving model to best.model\n",
      "0s - loss: 0.8167 - acc: 0.6404 - val_loss: 0.7215 - val_acc: 0.6522\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.72147 to 0.70779, saving model to best.model\n",
      "0s - loss: 0.8258 - acc: 0.6292 - val_loss: 0.7078 - val_acc: 0.6522\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.70779 to 0.69333, saving model to best.model\n",
      "0s - loss: 0.8100 - acc: 0.5618 - val_loss: 0.6933 - val_acc: 0.7391\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.69333 to 0.67897, saving model to best.model\n",
      "0s - loss: 0.8092 - acc: 0.6517 - val_loss: 0.6790 - val_acc: 0.7826\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67897 to 0.66563, saving model to best.model\n",
      "0s - loss: 0.7676 - acc: 0.5843 - val_loss: 0.6656 - val_acc: 0.7826\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66563 to 0.65376, saving model to best.model\n",
      "0s - loss: 0.7577 - acc: 0.6854 - val_loss: 0.6538 - val_acc: 0.7826\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65376 to 0.64349, saving model to best.model\n",
      "0s - loss: 0.7896 - acc: 0.5955 - val_loss: 0.6435 - val_acc: 0.7826\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.64349 to 0.63424, saving model to best.model\n",
      "0s - loss: 0.7483 - acc: 0.6292 - val_loss: 0.6342 - val_acc: 0.7826\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.63424 to 0.62612, saving model to best.model\n",
      "0s - loss: 0.8175 - acc: 0.5955 - val_loss: 0.6261 - val_acc: 0.7826\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.62612 to 0.61716, saving model to best.model\n",
      "0s - loss: 0.7328 - acc: 0.6517 - val_loss: 0.6172 - val_acc: 0.7826\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.61716 to 0.60714, saving model to best.model\n",
      "0s - loss: 0.7628 - acc: 0.6629 - val_loss: 0.6071 - val_acc: 0.7826\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.60714 to 0.59704, saving model to best.model\n",
      "0s - loss: 0.7303 - acc: 0.7079 - val_loss: 0.5970 - val_acc: 0.8696\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.59704 to 0.58769, saving model to best.model\n",
      "0s - loss: 0.7138 - acc: 0.6966 - val_loss: 0.5877 - val_acc: 0.8696\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.58769 to 0.57771, saving model to best.model\n",
      "0s - loss: 0.6724 - acc: 0.7416 - val_loss: 0.5777 - val_acc: 0.9130\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.57771 to 0.57006, saving model to best.model\n",
      "0s - loss: 0.6739 - acc: 0.7191 - val_loss: 0.5701 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.57006 to 0.56303, saving model to best.model\n",
      "0s - loss: 0.6708 - acc: 0.6966 - val_loss: 0.5630 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.56303 to 0.55601, saving model to best.model\n",
      "0s - loss: 0.7358 - acc: 0.6292 - val_loss: 0.5560 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.55601 to 0.54850, saving model to best.model\n",
      "0s - loss: 0.6453 - acc: 0.7079 - val_loss: 0.5485 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.54850 to 0.54030, saving model to best.model\n",
      "0s - loss: 0.6932 - acc: 0.6517 - val_loss: 0.5403 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.54030 to 0.53132, saving model to best.model\n",
      "0s - loss: 0.7144 - acc: 0.7303 - val_loss: 0.5313 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.53132 to 0.52304, saving model to best.model\n",
      "0s - loss: 0.6270 - acc: 0.7303 - val_loss: 0.5230 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.52304 to 0.51501, saving model to best.model\n",
      "0s - loss: 0.5996 - acc: 0.7978 - val_loss: 0.5150 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.51501 to 0.50679, saving model to best.model\n",
      "0s - loss: 0.5549 - acc: 0.7978 - val_loss: 0.5068 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.50679 to 0.49869, saving model to best.model\n",
      "0s - loss: 0.6541 - acc: 0.7191 - val_loss: 0.4987 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.49869 to 0.49089, saving model to best.model\n",
      "0s - loss: 0.6030 - acc: 0.7303 - val_loss: 0.4909 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.49089 to 0.48362, saving model to best.model\n",
      "0s - loss: 0.6326 - acc: 0.6966 - val_loss: 0.4836 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.48362 to 0.47659, saving model to best.model\n",
      "0s - loss: 0.5476 - acc: 0.7978 - val_loss: 0.4766 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.47659 to 0.47000, saving model to best.model\n",
      "0s - loss: 0.5845 - acc: 0.7079 - val_loss: 0.4700 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.47000 to 0.46357, saving model to best.model\n",
      "0s - loss: 0.6533 - acc: 0.6517 - val_loss: 0.4636 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.46357 to 0.45619, saving model to best.model\n",
      "0s - loss: 0.6017 - acc: 0.7416 - val_loss: 0.4562 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.45619 to 0.44830, saving model to best.model\n",
      "0s - loss: 0.6009 - acc: 0.8090 - val_loss: 0.4483 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.44830 to 0.44042, saving model to best.model\n",
      "0s - loss: 0.5571 - acc: 0.7753 - val_loss: 0.4404 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.44042 to 0.43282, saving model to best.model\n",
      "0s - loss: 0.5305 - acc: 0.7978 - val_loss: 0.4328 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.43282 to 0.42412, saving model to best.model\n",
      "0s - loss: 0.5858 - acc: 0.7191 - val_loss: 0.4241 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.42412 to 0.41467, saving model to best.model\n",
      "0s - loss: 0.5056 - acc: 0.8202 - val_loss: 0.4147 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.41467 to 0.40598, saving model to best.model\n",
      "0s - loss: 0.5100 - acc: 0.8202 - val_loss: 0.4060 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.40598 to 0.39875, saving model to best.model\n",
      "0s - loss: 0.5128 - acc: 0.8315 - val_loss: 0.3987 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.39875 to 0.39325, saving model to best.model\n",
      "0s - loss: 0.5759 - acc: 0.7640 - val_loss: 0.3933 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.39325 to 0.38776, saving model to best.model\n",
      "0s - loss: 0.4565 - acc: 0.7978 - val_loss: 0.3878 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.38776 to 0.38219, saving model to best.model\n",
      "0s - loss: 0.4957 - acc: 0.8202 - val_loss: 0.3822 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.38219 to 0.37612, saving model to best.model\n",
      "0s - loss: 0.5022 - acc: 0.8315 - val_loss: 0.3761 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.37612 to 0.37023, saving model to best.model\n",
      "0s - loss: 0.4300 - acc: 0.8539 - val_loss: 0.3702 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.37023 to 0.36487, saving model to best.model\n",
      "0s - loss: 0.4730 - acc: 0.8539 - val_loss: 0.3649 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.36487 to 0.35904, saving model to best.model\n",
      "0s - loss: 0.4245 - acc: 0.8764 - val_loss: 0.3590 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.35904 to 0.35433, saving model to best.model\n",
      "0s - loss: 0.4221 - acc: 0.8202 - val_loss: 0.3543 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.35433 to 0.35079, saving model to best.model\n",
      "0s - loss: 0.4492 - acc: 0.8764 - val_loss: 0.3508 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.35079 to 0.34756, saving model to best.model\n",
      "0s - loss: 0.5001 - acc: 0.7865 - val_loss: 0.3476 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.34756 to 0.34525, saving model to best.model\n",
      "0s - loss: 0.4421 - acc: 0.8539 - val_loss: 0.3453 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.34525 to 0.34360, saving model to best.model\n",
      "0s - loss: 0.4370 - acc: 0.8764 - val_loss: 0.3436 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.34360 to 0.34140, saving model to best.model\n",
      "0s - loss: 0.4980 - acc: 0.8202 - val_loss: 0.3414 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.34140 to 0.33931, saving model to best.model\n",
      "0s - loss: 0.3937 - acc: 0.8764 - val_loss: 0.3393 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.33931 to 0.33643, saving model to best.model\n",
      "0s - loss: 0.4448 - acc: 0.8090 - val_loss: 0.3364 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.33643 to 0.33276, saving model to best.model\n",
      "0s - loss: 0.4189 - acc: 0.8427 - val_loss: 0.3328 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.33276 to 0.32804, saving model to best.model\n",
      "0s - loss: 0.4211 - acc: 0.8427 - val_loss: 0.3280 - val_acc: 0.9130\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.32804 to 0.32326, saving model to best.model\n",
      "0s - loss: 0.4278 - acc: 0.8090 - val_loss: 0.3233 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.32326 to 0.31762, saving model to best.model\n",
      "0s - loss: 0.3364 - acc: 0.9101 - val_loss: 0.3176 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.31762 to 0.31068, saving model to best.model\n",
      "0s - loss: 0.3960 - acc: 0.8989 - val_loss: 0.3107 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.31068 to 0.30351, saving model to best.model\n",
      "0s - loss: 0.3872 - acc: 0.8315 - val_loss: 0.3035 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.30351 to 0.29629, saving model to best.model\n",
      "0s - loss: 0.4146 - acc: 0.8539 - val_loss: 0.2963 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.29629 to 0.28883, saving model to best.model\n",
      "0s - loss: 0.3946 - acc: 0.8427 - val_loss: 0.2888 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.28883 to 0.28250, saving model to best.model\n",
      "0s - loss: 0.4094 - acc: 0.8652 - val_loss: 0.2825 - val_acc: 0.9130\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.28250 to 0.27672, saving model to best.model\n",
      "0s - loss: 0.4043 - acc: 0.8427 - val_loss: 0.2767 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.27672 to 0.27024, saving model to best.model\n",
      "0s - loss: 0.3834 - acc: 0.8315 - val_loss: 0.2702 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.27024 to 0.26397, saving model to best.model\n",
      "0s - loss: 0.3961 - acc: 0.8539 - val_loss: 0.2640 - val_acc: 0.9130\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.26397 to 0.25698, saving model to best.model\n",
      "0s - loss: 0.3434 - acc: 0.8652 - val_loss: 0.2570 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.25698 to 0.24968, saving model to best.model\n",
      "0s - loss: 0.3441 - acc: 0.8764 - val_loss: 0.2497 - val_acc: 0.9130\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.24968 to 0.24227, saving model to best.model\n",
      "0s - loss: 0.3823 - acc: 0.8764 - val_loss: 0.2423 - val_acc: 0.9130\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.24227 to 0.23584, saving model to best.model\n",
      "0s - loss: 0.3141 - acc: 0.9326 - val_loss: 0.2358 - val_acc: 0.9130\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.23584 to 0.22927, saving model to best.model\n",
      "0s - loss: 0.2941 - acc: 0.9213 - val_loss: 0.2293 - val_acc: 0.9130\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.22927 to 0.22304, saving model to best.model\n",
      "0s - loss: 0.3149 - acc: 0.9101 - val_loss: 0.2230 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.22304 to 0.21768, saving model to best.model\n",
      "0s - loss: 0.3098 - acc: 0.8989 - val_loss: 0.2177 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.21768 to 0.21300, saving model to best.model\n",
      "0s - loss: 0.3259 - acc: 0.9438 - val_loss: 0.2130 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.21300 to 0.20905, saving model to best.model\n",
      "0s - loss: 0.2628 - acc: 0.9101 - val_loss: 0.2091 - val_acc: 0.9565\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.20905 to 0.20493, saving model to best.model\n",
      "0s - loss: 0.2859 - acc: 0.9326 - val_loss: 0.2049 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.20493 to 0.20065, saving model to best.model\n",
      "0s - loss: 0.2611 - acc: 0.9551 - val_loss: 0.2007 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.20065 to 0.19721, saving model to best.model\n",
      "0s - loss: 0.3131 - acc: 0.8876 - val_loss: 0.1972 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.19721 to 0.19354, saving model to best.model\n",
      "0s - loss: 0.2560 - acc: 0.9326 - val_loss: 0.1935 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.19354 to 0.18978, saving model to best.model\n",
      "0s - loss: 0.2541 - acc: 0.8989 - val_loss: 0.1898 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.18978 to 0.18571, saving model to best.model\n",
      "0s - loss: 0.2693 - acc: 0.8989 - val_loss: 0.1857 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.18571 to 0.18237, saving model to best.model\n",
      "0s - loss: 0.2751 - acc: 0.8876 - val_loss: 0.1824 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.18237 to 0.17821, saving model to best.model\n",
      "0s - loss: 0.2725 - acc: 0.9326 - val_loss: 0.1782 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.17821 to 0.17427, saving model to best.model\n",
      "0s - loss: 0.2785 - acc: 0.9101 - val_loss: 0.1743 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.17427 to 0.17168, saving model to best.model\n",
      "0s - loss: 0.3639 - acc: 0.8652 - val_loss: 0.1717 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.17168 to 0.16943, saving model to best.model\n",
      "0s - loss: 0.2878 - acc: 0.9101 - val_loss: 0.1694 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.16943 to 0.16641, saving model to best.model\n",
      "0s - loss: 0.2205 - acc: 0.9213 - val_loss: 0.1664 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.16641 to 0.16428, saving model to best.model\n",
      "0s - loss: 0.2641 - acc: 0.8989 - val_loss: 0.1643 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.16428 to 0.16174, saving model to best.model\n",
      "0s - loss: 0.2574 - acc: 0.9101 - val_loss: 0.1617 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.16174 to 0.15868, saving model to best.model\n",
      "0s - loss: 0.2264 - acc: 0.9663 - val_loss: 0.1587 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.15868 to 0.15484, saving model to best.model\n",
      "0s - loss: 0.2222 - acc: 0.9101 - val_loss: 0.1548 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.15484 to 0.15135, saving model to best.model\n",
      "0s - loss: 0.2069 - acc: 0.9551 - val_loss: 0.1513 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.15135 to 0.14715, saving model to best.model\n",
      "0s - loss: 0.2009 - acc: 0.9663 - val_loss: 0.1471 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.14715 to 0.14330, saving model to best.model\n",
      "0s - loss: 0.1831 - acc: 0.9551 - val_loss: 0.1433 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.14330 to 0.13911, saving model to best.model\n",
      "0s - loss: 0.2754 - acc: 0.9213 - val_loss: 0.1391 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.13911 to 0.13492, saving model to best.model\n",
      "0s - loss: 0.2405 - acc: 0.9101 - val_loss: 0.1349 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.13492 to 0.13081, saving model to best.model\n",
      "0s - loss: 0.2828 - acc: 0.8876 - val_loss: 0.1308 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.13081 to 0.12743, saving model to best.model\n",
      "0s - loss: 0.1948 - acc: 0.9213 - val_loss: 0.1274 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.12743 to 0.12445, saving model to best.model\n",
      "0s - loss: 0.2613 - acc: 0.9101 - val_loss: 0.1244 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.12445 to 0.12155, saving model to best.model\n",
      "0s - loss: 0.2123 - acc: 0.9101 - val_loss: 0.1215 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.12155 to 0.11843, saving model to best.model\n",
      "0s - loss: 0.1495 - acc: 0.9663 - val_loss: 0.1184 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.11843 to 0.11571, saving model to best.model\n",
      "0s - loss: 0.1630 - acc: 0.9438 - val_loss: 0.1157 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.11571 to 0.11336, saving model to best.model\n",
      "0s - loss: 0.1756 - acc: 0.9551 - val_loss: 0.1134 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.11336 to 0.11132, saving model to best.model\n",
      "0s - loss: 0.1703 - acc: 0.9663 - val_loss: 0.1113 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.11132 to 0.10994, saving model to best.model\n",
      "0s - loss: 0.1362 - acc: 0.9775 - val_loss: 0.1099 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.17567, saving model to best.model\n",
      "0s - loss: 1.4871 - acc: 0.3483 - val_loss: 1.1757 - val_acc: 0.4783\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.17567 to 1.13093, saving model to best.model\n",
      "0s - loss: 1.5270 - acc: 0.2809 - val_loss: 1.1309 - val_acc: 0.1739\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.13093 to 1.10162, saving model to best.model\n",
      "0s - loss: 1.3105 - acc: 0.3708 - val_loss: 1.1016 - val_acc: 0.1304\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.10162 to 1.09073, saving model to best.model\n",
      "0s - loss: 1.3027 - acc: 0.3034 - val_loss: 1.0907 - val_acc: 0.5217\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2406 - acc: 0.3596 - val_loss: 1.1009 - val_acc: 0.3043\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.3736 - acc: 0.2809 - val_loss: 1.1240 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2746 - acc: 0.3371 - val_loss: 1.1499 - val_acc: 0.3043\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1789 - acc: 0.3820 - val_loss: 1.1694 - val_acc: 0.3043\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2154 - acc: 0.3708 - val_loss: 1.1812 - val_acc: 0.3043\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.3814 - acc: 0.2921 - val_loss: 1.1862 - val_acc: 0.3043\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.3115 - acc: 0.3146 - val_loss: 1.1854 - val_acc: 0.3043\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1849 - acc: 0.4382 - val_loss: 1.1771 - val_acc: 0.3043\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2064 - acc: 0.3371 - val_loss: 1.1620 - val_acc: 0.3043\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2280 - acc: 0.3371 - val_loss: 1.1482 - val_acc: 0.3043\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2657 - acc: 0.3258 - val_loss: 1.1334 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2204 - acc: 0.3258 - val_loss: 1.1209 - val_acc: 0.3043\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.3349 - acc: 0.2921 - val_loss: 1.1108 - val_acc: 0.3043\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1172 - acc: 0.3371 - val_loss: 1.1020 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2617 - acc: 0.3820 - val_loss: 1.0925 - val_acc: 0.3043\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.09073 to 1.08469, saving model to best.model\n",
      "0s - loss: 1.3494 - acc: 0.3146 - val_loss: 1.0847 - val_acc: 0.5217\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.08469 to 1.07810, saving model to best.model\n",
      "0s - loss: 1.2016 - acc: 0.3483 - val_loss: 1.0781 - val_acc: 0.5217\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.07810 to 1.07271, saving model to best.model\n",
      "0s - loss: 1.2417 - acc: 0.2921 - val_loss: 1.0727 - val_acc: 0.5217\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.07271 to 1.06746, saving model to best.model\n",
      "0s - loss: 1.2911 - acc: 0.3146 - val_loss: 1.0675 - val_acc: 0.5217\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.06746 to 1.06183, saving model to best.model\n",
      "0s - loss: 1.1499 - acc: 0.3483 - val_loss: 1.0618 - val_acc: 0.6522\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.06183 to 1.05805, saving model to best.model\n",
      "0s - loss: 1.1829 - acc: 0.3258 - val_loss: 1.0580 - val_acc: 0.6957\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.05805 to 1.05577, saving model to best.model\n",
      "0s - loss: 1.1821 - acc: 0.3820 - val_loss: 1.0558 - val_acc: 0.6522\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.05577 to 1.05515, saving model to best.model\n",
      "0s - loss: 1.1612 - acc: 0.3258 - val_loss: 1.0551 - val_acc: 0.5652\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1582 - acc: 0.3933 - val_loss: 1.0556 - val_acc: 0.5217\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1834 - acc: 0.2809 - val_loss: 1.0565 - val_acc: 0.5217\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1988 - acc: 0.3933 - val_loss: 1.0586 - val_acc: 0.5217\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.2089 - acc: 0.3258 - val_loss: 1.0594 - val_acc: 0.5217\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1893 - acc: 0.3596 - val_loss: 1.0585 - val_acc: 0.5217\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.2200 - acc: 0.2921 - val_loss: 1.0581 - val_acc: 0.5217\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.1212 - acc: 0.4157 - val_loss: 1.0569 - val_acc: 0.5217\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 1.1157 - acc: 0.4157 - val_loss: 1.0559 - val_acc: 0.5217\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.05515 to 1.05396, saving model to best.model\n",
      "0s - loss: 1.1392 - acc: 0.4157 - val_loss: 1.0540 - val_acc: 0.5217\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.05396 to 1.05228, saving model to best.model\n",
      "0s - loss: 1.0735 - acc: 0.4270 - val_loss: 1.0523 - val_acc: 0.5217\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.05228 to 1.04895, saving model to best.model\n",
      "0s - loss: 1.0710 - acc: 0.3820 - val_loss: 1.0489 - val_acc: 0.5217\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.04895 to 1.04689, saving model to best.model\n",
      "0s - loss: 1.1287 - acc: 0.4157 - val_loss: 1.0469 - val_acc: 0.5217\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.04689 to 1.04546, saving model to best.model\n",
      "0s - loss: 1.0431 - acc: 0.5056 - val_loss: 1.0455 - val_acc: 0.5217\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.04546 to 1.04284, saving model to best.model\n",
      "0s - loss: 1.0613 - acc: 0.4719 - val_loss: 1.0428 - val_acc: 0.5217\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.04284 to 1.03983, saving model to best.model\n",
      "0s - loss: 1.0649 - acc: 0.4157 - val_loss: 1.0398 - val_acc: 0.5217\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.03983 to 1.03610, saving model to best.model\n",
      "0s - loss: 1.2469 - acc: 0.3933 - val_loss: 1.0361 - val_acc: 0.5217\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.03610 to 1.03138, saving model to best.model\n",
      "0s - loss: 1.0669 - acc: 0.5281 - val_loss: 1.0314 - val_acc: 0.5217\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.03138 to 1.02722, saving model to best.model\n",
      "0s - loss: 1.1541 - acc: 0.3820 - val_loss: 1.0272 - val_acc: 0.5217\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.02722 to 1.02500, saving model to best.model\n",
      "0s - loss: 1.1710 - acc: 0.3596 - val_loss: 1.0250 - val_acc: 0.5217\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.02500 to 1.02370, saving model to best.model\n",
      "0s - loss: 1.1534 - acc: 0.4270 - val_loss: 1.0237 - val_acc: 0.5217\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.02370 to 1.02282, saving model to best.model\n",
      "0s - loss: 1.0416 - acc: 0.4270 - val_loss: 1.0228 - val_acc: 0.5217\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.02282 to 1.02176, saving model to best.model\n",
      "0s - loss: 1.0547 - acc: 0.4719 - val_loss: 1.0218 - val_acc: 0.5217\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.02176 to 1.01809, saving model to best.model\n",
      "0s - loss: 1.0976 - acc: 0.3596 - val_loss: 1.0181 - val_acc: 0.5217\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.01809 to 1.01343, saving model to best.model\n",
      "0s - loss: 1.0610 - acc: 0.4270 - val_loss: 1.0134 - val_acc: 0.5217\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 1.01343 to 1.00721, saving model to best.model\n",
      "0s - loss: 1.0975 - acc: 0.4045 - val_loss: 1.0072 - val_acc: 0.5217\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 1.00721 to 1.00130, saving model to best.model\n",
      "0s - loss: 1.0488 - acc: 0.4944 - val_loss: 1.0013 - val_acc: 0.5217\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 1.00130 to 0.99503, saving model to best.model\n",
      "0s - loss: 1.1466 - acc: 0.4494 - val_loss: 0.9950 - val_acc: 0.5217\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.99503 to 0.98844, saving model to best.model\n",
      "0s - loss: 1.0804 - acc: 0.4494 - val_loss: 0.9884 - val_acc: 0.5217\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.98844 to 0.98110, saving model to best.model\n",
      "0s - loss: 1.0559 - acc: 0.4719 - val_loss: 0.9811 - val_acc: 0.5217\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.98110 to 0.97310, saving model to best.model\n",
      "0s - loss: 1.0943 - acc: 0.4045 - val_loss: 0.9731 - val_acc: 0.5217\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.97310 to 0.96562, saving model to best.model\n",
      "0s - loss: 1.1549 - acc: 0.3483 - val_loss: 0.9656 - val_acc: 0.5217\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.96562 to 0.95798, saving model to best.model\n",
      "0s - loss: 1.0563 - acc: 0.4494 - val_loss: 0.9580 - val_acc: 0.5217\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.95798 to 0.94957, saving model to best.model\n",
      "0s - loss: 1.0465 - acc: 0.5169 - val_loss: 0.9496 - val_acc: 0.5217\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.94957 to 0.94095, saving model to best.model\n",
      "0s - loss: 1.0356 - acc: 0.4944 - val_loss: 0.9409 - val_acc: 0.5217\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.94095 to 0.93230, saving model to best.model\n",
      "0s - loss: 0.9198 - acc: 0.5281 - val_loss: 0.9323 - val_acc: 0.5217\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.93230 to 0.92456, saving model to best.model\n",
      "0s - loss: 0.9713 - acc: 0.5843 - val_loss: 0.9246 - val_acc: 0.5652\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.92456 to 0.91780, saving model to best.model\n",
      "0s - loss: 1.0181 - acc: 0.4494 - val_loss: 0.9178 - val_acc: 0.5652\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.91780 to 0.91203, saving model to best.model\n",
      "0s - loss: 0.9981 - acc: 0.4045 - val_loss: 0.9120 - val_acc: 0.6522\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.91203 to 0.90716, saving model to best.model\n",
      "0s - loss: 1.0363 - acc: 0.4607 - val_loss: 0.9072 - val_acc: 0.6522\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.90716 to 0.90192, saving model to best.model\n",
      "0s - loss: 0.9856 - acc: 0.5506 - val_loss: 0.9019 - val_acc: 0.6522\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.90192 to 0.89662, saving model to best.model\n",
      "0s - loss: 0.9218 - acc: 0.5843 - val_loss: 0.8966 - val_acc: 0.6522\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.89662 to 0.89082, saving model to best.model\n",
      "0s - loss: 0.9652 - acc: 0.5393 - val_loss: 0.8908 - val_acc: 0.6522\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.89082 to 0.88561, saving model to best.model\n",
      "0s - loss: 0.9457 - acc: 0.5506 - val_loss: 0.8856 - val_acc: 0.6522\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.88561 to 0.87877, saving model to best.model\n",
      "0s - loss: 0.9245 - acc: 0.5730 - val_loss: 0.8788 - val_acc: 0.6522\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.87877 to 0.87081, saving model to best.model\n",
      "0s - loss: 0.9211 - acc: 0.5730 - val_loss: 0.8708 - val_acc: 0.6522\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.87081 to 0.86175, saving model to best.model\n",
      "0s - loss: 0.9075 - acc: 0.6067 - val_loss: 0.8618 - val_acc: 0.6957\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.86175 to 0.85159, saving model to best.model\n",
      "0s - loss: 0.9713 - acc: 0.5393 - val_loss: 0.8516 - val_acc: 0.6957\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.85159 to 0.84143, saving model to best.model\n",
      "0s - loss: 0.9694 - acc: 0.5730 - val_loss: 0.8414 - val_acc: 0.6957\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.84143 to 0.83037, saving model to best.model\n",
      "0s - loss: 0.9569 - acc: 0.5393 - val_loss: 0.8304 - val_acc: 0.6957\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.83037 to 0.81943, saving model to best.model\n",
      "0s - loss: 0.9515 - acc: 0.5393 - val_loss: 0.8194 - val_acc: 0.7826\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.81943 to 0.80862, saving model to best.model\n",
      "0s - loss: 0.9402 - acc: 0.5281 - val_loss: 0.8086 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.80862 to 0.79851, saving model to best.model\n",
      "0s - loss: 0.9064 - acc: 0.5169 - val_loss: 0.7985 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.79851 to 0.78909, saving model to best.model\n",
      "0s - loss: 0.9217 - acc: 0.5955 - val_loss: 0.7891 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.78909 to 0.78017, saving model to best.model\n",
      "0s - loss: 0.8820 - acc: 0.6292 - val_loss: 0.7802 - val_acc: 0.7826\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.78017 to 0.77057, saving model to best.model\n",
      "0s - loss: 0.8528 - acc: 0.6854 - val_loss: 0.7706 - val_acc: 0.8261\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.77057 to 0.76166, saving model to best.model\n",
      "0s - loss: 0.8234 - acc: 0.6404 - val_loss: 0.7617 - val_acc: 0.8261\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.76166 to 0.75221, saving model to best.model\n",
      "0s - loss: 0.8232 - acc: 0.6404 - val_loss: 0.7522 - val_acc: 0.8261\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.75221 to 0.74366, saving model to best.model\n",
      "0s - loss: 0.8915 - acc: 0.5843 - val_loss: 0.7437 - val_acc: 0.8261\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.74366 to 0.73479, saving model to best.model\n",
      "0s - loss: 0.8149 - acc: 0.6404 - val_loss: 0.7348 - val_acc: 0.8261\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.73479 to 0.72578, saving model to best.model\n",
      "0s - loss: 0.7547 - acc: 0.7079 - val_loss: 0.7258 - val_acc: 0.8261\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.72578 to 0.71724, saving model to best.model\n",
      "0s - loss: 0.8291 - acc: 0.6067 - val_loss: 0.7172 - val_acc: 0.8261\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.71724 to 0.70775, saving model to best.model\n",
      "0s - loss: 0.7846 - acc: 0.6180 - val_loss: 0.7078 - val_acc: 0.8261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.70775 to 0.69639, saving model to best.model\n",
      "0s - loss: 0.7165 - acc: 0.7303 - val_loss: 0.6964 - val_acc: 0.8261\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.69639 to 0.68518, saving model to best.model\n",
      "0s - loss: 0.8184 - acc: 0.5843 - val_loss: 0.6852 - val_acc: 0.8261\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68518 to 0.67424, saving model to best.model\n",
      "0s - loss: 0.7486 - acc: 0.6966 - val_loss: 0.6742 - val_acc: 0.8261\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67424 to 0.66264, saving model to best.model\n",
      "0s - loss: 0.6917 - acc: 0.7303 - val_loss: 0.6626 - val_acc: 0.8261\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66264 to 0.65125, saving model to best.model\n",
      "0s - loss: 0.7740 - acc: 0.6517 - val_loss: 0.6513 - val_acc: 0.8261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.65125 to 0.63988, saving model to best.model\n",
      "0s - loss: 0.7007 - acc: 0.6742 - val_loss: 0.6399 - val_acc: 0.8261\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.63988 to 0.62822, saving model to best.model\n",
      "0s - loss: 0.6771 - acc: 0.7753 - val_loss: 0.6282 - val_acc: 0.8261\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.62822 to 0.61655, saving model to best.model\n",
      "0s - loss: 0.6562 - acc: 0.7528 - val_loss: 0.6166 - val_acc: 0.8261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.61655 to 0.60421, saving model to best.model\n",
      "0s - loss: 0.5928 - acc: 0.7640 - val_loss: 0.6042 - val_acc: 0.8696\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.60421 to 0.59115, saving model to best.model\n",
      "0s - loss: 0.6601 - acc: 0.7528 - val_loss: 0.5911 - val_acc: 0.8696\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.59115 to 0.57892, saving model to best.model\n",
      "0s - loss: 0.6820 - acc: 0.6854 - val_loss: 0.5789 - val_acc: 0.8696\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.57892 to 0.56762, saving model to best.model\n",
      "0s - loss: 0.7139 - acc: 0.6854 - val_loss: 0.5676 - val_acc: 0.8696\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.56762 to 0.55626, saving model to best.model\n",
      "0s - loss: 0.6599 - acc: 0.6966 - val_loss: 0.5563 - val_acc: 0.8696\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.55626 to 0.54535, saving model to best.model\n",
      "0s - loss: 0.6754 - acc: 0.7079 - val_loss: 0.5454 - val_acc: 0.8696\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.54535 to 0.53482, saving model to best.model\n",
      "0s - loss: 0.6678 - acc: 0.7079 - val_loss: 0.5348 - val_acc: 0.8696\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.53482 to 0.52630, saving model to best.model\n",
      "0s - loss: 0.5991 - acc: 0.8090 - val_loss: 0.5263 - val_acc: 0.8696\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.52630 to 0.51782, saving model to best.model\n",
      "0s - loss: 0.6296 - acc: 0.6854 - val_loss: 0.5178 - val_acc: 0.8696\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.51782 to 0.51104, saving model to best.model\n",
      "0s - loss: 0.6437 - acc: 0.7865 - val_loss: 0.5110 - val_acc: 0.8696\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.51104 to 0.50576, saving model to best.model\n",
      "0s - loss: 0.6206 - acc: 0.7528 - val_loss: 0.5058 - val_acc: 0.8696\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.50576 to 0.50113, saving model to best.model\n",
      "0s - loss: 0.6137 - acc: 0.7303 - val_loss: 0.5011 - val_acc: 0.8696\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.50113 to 0.49580, saving model to best.model\n",
      "0s - loss: 0.5548 - acc: 0.7865 - val_loss: 0.4958 - val_acc: 0.8696\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.49580 to 0.49054, saving model to best.model\n",
      "0s - loss: 0.5123 - acc: 0.7865 - val_loss: 0.4905 - val_acc: 0.8696\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.49054 to 0.48525, saving model to best.model\n",
      "0s - loss: 0.6182 - acc: 0.7416 - val_loss: 0.4852 - val_acc: 0.8696\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.48525 to 0.48030, saving model to best.model\n",
      "0s - loss: 0.5837 - acc: 0.7865 - val_loss: 0.4803 - val_acc: 0.8696\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.48030 to 0.47472, saving model to best.model\n",
      "0s - loss: 0.5743 - acc: 0.7079 - val_loss: 0.4747 - val_acc: 0.8696\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.47472 to 0.46829, saving model to best.model\n",
      "0s - loss: 0.5596 - acc: 0.7640 - val_loss: 0.4683 - val_acc: 0.8696\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.46829 to 0.46105, saving model to best.model\n",
      "0s - loss: 0.5851 - acc: 0.7079 - val_loss: 0.4611 - val_acc: 0.8696\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.46105 to 0.45349, saving model to best.model\n",
      "0s - loss: 0.5549 - acc: 0.7416 - val_loss: 0.4535 - val_acc: 0.8696\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.45349 to 0.44487, saving model to best.model\n",
      "0s - loss: 0.5352 - acc: 0.7753 - val_loss: 0.4449 - val_acc: 0.8696\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.44487 to 0.43649, saving model to best.model\n",
      "0s - loss: 0.4982 - acc: 0.7978 - val_loss: 0.4365 - val_acc: 0.8696\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.43649 to 0.42803, saving model to best.model\n",
      "0s - loss: 0.5193 - acc: 0.7416 - val_loss: 0.4280 - val_acc: 0.8696\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.42803 to 0.41878, saving model to best.model\n",
      "0s - loss: 0.4496 - acc: 0.8202 - val_loss: 0.4188 - val_acc: 0.8696\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.41878 to 0.40863, saving model to best.model\n",
      "0s - loss: 0.4691 - acc: 0.8315 - val_loss: 0.4086 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.40863 to 0.39960, saving model to best.model\n",
      "0s - loss: 0.4782 - acc: 0.8090 - val_loss: 0.3996 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.39960 to 0.39088, saving model to best.model\n",
      "0s - loss: 0.4912 - acc: 0.7978 - val_loss: 0.3909 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.39088 to 0.38411, saving model to best.model\n",
      "0s - loss: 0.4865 - acc: 0.8090 - val_loss: 0.3841 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.38411 to 0.37629, saving model to best.model\n",
      "0s - loss: 0.5258 - acc: 0.8202 - val_loss: 0.3763 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.37629 to 0.36848, saving model to best.model\n",
      "0s - loss: 0.5131 - acc: 0.7978 - val_loss: 0.3685 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.36848 to 0.36076, saving model to best.model\n",
      "0s - loss: 0.4953 - acc: 0.7865 - val_loss: 0.3608 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.36076 to 0.35559, saving model to best.model\n",
      "0s - loss: 0.4849 - acc: 0.7865 - val_loss: 0.3556 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.35559 to 0.35014, saving model to best.model\n",
      "0s - loss: 0.4560 - acc: 0.8652 - val_loss: 0.3501 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.35014 to 0.34443, saving model to best.model\n",
      "0s - loss: 0.4567 - acc: 0.8090 - val_loss: 0.3444 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.34443 to 0.33857, saving model to best.model\n",
      "0s - loss: 0.4575 - acc: 0.8539 - val_loss: 0.3386 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.33857 to 0.33295, saving model to best.model\n",
      "0s - loss: 0.4177 - acc: 0.8764 - val_loss: 0.3329 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.33295 to 0.32679, saving model to best.model\n",
      "0s - loss: 0.4233 - acc: 0.8315 - val_loss: 0.3268 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.32679 to 0.32173, saving model to best.model\n",
      "0s - loss: 0.4168 - acc: 0.8427 - val_loss: 0.3217 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.32173 to 0.31694, saving model to best.model\n",
      "0s - loss: 0.3462 - acc: 0.8876 - val_loss: 0.3169 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.31694 to 0.31261, saving model to best.model\n",
      "0s - loss: 0.4562 - acc: 0.7865 - val_loss: 0.3126 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.31261 to 0.30895, saving model to best.model\n",
      "0s - loss: 0.4412 - acc: 0.8202 - val_loss: 0.3089 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.30895 to 0.30478, saving model to best.model\n",
      "0s - loss: 0.3732 - acc: 0.8764 - val_loss: 0.3048 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.30478 to 0.30136, saving model to best.model\n",
      "0s - loss: 0.3571 - acc: 0.8539 - val_loss: 0.3014 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.30136 to 0.29716, saving model to best.model\n",
      "0s - loss: 0.4291 - acc: 0.7978 - val_loss: 0.2972 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.29716 to 0.29285, saving model to best.model\n",
      "0s - loss: 0.3983 - acc: 0.8764 - val_loss: 0.2928 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.29285 to 0.28789, saving model to best.model\n",
      "0s - loss: 0.4051 - acc: 0.8202 - val_loss: 0.2879 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.28789 to 0.28235, saving model to best.model\n",
      "0s - loss: 0.3310 - acc: 0.8539 - val_loss: 0.2823 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.28235 to 0.27761, saving model to best.model\n",
      "0s - loss: 0.3520 - acc: 0.8764 - val_loss: 0.2776 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.27761 to 0.27295, saving model to best.model\n",
      "0s - loss: 0.4431 - acc: 0.8202 - val_loss: 0.2729 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.27295 to 0.26810, saving model to best.model\n",
      "0s - loss: 0.3795 - acc: 0.8315 - val_loss: 0.2681 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.26810 to 0.26342, saving model to best.model\n",
      "0s - loss: 0.3544 - acc: 0.8876 - val_loss: 0.2634 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.26342 to 0.25962, saving model to best.model\n",
      "0s - loss: 0.3386 - acc: 0.9213 - val_loss: 0.2596 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.25962 to 0.25706, saving model to best.model\n",
      "0s - loss: 0.3397 - acc: 0.8764 - val_loss: 0.2571 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.25706 to 0.25415, saving model to best.model\n",
      "0s - loss: 0.3217 - acc: 0.8764 - val_loss: 0.2542 - val_acc: 0.9130\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.25415 to 0.25097, saving model to best.model\n",
      "0s - loss: 0.3301 - acc: 0.8764 - val_loss: 0.2510 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.25097 to 0.24718, saving model to best.model\n",
      "0s - loss: 0.2729 - acc: 0.9213 - val_loss: 0.2472 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.24718 to 0.24266, saving model to best.model\n",
      "0s - loss: 0.3213 - acc: 0.8989 - val_loss: 0.2427 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.24266 to 0.23844, saving model to best.model\n",
      "0s - loss: 0.3214 - acc: 0.8989 - val_loss: 0.2384 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.23844 to 0.23377, saving model to best.model\n",
      "0s - loss: 0.2787 - acc: 0.9101 - val_loss: 0.2338 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.23377 to 0.22814, saving model to best.model\n",
      "0s - loss: 0.3209 - acc: 0.9101 - val_loss: 0.2281 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.22814 to 0.22270, saving model to best.model\n",
      "0s - loss: 0.2505 - acc: 0.8876 - val_loss: 0.2227 - val_acc: 0.9130\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.22270 to 0.21834, saving model to best.model\n",
      "0s - loss: 0.3472 - acc: 0.8539 - val_loss: 0.2183 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.21834 to 0.21495, saving model to best.model\n",
      "0s - loss: 0.3550 - acc: 0.8876 - val_loss: 0.2149 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.21495 to 0.21122, saving model to best.model\n",
      "0s - loss: 0.3076 - acc: 0.9101 - val_loss: 0.2112 - val_acc: 0.9130\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.21122 to 0.20849, saving model to best.model\n",
      "0s - loss: 0.2614 - acc: 0.8764 - val_loss: 0.2085 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.20849 to 0.20580, saving model to best.model\n",
      "0s - loss: 0.2765 - acc: 0.9213 - val_loss: 0.2058 - val_acc: 0.9130\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.20580 to 0.20273, saving model to best.model\n",
      "0s - loss: 0.2423 - acc: 0.9213 - val_loss: 0.2027 - val_acc: 0.9130\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.20273 to 0.20085, saving model to best.model\n",
      "0s - loss: 0.2460 - acc: 0.9326 - val_loss: 0.2008 - val_acc: 0.9130\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.20085 to 0.19877, saving model to best.model\n",
      "0s - loss: 0.3206 - acc: 0.8764 - val_loss: 0.1988 - val_acc: 0.9130\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.19877 to 0.19655, saving model to best.model\n",
      "0s - loss: 0.2185 - acc: 0.9438 - val_loss: 0.1966 - val_acc: 0.9130\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.19655 to 0.19426, saving model to best.model\n",
      "0s - loss: 0.3014 - acc: 0.8764 - val_loss: 0.1943 - val_acc: 0.9130\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.19426 to 0.19239, saving model to best.model\n",
      "0s - loss: 0.1913 - acc: 0.9438 - val_loss: 0.1924 - val_acc: 0.9130\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.19239 to 0.18960, saving model to best.model\n",
      "0s - loss: 0.2760 - acc: 0.8989 - val_loss: 0.1896 - val_acc: 0.9130\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.18960 to 0.18630, saving model to best.model\n",
      "0s - loss: 0.2175 - acc: 0.9551 - val_loss: 0.1863 - val_acc: 0.9130\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.18630 to 0.18226, saving model to best.model\n",
      "0s - loss: 0.2591 - acc: 0.8989 - val_loss: 0.1823 - val_acc: 0.9130\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.18226 to 0.17792, saving model to best.model\n",
      "0s - loss: 0.1842 - acc: 0.9551 - val_loss: 0.1779 - val_acc: 0.9130\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.17792 to 0.17361, saving model to best.model\n",
      "0s - loss: 0.2285 - acc: 0.9551 - val_loss: 0.1736 - val_acc: 0.9130\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.17361 to 0.16999, saving model to best.model\n",
      "0s - loss: 0.2250 - acc: 0.9101 - val_loss: 0.1700 - val_acc: 0.9130\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.16999 to 0.16636, saving model to best.model\n",
      "0s - loss: 0.2349 - acc: 0.9438 - val_loss: 0.1664 - val_acc: 0.9130\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.16636 to 0.16277, saving model to best.model\n",
      "0s - loss: 0.2446 - acc: 0.9326 - val_loss: 0.1628 - val_acc: 0.9130\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.16277 to 0.15999, saving model to best.model\n",
      "0s - loss: 0.2425 - acc: 0.9326 - val_loss: 0.1600 - val_acc: 0.9130\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.15999 to 0.15733, saving model to best.model\n",
      "0s - loss: 0.2414 - acc: 0.9101 - val_loss: 0.1573 - val_acc: 0.9130\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.15733 to 0.15564, saving model to best.model\n",
      "0s - loss: 0.2412 - acc: 0.9101 - val_loss: 0.1556 - val_acc: 0.9130\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.15564 to 0.15339, saving model to best.model\n",
      "0s - loss: 0.2324 - acc: 0.9213 - val_loss: 0.1534 - val_acc: 0.9130\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.15339 to 0.14961, saving model to best.model\n",
      "0s - loss: 0.2319 - acc: 0.9326 - val_loss: 0.1496 - val_acc: 0.9130\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.14961 to 0.14656, saving model to best.model\n",
      "0s - loss: 0.2168 - acc: 0.9213 - val_loss: 0.1466 - val_acc: 0.9130\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.14656 to 0.14291, saving model to best.model\n",
      "0s - loss: 0.1865 - acc: 0.9551 - val_loss: 0.1429 - val_acc: 0.9130\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.14291 to 0.14010, saving model to best.model\n",
      "0s - loss: 0.2644 - acc: 0.8876 - val_loss: 0.1401 - val_acc: 0.9130\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.14010 to 0.13792, saving model to best.model\n",
      "0s - loss: 0.2308 - acc: 0.8764 - val_loss: 0.1379 - val_acc: 0.9130\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.13792 to 0.13507, saving model to best.model\n",
      "0s - loss: 0.2393 - acc: 0.8989 - val_loss: 0.1351 - val_acc: 0.9130\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.13507 to 0.13332, saving model to best.model\n",
      "0s - loss: 0.1746 - acc: 0.9551 - val_loss: 0.1333 - val_acc: 0.9130\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.13332 to 0.13278, saving model to best.model\n",
      "0s - loss: 0.2170 - acc: 0.8989 - val_loss: 0.1328 - val_acc: 0.9130\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.13278 to 0.13175, saving model to best.model\n",
      "0s - loss: 0.1993 - acc: 0.9326 - val_loss: 0.1317 - val_acc: 0.9130\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.13175 to 0.13053, saving model to best.model\n",
      "0s - loss: 0.1681 - acc: 0.9326 - val_loss: 0.1305 - val_acc: 0.9130\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.13053 to 0.12854, saving model to best.model\n",
      "0s - loss: 0.2053 - acc: 0.9213 - val_loss: 0.1285 - val_acc: 0.9130\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.12854 to 0.12689, saving model to best.model\n",
      "0s - loss: 0.1969 - acc: 0.9326 - val_loss: 0.1269 - val_acc: 0.9130\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.12689 to 0.12496, saving model to best.model\n",
      "0s - loss: 0.1840 - acc: 0.9438 - val_loss: 0.1250 - val_acc: 0.9130\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.12496 to 0.12350, saving model to best.model\n",
      "0s - loss: 0.1987 - acc: 0.8876 - val_loss: 0.1235 - val_acc: 0.9130\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.12350 to 0.12242, saving model to best.model\n",
      "0s - loss: 0.1782 - acc: 0.9326 - val_loss: 0.1224 - val_acc: 0.9130\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.12242 to 0.11995, saving model to best.model\n",
      "0s - loss: 0.1854 - acc: 0.9213 - val_loss: 0.1199 - val_acc: 0.9130\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.11995 to 0.11749, saving model to best.model\n",
      "0s - loss: 0.1638 - acc: 0.9663 - val_loss: 0.1175 - val_acc: 0.9130\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.11749 to 0.11442, saving model to best.model\n",
      "0s - loss: 0.1727 - acc: 0.9551 - val_loss: 0.1144 - val_acc: 0.9130\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.11442 to 0.11098, saving model to best.model\n",
      "0s - loss: 0.2129 - acc: 0.9101 - val_loss: 0.1110 - val_acc: 0.9130\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.53430, saving model to best.model\n",
      "0s - loss: 1.9091 - acc: 0.2697 - val_loss: 1.5343 - val_acc: 0.2609\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.53430 to 1.36472, saving model to best.model\n",
      "0s - loss: 1.6011 - acc: 0.2809 - val_loss: 1.3647 - val_acc: 0.2609\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.36472 to 1.23069, saving model to best.model\n",
      "0s - loss: 1.5101 - acc: 0.2584 - val_loss: 1.2307 - val_acc: 0.2609\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.23069 to 1.13040, saving model to best.model\n",
      "0s - loss: 1.3508 - acc: 0.2809 - val_loss: 1.1304 - val_acc: 0.2609\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.13040 to 1.06069, saving model to best.model\n",
      "0s - loss: 1.3259 - acc: 0.2921 - val_loss: 1.0607 - val_acc: 0.5217\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.06069 to 1.01666, saving model to best.model\n",
      "0s - loss: 1.2870 - acc: 0.3034 - val_loss: 1.0167 - val_acc: 0.5652\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.01666 to 0.99534, saving model to best.model\n",
      "0s - loss: 1.1445 - acc: 0.4157 - val_loss: 0.9953 - val_acc: 0.5652\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.99534 to 0.99172, saving model to best.model\n",
      "0s - loss: 1.1750 - acc: 0.3933 - val_loss: 0.9917 - val_acc: 0.5652\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1535 - acc: 0.4719 - val_loss: 0.9986 - val_acc: 0.5652\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.3009 - acc: 0.3820 - val_loss: 1.0095 - val_acc: 0.5652\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2472 - acc: 0.4831 - val_loss: 1.0200 - val_acc: 0.5652\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2594 - acc: 0.4157 - val_loss: 1.0290 - val_acc: 0.5652\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2730 - acc: 0.4270 - val_loss: 1.0335 - val_acc: 0.5652\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2710 - acc: 0.4719 - val_loss: 1.0339 - val_acc: 0.5652\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2795 - acc: 0.4607 - val_loss: 1.0304 - val_acc: 0.5652\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.1441 - acc: 0.4719 - val_loss: 1.0244 - val_acc: 0.5652\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1773 - acc: 0.5056 - val_loss: 1.0169 - val_acc: 0.5652\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1439 - acc: 0.4719 - val_loss: 1.0085 - val_acc: 0.5652\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1929 - acc: 0.4270 - val_loss: 1.0006 - val_acc: 0.5652\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1195 - acc: 0.4607 - val_loss: 0.9940 - val_acc: 0.5652\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.99172 to 0.98834, saving model to best.model\n",
      "0s - loss: 1.2388 - acc: 0.4045 - val_loss: 0.9883 - val_acc: 0.5652\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.98834 to 0.98342, saving model to best.model\n",
      "0s - loss: 1.1061 - acc: 0.4270 - val_loss: 0.9834 - val_acc: 0.5652\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.98342 to 0.97925, saving model to best.model\n",
      "0s - loss: 1.1131 - acc: 0.4157 - val_loss: 0.9792 - val_acc: 0.5652\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.97925 to 0.97597, saving model to best.model\n",
      "0s - loss: 1.1492 - acc: 0.3596 - val_loss: 0.9760 - val_acc: 0.5652\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.97597 to 0.97326, saving model to best.model\n",
      "0s - loss: 1.1549 - acc: 0.4045 - val_loss: 0.9733 - val_acc: 0.5652\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.97326 to 0.97157, saving model to best.model\n",
      "0s - loss: 1.0962 - acc: 0.4719 - val_loss: 0.9716 - val_acc: 0.5652\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.97157 to 0.97052, saving model to best.model\n",
      "0s - loss: 1.2131 - acc: 0.3483 - val_loss: 0.9705 - val_acc: 0.5652\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.97052 to 0.96936, saving model to best.model\n",
      "0s - loss: 1.2934 - acc: 0.3371 - val_loss: 0.9694 - val_acc: 0.5652\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.96936 to 0.96839, saving model to best.model\n",
      "0s - loss: 1.2004 - acc: 0.4157 - val_loss: 0.9684 - val_acc: 0.5652\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.96839 to 0.96762, saving model to best.model\n",
      "0s - loss: 1.0498 - acc: 0.4719 - val_loss: 0.9676 - val_acc: 0.5652\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.96762 to 0.96632, saving model to best.model\n",
      "0s - loss: 1.1140 - acc: 0.4494 - val_loss: 0.9663 - val_acc: 0.5652\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.96632 to 0.96506, saving model to best.model\n",
      "0s - loss: 1.1276 - acc: 0.4494 - val_loss: 0.9651 - val_acc: 0.5652\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.96506 to 0.96344, saving model to best.model\n",
      "0s - loss: 1.1357 - acc: 0.4270 - val_loss: 0.9634 - val_acc: 0.5652\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.96344 to 0.96098, saving model to best.model\n",
      "0s - loss: 1.1910 - acc: 0.4045 - val_loss: 0.9610 - val_acc: 0.5652\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.96098 to 0.95821, saving model to best.model\n",
      "0s - loss: 1.1006 - acc: 0.3933 - val_loss: 0.9582 - val_acc: 0.5652\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.95821 to 0.95553, saving model to best.model\n",
      "0s - loss: 1.1233 - acc: 0.3933 - val_loss: 0.9555 - val_acc: 0.5652\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.95553 to 0.95310, saving model to best.model\n",
      "0s - loss: 1.1002 - acc: 0.4944 - val_loss: 0.9531 - val_acc: 0.5652\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.95310 to 0.95063, saving model to best.model\n",
      "0s - loss: 1.1162 - acc: 0.4157 - val_loss: 0.9506 - val_acc: 0.5652\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.95063 to 0.94788, saving model to best.model\n",
      "0s - loss: 1.1211 - acc: 0.4157 - val_loss: 0.9479 - val_acc: 0.5652\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.94788 to 0.94537, saving model to best.model\n",
      "0s - loss: 1.1333 - acc: 0.3820 - val_loss: 0.9454 - val_acc: 0.5652\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.94537 to 0.94296, saving model to best.model\n",
      "0s - loss: 1.1177 - acc: 0.4270 - val_loss: 0.9430 - val_acc: 0.5652\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.94296 to 0.94087, saving model to best.model\n",
      "0s - loss: 0.9775 - acc: 0.5618 - val_loss: 0.9409 - val_acc: 0.5652\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.94087 to 0.93889, saving model to best.model\n",
      "0s - loss: 1.2147 - acc: 0.3820 - val_loss: 0.9389 - val_acc: 0.5652\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.93889 to 0.93742, saving model to best.model\n",
      "0s - loss: 1.0773 - acc: 0.4382 - val_loss: 0.9374 - val_acc: 0.5652\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.93742 to 0.93615, saving model to best.model\n",
      "0s - loss: 1.0677 - acc: 0.4719 - val_loss: 0.9362 - val_acc: 0.5652\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.93615 to 0.93469, saving model to best.model\n",
      "0s - loss: 1.1428 - acc: 0.4494 - val_loss: 0.9347 - val_acc: 0.5652\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.93469 to 0.93311, saving model to best.model\n",
      "0s - loss: 1.0760 - acc: 0.4270 - val_loss: 0.9331 - val_acc: 0.5652\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.93311 to 0.93109, saving model to best.model\n",
      "0s - loss: 1.0746 - acc: 0.4831 - val_loss: 0.9311 - val_acc: 0.5652\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.93109 to 0.92898, saving model to best.model\n",
      "0s - loss: 1.1243 - acc: 0.4045 - val_loss: 0.9290 - val_acc: 0.5652\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.92898 to 0.92694, saving model to best.model\n",
      "0s - loss: 1.0681 - acc: 0.4607 - val_loss: 0.9269 - val_acc: 0.5652\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.92694 to 0.92484, saving model to best.model\n",
      "0s - loss: 1.1408 - acc: 0.3820 - val_loss: 0.9248 - val_acc: 0.5652\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.92484 to 0.92297, saving model to best.model\n",
      "0s - loss: 0.9958 - acc: 0.4944 - val_loss: 0.9230 - val_acc: 0.5652\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.92297 to 0.92131, saving model to best.model\n",
      "0s - loss: 1.0543 - acc: 0.4831 - val_loss: 0.9213 - val_acc: 0.5652\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.92131 to 0.91964, saving model to best.model\n",
      "0s - loss: 1.0694 - acc: 0.4719 - val_loss: 0.9196 - val_acc: 0.5652\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.91964 to 0.91801, saving model to best.model\n",
      "0s - loss: 1.0465 - acc: 0.4719 - val_loss: 0.9180 - val_acc: 0.5652\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.91801 to 0.91653, saving model to best.model\n",
      "0s - loss: 1.0463 - acc: 0.4045 - val_loss: 0.9165 - val_acc: 0.5652\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.91653 to 0.91520, saving model to best.model\n",
      "0s - loss: 1.0510 - acc: 0.5169 - val_loss: 0.9152 - val_acc: 0.5652\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.91520 to 0.91396, saving model to best.model\n",
      "0s - loss: 0.9957 - acc: 0.4831 - val_loss: 0.9140 - val_acc: 0.5652\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.91396 to 0.91274, saving model to best.model\n",
      "0s - loss: 1.0331 - acc: 0.4719 - val_loss: 0.9127 - val_acc: 0.5652\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.91274 to 0.91121, saving model to best.model\n",
      "0s - loss: 1.0428 - acc: 0.3933 - val_loss: 0.9112 - val_acc: 0.5652\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.91121 to 0.90949, saving model to best.model\n",
      "0s - loss: 1.0151 - acc: 0.4607 - val_loss: 0.9095 - val_acc: 0.5652\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.90949 to 0.90757, saving model to best.model\n",
      "0s - loss: 1.0221 - acc: 0.4494 - val_loss: 0.9076 - val_acc: 0.5652\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.90757 to 0.90487, saving model to best.model\n",
      "0s - loss: 1.0597 - acc: 0.4831 - val_loss: 0.9049 - val_acc: 0.5652\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.90487 to 0.90162, saving model to best.model\n",
      "0s - loss: 1.0297 - acc: 0.4494 - val_loss: 0.9016 - val_acc: 0.5652\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.90162 to 0.89785, saving model to best.model\n",
      "0s - loss: 0.9930 - acc: 0.4607 - val_loss: 0.8979 - val_acc: 0.5652\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.89785 to 0.89372, saving model to best.model\n",
      "0s - loss: 0.9516 - acc: 0.5169 - val_loss: 0.8937 - val_acc: 0.5652\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.89372 to 0.88910, saving model to best.model\n",
      "0s - loss: 1.0315 - acc: 0.4719 - val_loss: 0.8891 - val_acc: 0.5652\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.88910 to 0.88410, saving model to best.model\n",
      "0s - loss: 1.0300 - acc: 0.4607 - val_loss: 0.8841 - val_acc: 0.5652\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.88410 to 0.87887, saving model to best.model\n",
      "0s - loss: 0.8928 - acc: 0.5955 - val_loss: 0.8789 - val_acc: 0.5652\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.87887 to 0.87362, saving model to best.model\n",
      "0s - loss: 0.9949 - acc: 0.5169 - val_loss: 0.8736 - val_acc: 0.5652\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.87362 to 0.86856, saving model to best.model\n",
      "0s - loss: 0.9716 - acc: 0.5169 - val_loss: 0.8686 - val_acc: 0.5652\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.86856 to 0.86348, saving model to best.model\n",
      "0s - loss: 0.9799 - acc: 0.5393 - val_loss: 0.8635 - val_acc: 0.5652\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.86348 to 0.85853, saving model to best.model\n",
      "0s - loss: 0.9949 - acc: 0.5056 - val_loss: 0.8585 - val_acc: 0.5652\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.85853 to 0.85357, saving model to best.model\n",
      "0s - loss: 1.0475 - acc: 0.4831 - val_loss: 0.8536 - val_acc: 0.5652\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.85357 to 0.84865, saving model to best.model\n",
      "0s - loss: 1.0086 - acc: 0.4607 - val_loss: 0.8487 - val_acc: 0.5652\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.84865 to 0.84412, saving model to best.model\n",
      "0s - loss: 1.0465 - acc: 0.4719 - val_loss: 0.8441 - val_acc: 0.5652\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.84412 to 0.83961, saving model to best.model\n",
      "0s - loss: 0.9197 - acc: 0.5393 - val_loss: 0.8396 - val_acc: 0.5652\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.83961 to 0.83516, saving model to best.model\n",
      "0s - loss: 0.8580 - acc: 0.6629 - val_loss: 0.8352 - val_acc: 0.5652\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.83516 to 0.83081, saving model to best.model\n",
      "0s - loss: 0.9190 - acc: 0.5281 - val_loss: 0.8308 - val_acc: 0.5652\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.83081 to 0.82666, saving model to best.model\n",
      "0s - loss: 0.9611 - acc: 0.4944 - val_loss: 0.8267 - val_acc: 0.5652\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.82666 to 0.82204, saving model to best.model\n",
      "0s - loss: 0.8424 - acc: 0.6629 - val_loss: 0.8220 - val_acc: 0.5652\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.82204 to 0.81707, saving model to best.model\n",
      "0s - loss: 0.9941 - acc: 0.4719 - val_loss: 0.8171 - val_acc: 0.5652\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.81707 to 0.81169, saving model to best.model\n",
      "0s - loss: 0.9229 - acc: 0.6292 - val_loss: 0.8117 - val_acc: 0.5652\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.81169 to 0.80590, saving model to best.model\n",
      "0s - loss: 0.9090 - acc: 0.5618 - val_loss: 0.8059 - val_acc: 0.5652\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.80590 to 0.79970, saving model to best.model\n",
      "0s - loss: 0.9792 - acc: 0.5169 - val_loss: 0.7997 - val_acc: 0.5652\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.79970 to 0.79305, saving model to best.model\n",
      "0s - loss: 0.9092 - acc: 0.6067 - val_loss: 0.7931 - val_acc: 0.6522\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.79305 to 0.78608, saving model to best.model\n",
      "0s - loss: 0.8859 - acc: 0.5730 - val_loss: 0.7861 - val_acc: 0.6522\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.78608 to 0.77883, saving model to best.model\n",
      "0s - loss: 0.8765 - acc: 0.6404 - val_loss: 0.7788 - val_acc: 0.6522\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.77883 to 0.77134, saving model to best.model\n",
      "0s - loss: 0.9105 - acc: 0.4944 - val_loss: 0.7713 - val_acc: 0.6522\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.77134 to 0.76381, saving model to best.model\n",
      "0s - loss: 0.9397 - acc: 0.5281 - val_loss: 0.7638 - val_acc: 0.6957\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.76381 to 0.75600, saving model to best.model\n",
      "0s - loss: 0.9941 - acc: 0.5393 - val_loss: 0.7560 - val_acc: 0.6957\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.75600 to 0.74788, saving model to best.model\n",
      "0s - loss: 0.8878 - acc: 0.5393 - val_loss: 0.7479 - val_acc: 0.6957\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.74788 to 0.73968, saving model to best.model\n",
      "0s - loss: 0.8699 - acc: 0.5618 - val_loss: 0.7397 - val_acc: 0.6957\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.73968 to 0.73138, saving model to best.model\n",
      "0s - loss: 0.9057 - acc: 0.5393 - val_loss: 0.7314 - val_acc: 0.6957\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.73138 to 0.72316, saving model to best.model\n",
      "0s - loss: 0.8791 - acc: 0.5955 - val_loss: 0.7232 - val_acc: 0.6957\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.72316 to 0.71478, saving model to best.model\n",
      "0s - loss: 0.8533 - acc: 0.6180 - val_loss: 0.7148 - val_acc: 0.6957\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.71478 to 0.70629, saving model to best.model\n",
      "0s - loss: 0.8313 - acc: 0.6067 - val_loss: 0.7063 - val_acc: 0.6957\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.70629 to 0.69760, saving model to best.model\n",
      "0s - loss: 0.7136 - acc: 0.7079 - val_loss: 0.6976 - val_acc: 0.6957\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.69760 to 0.68900, saving model to best.model\n",
      "0s - loss: 0.7898 - acc: 0.6517 - val_loss: 0.6890 - val_acc: 0.6957\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.68900 to 0.68035, saving model to best.model\n",
      "0s - loss: 0.7131 - acc: 0.6854 - val_loss: 0.6803 - val_acc: 0.6957\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.68035 to 0.67167, saving model to best.model\n",
      "0s - loss: 0.7522 - acc: 0.6629 - val_loss: 0.6717 - val_acc: 0.6957\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67167 to 0.66292, saving model to best.model\n",
      "0s - loss: 0.8169 - acc: 0.6180 - val_loss: 0.6629 - val_acc: 0.7391\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66292 to 0.65431, saving model to best.model\n",
      "0s - loss: 0.8558 - acc: 0.6067 - val_loss: 0.6543 - val_acc: 0.7391\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65431 to 0.64594, saving model to best.model\n",
      "0s - loss: 0.7799 - acc: 0.6629 - val_loss: 0.6459 - val_acc: 0.7391\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.64594 to 0.63762, saving model to best.model\n",
      "0s - loss: 0.7195 - acc: 0.7303 - val_loss: 0.6376 - val_acc: 0.7391\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.63762 to 0.62936, saving model to best.model\n",
      "0s - loss: 0.7757 - acc: 0.6404 - val_loss: 0.6294 - val_acc: 0.7391\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.62936 to 0.62076, saving model to best.model\n",
      "0s - loss: 0.7732 - acc: 0.6742 - val_loss: 0.6208 - val_acc: 0.7391\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.62076 to 0.61196, saving model to best.model\n",
      "0s - loss: 0.7508 - acc: 0.6742 - val_loss: 0.6120 - val_acc: 0.7391\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.61196 to 0.60303, saving model to best.model\n",
      "0s - loss: 0.7090 - acc: 0.7303 - val_loss: 0.6030 - val_acc: 0.7391\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.60303 to 0.59413, saving model to best.model\n",
      "0s - loss: 0.6800 - acc: 0.7303 - val_loss: 0.5941 - val_acc: 0.7391\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.59413 to 0.58510, saving model to best.model\n",
      "0s - loss: 0.6572 - acc: 0.7416 - val_loss: 0.5851 - val_acc: 0.7391\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.58510 to 0.57628, saving model to best.model\n",
      "0s - loss: 0.6901 - acc: 0.7416 - val_loss: 0.5763 - val_acc: 0.7391\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.57628 to 0.56729, saving model to best.model\n",
      "0s - loss: 0.7080 - acc: 0.7191 - val_loss: 0.5673 - val_acc: 0.8261\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.56729 to 0.55837, saving model to best.model\n",
      "0s - loss: 0.6807 - acc: 0.6966 - val_loss: 0.5584 - val_acc: 0.8261\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.55837 to 0.54987, saving model to best.model\n",
      "0s - loss: 0.6711 - acc: 0.6517 - val_loss: 0.5499 - val_acc: 0.8696\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.54987 to 0.54144, saving model to best.model\n",
      "0s - loss: 0.6183 - acc: 0.7528 - val_loss: 0.5414 - val_acc: 0.8696\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.54144 to 0.53273, saving model to best.model\n",
      "0s - loss: 0.6521 - acc: 0.7303 - val_loss: 0.5327 - val_acc: 0.8696\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.53273 to 0.52373, saving model to best.model\n",
      "0s - loss: 0.6033 - acc: 0.7303 - val_loss: 0.5237 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.52373 to 0.51483, saving model to best.model\n",
      "0s - loss: 0.6627 - acc: 0.6742 - val_loss: 0.5148 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.51483 to 0.50614, saving model to best.model\n",
      "0s - loss: 0.6581 - acc: 0.7303 - val_loss: 0.5061 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.50614 to 0.49775, saving model to best.model\n",
      "0s - loss: 0.6803 - acc: 0.6966 - val_loss: 0.4978 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.49775 to 0.48957, saving model to best.model\n",
      "0s - loss: 0.6566 - acc: 0.6854 - val_loss: 0.4896 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.48957 to 0.48156, saving model to best.model\n",
      "0s - loss: 0.6248 - acc: 0.6966 - val_loss: 0.4816 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.48156 to 0.47334, saving model to best.model\n",
      "0s - loss: 0.5758 - acc: 0.7640 - val_loss: 0.4733 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.47334 to 0.46504, saving model to best.model\n",
      "0s - loss: 0.5519 - acc: 0.7865 - val_loss: 0.4650 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.46504 to 0.45686, saving model to best.model\n",
      "0s - loss: 0.5894 - acc: 0.7303 - val_loss: 0.4569 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.45686 to 0.44899, saving model to best.model\n",
      "0s - loss: 0.5756 - acc: 0.7528 - val_loss: 0.4490 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.44899 to 0.44165, saving model to best.model\n",
      "0s - loss: 0.6231 - acc: 0.6966 - val_loss: 0.4416 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.44165 to 0.43450, saving model to best.model\n",
      "0s - loss: 0.5448 - acc: 0.7865 - val_loss: 0.4345 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.43450 to 0.42759, saving model to best.model\n",
      "0s - loss: 0.5644 - acc: 0.7865 - val_loss: 0.4276 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.42759 to 0.42052, saving model to best.model\n",
      "0s - loss: 0.5968 - acc: 0.7978 - val_loss: 0.4205 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.42052 to 0.41357, saving model to best.model\n",
      "0s - loss: 0.5192 - acc: 0.8427 - val_loss: 0.4136 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.41357 to 0.40673, saving model to best.model\n",
      "0s - loss: 0.5948 - acc: 0.7416 - val_loss: 0.4067 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.40673 to 0.39955, saving model to best.model\n",
      "0s - loss: 0.5822 - acc: 0.7978 - val_loss: 0.3995 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.39955 to 0.39231, saving model to best.model\n",
      "0s - loss: 0.5056 - acc: 0.7865 - val_loss: 0.3923 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.39231 to 0.38445, saving model to best.model\n",
      "0s - loss: 0.5656 - acc: 0.7753 - val_loss: 0.3844 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.38445 to 0.37680, saving model to best.model\n",
      "0s - loss: 0.4970 - acc: 0.8315 - val_loss: 0.3768 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.37680 to 0.36903, saving model to best.model\n",
      "0s - loss: 0.4706 - acc: 0.7978 - val_loss: 0.3690 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.36903 to 0.36191, saving model to best.model\n",
      "0s - loss: 0.5473 - acc: 0.7978 - val_loss: 0.3619 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.36191 to 0.35492, saving model to best.model\n",
      "0s - loss: 0.4521 - acc: 0.7865 - val_loss: 0.3549 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.35492 to 0.34825, saving model to best.model\n",
      "0s - loss: 0.4768 - acc: 0.7978 - val_loss: 0.3482 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.34825 to 0.34169, saving model to best.model\n",
      "0s - loss: 0.4485 - acc: 0.8202 - val_loss: 0.3417 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.34169 to 0.33500, saving model to best.model\n",
      "0s - loss: 0.4470 - acc: 0.7753 - val_loss: 0.3350 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.33500 to 0.32846, saving model to best.model\n",
      "0s - loss: 0.4948 - acc: 0.7865 - val_loss: 0.3285 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.32846 to 0.32195, saving model to best.model\n",
      "0s - loss: 0.4381 - acc: 0.8876 - val_loss: 0.3219 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.32195 to 0.31558, saving model to best.model\n",
      "0s - loss: 0.4431 - acc: 0.8539 - val_loss: 0.3156 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.31558 to 0.30907, saving model to best.model\n",
      "0s - loss: 0.4085 - acc: 0.8764 - val_loss: 0.3091 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.30907 to 0.30232, saving model to best.model\n",
      "0s - loss: 0.4282 - acc: 0.8539 - val_loss: 0.3023 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.30232 to 0.29571, saving model to best.model\n",
      "0s - loss: 0.3937 - acc: 0.8652 - val_loss: 0.2957 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.29571 to 0.28894, saving model to best.model\n",
      "0s - loss: 0.3897 - acc: 0.8539 - val_loss: 0.2889 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.28894 to 0.28238, saving model to best.model\n",
      "0s - loss: 0.3921 - acc: 0.8427 - val_loss: 0.2824 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.28238 to 0.27598, saving model to best.model\n",
      "0s - loss: 0.4195 - acc: 0.8876 - val_loss: 0.2760 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.27598 to 0.26954, saving model to best.model\n",
      "0s - loss: 0.3938 - acc: 0.8539 - val_loss: 0.2695 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.26954 to 0.26317, saving model to best.model\n",
      "0s - loss: 0.4103 - acc: 0.8539 - val_loss: 0.2632 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.26317 to 0.25702, saving model to best.model\n",
      "0s - loss: 0.3615 - acc: 0.8539 - val_loss: 0.2570 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.25702 to 0.25089, saving model to best.model\n",
      "0s - loss: 0.4181 - acc: 0.8539 - val_loss: 0.2509 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.25089 to 0.24466, saving model to best.model\n",
      "0s - loss: 0.3992 - acc: 0.8315 - val_loss: 0.2447 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.24466 to 0.23870, saving model to best.model\n",
      "0s - loss: 0.3925 - acc: 0.8539 - val_loss: 0.2387 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.23870 to 0.23300, saving model to best.model\n",
      "0s - loss: 0.3694 - acc: 0.8315 - val_loss: 0.2330 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.23300 to 0.22749, saving model to best.model\n",
      "0s - loss: 0.3550 - acc: 0.8427 - val_loss: 0.2275 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.22749 to 0.22207, saving model to best.model\n",
      "0s - loss: 0.3347 - acc: 0.9101 - val_loss: 0.2221 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.22207 to 0.21676, saving model to best.model\n",
      "0s - loss: 0.3404 - acc: 0.8764 - val_loss: 0.2168 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.21676 to 0.21153, saving model to best.model\n",
      "0s - loss: 0.3365 - acc: 0.8876 - val_loss: 0.2115 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.21153 to 0.20661, saving model to best.model\n",
      "0s - loss: 0.3151 - acc: 0.9213 - val_loss: 0.2066 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.20661 to 0.20196, saving model to best.model\n",
      "0s - loss: 0.2780 - acc: 0.9326 - val_loss: 0.2020 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.20196 to 0.19752, saving model to best.model\n",
      "0s - loss: 0.3227 - acc: 0.8652 - val_loss: 0.1975 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.19752 to 0.19341, saving model to best.model\n",
      "0s - loss: 0.3559 - acc: 0.8989 - val_loss: 0.1934 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.19341 to 0.18935, saving model to best.model\n",
      "0s - loss: 0.2890 - acc: 0.8989 - val_loss: 0.1893 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.18935 to 0.18519, saving model to best.model\n",
      "0s - loss: 0.3152 - acc: 0.8989 - val_loss: 0.1852 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.18519 to 0.18120, saving model to best.model\n",
      "0s - loss: 0.3298 - acc: 0.8876 - val_loss: 0.1812 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.18120 to 0.17682, saving model to best.model\n",
      "0s - loss: 0.3361 - acc: 0.8539 - val_loss: 0.1768 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.17682 to 0.17234, saving model to best.model\n",
      "0s - loss: 0.3262 - acc: 0.8539 - val_loss: 0.1723 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.17234 to 0.16809, saving model to best.model\n",
      "0s - loss: 0.2918 - acc: 0.8989 - val_loss: 0.1681 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.16809 to 0.16427, saving model to best.model\n",
      "0s - loss: 0.2806 - acc: 0.8876 - val_loss: 0.1643 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.16427 to 0.16021, saving model to best.model\n",
      "0s - loss: 0.2735 - acc: 0.9326 - val_loss: 0.1602 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.16021 to 0.15566, saving model to best.model\n",
      "0s - loss: 0.2813 - acc: 0.9101 - val_loss: 0.1557 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.15566 to 0.15087, saving model to best.model\n",
      "0s - loss: 0.3393 - acc: 0.8652 - val_loss: 0.1509 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.15087 to 0.14602, saving model to best.model\n",
      "0s - loss: 0.2315 - acc: 0.9326 - val_loss: 0.1460 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.14602 to 0.14123, saving model to best.model\n",
      "0s - loss: 0.2248 - acc: 0.9438 - val_loss: 0.1412 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.14123 to 0.13679, saving model to best.model\n",
      "0s - loss: 0.2854 - acc: 0.8876 - val_loss: 0.1368 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.13679 to 0.13241, saving model to best.model\n",
      "0s - loss: 0.2360 - acc: 0.9213 - val_loss: 0.1324 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.13241 to 0.12802, saving model to best.model\n",
      "0s - loss: 0.2449 - acc: 0.9326 - val_loss: 0.1280 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.12802 to 0.12375, saving model to best.model\n",
      "0s - loss: 0.2730 - acc: 0.8989 - val_loss: 0.1238 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.12375 to 0.11984, saving model to best.model\n",
      "0s - loss: 0.2499 - acc: 0.9326 - val_loss: 0.1198 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.11984 to 0.11592, saving model to best.model\n",
      "0s - loss: 0.2837 - acc: 0.8876 - val_loss: 0.1159 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.11592 to 0.11241, saving model to best.model\n",
      "0s - loss: 0.2579 - acc: 0.9213 - val_loss: 0.1124 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.11241 to 0.10917, saving model to best.model\n",
      "0s - loss: 0.2110 - acc: 0.9438 - val_loss: 0.1092 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.10917 to 0.10605, saving model to best.model\n",
      "0s - loss: 0.2378 - acc: 0.8989 - val_loss: 0.1060 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.10605 to 0.10298, saving model to best.model\n",
      "0s - loss: 0.2745 - acc: 0.9101 - val_loss: 0.1030 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.10298 to 0.10011, saving model to best.model\n",
      "0s - loss: 0.2651 - acc: 0.8989 - val_loss: 0.1001 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.10011 to 0.09736, saving model to best.model\n",
      "0s - loss: 0.2074 - acc: 0.9663 - val_loss: 0.0974 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.09736 to 0.09480, saving model to best.model\n",
      "0s - loss: 0.2774 - acc: 0.9213 - val_loss: 0.0948 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.09480 to 0.09231, saving model to best.model\n",
      "0s - loss: 0.2338 - acc: 0.9213 - val_loss: 0.0923 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.09231 to 0.08996, saving model to best.model\n",
      "0s - loss: 0.2068 - acc: 0.9438 - val_loss: 0.0900 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.08996 to 0.08768, saving model to best.model\n",
      "0s - loss: 0.1870 - acc: 0.9663 - val_loss: 0.0877 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.08768 to 0.08554, saving model to best.model\n",
      "0s - loss: 0.1726 - acc: 0.9438 - val_loss: 0.0855 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.08554 to 0.08337, saving model to best.model\n",
      "0s - loss: 0.2393 - acc: 0.9438 - val_loss: 0.0834 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.08337 to 0.08136, saving model to best.model\n",
      "0s - loss: 0.1750 - acc: 0.9438 - val_loss: 0.0814 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.08136 to 0.07951, saving model to best.model\n",
      "0s - loss: 0.2642 - acc: 0.9213 - val_loss: 0.0795 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.07951 to 0.07769, saving model to best.model\n",
      "0s - loss: 0.1919 - acc: 0.9101 - val_loss: 0.0777 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.12474, saving model to best.model\n",
      "0s - loss: 1.3248 - acc: 0.3034 - val_loss: 1.1247 - val_acc: 0.3913\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.12474 to 1.09919, saving model to best.model\n",
      "0s - loss: 1.2324 - acc: 0.4045 - val_loss: 1.0992 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.09919 to 1.09578, saving model to best.model\n",
      "0s - loss: 1.2110 - acc: 0.3371 - val_loss: 1.0958 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2184 - acc: 0.3483 - val_loss: 1.1087 - val_acc: 0.3043\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.3418 - acc: 0.2360 - val_loss: 1.1276 - val_acc: 0.3043\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2415 - acc: 0.3708 - val_loss: 1.1426 - val_acc: 0.3043\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.3149 - acc: 0.3820 - val_loss: 1.1520 - val_acc: 0.3043\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1793 - acc: 0.3933 - val_loss: 1.1528 - val_acc: 0.3043\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2729 - acc: 0.3483 - val_loss: 1.1456 - val_acc: 0.3043\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2387 - acc: 0.3820 - val_loss: 1.1343 - val_acc: 0.3043\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2811 - acc: 0.2697 - val_loss: 1.1205 - val_acc: 0.3043\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1961 - acc: 0.3371 - val_loss: 1.1090 - val_acc: 0.3043\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2319 - acc: 0.3258 - val_loss: 1.0987 - val_acc: 0.3043\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.09578 to 1.09033, saving model to best.model\n",
      "0s - loss: 1.2901 - acc: 0.3146 - val_loss: 1.0903 - val_acc: 0.3043\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.09033 to 1.08389, saving model to best.model\n",
      "0s - loss: 1.1031 - acc: 0.3933 - val_loss: 1.0839 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.08389 to 1.07963, saving model to best.model\n",
      "0s - loss: 1.2223 - acc: 0.3258 - val_loss: 1.0796 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.07963 to 1.07682, saving model to best.model\n",
      "0s - loss: 1.2247 - acc: 0.3371 - val_loss: 1.0768 - val_acc: 0.3913\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.07682 to 1.07549, saving model to best.model\n",
      "0s - loss: 1.1520 - acc: 0.3371 - val_loss: 1.0755 - val_acc: 0.3913\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.07549 to 1.07443, saving model to best.model\n",
      "0s - loss: 1.1142 - acc: 0.3933 - val_loss: 1.0744 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.07443 to 1.07366, saving model to best.model\n",
      "0s - loss: 1.1508 - acc: 0.3933 - val_loss: 1.0737 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.07366 to 1.07250, saving model to best.model\n",
      "0s - loss: 1.1573 - acc: 0.3708 - val_loss: 1.0725 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.07250 to 1.07100, saving model to best.model\n",
      "0s - loss: 1.2291 - acc: 0.2921 - val_loss: 1.0710 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.07100 to 1.06913, saving model to best.model\n",
      "0s - loss: 1.1393 - acc: 0.3820 - val_loss: 1.0691 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.06913 to 1.06702, saving model to best.model\n",
      "0s - loss: 1.0794 - acc: 0.4382 - val_loss: 1.0670 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.06702 to 1.06571, saving model to best.model\n",
      "0s - loss: 1.1107 - acc: 0.4157 - val_loss: 1.0657 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.06571 to 1.06472, saving model to best.model\n",
      "0s - loss: 1.1930 - acc: 0.3820 - val_loss: 1.0647 - val_acc: 0.6957\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.06472 to 1.06450, saving model to best.model\n",
      "0s - loss: 1.0815 - acc: 0.3708 - val_loss: 1.0645 - val_acc: 0.3913\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.2668 - acc: 0.3371 - val_loss: 1.0652 - val_acc: 0.3043\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1541 - acc: 0.4382 - val_loss: 1.0652 - val_acc: 0.3043\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.1210 - acc: 0.3258 - val_loss: 1.0656 - val_acc: 0.3043\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1238 - acc: 0.3933 - val_loss: 1.0665 - val_acc: 0.3043\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.1031 - acc: 0.3933 - val_loss: 1.0664 - val_acc: 0.3043\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.2165 - acc: 0.3034 - val_loss: 1.0650 - val_acc: 0.3043\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.06450 to 1.06276, saving model to best.model\n",
      "0s - loss: 1.2030 - acc: 0.4045 - val_loss: 1.0628 - val_acc: 0.3043\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.06276 to 1.05978, saving model to best.model\n",
      "0s - loss: 1.1050 - acc: 0.4382 - val_loss: 1.0598 - val_acc: 0.3043\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.05978 to 1.05509, saving model to best.model\n",
      "0s - loss: 1.1087 - acc: 0.3933 - val_loss: 1.0551 - val_acc: 0.3043\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.05509 to 1.05000, saving model to best.model\n",
      "0s - loss: 1.0051 - acc: 0.4719 - val_loss: 1.0500 - val_acc: 0.3043\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.05000 to 1.04477, saving model to best.model\n",
      "0s - loss: 1.1751 - acc: 0.3596 - val_loss: 1.0448 - val_acc: 0.3043\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.04477 to 1.04002, saving model to best.model\n",
      "0s - loss: 1.0687 - acc: 0.4270 - val_loss: 1.0400 - val_acc: 0.3043\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.04002 to 1.03533, saving model to best.model\n",
      "0s - loss: 1.1057 - acc: 0.3820 - val_loss: 1.0353 - val_acc: 0.3043\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.03533 to 1.03074, saving model to best.model\n",
      "0s - loss: 1.1142 - acc: 0.3596 - val_loss: 1.0307 - val_acc: 0.5217\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.03074 to 1.02683, saving model to best.model\n",
      "0s - loss: 1.1027 - acc: 0.3708 - val_loss: 1.0268 - val_acc: 0.6957\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.02683 to 1.02277, saving model to best.model\n",
      "0s - loss: 1.1250 - acc: 0.3146 - val_loss: 1.0228 - val_acc: 0.6957\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.02277 to 1.01933, saving model to best.model\n",
      "0s - loss: 1.0807 - acc: 0.4270 - val_loss: 1.0193 - val_acc: 0.6957\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.01933 to 1.01611, saving model to best.model\n",
      "0s - loss: 1.0951 - acc: 0.3933 - val_loss: 1.0161 - val_acc: 0.6957\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.01611 to 1.01318, saving model to best.model\n",
      "0s - loss: 1.0630 - acc: 0.4382 - val_loss: 1.0132 - val_acc: 0.6957\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.01318 to 1.00991, saving model to best.model\n",
      "0s - loss: 1.0656 - acc: 0.4831 - val_loss: 1.0099 - val_acc: 0.6957\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.00991 to 1.00647, saving model to best.model\n",
      "0s - loss: 1.1431 - acc: 0.3596 - val_loss: 1.0065 - val_acc: 0.6957\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.00647 to 1.00287, saving model to best.model\n",
      "0s - loss: 1.1621 - acc: 0.3146 - val_loss: 1.0029 - val_acc: 0.6957\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.00287 to 0.99921, saving model to best.model\n",
      "0s - loss: 1.1151 - acc: 0.3596 - val_loss: 0.9992 - val_acc: 0.6957\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.99921 to 0.99515, saving model to best.model\n",
      "0s - loss: 1.0060 - acc: 0.4944 - val_loss: 0.9952 - val_acc: 0.6957\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.99515 to 0.99099, saving model to best.model\n",
      "0s - loss: 1.1176 - acc: 0.3596 - val_loss: 0.9910 - val_acc: 0.6957\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.99099 to 0.98703, saving model to best.model\n",
      "0s - loss: 1.1007 - acc: 0.3820 - val_loss: 0.9870 - val_acc: 0.6957\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.98703 to 0.98304, saving model to best.model\n",
      "0s - loss: 1.0106 - acc: 0.3933 - val_loss: 0.9830 - val_acc: 0.6957\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.98304 to 0.97900, saving model to best.model\n",
      "0s - loss: 1.0317 - acc: 0.4270 - val_loss: 0.9790 - val_acc: 0.6957\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.97900 to 0.97473, saving model to best.model\n",
      "0s - loss: 1.0998 - acc: 0.4045 - val_loss: 0.9747 - val_acc: 0.6957\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.97473 to 0.97071, saving model to best.model\n",
      "0s - loss: 1.0954 - acc: 0.4157 - val_loss: 0.9707 - val_acc: 0.6522\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.97071 to 0.96628, saving model to best.model\n",
      "0s - loss: 1.0410 - acc: 0.4382 - val_loss: 0.9663 - val_acc: 0.6522\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.96628 to 0.96181, saving model to best.model\n",
      "0s - loss: 0.9863 - acc: 0.4607 - val_loss: 0.9618 - val_acc: 0.6522\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.96181 to 0.95746, saving model to best.model\n",
      "0s - loss: 1.0669 - acc: 0.4270 - val_loss: 0.9575 - val_acc: 0.6087\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.95746 to 0.95293, saving model to best.model\n",
      "0s - loss: 0.9953 - acc: 0.4607 - val_loss: 0.9529 - val_acc: 0.5652\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.95293 to 0.94834, saving model to best.model\n",
      "0s - loss: 1.0961 - acc: 0.3820 - val_loss: 0.9483 - val_acc: 0.4348\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.94834 to 0.94311, saving model to best.model\n",
      "0s - loss: 1.0108 - acc: 0.4719 - val_loss: 0.9431 - val_acc: 0.4348\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.94311 to 0.93760, saving model to best.model\n",
      "0s - loss: 1.0780 - acc: 0.4270 - val_loss: 0.9376 - val_acc: 0.6957\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.93760 to 0.93137, saving model to best.model\n",
      "0s - loss: 1.0024 - acc: 0.4944 - val_loss: 0.9314 - val_acc: 0.7391\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.93137 to 0.92496, saving model to best.model\n",
      "0s - loss: 1.0601 - acc: 0.4831 - val_loss: 0.9250 - val_acc: 0.7391\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.92496 to 0.91830, saving model to best.model\n",
      "0s - loss: 1.0116 - acc: 0.5056 - val_loss: 0.9183 - val_acc: 0.7391\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.91830 to 0.91111, saving model to best.model\n",
      "0s - loss: 0.9192 - acc: 0.5281 - val_loss: 0.9111 - val_acc: 0.7391\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.91111 to 0.90366, saving model to best.model\n",
      "0s - loss: 1.0355 - acc: 0.4607 - val_loss: 0.9037 - val_acc: 0.7826\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.90366 to 0.89581, saving model to best.model\n",
      "0s - loss: 0.9379 - acc: 0.4831 - val_loss: 0.8958 - val_acc: 0.8261\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.89581 to 0.88798, saving model to best.model\n",
      "0s - loss: 0.9426 - acc: 0.5955 - val_loss: 0.8880 - val_acc: 0.8696\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.88798 to 0.87951, saving model to best.model\n",
      "0s - loss: 0.9494 - acc: 0.5169 - val_loss: 0.8795 - val_acc: 0.9565\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.87951 to 0.87102, saving model to best.model\n",
      "0s - loss: 0.9360 - acc: 0.5843 - val_loss: 0.8710 - val_acc: 0.9565\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.87102 to 0.86237, saving model to best.model\n",
      "0s - loss: 1.0022 - acc: 0.4494 - val_loss: 0.8624 - val_acc: 0.9565\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.86237 to 0.85328, saving model to best.model\n",
      "0s - loss: 1.0060 - acc: 0.5056 - val_loss: 0.8533 - val_acc: 0.9565\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.85328 to 0.84366, saving model to best.model\n",
      "0s - loss: 0.9521 - acc: 0.5281 - val_loss: 0.8437 - val_acc: 1.0000\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.84366 to 0.83397, saving model to best.model\n",
      "0s - loss: 0.9211 - acc: 0.5393 - val_loss: 0.8340 - val_acc: 1.0000\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.83397 to 0.82377, saving model to best.model\n",
      "0s - loss: 0.9300 - acc: 0.5730 - val_loss: 0.8238 - val_acc: 1.0000\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.82377 to 0.81317, saving model to best.model\n",
      "0s - loss: 0.8693 - acc: 0.5955 - val_loss: 0.8132 - val_acc: 1.0000\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.81317 to 0.80224, saving model to best.model\n",
      "0s - loss: 0.9026 - acc: 0.5506 - val_loss: 0.8022 - val_acc: 1.0000\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.80224 to 0.79132, saving model to best.model\n",
      "0s - loss: 0.9174 - acc: 0.5393 - val_loss: 0.7913 - val_acc: 1.0000\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.79132 to 0.78026, saving model to best.model\n",
      "0s - loss: 0.8338 - acc: 0.6292 - val_loss: 0.7803 - val_acc: 1.0000\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.78026 to 0.76912, saving model to best.model\n",
      "0s - loss: 0.8396 - acc: 0.5618 - val_loss: 0.7691 - val_acc: 1.0000\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.76912 to 0.75780, saving model to best.model\n",
      "0s - loss: 0.8743 - acc: 0.6404 - val_loss: 0.7578 - val_acc: 1.0000\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.75780 to 0.74568, saving model to best.model\n",
      "0s - loss: 0.8451 - acc: 0.6629 - val_loss: 0.7457 - val_acc: 1.0000\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.74568 to 0.73252, saving model to best.model\n",
      "0s - loss: 0.8620 - acc: 0.5730 - val_loss: 0.7325 - val_acc: 1.0000\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.73252 to 0.71897, saving model to best.model\n",
      "0s - loss: 0.8314 - acc: 0.6404 - val_loss: 0.7190 - val_acc: 1.0000\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.71897 to 0.70543, saving model to best.model\n",
      "0s - loss: 0.8596 - acc: 0.6067 - val_loss: 0.7054 - val_acc: 1.0000\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.70543 to 0.69175, saving model to best.model\n",
      "0s - loss: 0.8969 - acc: 0.5281 - val_loss: 0.6918 - val_acc: 1.0000\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69175 to 0.67804, saving model to best.model\n",
      "0s - loss: 0.8015 - acc: 0.7079 - val_loss: 0.6780 - val_acc: 1.0000\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67804 to 0.66382, saving model to best.model\n",
      "0s - loss: 0.7984 - acc: 0.6629 - val_loss: 0.6638 - val_acc: 1.0000\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66382 to 0.64936, saving model to best.model\n",
      "0s - loss: 0.7914 - acc: 0.6742 - val_loss: 0.6494 - val_acc: 1.0000\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.64936 to 0.63454, saving model to best.model\n",
      "0s - loss: 0.7807 - acc: 0.6966 - val_loss: 0.6345 - val_acc: 1.0000\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.63454 to 0.61998, saving model to best.model\n",
      "0s - loss: 0.7471 - acc: 0.6966 - val_loss: 0.6200 - val_acc: 1.0000\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.61998 to 0.60553, saving model to best.model\n",
      "0s - loss: 0.7775 - acc: 0.6742 - val_loss: 0.6055 - val_acc: 1.0000\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.60553 to 0.59085, saving model to best.model\n",
      "0s - loss: 0.7763 - acc: 0.6629 - val_loss: 0.5908 - val_acc: 1.0000\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.59085 to 0.57661, saving model to best.model\n",
      "0s - loss: 0.7887 - acc: 0.6629 - val_loss: 0.5766 - val_acc: 1.0000\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.57661 to 0.56254, saving model to best.model\n",
      "0s - loss: 0.7508 - acc: 0.7303 - val_loss: 0.5625 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.56254 to 0.54856, saving model to best.model\n",
      "0s - loss: 0.6905 - acc: 0.7865 - val_loss: 0.5486 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.54856 to 0.53466, saving model to best.model\n",
      "0s - loss: 0.7121 - acc: 0.7416 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.53466 to 0.52040, saving model to best.model\n",
      "0s - loss: 0.6974 - acc: 0.7079 - val_loss: 0.5204 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.52040 to 0.50621, saving model to best.model\n",
      "0s - loss: 0.6639 - acc: 0.7191 - val_loss: 0.5062 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.50621 to 0.49221, saving model to best.model\n",
      "0s - loss: 0.6153 - acc: 0.7753 - val_loss: 0.4922 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.49221 to 0.47805, saving model to best.model\n",
      "0s - loss: 0.6616 - acc: 0.7416 - val_loss: 0.4781 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.47805 to 0.46414, saving model to best.model\n",
      "0s - loss: 0.6054 - acc: 0.8315 - val_loss: 0.4641 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.46414 to 0.45088, saving model to best.model\n",
      "0s - loss: 0.6107 - acc: 0.7528 - val_loss: 0.4509 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.45088 to 0.43822, saving model to best.model\n",
      "0s - loss: 0.6915 - acc: 0.7079 - val_loss: 0.4382 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.43822 to 0.42542, saving model to best.model\n",
      "0s - loss: 0.5671 - acc: 0.8652 - val_loss: 0.4254 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.42542 to 0.41269, saving model to best.model\n",
      "0s - loss: 0.6250 - acc: 0.7753 - val_loss: 0.4127 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.41269 to 0.39982, saving model to best.model\n",
      "0s - loss: 0.5475 - acc: 0.8652 - val_loss: 0.3998 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.39982 to 0.38781, saving model to best.model\n",
      "0s - loss: 0.5878 - acc: 0.7865 - val_loss: 0.3878 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.38781 to 0.37557, saving model to best.model\n",
      "0s - loss: 0.5893 - acc: 0.8090 - val_loss: 0.3756 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.37557 to 0.36236, saving model to best.model\n",
      "0s - loss: 0.5549 - acc: 0.8090 - val_loss: 0.3624 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.36236 to 0.34978, saving model to best.model\n",
      "0s - loss: 0.5212 - acc: 0.8315 - val_loss: 0.3498 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.34978 to 0.33764, saving model to best.model\n",
      "0s - loss: 0.5014 - acc: 0.8202 - val_loss: 0.3376 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.33764 to 0.32582, saving model to best.model\n",
      "0s - loss: 0.5314 - acc: 0.8427 - val_loss: 0.3258 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.32582 to 0.31437, saving model to best.model\n",
      "0s - loss: 0.5821 - acc: 0.7416 - val_loss: 0.3144 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.31437 to 0.30234, saving model to best.model\n",
      "0s - loss: 0.4705 - acc: 0.8652 - val_loss: 0.3023 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.30234 to 0.29057, saving model to best.model\n",
      "0s - loss: 0.4766 - acc: 0.8427 - val_loss: 0.2906 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.29057 to 0.27954, saving model to best.model\n",
      "0s - loss: 0.4914 - acc: 0.7978 - val_loss: 0.2795 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.27954 to 0.26909, saving model to best.model\n",
      "0s - loss: 0.4879 - acc: 0.8315 - val_loss: 0.2691 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.26909 to 0.25858, saving model to best.model\n",
      "0s - loss: 0.4695 - acc: 0.8427 - val_loss: 0.2586 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.25858 to 0.24828, saving model to best.model\n",
      "0s - loss: 0.4785 - acc: 0.8090 - val_loss: 0.2483 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.24828 to 0.23831, saving model to best.model\n",
      "0s - loss: 0.4154 - acc: 0.8764 - val_loss: 0.2383 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.23831 to 0.22863, saving model to best.model\n",
      "0s - loss: 0.4342 - acc: 0.8652 - val_loss: 0.2286 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.22863 to 0.21904, saving model to best.model\n",
      "0s - loss: 0.4475 - acc: 0.8876 - val_loss: 0.2190 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.21904 to 0.21002, saving model to best.model\n",
      "0s - loss: 0.4215 - acc: 0.8427 - val_loss: 0.2100 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.21002 to 0.20146, saving model to best.model\n",
      "0s - loss: 0.4601 - acc: 0.8202 - val_loss: 0.2015 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.20146 to 0.19339, saving model to best.model\n",
      "0s - loss: 0.3723 - acc: 0.8652 - val_loss: 0.1934 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.19339 to 0.18567, saving model to best.model\n",
      "0s - loss: 0.4658 - acc: 0.7978 - val_loss: 0.1857 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.18567 to 0.17833, saving model to best.model\n",
      "0s - loss: 0.4161 - acc: 0.8427 - val_loss: 0.1783 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.17833 to 0.17127, saving model to best.model\n",
      "0s - loss: 0.4068 - acc: 0.8652 - val_loss: 0.1713 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.17127 to 0.16427, saving model to best.model\n",
      "0s - loss: 0.4534 - acc: 0.8427 - val_loss: 0.1643 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.16427 to 0.15743, saving model to best.model\n",
      "0s - loss: 0.3829 - acc: 0.8652 - val_loss: 0.1574 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.15743 to 0.15102, saving model to best.model\n",
      "0s - loss: 0.3395 - acc: 0.8764 - val_loss: 0.1510 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.15102 to 0.14513, saving model to best.model\n",
      "0s - loss: 0.3374 - acc: 0.8764 - val_loss: 0.1451 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.14513 to 0.13958, saving model to best.model\n",
      "0s - loss: 0.3890 - acc: 0.8539 - val_loss: 0.1396 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.13958 to 0.13419, saving model to best.model\n",
      "0s - loss: 0.3080 - acc: 0.8876 - val_loss: 0.1342 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.13419 to 0.12901, saving model to best.model\n",
      "0s - loss: 0.3492 - acc: 0.8989 - val_loss: 0.1290 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.12901 to 0.12372, saving model to best.model\n",
      "0s - loss: 0.3277 - acc: 0.9326 - val_loss: 0.1237 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.12372 to 0.11850, saving model to best.model\n",
      "0s - loss: 0.3220 - acc: 0.8764 - val_loss: 0.1185 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.11850 to 0.11370, saving model to best.model\n",
      "0s - loss: 0.2934 - acc: 0.8989 - val_loss: 0.1137 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.11370 to 0.10912, saving model to best.model\n",
      "0s - loss: 0.2907 - acc: 0.9101 - val_loss: 0.1091 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.10912 to 0.10462, saving model to best.model\n",
      "0s - loss: 0.2680 - acc: 0.9438 - val_loss: 0.1046 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.10462 to 0.10016, saving model to best.model\n",
      "0s - loss: 0.2845 - acc: 0.8764 - val_loss: 0.1002 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.10016 to 0.09589, saving model to best.model\n",
      "0s - loss: 0.3335 - acc: 0.8539 - val_loss: 0.0959 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.09589 to 0.09169, saving model to best.model\n",
      "0s - loss: 0.2818 - acc: 0.9326 - val_loss: 0.0917 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.09169 to 0.08770, saving model to best.model\n",
      "0s - loss: 0.3160 - acc: 0.8876 - val_loss: 0.0877 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.08770 to 0.08389, saving model to best.model\n",
      "0s - loss: 0.2466 - acc: 0.9438 - val_loss: 0.0839 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.08389 to 0.08017, saving model to best.model\n",
      "0s - loss: 0.2910 - acc: 0.9101 - val_loss: 0.0802 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.08017 to 0.07665, saving model to best.model\n",
      "0s - loss: 0.2828 - acc: 0.9213 - val_loss: 0.0766 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.07665 to 0.07347, saving model to best.model\n",
      "0s - loss: 0.2638 - acc: 0.8989 - val_loss: 0.0735 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.07347 to 0.07043, saving model to best.model\n",
      "0s - loss: 0.2328 - acc: 0.9213 - val_loss: 0.0704 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.07043 to 0.06744, saving model to best.model\n",
      "0s - loss: 0.2500 - acc: 0.9213 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.06744 to 0.06459, saving model to best.model\n",
      "0s - loss: 0.2460 - acc: 0.9213 - val_loss: 0.0646 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.06459 to 0.06193, saving model to best.model\n",
      "0s - loss: 0.2958 - acc: 0.8652 - val_loss: 0.0619 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.06193 to 0.05940, saving model to best.model\n",
      "0s - loss: 0.2675 - acc: 0.9101 - val_loss: 0.0594 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.05940 to 0.05707, saving model to best.model\n",
      "0s - loss: 0.2593 - acc: 0.9438 - val_loss: 0.0571 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.05707 to 0.05480, saving model to best.model\n",
      "0s - loss: 0.2298 - acc: 0.9326 - val_loss: 0.0548 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.05480 to 0.05269, saving model to best.model\n",
      "0s - loss: 0.2599 - acc: 0.8989 - val_loss: 0.0527 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.05269 to 0.05065, saving model to best.model\n",
      "0s - loss: 0.2265 - acc: 0.9101 - val_loss: 0.0506 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.05065 to 0.04879, saving model to best.model\n",
      "0s - loss: 0.2011 - acc: 0.9551 - val_loss: 0.0488 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.04879 to 0.04706, saving model to best.model\n",
      "0s - loss: 0.2269 - acc: 0.9438 - val_loss: 0.0471 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.04706 to 0.04536, saving model to best.model\n",
      "0s - loss: 0.2010 - acc: 0.9326 - val_loss: 0.0454 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.04536 to 0.04371, saving model to best.model\n",
      "0s - loss: 0.2188 - acc: 0.9326 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.04371 to 0.04202, saving model to best.model\n",
      "0s - loss: 0.1669 - acc: 0.9551 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.04202 to 0.04040, saving model to best.model\n",
      "0s - loss: 0.2347 - acc: 0.9213 - val_loss: 0.0404 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.04040 to 0.03890, saving model to best.model\n",
      "0s - loss: 0.1982 - acc: 0.9551 - val_loss: 0.0389 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.03890 to 0.03751, saving model to best.model\n",
      "0s - loss: 0.2184 - acc: 0.8876 - val_loss: 0.0375 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.03751 to 0.03617, saving model to best.model\n",
      "0s - loss: 0.2329 - acc: 0.9101 - val_loss: 0.0362 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.03617 to 0.03486, saving model to best.model\n",
      "0s - loss: 0.2074 - acc: 0.9101 - val_loss: 0.0349 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.03486 to 0.03368, saving model to best.model\n",
      "0s - loss: 0.2277 - acc: 0.9101 - val_loss: 0.0337 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.03368 to 0.03260, saving model to best.model\n",
      "0s - loss: 0.1969 - acc: 0.9213 - val_loss: 0.0326 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.03260 to 0.03151, saving model to best.model\n",
      "0s - loss: 0.1842 - acc: 0.9326 - val_loss: 0.0315 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.03151 to 0.03041, saving model to best.model\n",
      "0s - loss: 0.1896 - acc: 0.9551 - val_loss: 0.0304 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.03041 to 0.02934, saving model to best.model\n",
      "0s - loss: 0.1845 - acc: 0.9551 - val_loss: 0.0293 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.02934 to 0.02832, saving model to best.model\n",
      "0s - loss: 0.1776 - acc: 0.9438 - val_loss: 0.0283 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.02832 to 0.02740, saving model to best.model\n",
      "0s - loss: 0.1225 - acc: 0.9888 - val_loss: 0.0274 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.02740 to 0.02657, saving model to best.model\n",
      "0s - loss: 0.2209 - acc: 0.9438 - val_loss: 0.0266 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.02657 to 0.02569, saving model to best.model\n",
      "0s - loss: 0.1751 - acc: 0.9213 - val_loss: 0.0257 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.02569 to 0.02482, saving model to best.model\n",
      "0s - loss: 0.1486 - acc: 0.9775 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.02482 to 0.02392, saving model to best.model\n",
      "0s - loss: 0.2094 - acc: 0.8989 - val_loss: 0.0239 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.02392 to 0.02303, saving model to best.model\n",
      "0s - loss: 0.1472 - acc: 0.9663 - val_loss: 0.0230 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.02303 to 0.02220, saving model to best.model\n",
      "0s - loss: 0.2095 - acc: 0.9438 - val_loss: 0.0222 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.02220 to 0.02138, saving model to best.model\n",
      "0s - loss: 0.2117 - acc: 0.9101 - val_loss: 0.0214 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.02138 to 0.02068, saving model to best.model\n",
      "0s - loss: 0.2518 - acc: 0.9326 - val_loss: 0.0207 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.02068 to 0.02001, saving model to best.model\n",
      "0s - loss: 0.1510 - acc: 0.9775 - val_loss: 0.0200 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.02001 to 0.01936, saving model to best.model\n",
      "0s - loss: 0.1570 - acc: 0.9438 - val_loss: 0.0194 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.01936 to 0.01878, saving model to best.model\n",
      "0s - loss: 0.1616 - acc: 0.9438 - val_loss: 0.0188 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.01878 to 0.01820, saving model to best.model\n",
      "0s - loss: 0.1668 - acc: 0.9438 - val_loss: 0.0182 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.01820 to 0.01767, saving model to best.model\n",
      "0s - loss: 0.1671 - acc: 0.9438 - val_loss: 0.0177 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.01767 to 0.01719, saving model to best.model\n",
      "0s - loss: 0.1241 - acc: 0.9663 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.01719 to 0.01673, saving model to best.model\n",
      "0s - loss: 0.1434 - acc: 0.9438 - val_loss: 0.0167 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.01673 to 0.01630, saving model to best.model\n",
      "0s - loss: 0.1939 - acc: 0.9213 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.01630 to 0.01584, saving model to best.model\n",
      "0s - loss: 0.1949 - acc: 0.8989 - val_loss: 0.0158 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.01584 to 0.01542, saving model to best.model\n",
      "0s - loss: 0.1556 - acc: 0.9551 - val_loss: 0.0154 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.01542 to 0.01497, saving model to best.model\n",
      "0s - loss: 0.1054 - acc: 0.9888 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.01497 to 0.01455, saving model to best.model\n",
      "0s - loss: 0.1081 - acc: 0.9775 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.01455 to 0.01415, saving model to best.model\n",
      "0s - loss: 0.1260 - acc: 0.9551 - val_loss: 0.0142 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.01415 to 0.01379, saving model to best.model\n",
      "0s - loss: 0.1690 - acc: 0.9551 - val_loss: 0.0138 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.10059, saving model to best.model\n",
      "0s - loss: 1.3087 - acc: 0.3034 - val_loss: 1.1006 - val_acc: 0.0870\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.3338 - acc: 0.3258 - val_loss: 1.1173 - val_acc: 0.2174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.1903 - acc: 0.3933 - val_loss: 1.1328 - val_acc: 0.2174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.3171 - acc: 0.2697 - val_loss: 1.1419 - val_acc: 0.2174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2656 - acc: 0.2921 - val_loss: 1.1464 - val_acc: 0.2174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1285 - acc: 0.4382 - val_loss: 1.1443 - val_acc: 0.2174\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2763 - acc: 0.3820 - val_loss: 1.1318 - val_acc: 0.2174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.3969 - acc: 0.2247 - val_loss: 1.1156 - val_acc: 0.2174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.10059 to 1.09789, saving model to best.model\n",
      "0s - loss: 1.1434 - acc: 0.4045 - val_loss: 1.0979 - val_acc: 0.2174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.09789 to 1.08402, saving model to best.model\n",
      "0s - loss: 1.1099 - acc: 0.3933 - val_loss: 1.0840 - val_acc: 0.5652\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.08402 to 1.07300, saving model to best.model\n",
      "0s - loss: 1.1649 - acc: 0.3933 - val_loss: 1.0730 - val_acc: 0.5652\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.07300 to 1.06515, saving model to best.model\n",
      "0s - loss: 1.2646 - acc: 0.3034 - val_loss: 1.0652 - val_acc: 0.4348\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.06515 to 1.06079, saving model to best.model\n",
      "0s - loss: 1.1926 - acc: 0.3708 - val_loss: 1.0608 - val_acc: 0.4348\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.06079 to 1.05793, saving model to best.model\n",
      "0s - loss: 1.1133 - acc: 0.4270 - val_loss: 1.0579 - val_acc: 0.4348\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.05793 to 1.05629, saving model to best.model\n",
      "0s - loss: 1.1300 - acc: 0.3820 - val_loss: 1.0563 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.05629 to 1.05621, saving model to best.model\n",
      "0s - loss: 1.1670 - acc: 0.3820 - val_loss: 1.0562 - val_acc: 0.6087\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.3450 - acc: 0.2921 - val_loss: 1.0574 - val_acc: 0.6957\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1347 - acc: 0.3933 - val_loss: 1.0599 - val_acc: 0.9130\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2798 - acc: 0.2921 - val_loss: 1.0635 - val_acc: 0.7826\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1398 - acc: 0.4382 - val_loss: 1.0670 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1690 - acc: 0.4270 - val_loss: 1.0699 - val_acc: 0.2609\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.2291 - acc: 0.3258 - val_loss: 1.0734 - val_acc: 0.2174\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2251 - acc: 0.4045 - val_loss: 1.0764 - val_acc: 0.2174\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.2496 - acc: 0.2697 - val_loss: 1.0767 - val_acc: 0.2174\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.0991 - acc: 0.3933 - val_loss: 1.0752 - val_acc: 0.2174\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1687 - acc: 0.3933 - val_loss: 1.0733 - val_acc: 0.2174\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1560 - acc: 0.3034 - val_loss: 1.0707 - val_acc: 0.2174\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.0657 - acc: 0.4270 - val_loss: 1.0655 - val_acc: 0.2174\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.2170 - acc: 0.3708 - val_loss: 1.0595 - val_acc: 0.3043\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.05621 to 1.05326, saving model to best.model\n",
      "0s - loss: 1.0757 - acc: 0.4382 - val_loss: 1.0533 - val_acc: 0.5652\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.05326 to 1.04670, saving model to best.model\n",
      "0s - loss: 1.0298 - acc: 0.4270 - val_loss: 1.0467 - val_acc: 0.5652\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.04670 to 1.04099, saving model to best.model\n",
      "0s - loss: 1.1668 - acc: 0.3146 - val_loss: 1.0410 - val_acc: 0.5652\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.04099 to 1.03499, saving model to best.model\n",
      "0s - loss: 1.1533 - acc: 0.3820 - val_loss: 1.0350 - val_acc: 0.6087\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.03499 to 1.02979, saving model to best.model\n",
      "0s - loss: 1.0715 - acc: 0.4607 - val_loss: 1.0298 - val_acc: 0.6087\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.02979 to 1.02333, saving model to best.model\n",
      "0s - loss: 1.0883 - acc: 0.4157 - val_loss: 1.0233 - val_acc: 0.6087\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.02333 to 1.01711, saving model to best.model\n",
      "0s - loss: 1.1807 - acc: 0.3820 - val_loss: 1.0171 - val_acc: 0.6087\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.01711 to 1.01209, saving model to best.model\n",
      "0s - loss: 1.1427 - acc: 0.3708 - val_loss: 1.0121 - val_acc: 0.6522\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.01209 to 1.00763, saving model to best.model\n",
      "0s - loss: 1.1655 - acc: 0.3596 - val_loss: 1.0076 - val_acc: 0.8261\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.00763 to 1.00370, saving model to best.model\n",
      "0s - loss: 1.1571 - acc: 0.4382 - val_loss: 1.0037 - val_acc: 0.8696\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.00370 to 0.99987, saving model to best.model\n",
      "0s - loss: 1.1347 - acc: 0.4494 - val_loss: 0.9999 - val_acc: 0.9565\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.99987 to 0.99756, saving model to best.model\n",
      "0s - loss: 1.1272 - acc: 0.3483 - val_loss: 0.9976 - val_acc: 0.9130\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.99756 to 0.99517, saving model to best.model\n",
      "0s - loss: 1.1207 - acc: 0.3708 - val_loss: 0.9952 - val_acc: 0.9130\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.99517 to 0.99262, saving model to best.model\n",
      "0s - loss: 1.1037 - acc: 0.3933 - val_loss: 0.9926 - val_acc: 0.9130\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.99262 to 0.99075, saving model to best.model\n",
      "0s - loss: 1.1488 - acc: 0.3483 - val_loss: 0.9907 - val_acc: 0.9130\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.99075 to 0.98876, saving model to best.model\n",
      "0s - loss: 1.0648 - acc: 0.5056 - val_loss: 0.9888 - val_acc: 0.8696\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.98876 to 0.98569, saving model to best.model\n",
      "0s - loss: 1.0617 - acc: 0.4382 - val_loss: 0.9857 - val_acc: 0.8696\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.98569 to 0.98165, saving model to best.model\n",
      "0s - loss: 1.0708 - acc: 0.4494 - val_loss: 0.9817 - val_acc: 0.8696\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.98165 to 0.97685, saving model to best.model\n",
      "0s - loss: 1.0199 - acc: 0.4494 - val_loss: 0.9769 - val_acc: 0.8261\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.97685 to 0.97133, saving model to best.model\n",
      "0s - loss: 1.0234 - acc: 0.4157 - val_loss: 0.9713 - val_acc: 0.8261\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.97133 to 0.96527, saving model to best.model\n",
      "0s - loss: 1.1003 - acc: 0.4045 - val_loss: 0.9653 - val_acc: 0.7826\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.96527 to 0.95984, saving model to best.model\n",
      "0s - loss: 1.0407 - acc: 0.4270 - val_loss: 0.9598 - val_acc: 0.7391\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.95984 to 0.95417, saving model to best.model\n",
      "0s - loss: 1.0310 - acc: 0.4831 - val_loss: 0.9542 - val_acc: 0.7826\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.95417 to 0.94794, saving model to best.model\n",
      "0s - loss: 0.9358 - acc: 0.5393 - val_loss: 0.9479 - val_acc: 0.7826\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.94794 to 0.94155, saving model to best.model\n",
      "0s - loss: 0.9220 - acc: 0.5056 - val_loss: 0.9416 - val_acc: 0.7391\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.94155 to 0.93451, saving model to best.model\n",
      "0s - loss: 1.0713 - acc: 0.4045 - val_loss: 0.9345 - val_acc: 0.7391\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.93451 to 0.92703, saving model to best.model\n",
      "0s - loss: 1.1026 - acc: 0.4157 - val_loss: 0.9270 - val_acc: 0.7391\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.92703 to 0.91965, saving model to best.model\n",
      "0s - loss: 1.0360 - acc: 0.5169 - val_loss: 0.9196 - val_acc: 0.7391\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.91965 to 0.91322, saving model to best.model\n",
      "0s - loss: 1.0177 - acc: 0.4719 - val_loss: 0.9132 - val_acc: 0.7391\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.91322 to 0.90746, saving model to best.model\n",
      "0s - loss: 0.9658 - acc: 0.5618 - val_loss: 0.9075 - val_acc: 0.7391\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.90746 to 0.90272, saving model to best.model\n",
      "0s - loss: 0.9627 - acc: 0.5169 - val_loss: 0.9027 - val_acc: 0.7391\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.90272 to 0.89740, saving model to best.model\n",
      "0s - loss: 1.0264 - acc: 0.4719 - val_loss: 0.8974 - val_acc: 0.7826\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.89740 to 0.89087, saving model to best.model\n",
      "0s - loss: 0.9448 - acc: 0.5730 - val_loss: 0.8909 - val_acc: 0.7826\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.89087 to 0.88486, saving model to best.model\n",
      "0s - loss: 1.0047 - acc: 0.4270 - val_loss: 0.8849 - val_acc: 0.7826\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.88486 to 0.87781, saving model to best.model\n",
      "0s - loss: 0.9512 - acc: 0.5843 - val_loss: 0.8778 - val_acc: 0.7826\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.87781 to 0.86888, saving model to best.model\n",
      "0s - loss: 0.9599 - acc: 0.5281 - val_loss: 0.8689 - val_acc: 0.7391\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.86888 to 0.85981, saving model to best.model\n",
      "0s - loss: 0.9596 - acc: 0.5618 - val_loss: 0.8598 - val_acc: 0.7391\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.85981 to 0.85067, saving model to best.model\n",
      "0s - loss: 0.9907 - acc: 0.4494 - val_loss: 0.8507 - val_acc: 0.7391\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.85067 to 0.84090, saving model to best.model\n",
      "0s - loss: 0.9592 - acc: 0.5281 - val_loss: 0.8409 - val_acc: 0.7391\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.84090 to 0.83087, saving model to best.model\n",
      "0s - loss: 0.8756 - acc: 0.5506 - val_loss: 0.8309 - val_acc: 0.7391\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.83087 to 0.81989, saving model to best.model\n",
      "0s - loss: 0.9236 - acc: 0.5393 - val_loss: 0.8199 - val_acc: 0.7826\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.81989 to 0.80864, saving model to best.model\n",
      "0s - loss: 0.9160 - acc: 0.5281 - val_loss: 0.8086 - val_acc: 0.7826\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.80864 to 0.79751, saving model to best.model\n",
      "0s - loss: 0.7982 - acc: 0.5843 - val_loss: 0.7975 - val_acc: 0.7826\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.79751 to 0.78633, saving model to best.model\n",
      "0s - loss: 0.8697 - acc: 0.5843 - val_loss: 0.7863 - val_acc: 0.7826\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.78633 to 0.77541, saving model to best.model\n",
      "0s - loss: 0.9209 - acc: 0.5393 - val_loss: 0.7754 - val_acc: 0.8261\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.77541 to 0.76494, saving model to best.model\n",
      "0s - loss: 0.8515 - acc: 0.6292 - val_loss: 0.7649 - val_acc: 0.8261\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.76494 to 0.75538, saving model to best.model\n",
      "0s - loss: 0.9310 - acc: 0.4719 - val_loss: 0.7554 - val_acc: 0.8261\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.75538 to 0.74694, saving model to best.model\n",
      "0s - loss: 0.8525 - acc: 0.6292 - val_loss: 0.7469 - val_acc: 0.7826\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.74694 to 0.73948, saving model to best.model\n",
      "0s - loss: 0.9081 - acc: 0.5506 - val_loss: 0.7395 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.73948 to 0.73217, saving model to best.model\n",
      "0s - loss: 0.9070 - acc: 0.5281 - val_loss: 0.7322 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.73217 to 0.72584, saving model to best.model\n",
      "0s - loss: 0.8734 - acc: 0.5506 - val_loss: 0.7258 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.72584 to 0.71998, saving model to best.model\n",
      "0s - loss: 0.8311 - acc: 0.6629 - val_loss: 0.7200 - val_acc: 0.7826\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.71998 to 0.71359, saving model to best.model\n",
      "0s - loss: 0.8198 - acc: 0.6180 - val_loss: 0.7136 - val_acc: 0.7391\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.71359 to 0.70658, saving model to best.model\n",
      "0s - loss: 0.8101 - acc: 0.6404 - val_loss: 0.7066 - val_acc: 0.7391\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.70658 to 0.69962, saving model to best.model\n",
      "0s - loss: 0.8773 - acc: 0.5393 - val_loss: 0.6996 - val_acc: 0.7391\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69962 to 0.69166, saving model to best.model\n",
      "0s - loss: 0.8287 - acc: 0.5955 - val_loss: 0.6917 - val_acc: 0.7391\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69166 to 0.68189, saving model to best.model\n",
      "0s - loss: 0.7926 - acc: 0.5955 - val_loss: 0.6819 - val_acc: 0.7391\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68189 to 0.67162, saving model to best.model\n",
      "0s - loss: 0.7543 - acc: 0.6854 - val_loss: 0.6716 - val_acc: 0.7391\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67162 to 0.65987, saving model to best.model\n",
      "0s - loss: 0.7366 - acc: 0.6742 - val_loss: 0.6599 - val_acc: 0.7391\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.65987 to 0.64728, saving model to best.model\n",
      "0s - loss: 0.7403 - acc: 0.6854 - val_loss: 0.6473 - val_acc: 0.7391\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.64728 to 0.63640, saving model to best.model\n",
      "0s - loss: 0.7280 - acc: 0.6517 - val_loss: 0.6364 - val_acc: 0.7391\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.63640 to 0.62674, saving model to best.model\n",
      "0s - loss: 0.7444 - acc: 0.6517 - val_loss: 0.6267 - val_acc: 0.7391\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.62674 to 0.61634, saving model to best.model\n",
      "0s - loss: 0.7033 - acc: 0.6966 - val_loss: 0.6163 - val_acc: 0.7391\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.61634 to 0.60660, saving model to best.model\n",
      "0s - loss: 0.6827 - acc: 0.7303 - val_loss: 0.6066 - val_acc: 0.7391\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.60660 to 0.59570, saving model to best.model\n",
      "0s - loss: 0.6693 - acc: 0.6854 - val_loss: 0.5957 - val_acc: 0.7391\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.59570 to 0.58527, saving model to best.model\n",
      "0s - loss: 0.7042 - acc: 0.7416 - val_loss: 0.5853 - val_acc: 0.7391\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.58527 to 0.57531, saving model to best.model\n",
      "0s - loss: 0.6763 - acc: 0.7079 - val_loss: 0.5753 - val_acc: 0.7826\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.57531 to 0.56637, saving model to best.model\n",
      "0s - loss: 0.6818 - acc: 0.7191 - val_loss: 0.5664 - val_acc: 0.7826\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.56637 to 0.55893, saving model to best.model\n",
      "0s - loss: 0.6425 - acc: 0.7191 - val_loss: 0.5589 - val_acc: 0.7826\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.55893 to 0.55393, saving model to best.model\n",
      "0s - loss: 0.6271 - acc: 0.7753 - val_loss: 0.5539 - val_acc: 0.7826\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.55393 to 0.54780, saving model to best.model\n",
      "0s - loss: 0.6558 - acc: 0.7079 - val_loss: 0.5478 - val_acc: 0.7826\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.54780 to 0.54371, saving model to best.model\n",
      "0s - loss: 0.6702 - acc: 0.7079 - val_loss: 0.5437 - val_acc: 0.7826\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.54371 to 0.53734, saving model to best.model\n",
      "0s - loss: 0.6855 - acc: 0.7079 - val_loss: 0.5373 - val_acc: 0.7826\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.53734 to 0.53087, saving model to best.model\n",
      "0s - loss: 0.6175 - acc: 0.7079 - val_loss: 0.5309 - val_acc: 0.7826\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.53087 to 0.52439, saving model to best.model\n",
      "0s - loss: 0.7012 - acc: 0.7416 - val_loss: 0.5244 - val_acc: 0.7826\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.52439 to 0.51874, saving model to best.model\n",
      "0s - loss: 0.6240 - acc: 0.6854 - val_loss: 0.5187 - val_acc: 0.7826\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.51874 to 0.51334, saving model to best.model\n",
      "0s - loss: 0.5708 - acc: 0.7865 - val_loss: 0.5133 - val_acc: 0.7826\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.51334 to 0.50768, saving model to best.model\n",
      "0s - loss: 0.5691 - acc: 0.8090 - val_loss: 0.5077 - val_acc: 0.7826\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.50768 to 0.50172, saving model to best.model\n",
      "0s - loss: 0.5914 - acc: 0.7528 - val_loss: 0.5017 - val_acc: 0.7826\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.50172 to 0.49764, saving model to best.model\n",
      "0s - loss: 0.6425 - acc: 0.7191 - val_loss: 0.4976 - val_acc: 0.7826\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.49764 to 0.49385, saving model to best.model\n",
      "0s - loss: 0.5951 - acc: 0.7191 - val_loss: 0.4939 - val_acc: 0.7826\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.49385 to 0.49010, saving model to best.model\n",
      "0s - loss: 0.5556 - acc: 0.7865 - val_loss: 0.4901 - val_acc: 0.7826\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.49010 to 0.48503, saving model to best.model\n",
      "0s - loss: 0.6696 - acc: 0.6966 - val_loss: 0.4850 - val_acc: 0.7826\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.48503 to 0.48116, saving model to best.model\n",
      "0s - loss: 0.5606 - acc: 0.7528 - val_loss: 0.4812 - val_acc: 0.7826\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.48116 to 0.47712, saving model to best.model\n",
      "0s - loss: 0.6377 - acc: 0.6742 - val_loss: 0.4771 - val_acc: 0.7826\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.47712 to 0.47291, saving model to best.model\n",
      "0s - loss: 0.5392 - acc: 0.7865 - val_loss: 0.4729 - val_acc: 0.7826\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.47291 to 0.46695, saving model to best.model\n",
      "0s - loss: 0.5639 - acc: 0.7416 - val_loss: 0.4670 - val_acc: 0.7826\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.46695 to 0.46234, saving model to best.model\n",
      "0s - loss: 0.5549 - acc: 0.8315 - val_loss: 0.4623 - val_acc: 0.7826\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.46234 to 0.45784, saving model to best.model\n",
      "0s - loss: 0.4429 - acc: 0.8090 - val_loss: 0.4578 - val_acc: 0.7826\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.45784 to 0.45308, saving model to best.model\n",
      "0s - loss: 0.5530 - acc: 0.7303 - val_loss: 0.4531 - val_acc: 0.7826\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.45308 to 0.44603, saving model to best.model\n",
      "0s - loss: 0.5079 - acc: 0.8202 - val_loss: 0.4460 - val_acc: 0.7826\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.44603 to 0.43775, saving model to best.model\n",
      "0s - loss: 0.5584 - acc: 0.7416 - val_loss: 0.4378 - val_acc: 0.7826\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.43775 to 0.43082, saving model to best.model\n",
      "0s - loss: 0.5473 - acc: 0.7416 - val_loss: 0.4308 - val_acc: 0.7826\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.43082 to 0.42536, saving model to best.model\n",
      "0s - loss: 0.4834 - acc: 0.8090 - val_loss: 0.4254 - val_acc: 0.7826\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.42536 to 0.42121, saving model to best.model\n",
      "0s - loss: 0.5378 - acc: 0.7528 - val_loss: 0.4212 - val_acc: 0.7826\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.42121 to 0.41683, saving model to best.model\n",
      "0s - loss: 0.5339 - acc: 0.7303 - val_loss: 0.4168 - val_acc: 0.7826\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.41683 to 0.41527, saving model to best.model\n",
      "0s - loss: 0.5415 - acc: 0.7753 - val_loss: 0.4153 - val_acc: 0.7826\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.41527 to 0.41480, saving model to best.model\n",
      "0s - loss: 0.5031 - acc: 0.7865 - val_loss: 0.4148 - val_acc: 0.7826\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.4886 - acc: 0.7640 - val_loss: 0.4158 - val_acc: 0.7826\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.5316 - acc: 0.7753 - val_loss: 0.4161 - val_acc: 0.7826\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.41480 to 0.41442, saving model to best.model\n",
      "0s - loss: 0.4925 - acc: 0.8202 - val_loss: 0.4144 - val_acc: 0.7826\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.41442 to 0.41030, saving model to best.model\n",
      "0s - loss: 0.4457 - acc: 0.8315 - val_loss: 0.4103 - val_acc: 0.7826\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.41030 to 0.40398, saving model to best.model\n",
      "0s - loss: 0.4639 - acc: 0.7865 - val_loss: 0.4040 - val_acc: 0.7826\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.40398 to 0.39730, saving model to best.model\n",
      "0s - loss: 0.4881 - acc: 0.8202 - val_loss: 0.3973 - val_acc: 0.7826\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.39730 to 0.39296, saving model to best.model\n",
      "0s - loss: 0.4807 - acc: 0.8090 - val_loss: 0.3930 - val_acc: 0.7826\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.39296 to 0.38661, saving model to best.model\n",
      "0s - loss: 0.4440 - acc: 0.8427 - val_loss: 0.3866 - val_acc: 0.7826\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.38661 to 0.37811, saving model to best.model\n",
      "0s - loss: 0.4472 - acc: 0.8427 - val_loss: 0.3781 - val_acc: 0.7826\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.37811 to 0.37156, saving model to best.model\n",
      "0s - loss: 0.4443 - acc: 0.8090 - val_loss: 0.3716 - val_acc: 0.7826\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.37156 to 0.36511, saving model to best.model\n",
      "0s - loss: 0.4522 - acc: 0.7753 - val_loss: 0.3651 - val_acc: 0.7826\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.36511 to 0.35919, saving model to best.model\n",
      "0s - loss: 0.4066 - acc: 0.8539 - val_loss: 0.3592 - val_acc: 0.7826\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.35919 to 0.35391, saving model to best.model\n",
      "0s - loss: 0.4256 - acc: 0.8315 - val_loss: 0.3539 - val_acc: 0.7826\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.35391 to 0.34763, saving model to best.model\n",
      "0s - loss: 0.4030 - acc: 0.8315 - val_loss: 0.3476 - val_acc: 0.7826\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.34763 to 0.34199, saving model to best.model\n",
      "0s - loss: 0.4512 - acc: 0.8539 - val_loss: 0.3420 - val_acc: 0.7826\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.34199 to 0.33713, saving model to best.model\n",
      "0s - loss: 0.4510 - acc: 0.8315 - val_loss: 0.3371 - val_acc: 0.7826\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.33713 to 0.33324, saving model to best.model\n",
      "0s - loss: 0.4143 - acc: 0.8427 - val_loss: 0.3332 - val_acc: 0.7826\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.33324 to 0.32976, saving model to best.model\n",
      "0s - loss: 0.4378 - acc: 0.8315 - val_loss: 0.3298 - val_acc: 0.7826\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.32976 to 0.32773, saving model to best.model\n",
      "0s - loss: 0.4227 - acc: 0.8427 - val_loss: 0.3277 - val_acc: 0.7826\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.32773 to 0.32521, saving model to best.model\n",
      "0s - loss: 0.4003 - acc: 0.8427 - val_loss: 0.3252 - val_acc: 0.7826\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.32521 to 0.32170, saving model to best.model\n",
      "0s - loss: 0.4851 - acc: 0.7528 - val_loss: 0.3217 - val_acc: 0.7826\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.32170 to 0.31961, saving model to best.model\n",
      "0s - loss: 0.4626 - acc: 0.8202 - val_loss: 0.3196 - val_acc: 0.8261\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.31961 to 0.31658, saving model to best.model\n",
      "0s - loss: 0.4315 - acc: 0.8315 - val_loss: 0.3166 - val_acc: 0.8261\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.31658 to 0.31404, saving model to best.model\n",
      "0s - loss: 0.3350 - acc: 0.8876 - val_loss: 0.3140 - val_acc: 0.8261\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.31404 to 0.31155, saving model to best.model\n",
      "0s - loss: 0.3584 - acc: 0.8876 - val_loss: 0.3115 - val_acc: 0.8696\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.31155 to 0.30730, saving model to best.model\n",
      "0s - loss: 0.3462 - acc: 0.8652 - val_loss: 0.3073 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.30730 to 0.30165, saving model to best.model\n",
      "0s - loss: 0.3944 - acc: 0.8202 - val_loss: 0.3016 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.30165 to 0.29669, saving model to best.model\n",
      "0s - loss: 0.4298 - acc: 0.8539 - val_loss: 0.2967 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.29669 to 0.29100, saving model to best.model\n",
      "0s - loss: 0.3771 - acc: 0.8427 - val_loss: 0.2910 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.29100 to 0.28745, saving model to best.model\n",
      "0s - loss: 0.4343 - acc: 0.8202 - val_loss: 0.2875 - val_acc: 0.8696\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.28745 to 0.28288, saving model to best.model\n",
      "0s - loss: 0.4268 - acc: 0.8202 - val_loss: 0.2829 - val_acc: 0.8696\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.28288 to 0.27961, saving model to best.model\n",
      "0s - loss: 0.3980 - acc: 0.8315 - val_loss: 0.2796 - val_acc: 0.8696\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.27961 to 0.27722, saving model to best.model\n",
      "0s - loss: 0.4323 - acc: 0.8090 - val_loss: 0.2772 - val_acc: 0.8696\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.27722 to 0.27639, saving model to best.model\n",
      "0s - loss: 0.3722 - acc: 0.8764 - val_loss: 0.2764 - val_acc: 0.8696\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.27639 to 0.27460, saving model to best.model\n",
      "0s - loss: 0.3947 - acc: 0.8202 - val_loss: 0.2746 - val_acc: 0.8696\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.27460 to 0.27283, saving model to best.model\n",
      "0s - loss: 0.3287 - acc: 0.9101 - val_loss: 0.2728 - val_acc: 0.8696\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.27283 to 0.27199, saving model to best.model\n",
      "0s - loss: 0.3348 - acc: 0.8427 - val_loss: 0.2720 - val_acc: 0.8696\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.27199 to 0.27040, saving model to best.model\n",
      "0s - loss: 0.3416 - acc: 0.8539 - val_loss: 0.2704 - val_acc: 0.8696\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.27040 to 0.26953, saving model to best.model\n",
      "0s - loss: 0.3345 - acc: 0.8876 - val_loss: 0.2695 - val_acc: 0.8696\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.26953 to 0.26679, saving model to best.model\n",
      "0s - loss: 0.3009 - acc: 0.8989 - val_loss: 0.2668 - val_acc: 0.8696\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.26679 to 0.26375, saving model to best.model\n",
      "0s - loss: 0.2888 - acc: 0.9101 - val_loss: 0.2637 - val_acc: 0.8696\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.26375 to 0.26124, saving model to best.model\n",
      "0s - loss: 0.4303 - acc: 0.8539 - val_loss: 0.2612 - val_acc: 0.8696\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.26124 to 0.26006, saving model to best.model\n",
      "0s - loss: 0.3960 - acc: 0.7865 - val_loss: 0.2601 - val_acc: 0.8696\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.26006 to 0.25688, saving model to best.model\n",
      "0s - loss: 0.3480 - acc: 0.8539 - val_loss: 0.2569 - val_acc: 0.8696\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.25688 to 0.25259, saving model to best.model\n",
      "0s - loss: 0.3466 - acc: 0.9101 - val_loss: 0.2526 - val_acc: 0.9130\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.25259 to 0.24732, saving model to best.model\n",
      "0s - loss: 0.3382 - acc: 0.8652 - val_loss: 0.2473 - val_acc: 0.9130\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.24732 to 0.24263, saving model to best.model\n",
      "0s - loss: 0.2686 - acc: 0.9101 - val_loss: 0.2426 - val_acc: 0.9130\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.24263 to 0.23649, saving model to best.model\n",
      "0s - loss: 0.3348 - acc: 0.8539 - val_loss: 0.2365 - val_acc: 0.9130\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.23649 to 0.22851, saving model to best.model\n",
      "0s - loss: 0.2798 - acc: 0.8989 - val_loss: 0.2285 - val_acc: 0.9130\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.22851 to 0.22111, saving model to best.model\n",
      "0s - loss: 0.3619 - acc: 0.8202 - val_loss: 0.2211 - val_acc: 0.9130\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.22111 to 0.21375, saving model to best.model\n",
      "0s - loss: 0.3069 - acc: 0.8876 - val_loss: 0.2138 - val_acc: 0.9130\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.21375 to 0.20814, saving model to best.model\n",
      "0s - loss: 0.3299 - acc: 0.8652 - val_loss: 0.2081 - val_acc: 0.9130\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.20814 to 0.20368, saving model to best.model\n",
      "0s - loss: 0.3275 - acc: 0.8764 - val_loss: 0.2037 - val_acc: 0.9130\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.20368 to 0.19907, saving model to best.model\n",
      "0s - loss: 0.3840 - acc: 0.8539 - val_loss: 0.1991 - val_acc: 0.9130\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.19907 to 0.19547, saving model to best.model\n",
      "0s - loss: 0.3091 - acc: 0.9101 - val_loss: 0.1955 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.19547 to 0.19296, saving model to best.model\n",
      "0s - loss: 0.3281 - acc: 0.8764 - val_loss: 0.1930 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.19296 to 0.19003, saving model to best.model\n",
      "0s - loss: 0.3046 - acc: 0.8989 - val_loss: 0.1900 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.19003 to 0.18834, saving model to best.model\n",
      "0s - loss: 0.3630 - acc: 0.7865 - val_loss: 0.1883 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.18834 to 0.18642, saving model to best.model\n",
      "0s - loss: 0.2535 - acc: 0.9326 - val_loss: 0.1864 - val_acc: 0.9130\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.18642 to 0.18523, saving model to best.model\n",
      "0s - loss: 0.2830 - acc: 0.8989 - val_loss: 0.1852 - val_acc: 0.9130\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.3016 - acc: 0.9326 - val_loss: 0.1853 - val_acc: 0.9130\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.18523 to 0.18479, saving model to best.model\n",
      "0s - loss: 0.2974 - acc: 0.8989 - val_loss: 0.1848 - val_acc: 0.9130\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.18479 to 0.18233, saving model to best.model\n",
      "0s - loss: 0.3563 - acc: 0.8539 - val_loss: 0.1823 - val_acc: 0.9130\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.18233 to 0.18050, saving model to best.model\n",
      "0s - loss: 0.3901 - acc: 0.8427 - val_loss: 0.1805 - val_acc: 0.9130\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.18050 to 0.17967, saving model to best.model\n",
      "0s - loss: 0.2498 - acc: 0.9326 - val_loss: 0.1797 - val_acc: 0.9130\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.17967 to 0.17851, saving model to best.model\n",
      "0s - loss: 0.2715 - acc: 0.9213 - val_loss: 0.1785 - val_acc: 0.9130\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.17851 to 0.17704, saving model to best.model\n",
      "0s - loss: 0.2258 - acc: 0.9326 - val_loss: 0.1770 - val_acc: 0.9130\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.17704 to 0.17581, saving model to best.model\n",
      "0s - loss: 0.2731 - acc: 0.8764 - val_loss: 0.1758 - val_acc: 0.9130\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.17581 to 0.17387, saving model to best.model\n",
      "0s - loss: 0.2483 - acc: 0.8764 - val_loss: 0.1739 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.17387 to 0.17151, saving model to best.model\n",
      "0s - loss: 0.2720 - acc: 0.8876 - val_loss: 0.1715 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.17151 to 0.16986, saving model to best.model\n",
      "0s - loss: 0.3104 - acc: 0.8989 - val_loss: 0.1699 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.16986 to 0.16742, saving model to best.model\n",
      "0s - loss: 0.2301 - acc: 0.9213 - val_loss: 0.1674 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.16742 to 0.16360, saving model to best.model\n",
      "0s - loss: 0.2589 - acc: 0.9326 - val_loss: 0.1636 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.17489, saving model to best.model\n",
      "0s - loss: 1.2890 - acc: 0.3820 - val_loss: 1.1749 - val_acc: 0.1304\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.17489 to 1.09466, saving model to best.model\n",
      "0s - loss: 1.2107 - acc: 0.3034 - val_loss: 1.0947 - val_acc: 0.4348\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.09466 to 1.04773, saving model to best.model\n",
      "0s - loss: 1.1436 - acc: 0.4382 - val_loss: 1.0477 - val_acc: 0.8261\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.04773 to 1.02334, saving model to best.model\n",
      "0s - loss: 1.2436 - acc: 0.4157 - val_loss: 1.0233 - val_acc: 0.4348\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.02334 to 1.00987, saving model to best.model\n",
      "0s - loss: 1.1780 - acc: 0.3933 - val_loss: 1.0099 - val_acc: 0.4348\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.00987 to 1.00273, saving model to best.model\n",
      "0s - loss: 1.1529 - acc: 0.4382 - val_loss: 1.0027 - val_acc: 0.4348\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.00273 to 0.99927, saving model to best.model\n",
      "0s - loss: 1.2498 - acc: 0.3596 - val_loss: 0.9993 - val_acc: 0.4348\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.3788 - acc: 0.2472 - val_loss: 0.9996 - val_acc: 0.4348\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.3214 - acc: 0.3258 - val_loss: 1.0022 - val_acc: 0.4348\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.3404 - acc: 0.2360 - val_loss: 1.0068 - val_acc: 0.4348\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2689 - acc: 0.3034 - val_loss: 1.0120 - val_acc: 0.4348\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2674 - acc: 0.3820 - val_loss: 1.0170 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2512 - acc: 0.3034 - val_loss: 1.0220 - val_acc: 0.8696\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2522 - acc: 0.3034 - val_loss: 1.0281 - val_acc: 0.7826\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2624 - acc: 0.2809 - val_loss: 1.0350 - val_acc: 0.5652\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2238 - acc: 0.3708 - val_loss: 1.0372 - val_acc: 0.4783\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2167 - acc: 0.3258 - val_loss: 1.0377 - val_acc: 0.4783\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2021 - acc: 0.3483 - val_loss: 1.0369 - val_acc: 0.4783\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.3580 - acc: 0.3034 - val_loss: 1.0365 - val_acc: 0.4783\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2298 - acc: 0.3596 - val_loss: 1.0338 - val_acc: 0.5217\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.2083 - acc: 0.3596 - val_loss: 1.0316 - val_acc: 0.5652\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1841 - acc: 0.4157 - val_loss: 1.0274 - val_acc: 0.5652\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1376 - acc: 0.3820 - val_loss: 1.0224 - val_acc: 0.6522\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1453 - acc: 0.4270 - val_loss: 1.0185 - val_acc: 0.6957\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1488 - acc: 0.3933 - val_loss: 1.0127 - val_acc: 0.7391\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.2086 - acc: 0.3483 - val_loss: 1.0044 - val_acc: 0.7826\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.99927 to 0.99703, saving model to best.model\n",
      "0s - loss: 1.1888 - acc: 0.3483 - val_loss: 0.9970 - val_acc: 0.8261\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.99703 to 0.99021, saving model to best.model\n",
      "0s - loss: 1.1361 - acc: 0.3708 - val_loss: 0.9902 - val_acc: 0.7826\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.99021 to 0.98447, saving model to best.model\n",
      "0s - loss: 1.0790 - acc: 0.5169 - val_loss: 0.9845 - val_acc: 0.8261\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.98447 to 0.97878, saving model to best.model\n",
      "0s - loss: 1.2049 - acc: 0.3483 - val_loss: 0.9788 - val_acc: 0.8261\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.97878 to 0.97489, saving model to best.model\n",
      "0s - loss: 1.0915 - acc: 0.4494 - val_loss: 0.9749 - val_acc: 0.8261\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.97489 to 0.97096, saving model to best.model\n",
      "0s - loss: 1.1220 - acc: 0.3596 - val_loss: 0.9710 - val_acc: 0.8261\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.97096 to 0.96762, saving model to best.model\n",
      "0s - loss: 1.2281 - acc: 0.3146 - val_loss: 0.9676 - val_acc: 0.8261\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.96762 to 0.96395, saving model to best.model\n",
      "0s - loss: 1.1338 - acc: 0.4157 - val_loss: 0.9640 - val_acc: 0.8261\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.96395 to 0.96011, saving model to best.model\n",
      "0s - loss: 1.0622 - acc: 0.4270 - val_loss: 0.9601 - val_acc: 0.7826\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.96011 to 0.95576, saving model to best.model\n",
      "0s - loss: 1.0634 - acc: 0.4494 - val_loss: 0.9558 - val_acc: 0.6957\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.95576 to 0.95286, saving model to best.model\n",
      "0s - loss: 1.1792 - acc: 0.3483 - val_loss: 0.9529 - val_acc: 0.6957\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.95286 to 0.95057, saving model to best.model\n",
      "0s - loss: 1.0925 - acc: 0.4831 - val_loss: 0.9506 - val_acc: 0.6957\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.95057 to 0.94710, saving model to best.model\n",
      "0s - loss: 1.0549 - acc: 0.4382 - val_loss: 0.9471 - val_acc: 0.6957\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.94710 to 0.94428, saving model to best.model\n",
      "0s - loss: 1.0939 - acc: 0.4607 - val_loss: 0.9443 - val_acc: 0.6957\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.94428 to 0.94160, saving model to best.model\n",
      "0s - loss: 1.0960 - acc: 0.4719 - val_loss: 0.9416 - val_acc: 0.6957\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.94160 to 0.93998, saving model to best.model\n",
      "0s - loss: 1.1434 - acc: 0.4045 - val_loss: 0.9400 - val_acc: 0.6957\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.93998 to 0.93855, saving model to best.model\n",
      "0s - loss: 1.0409 - acc: 0.3933 - val_loss: 0.9386 - val_acc: 0.6957\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.93855 to 0.93621, saving model to best.model\n",
      "0s - loss: 1.1391 - acc: 0.3933 - val_loss: 0.9362 - val_acc: 0.7391\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.93621 to 0.93345, saving model to best.model\n",
      "0s - loss: 1.0674 - acc: 0.4494 - val_loss: 0.9334 - val_acc: 0.8261\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.93345 to 0.93078, saving model to best.model\n",
      "0s - loss: 1.0317 - acc: 0.3933 - val_loss: 0.9308 - val_acc: 0.9130\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.93078 to 0.92864, saving model to best.model\n",
      "0s - loss: 1.1699 - acc: 0.3933 - val_loss: 0.9286 - val_acc: 0.9565\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.92864 to 0.92691, saving model to best.model\n",
      "0s - loss: 1.0657 - acc: 0.4270 - val_loss: 0.9269 - val_acc: 0.9565\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.92691 to 0.92494, saving model to best.model\n",
      "0s - loss: 1.0865 - acc: 0.4382 - val_loss: 0.9249 - val_acc: 0.9565\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.92494 to 0.92244, saving model to best.model\n",
      "0s - loss: 1.0317 - acc: 0.4494 - val_loss: 0.9224 - val_acc: 0.9565\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.92244 to 0.91879, saving model to best.model\n",
      "0s - loss: 1.0511 - acc: 0.4607 - val_loss: 0.9188 - val_acc: 0.9565\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.91879 to 0.91549, saving model to best.model\n",
      "0s - loss: 0.9779 - acc: 0.5281 - val_loss: 0.9155 - val_acc: 0.9565\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.91549 to 0.91033, saving model to best.model\n",
      "0s - loss: 1.0522 - acc: 0.4494 - val_loss: 0.9103 - val_acc: 0.9565\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.91033 to 0.90255, saving model to best.model\n",
      "0s - loss: 1.0781 - acc: 0.4719 - val_loss: 0.9025 - val_acc: 0.9565\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.90255 to 0.89418, saving model to best.model\n",
      "0s - loss: 1.0977 - acc: 0.4270 - val_loss: 0.8942 - val_acc: 0.9565\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.89418 to 0.88536, saving model to best.model\n",
      "0s - loss: 1.0917 - acc: 0.4607 - val_loss: 0.8854 - val_acc: 0.9565\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.88536 to 0.87511, saving model to best.model\n",
      "0s - loss: 0.9645 - acc: 0.5169 - val_loss: 0.8751 - val_acc: 0.9565\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.87511 to 0.86471, saving model to best.model\n",
      "0s - loss: 1.0093 - acc: 0.4494 - val_loss: 0.8647 - val_acc: 0.9565\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.86471 to 0.85389, saving model to best.model\n",
      "0s - loss: 0.9597 - acc: 0.4944 - val_loss: 0.8539 - val_acc: 0.9565\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.85389 to 0.84392, saving model to best.model\n",
      "0s - loss: 0.9918 - acc: 0.4944 - val_loss: 0.8439 - val_acc: 0.9565\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.84392 to 0.83411, saving model to best.model\n",
      "0s - loss: 1.0595 - acc: 0.4270 - val_loss: 0.8341 - val_acc: 0.9565\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.83411 to 0.82511, saving model to best.model\n",
      "0s - loss: 0.9236 - acc: 0.5843 - val_loss: 0.8251 - val_acc: 0.9565\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.82511 to 0.81551, saving model to best.model\n",
      "0s - loss: 0.9981 - acc: 0.4494 - val_loss: 0.8155 - val_acc: 0.9565\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.81551 to 0.80592, saving model to best.model\n",
      "0s - loss: 0.9516 - acc: 0.5618 - val_loss: 0.8059 - val_acc: 0.9565\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.80592 to 0.79664, saving model to best.model\n",
      "0s - loss: 0.8662 - acc: 0.6292 - val_loss: 0.7966 - val_acc: 0.9565\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.79664 to 0.78727, saving model to best.model\n",
      "0s - loss: 0.9060 - acc: 0.6067 - val_loss: 0.7873 - val_acc: 0.9565\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.78727 to 0.77914, saving model to best.model\n",
      "0s - loss: 0.9106 - acc: 0.5730 - val_loss: 0.7791 - val_acc: 0.9565\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.77914 to 0.77018, saving model to best.model\n",
      "0s - loss: 0.9358 - acc: 0.5169 - val_loss: 0.7702 - val_acc: 0.9565\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.77018 to 0.76132, saving model to best.model\n",
      "0s - loss: 0.9779 - acc: 0.5281 - val_loss: 0.7613 - val_acc: 0.9565\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.76132 to 0.75252, saving model to best.model\n",
      "0s - loss: 0.8940 - acc: 0.5618 - val_loss: 0.7525 - val_acc: 0.9565\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.75252 to 0.74291, saving model to best.model\n",
      "0s - loss: 0.8490 - acc: 0.6067 - val_loss: 0.7429 - val_acc: 0.9565\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.74291 to 0.73241, saving model to best.model\n",
      "0s - loss: 0.9338 - acc: 0.5730 - val_loss: 0.7324 - val_acc: 0.9565\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.73241 to 0.72222, saving model to best.model\n",
      "0s - loss: 0.9022 - acc: 0.6067 - val_loss: 0.7222 - val_acc: 0.9565\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.72222 to 0.71217, saving model to best.model\n",
      "0s - loss: 0.8560 - acc: 0.6180 - val_loss: 0.7122 - val_acc: 0.9565\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71217 to 0.70162, saving model to best.model\n",
      "0s - loss: 0.8609 - acc: 0.5618 - val_loss: 0.7016 - val_acc: 0.9565\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70162 to 0.69115, saving model to best.model\n",
      "0s - loss: 0.8028 - acc: 0.6742 - val_loss: 0.6912 - val_acc: 0.9565\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69115 to 0.68121, saving model to best.model\n",
      "0s - loss: 0.7911 - acc: 0.6742 - val_loss: 0.6812 - val_acc: 0.9565\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.68121 to 0.67087, saving model to best.model\n",
      "0s - loss: 0.8205 - acc: 0.6404 - val_loss: 0.6709 - val_acc: 0.9565\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.67087 to 0.66060, saving model to best.model\n",
      "0s - loss: 0.8806 - acc: 0.5843 - val_loss: 0.6606 - val_acc: 0.9565\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.66060 to 0.64994, saving model to best.model\n",
      "0s - loss: 0.8187 - acc: 0.6742 - val_loss: 0.6499 - val_acc: 0.9565\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.64994 to 0.63883, saving model to best.model\n",
      "0s - loss: 0.8185 - acc: 0.6180 - val_loss: 0.6388 - val_acc: 0.9565\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.63883 to 0.62732, saving model to best.model\n",
      "0s - loss: 0.7821 - acc: 0.6742 - val_loss: 0.6273 - val_acc: 0.9565\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.62732 to 0.61561, saving model to best.model\n",
      "0s - loss: 0.7884 - acc: 0.6180 - val_loss: 0.6156 - val_acc: 0.9565\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.61561 to 0.60351, saving model to best.model\n",
      "0s - loss: 0.7959 - acc: 0.6517 - val_loss: 0.6035 - val_acc: 0.9565\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.60351 to 0.59143, saving model to best.model\n",
      "0s - loss: 0.7348 - acc: 0.6966 - val_loss: 0.5914 - val_acc: 0.9565\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.59143 to 0.57878, saving model to best.model\n",
      "0s - loss: 0.7719 - acc: 0.6517 - val_loss: 0.5788 - val_acc: 0.9565\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.57878 to 0.56600, saving model to best.model\n",
      "0s - loss: 0.7263 - acc: 0.7191 - val_loss: 0.5660 - val_acc: 0.9565\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.56600 to 0.55379, saving model to best.model\n",
      "0s - loss: 0.7095 - acc: 0.7416 - val_loss: 0.5538 - val_acc: 0.9565\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.55379 to 0.54210, saving model to best.model\n",
      "0s - loss: 0.6778 - acc: 0.7191 - val_loss: 0.5421 - val_acc: 0.9565\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.54210 to 0.53053, saving model to best.model\n",
      "0s - loss: 0.7179 - acc: 0.6517 - val_loss: 0.5305 - val_acc: 0.9565\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.53053 to 0.51923, saving model to best.model\n",
      "0s - loss: 0.6310 - acc: 0.7865 - val_loss: 0.5192 - val_acc: 0.9565\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.51923 to 0.50773, saving model to best.model\n",
      "0s - loss: 0.6369 - acc: 0.7303 - val_loss: 0.5077 - val_acc: 0.9565\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.50773 to 0.49692, saving model to best.model\n",
      "0s - loss: 0.5979 - acc: 0.7865 - val_loss: 0.4969 - val_acc: 0.9565\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.49692 to 0.48697, saving model to best.model\n",
      "0s - loss: 0.7733 - acc: 0.6292 - val_loss: 0.4870 - val_acc: 0.9565\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.48697 to 0.47732, saving model to best.model\n",
      "0s - loss: 0.6177 - acc: 0.7640 - val_loss: 0.4773 - val_acc: 0.9565\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.47732 to 0.46779, saving model to best.model\n",
      "0s - loss: 0.6053 - acc: 0.7640 - val_loss: 0.4678 - val_acc: 0.9565\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.46779 to 0.45853, saving model to best.model\n",
      "0s - loss: 0.5801 - acc: 0.8427 - val_loss: 0.4585 - val_acc: 0.9565\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.45853 to 0.44877, saving model to best.model\n",
      "0s - loss: 0.5554 - acc: 0.7865 - val_loss: 0.4488 - val_acc: 0.9565\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.44877 to 0.43883, saving model to best.model\n",
      "0s - loss: 0.5880 - acc: 0.7528 - val_loss: 0.4388 - val_acc: 0.9565\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.43883 to 0.42870, saving model to best.model\n",
      "0s - loss: 0.6232 - acc: 0.7303 - val_loss: 0.4287 - val_acc: 0.9565\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.42870 to 0.41900, saving model to best.model\n",
      "0s - loss: 0.5727 - acc: 0.7528 - val_loss: 0.4190 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.41900 to 0.40907, saving model to best.model\n",
      "0s - loss: 0.5489 - acc: 0.8090 - val_loss: 0.4091 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.40907 to 0.39976, saving model to best.model\n",
      "0s - loss: 0.5721 - acc: 0.7416 - val_loss: 0.3998 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.39976 to 0.39030, saving model to best.model\n",
      "0s - loss: 0.5365 - acc: 0.7640 - val_loss: 0.3903 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.39030 to 0.38138, saving model to best.model\n",
      "0s - loss: 0.5723 - acc: 0.7640 - val_loss: 0.3814 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.38138 to 0.37268, saving model to best.model\n",
      "0s - loss: 0.5906 - acc: 0.7865 - val_loss: 0.3727 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.37268 to 0.36286, saving model to best.model\n",
      "0s - loss: 0.5439 - acc: 0.7640 - val_loss: 0.3629 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.36286 to 0.35330, saving model to best.model\n",
      "0s - loss: 0.5775 - acc: 0.7528 - val_loss: 0.3533 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.35330 to 0.34389, saving model to best.model\n",
      "0s - loss: 0.5054 - acc: 0.8090 - val_loss: 0.3439 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.34389 to 0.33427, saving model to best.model\n",
      "0s - loss: 0.4770 - acc: 0.8315 - val_loss: 0.3343 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.33427 to 0.32565, saving model to best.model\n",
      "0s - loss: 0.5122 - acc: 0.7528 - val_loss: 0.3257 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.32565 to 0.31807, saving model to best.model\n",
      "0s - loss: 0.4775 - acc: 0.8090 - val_loss: 0.3181 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.31807 to 0.30995, saving model to best.model\n",
      "0s - loss: 0.5187 - acc: 0.8090 - val_loss: 0.3100 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.30995 to 0.30190, saving model to best.model\n",
      "0s - loss: 0.4683 - acc: 0.8652 - val_loss: 0.3019 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.30190 to 0.29410, saving model to best.model\n",
      "0s - loss: 0.4825 - acc: 0.8315 - val_loss: 0.2941 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.29410 to 0.28665, saving model to best.model\n",
      "0s - loss: 0.4811 - acc: 0.8202 - val_loss: 0.2866 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.28665 to 0.27937, saving model to best.model\n",
      "0s - loss: 0.4590 - acc: 0.8315 - val_loss: 0.2794 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.27937 to 0.27196, saving model to best.model\n",
      "0s - loss: 0.4927 - acc: 0.7865 - val_loss: 0.2720 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.27196 to 0.26563, saving model to best.model\n",
      "0s - loss: 0.4429 - acc: 0.8652 - val_loss: 0.2656 - val_acc: 1.0000\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.26563 to 0.25986, saving model to best.model\n",
      "0s - loss: 0.5475 - acc: 0.7528 - val_loss: 0.2599 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.25986 to 0.25451, saving model to best.model\n",
      "0s - loss: 0.4371 - acc: 0.8202 - val_loss: 0.2545 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.25451 to 0.25033, saving model to best.model\n",
      "0s - loss: 0.4926 - acc: 0.8090 - val_loss: 0.2503 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.25033 to 0.24590, saving model to best.model\n",
      "0s - loss: 0.4544 - acc: 0.8090 - val_loss: 0.2459 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.24590 to 0.24103, saving model to best.model\n",
      "0s - loss: 0.4559 - acc: 0.8427 - val_loss: 0.2410 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.24103 to 0.23612, saving model to best.model\n",
      "0s - loss: 0.5234 - acc: 0.7753 - val_loss: 0.2361 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.23612 to 0.23167, saving model to best.model\n",
      "0s - loss: 0.4483 - acc: 0.8315 - val_loss: 0.2317 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.23167 to 0.22708, saving model to best.model\n",
      "0s - loss: 0.3720 - acc: 0.8764 - val_loss: 0.2271 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.22708 to 0.22208, saving model to best.model\n",
      "0s - loss: 0.4271 - acc: 0.8427 - val_loss: 0.2221 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.22208 to 0.21586, saving model to best.model\n",
      "0s - loss: 0.3898 - acc: 0.8427 - val_loss: 0.2159 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.21586 to 0.21010, saving model to best.model\n",
      "0s - loss: 0.4671 - acc: 0.8652 - val_loss: 0.2101 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.21010 to 0.20375, saving model to best.model\n",
      "0s - loss: 0.3666 - acc: 0.8652 - val_loss: 0.2038 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.20375 to 0.19687, saving model to best.model\n",
      "0s - loss: 0.3958 - acc: 0.8764 - val_loss: 0.1969 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.19687 to 0.19026, saving model to best.model\n",
      "0s - loss: 0.4315 - acc: 0.8539 - val_loss: 0.1903 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.19026 to 0.18381, saving model to best.model\n",
      "0s - loss: 0.4058 - acc: 0.8202 - val_loss: 0.1838 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.18381 to 0.17732, saving model to best.model\n",
      "0s - loss: 0.3849 - acc: 0.8652 - val_loss: 0.1773 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.17732 to 0.17152, saving model to best.model\n",
      "0s - loss: 0.4014 - acc: 0.8315 - val_loss: 0.1715 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.17152 to 0.16628, saving model to best.model\n",
      "0s - loss: 0.4090 - acc: 0.8652 - val_loss: 0.1663 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.16628 to 0.16186, saving model to best.model\n",
      "0s - loss: 0.3345 - acc: 0.8989 - val_loss: 0.1619 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.16186 to 0.15782, saving model to best.model\n",
      "0s - loss: 0.4149 - acc: 0.8090 - val_loss: 0.1578 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.15782 to 0.15404, saving model to best.model\n",
      "0s - loss: 0.3916 - acc: 0.8539 - val_loss: 0.1540 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.15404 to 0.15048, saving model to best.model\n",
      "0s - loss: 0.3402 - acc: 0.8989 - val_loss: 0.1505 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.15048 to 0.14709, saving model to best.model\n",
      "0s - loss: 0.3766 - acc: 0.8539 - val_loss: 0.1471 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.14709 to 0.14339, saving model to best.model\n",
      "0s - loss: 0.3122 - acc: 0.8876 - val_loss: 0.1434 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.14339 to 0.13981, saving model to best.model\n",
      "0s - loss: 0.3591 - acc: 0.8652 - val_loss: 0.1398 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.13981 to 0.13603, saving model to best.model\n",
      "0s - loss: 0.3558 - acc: 0.8652 - val_loss: 0.1360 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.13603 to 0.13222, saving model to best.model\n",
      "0s - loss: 0.3249 - acc: 0.8652 - val_loss: 0.1322 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.13222 to 0.12847, saving model to best.model\n",
      "0s - loss: 0.3463 - acc: 0.8876 - val_loss: 0.1285 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.12847 to 0.12482, saving model to best.model\n",
      "0s - loss: 0.2740 - acc: 0.9213 - val_loss: 0.1248 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.12482 to 0.12175, saving model to best.model\n",
      "0s - loss: 0.3516 - acc: 0.8876 - val_loss: 0.1217 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.12175 to 0.11892, saving model to best.model\n",
      "0s - loss: 0.3124 - acc: 0.8876 - val_loss: 0.1189 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.11892 to 0.11588, saving model to best.model\n",
      "0s - loss: 0.3107 - acc: 0.8876 - val_loss: 0.1159 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.11588 to 0.11268, saving model to best.model\n",
      "0s - loss: 0.3582 - acc: 0.8764 - val_loss: 0.1127 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.11268 to 0.10947, saving model to best.model\n",
      "0s - loss: 0.3589 - acc: 0.8876 - val_loss: 0.1095 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.10947 to 0.10629, saving model to best.model\n",
      "0s - loss: 0.2967 - acc: 0.8876 - val_loss: 0.1063 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.10629 to 0.10332, saving model to best.model\n",
      "0s - loss: 0.2529 - acc: 0.9438 - val_loss: 0.1033 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.10332 to 0.10026, saving model to best.model\n",
      "0s - loss: 0.3307 - acc: 0.8764 - val_loss: 0.1003 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.10026 to 0.09754, saving model to best.model\n",
      "0s - loss: 0.3053 - acc: 0.8876 - val_loss: 0.0975 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.09754 to 0.09501, saving model to best.model\n",
      "0s - loss: 0.2760 - acc: 0.8876 - val_loss: 0.0950 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.09501 to 0.09221, saving model to best.model\n",
      "0s - loss: 0.3253 - acc: 0.8764 - val_loss: 0.0922 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.09221 to 0.08992, saving model to best.model\n",
      "0s - loss: 0.2440 - acc: 0.9438 - val_loss: 0.0899 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.08992 to 0.08713, saving model to best.model\n",
      "0s - loss: 0.2456 - acc: 0.9438 - val_loss: 0.0871 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.08713 to 0.08432, saving model to best.model\n",
      "0s - loss: 0.2942 - acc: 0.8876 - val_loss: 0.0843 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.08432 to 0.08153, saving model to best.model\n",
      "0s - loss: 0.2313 - acc: 0.9551 - val_loss: 0.0815 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.08153 to 0.07907, saving model to best.model\n",
      "0s - loss: 0.2909 - acc: 0.9101 - val_loss: 0.0791 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.07907 to 0.07657, saving model to best.model\n",
      "0s - loss: 0.2188 - acc: 0.9326 - val_loss: 0.0766 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.07657 to 0.07425, saving model to best.model\n",
      "0s - loss: 0.2219 - acc: 0.9101 - val_loss: 0.0742 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.07425 to 0.07212, saving model to best.model\n",
      "0s - loss: 0.2631 - acc: 0.9101 - val_loss: 0.0721 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.07212 to 0.06978, saving model to best.model\n",
      "0s - loss: 0.2269 - acc: 0.9326 - val_loss: 0.0698 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.06978 to 0.06740, saving model to best.model\n",
      "0s - loss: 0.2896 - acc: 0.8876 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.06740 to 0.06514, saving model to best.model\n",
      "0s - loss: 0.2630 - acc: 0.8989 - val_loss: 0.0651 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.06514 to 0.06306, saving model to best.model\n",
      "0s - loss: 0.2008 - acc: 0.9326 - val_loss: 0.0631 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.06306 to 0.06085, saving model to best.model\n",
      "0s - loss: 0.2028 - acc: 0.9438 - val_loss: 0.0609 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.06085 to 0.05875, saving model to best.model\n",
      "0s - loss: 0.2183 - acc: 0.9326 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.05875 to 0.05668, saving model to best.model\n",
      "0s - loss: 0.2304 - acc: 0.8876 - val_loss: 0.0567 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.05668 to 0.05483, saving model to best.model\n",
      "0s - loss: 0.2586 - acc: 0.8989 - val_loss: 0.0548 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.05483 to 0.05316, saving model to best.model\n",
      "0s - loss: 0.3331 - acc: 0.8315 - val_loss: 0.0532 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.05316 to 0.05154, saving model to best.model\n",
      "0s - loss: 0.2198 - acc: 0.8989 - val_loss: 0.0515 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.05154 to 0.05003, saving model to best.model\n",
      "0s - loss: 0.2314 - acc: 0.9326 - val_loss: 0.0500 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.05003 to 0.04855, saving model to best.model\n",
      "0s - loss: 0.1872 - acc: 0.9213 - val_loss: 0.0485 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.04855 to 0.04728, saving model to best.model\n",
      "0s - loss: 0.2002 - acc: 0.9213 - val_loss: 0.0473 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.04728 to 0.04605, saving model to best.model\n",
      "0s - loss: 0.1905 - acc: 0.9551 - val_loss: 0.0461 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.04605 to 0.04513, saving model to best.model\n",
      "0s - loss: 0.2503 - acc: 0.8764 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.04513 to 0.04427, saving model to best.model\n",
      "0s - loss: 0.2312 - acc: 0.8989 - val_loss: 0.0443 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.04427 to 0.04354, saving model to best.model\n",
      "0s - loss: 0.2025 - acc: 0.9663 - val_loss: 0.0435 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.04354 to 0.04271, saving model to best.model\n",
      "0s - loss: 0.1900 - acc: 0.9438 - val_loss: 0.0427 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.04271 to 0.04183, saving model to best.model\n",
      "0s - loss: 0.2451 - acc: 0.8989 - val_loss: 0.0418 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.04183 to 0.04104, saving model to best.model\n",
      "0s - loss: 0.1997 - acc: 0.9551 - val_loss: 0.0410 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.04104 to 0.04031, saving model to best.model\n",
      "0s - loss: 0.2741 - acc: 0.8989 - val_loss: 0.0403 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.04031 to 0.03959, saving model to best.model\n",
      "0s - loss: 0.1502 - acc: 0.9663 - val_loss: 0.0396 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.03959 to 0.03880, saving model to best.model\n",
      "0s - loss: 0.1989 - acc: 0.9551 - val_loss: 0.0388 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.03880 to 0.03806, saving model to best.model\n",
      "0s - loss: 0.1461 - acc: 0.9775 - val_loss: 0.0381 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.03806 to 0.03740, saving model to best.model\n",
      "0s - loss: 0.2030 - acc: 0.9438 - val_loss: 0.0374 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.03740 to 0.03635, saving model to best.model\n",
      "0s - loss: 0.1688 - acc: 0.9551 - val_loss: 0.0364 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.03635 to 0.03522, saving model to best.model\n",
      "0s - loss: 0.1794 - acc: 0.9326 - val_loss: 0.0352 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.03522 to 0.03391, saving model to best.model\n",
      "0s - loss: 0.1570 - acc: 0.9551 - val_loss: 0.0339 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.03391 to 0.03244, saving model to best.model\n",
      "0s - loss: 0.2450 - acc: 0.9101 - val_loss: 0.0324 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.03244 to 0.03106, saving model to best.model\n",
      "0s - loss: 0.1696 - acc: 0.9551 - val_loss: 0.0311 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.03106 to 0.02978, saving model to best.model\n",
      "0s - loss: 0.1897 - acc: 0.9438 - val_loss: 0.0298 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.02978 to 0.02869, saving model to best.model\n",
      "0s - loss: 0.1880 - acc: 0.9551 - val_loss: 0.0287 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.02869 to 0.02756, saving model to best.model\n",
      "0s - loss: 0.2054 - acc: 0.9213 - val_loss: 0.0276 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.02171, saving model to best.model\n",
      "0s - loss: 1.1830 - acc: 0.4270 - val_loss: 1.0217 - val_acc: 0.3913\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.3050 - acc: 0.3258 - val_loss: 1.0499 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.1304 - acc: 0.4045 - val_loss: 1.0702 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2143 - acc: 0.4045 - val_loss: 1.0820 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.1773 - acc: 0.3708 - val_loss: 1.0799 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1310 - acc: 0.4157 - val_loss: 1.0762 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2278 - acc: 0.4045 - val_loss: 1.0702 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2374 - acc: 0.3258 - val_loss: 1.0600 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2774 - acc: 0.3371 - val_loss: 1.0488 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1729 - acc: 0.3708 - val_loss: 1.0409 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.1899 - acc: 0.3483 - val_loss: 1.0345 - val_acc: 0.3913\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.1343 - acc: 0.4382 - val_loss: 1.0299 - val_acc: 0.3913\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1399 - acc: 0.4494 - val_loss: 1.0251 - val_acc: 0.3913\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.02171 to 1.02091, saving model to best.model\n",
      "0s - loss: 1.1749 - acc: 0.3258 - val_loss: 1.0209 - val_acc: 0.3913\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.02091 to 1.01677, saving model to best.model\n",
      "0s - loss: 1.2422 - acc: 0.4157 - val_loss: 1.0168 - val_acc: 0.3913\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.01677 to 1.01374, saving model to best.model\n",
      "0s - loss: 1.2276 - acc: 0.3483 - val_loss: 1.0137 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.01374 to 1.01114, saving model to best.model\n",
      "0s - loss: 1.1306 - acc: 0.3820 - val_loss: 1.0111 - val_acc: 0.3913\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.01114 to 1.00857, saving model to best.model\n",
      "0s - loss: 1.1628 - acc: 0.4045 - val_loss: 1.0086 - val_acc: 0.3913\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.00857 to 1.00569, saving model to best.model\n",
      "0s - loss: 1.1879 - acc: 0.4382 - val_loss: 1.0057 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.00569 to 1.00535, saving model to best.model\n",
      "0s - loss: 1.2001 - acc: 0.3146 - val_loss: 1.0054 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.00535 to 1.00513, saving model to best.model\n",
      "0s - loss: 1.1944 - acc: 0.3820 - val_loss: 1.0051 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1115 - acc: 0.4270 - val_loss: 1.0060 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1490 - acc: 0.3596 - val_loss: 1.0074 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1113 - acc: 0.4157 - val_loss: 1.0073 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1660 - acc: 0.3146 - val_loss: 1.0065 - val_acc: 0.3913\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.0772 - acc: 0.4382 - val_loss: 1.0052 - val_acc: 0.3913\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.00513 to 1.00400, saving model to best.model\n",
      "0s - loss: 1.0252 - acc: 0.4719 - val_loss: 1.0040 - val_acc: 0.3913\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.00400 to 1.00223, saving model to best.model\n",
      "0s - loss: 1.0978 - acc: 0.3933 - val_loss: 1.0022 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.00223 to 0.99923, saving model to best.model\n",
      "0s - loss: 1.0442 - acc: 0.4382 - val_loss: 0.9992 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.99923 to 0.99564, saving model to best.model\n",
      "0s - loss: 1.1956 - acc: 0.3034 - val_loss: 0.9956 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.99564 to 0.99281, saving model to best.model\n",
      "0s - loss: 1.1437 - acc: 0.3708 - val_loss: 0.9928 - val_acc: 0.3913\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.99281 to 0.99121, saving model to best.model\n",
      "0s - loss: 1.1255 - acc: 0.3933 - val_loss: 0.9912 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.99121 to 0.98949, saving model to best.model\n",
      "0s - loss: 1.0804 - acc: 0.4607 - val_loss: 0.9895 - val_acc: 0.3913\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.98949 to 0.98868, saving model to best.model\n",
      "0s - loss: 1.1155 - acc: 0.4494 - val_loss: 0.9887 - val_acc: 0.3913\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.98868 to 0.98715, saving model to best.model\n",
      "0s - loss: 1.1208 - acc: 0.3708 - val_loss: 0.9872 - val_acc: 0.3913\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.98715 to 0.98692, saving model to best.model\n",
      "0s - loss: 1.0910 - acc: 0.4719 - val_loss: 0.9869 - val_acc: 0.3913\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.98692 to 0.98595, saving model to best.model\n",
      "0s - loss: 1.1222 - acc: 0.4045 - val_loss: 0.9859 - val_acc: 0.3913\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.98595 to 0.98468, saving model to best.model\n",
      "0s - loss: 1.1097 - acc: 0.4382 - val_loss: 0.9847 - val_acc: 0.3913\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.98468 to 0.98130, saving model to best.model\n",
      "0s - loss: 1.0746 - acc: 0.4382 - val_loss: 0.9813 - val_acc: 0.3913\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.98130 to 0.97846, saving model to best.model\n",
      "0s - loss: 1.1626 - acc: 0.3371 - val_loss: 0.9785 - val_acc: 0.3913\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.97846 to 0.97496, saving model to best.model\n",
      "0s - loss: 1.0903 - acc: 0.4157 - val_loss: 0.9750 - val_acc: 0.3913\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.97496 to 0.96910, saving model to best.model\n",
      "0s - loss: 1.0883 - acc: 0.4607 - val_loss: 0.9691 - val_acc: 0.3913\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.96910 to 0.96443, saving model to best.model\n",
      "0s - loss: 1.0962 - acc: 0.4157 - val_loss: 0.9644 - val_acc: 0.3913\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.96443 to 0.95915, saving model to best.model\n",
      "0s - loss: 1.0388 - acc: 0.4157 - val_loss: 0.9591 - val_acc: 0.3913\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.95915 to 0.95434, saving model to best.model\n",
      "0s - loss: 1.0696 - acc: 0.4607 - val_loss: 0.9543 - val_acc: 0.3913\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.95434 to 0.94958, saving model to best.model\n",
      "0s - loss: 1.0865 - acc: 0.4494 - val_loss: 0.9496 - val_acc: 0.3913\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.94958 to 0.94461, saving model to best.model\n",
      "0s - loss: 1.1672 - acc: 0.3371 - val_loss: 0.9446 - val_acc: 0.3913\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.94461 to 0.93985, saving model to best.model\n",
      "0s - loss: 1.0048 - acc: 0.5169 - val_loss: 0.9399 - val_acc: 0.4783\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.93985 to 0.93519, saving model to best.model\n",
      "0s - loss: 1.0967 - acc: 0.4045 - val_loss: 0.9352 - val_acc: 0.4783\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.93519 to 0.93190, saving model to best.model\n",
      "0s - loss: 1.0316 - acc: 0.4719 - val_loss: 0.9319 - val_acc: 0.4783\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.93190 to 0.92831, saving model to best.model\n",
      "0s - loss: 1.1225 - acc: 0.3820 - val_loss: 0.9283 - val_acc: 0.4783\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.92831 to 0.92528, saving model to best.model\n",
      "0s - loss: 1.0421 - acc: 0.4494 - val_loss: 0.9253 - val_acc: 0.4783\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.92528 to 0.92101, saving model to best.model\n",
      "0s - loss: 1.0582 - acc: 0.4494 - val_loss: 0.9210 - val_acc: 0.4783\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.92101 to 0.91557, saving model to best.model\n",
      "0s - loss: 1.0536 - acc: 0.5393 - val_loss: 0.9156 - val_acc: 0.4783\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.91557 to 0.90990, saving model to best.model\n",
      "0s - loss: 1.0112 - acc: 0.4607 - val_loss: 0.9099 - val_acc: 0.4783\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.90990 to 0.90463, saving model to best.model\n",
      "0s - loss: 1.0019 - acc: 0.4719 - val_loss: 0.9046 - val_acc: 0.5652\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.90463 to 0.89766, saving model to best.model\n",
      "0s - loss: 1.0180 - acc: 0.4494 - val_loss: 0.8977 - val_acc: 0.6522\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.89766 to 0.89170, saving model to best.model\n",
      "0s - loss: 0.9180 - acc: 0.5730 - val_loss: 0.8917 - val_acc: 0.6957\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.89170 to 0.88534, saving model to best.model\n",
      "0s - loss: 0.9362 - acc: 0.5393 - val_loss: 0.8853 - val_acc: 0.6957\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.88534 to 0.87850, saving model to best.model\n",
      "0s - loss: 0.9144 - acc: 0.5618 - val_loss: 0.8785 - val_acc: 0.6957\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.87850 to 0.87181, saving model to best.model\n",
      "0s - loss: 0.9649 - acc: 0.5281 - val_loss: 0.8718 - val_acc: 0.6957\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.87181 to 0.86502, saving model to best.model\n",
      "0s - loss: 1.0343 - acc: 0.4157 - val_loss: 0.8650 - val_acc: 0.6957\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.86502 to 0.85722, saving model to best.model\n",
      "0s - loss: 0.9250 - acc: 0.6067 - val_loss: 0.8572 - val_acc: 0.6957\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.85722 to 0.84955, saving model to best.model\n",
      "0s - loss: 0.9949 - acc: 0.4831 - val_loss: 0.8495 - val_acc: 0.6957\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.84955 to 0.84141, saving model to best.model\n",
      "0s - loss: 0.9919 - acc: 0.4831 - val_loss: 0.8414 - val_acc: 0.6957\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.84141 to 0.83490, saving model to best.model\n",
      "0s - loss: 0.9005 - acc: 0.5730 - val_loss: 0.8349 - val_acc: 0.6957\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.83490 to 0.82864, saving model to best.model\n",
      "0s - loss: 0.9336 - acc: 0.5843 - val_loss: 0.8286 - val_acc: 0.6957\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.82864 to 0.82188, saving model to best.model\n",
      "0s - loss: 0.9448 - acc: 0.5506 - val_loss: 0.8219 - val_acc: 0.6957\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.82188 to 0.81395, saving model to best.model\n",
      "0s - loss: 0.9427 - acc: 0.5169 - val_loss: 0.8139 - val_acc: 0.7391\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.81395 to 0.80457, saving model to best.model\n",
      "0s - loss: 0.8577 - acc: 0.6180 - val_loss: 0.8046 - val_acc: 0.7391\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.80457 to 0.79464, saving model to best.model\n",
      "0s - loss: 0.9160 - acc: 0.4944 - val_loss: 0.7946 - val_acc: 0.7826\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.79464 to 0.78619, saving model to best.model\n",
      "0s - loss: 0.8696 - acc: 0.5393 - val_loss: 0.7862 - val_acc: 0.7826\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.78619 to 0.77718, saving model to best.model\n",
      "0s - loss: 0.8863 - acc: 0.5730 - val_loss: 0.7772 - val_acc: 0.7826\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.77718 to 0.76700, saving model to best.model\n",
      "0s - loss: 0.8426 - acc: 0.6742 - val_loss: 0.7670 - val_acc: 0.8696\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.76700 to 0.75720, saving model to best.model\n",
      "0s - loss: 0.8125 - acc: 0.6067 - val_loss: 0.7572 - val_acc: 0.8696\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.75720 to 0.74623, saving model to best.model\n",
      "0s - loss: 0.8286 - acc: 0.6404 - val_loss: 0.7462 - val_acc: 0.9130\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.74623 to 0.73415, saving model to best.model\n",
      "0s - loss: 0.8249 - acc: 0.6180 - val_loss: 0.7341 - val_acc: 0.8696\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.73415 to 0.72075, saving model to best.model\n",
      "0s - loss: 0.7809 - acc: 0.7303 - val_loss: 0.7207 - val_acc: 0.8696\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.72075 to 0.70699, saving model to best.model\n",
      "0s - loss: 0.8518 - acc: 0.6292 - val_loss: 0.7070 - val_acc: 0.8696\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70699 to 0.69253, saving model to best.model\n",
      "0s - loss: 0.7760 - acc: 0.6742 - val_loss: 0.6925 - val_acc: 0.8696\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69253 to 0.67920, saving model to best.model\n",
      "0s - loss: 0.8061 - acc: 0.7191 - val_loss: 0.6792 - val_acc: 0.8696\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.67920 to 0.66594, saving model to best.model\n",
      "0s - loss: 0.7445 - acc: 0.7640 - val_loss: 0.6659 - val_acc: 0.8696\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.66594 to 0.65332, saving model to best.model\n",
      "0s - loss: 0.7616 - acc: 0.6629 - val_loss: 0.6533 - val_acc: 0.8696\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.65332 to 0.64190, saving model to best.model\n",
      "0s - loss: 0.7302 - acc: 0.6966 - val_loss: 0.6419 - val_acc: 0.8696\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.64190 to 0.63076, saving model to best.model\n",
      "0s - loss: 0.6866 - acc: 0.7416 - val_loss: 0.6308 - val_acc: 0.9130\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.63076 to 0.62051, saving model to best.model\n",
      "0s - loss: 0.7417 - acc: 0.6742 - val_loss: 0.6205 - val_acc: 0.9130\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.62051 to 0.60925, saving model to best.model\n",
      "0s - loss: 0.7256 - acc: 0.7191 - val_loss: 0.6093 - val_acc: 0.9130\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.60925 to 0.59688, saving model to best.model\n",
      "0s - loss: 0.7229 - acc: 0.7416 - val_loss: 0.5969 - val_acc: 0.9130\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.59688 to 0.58511, saving model to best.model\n",
      "0s - loss: 0.6273 - acc: 0.7865 - val_loss: 0.5851 - val_acc: 0.9130\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.58511 to 0.57211, saving model to best.model\n",
      "0s - loss: 0.6853 - acc: 0.7528 - val_loss: 0.5721 - val_acc: 0.9130\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.57211 to 0.56005, saving model to best.model\n",
      "0s - loss: 0.6155 - acc: 0.7640 - val_loss: 0.5601 - val_acc: 0.9130\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.56005 to 0.54819, saving model to best.model\n",
      "0s - loss: 0.6431 - acc: 0.8315 - val_loss: 0.5482 - val_acc: 0.9130\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.54819 to 0.53646, saving model to best.model\n",
      "0s - loss: 0.6056 - acc: 0.7753 - val_loss: 0.5365 - val_acc: 0.9130\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.53646 to 0.52465, saving model to best.model\n",
      "0s - loss: 0.6464 - acc: 0.7303 - val_loss: 0.5246 - val_acc: 0.9130\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.52465 to 0.51153, saving model to best.model\n",
      "0s - loss: 0.6450 - acc: 0.7640 - val_loss: 0.5115 - val_acc: 0.9130\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.51153 to 0.49920, saving model to best.model\n",
      "0s - loss: 0.6387 - acc: 0.7191 - val_loss: 0.4992 - val_acc: 0.9130\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.49920 to 0.48683, saving model to best.model\n",
      "0s - loss: 0.5674 - acc: 0.8090 - val_loss: 0.4868 - val_acc: 0.9130\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.48683 to 0.47543, saving model to best.model\n",
      "0s - loss: 0.5063 - acc: 0.8539 - val_loss: 0.4754 - val_acc: 0.9130\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.47543 to 0.46448, saving model to best.model\n",
      "0s - loss: 0.5227 - acc: 0.8876 - val_loss: 0.4645 - val_acc: 0.9130\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.46448 to 0.45438, saving model to best.model\n",
      "0s - loss: 0.5362 - acc: 0.8315 - val_loss: 0.4544 - val_acc: 0.9130\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.45438 to 0.44386, saving model to best.model\n",
      "0s - loss: 0.5780 - acc: 0.7640 - val_loss: 0.4439 - val_acc: 0.9130\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.44386 to 0.43353, saving model to best.model\n",
      "0s - loss: 0.5238 - acc: 0.8539 - val_loss: 0.4335 - val_acc: 0.9130\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.43353 to 0.42421, saving model to best.model\n",
      "0s - loss: 0.4629 - acc: 0.8427 - val_loss: 0.4242 - val_acc: 0.9130\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.42421 to 0.41527, saving model to best.model\n",
      "0s - loss: 0.5311 - acc: 0.7753 - val_loss: 0.4153 - val_acc: 0.9130\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.41527 to 0.40638, saving model to best.model\n",
      "0s - loss: 0.4595 - acc: 0.8652 - val_loss: 0.4064 - val_acc: 0.9130\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.40638 to 0.39826, saving model to best.model\n",
      "0s - loss: 0.4770 - acc: 0.8202 - val_loss: 0.3983 - val_acc: 0.9130\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.39826 to 0.39044, saving model to best.model\n",
      "0s - loss: 0.4076 - acc: 0.9101 - val_loss: 0.3904 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.39044 to 0.38254, saving model to best.model\n",
      "0s - loss: 0.5208 - acc: 0.7865 - val_loss: 0.3825 - val_acc: 0.9130\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.38254 to 0.37488, saving model to best.model\n",
      "0s - loss: 0.4473 - acc: 0.9213 - val_loss: 0.3749 - val_acc: 0.9130\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.37488 to 0.36680, saving model to best.model\n",
      "0s - loss: 0.4206 - acc: 0.8764 - val_loss: 0.3668 - val_acc: 0.9130\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.36680 to 0.35948, saving model to best.model\n",
      "0s - loss: 0.4413 - acc: 0.8764 - val_loss: 0.3595 - val_acc: 0.9130\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.35948 to 0.35260, saving model to best.model\n",
      "0s - loss: 0.4740 - acc: 0.8315 - val_loss: 0.3526 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.35260 to 0.34614, saving model to best.model\n",
      "0s - loss: 0.4268 - acc: 0.8764 - val_loss: 0.3461 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.34614 to 0.34043, saving model to best.model\n",
      "0s - loss: 0.4026 - acc: 0.8652 - val_loss: 0.3404 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.34043 to 0.33401, saving model to best.model\n",
      "0s - loss: 0.3972 - acc: 0.8876 - val_loss: 0.3340 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.33401 to 0.32715, saving model to best.model\n",
      "0s - loss: 0.3915 - acc: 0.8989 - val_loss: 0.3271 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.32715 to 0.31982, saving model to best.model\n",
      "0s - loss: 0.3953 - acc: 0.8989 - val_loss: 0.3198 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.31982 to 0.31211, saving model to best.model\n",
      "0s - loss: 0.3932 - acc: 0.8876 - val_loss: 0.3121 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.31211 to 0.30394, saving model to best.model\n",
      "0s - loss: 0.3880 - acc: 0.8427 - val_loss: 0.3039 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.30394 to 0.29655, saving model to best.model\n",
      "0s - loss: 0.3835 - acc: 0.9101 - val_loss: 0.2966 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.29655 to 0.28933, saving model to best.model\n",
      "0s - loss: 0.3637 - acc: 0.8764 - val_loss: 0.2893 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.28933 to 0.28244, saving model to best.model\n",
      "0s - loss: 0.3409 - acc: 0.8876 - val_loss: 0.2824 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.28244 to 0.27608, saving model to best.model\n",
      "0s - loss: 0.3678 - acc: 0.8876 - val_loss: 0.2761 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.27608 to 0.27068, saving model to best.model\n",
      "0s - loss: 0.3392 - acc: 0.8876 - val_loss: 0.2707 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.27068 to 0.26561, saving model to best.model\n",
      "0s - loss: 0.3083 - acc: 0.9326 - val_loss: 0.2656 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.26561 to 0.26117, saving model to best.model\n",
      "0s - loss: 0.3180 - acc: 0.8764 - val_loss: 0.2612 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.26117 to 0.25735, saving model to best.model\n",
      "0s - loss: 0.3118 - acc: 0.8989 - val_loss: 0.2574 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.25735 to 0.25321, saving model to best.model\n",
      "0s - loss: 0.2685 - acc: 0.9326 - val_loss: 0.2532 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.25321 to 0.24957, saving model to best.model\n",
      "0s - loss: 0.2738 - acc: 0.9775 - val_loss: 0.2496 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.24957 to 0.24547, saving model to best.model\n",
      "0s - loss: 0.3486 - acc: 0.8876 - val_loss: 0.2455 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.24547 to 0.24105, saving model to best.model\n",
      "0s - loss: 0.2427 - acc: 0.9663 - val_loss: 0.2411 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.24105 to 0.23685, saving model to best.model\n",
      "0s - loss: 0.3061 - acc: 0.8764 - val_loss: 0.2368 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.23685 to 0.23281, saving model to best.model\n",
      "0s - loss: 0.3268 - acc: 0.8989 - val_loss: 0.2328 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.23281 to 0.22906, saving model to best.model\n",
      "0s - loss: 0.2514 - acc: 0.9213 - val_loss: 0.2291 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.22906 to 0.22440, saving model to best.model\n",
      "0s - loss: 0.2860 - acc: 0.9101 - val_loss: 0.2244 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.22440 to 0.21906, saving model to best.model\n",
      "0s - loss: 0.2509 - acc: 0.9438 - val_loss: 0.2191 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.21906 to 0.21358, saving model to best.model\n",
      "0s - loss: 0.2465 - acc: 0.9551 - val_loss: 0.2136 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.21358 to 0.20833, saving model to best.model\n",
      "0s - loss: 0.2252 - acc: 0.9326 - val_loss: 0.2083 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.20833 to 0.20348, saving model to best.model\n",
      "0s - loss: 0.2398 - acc: 0.9551 - val_loss: 0.2035 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.20348 to 0.19950, saving model to best.model\n",
      "0s - loss: 0.2036 - acc: 0.9551 - val_loss: 0.1995 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.19950 to 0.19593, saving model to best.model\n",
      "0s - loss: 0.2430 - acc: 0.9101 - val_loss: 0.1959 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.19593 to 0.19255, saving model to best.model\n",
      "0s - loss: 0.2399 - acc: 0.9438 - val_loss: 0.1926 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.19255 to 0.18948, saving model to best.model\n",
      "0s - loss: 0.2246 - acc: 0.9438 - val_loss: 0.1895 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.18948 to 0.18709, saving model to best.model\n",
      "0s - loss: 0.2600 - acc: 0.9101 - val_loss: 0.1871 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.18709 to 0.18423, saving model to best.model\n",
      "0s - loss: 0.2232 - acc: 0.9438 - val_loss: 0.1842 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.18423 to 0.18172, saving model to best.model\n",
      "0s - loss: 0.1685 - acc: 0.9438 - val_loss: 0.1817 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.18172 to 0.17917, saving model to best.model\n",
      "0s - loss: 0.2294 - acc: 0.8989 - val_loss: 0.1792 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.17917 to 0.17637, saving model to best.model\n",
      "0s - loss: 0.1781 - acc: 0.9438 - val_loss: 0.1764 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.17637 to 0.17342, saving model to best.model\n",
      "0s - loss: 0.2413 - acc: 0.9438 - val_loss: 0.1734 - val_acc: 0.8696\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.17342 to 0.17030, saving model to best.model\n",
      "0s - loss: 0.1774 - acc: 0.9663 - val_loss: 0.1703 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.17030 to 0.16674, saving model to best.model\n",
      "0s - loss: 0.1479 - acc: 0.9775 - val_loss: 0.1667 - val_acc: 0.9130\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.16674 to 0.16330, saving model to best.model\n",
      "0s - loss: 0.2512 - acc: 0.9213 - val_loss: 0.1633 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.16330 to 0.16013, saving model to best.model\n",
      "0s - loss: 0.1560 - acc: 0.9663 - val_loss: 0.1601 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.16013 to 0.15703, saving model to best.model\n",
      "0s - loss: 0.1704 - acc: 0.9551 - val_loss: 0.1570 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.15703 to 0.15406, saving model to best.model\n",
      "0s - loss: 0.1284 - acc: 1.0000 - val_loss: 0.1541 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.15406 to 0.15153, saving model to best.model\n",
      "0s - loss: 0.1426 - acc: 0.9663 - val_loss: 0.1515 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.15153 to 0.14943, saving model to best.model\n",
      "0s - loss: 0.1712 - acc: 0.9663 - val_loss: 0.1494 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.14943 to 0.14788, saving model to best.model\n",
      "0s - loss: 0.1529 - acc: 0.9663 - val_loss: 0.1479 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.14788 to 0.14658, saving model to best.model\n",
      "0s - loss: 0.1652 - acc: 0.9438 - val_loss: 0.1466 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.14658 to 0.14496, saving model to best.model\n",
      "0s - loss: 0.1375 - acc: 0.9663 - val_loss: 0.1450 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.14496 to 0.14389, saving model to best.model\n",
      "0s - loss: 0.1468 - acc: 0.9663 - val_loss: 0.1439 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.14389 to 0.14289, saving model to best.model\n",
      "0s - loss: 0.1332 - acc: 0.9663 - val_loss: 0.1429 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.14289 to 0.14208, saving model to best.model\n",
      "0s - loss: 0.1417 - acc: 0.9888 - val_loss: 0.1421 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.14208 to 0.14205, saving model to best.model\n",
      "0s - loss: 0.1383 - acc: 0.9663 - val_loss: 0.1420 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.14205 to 0.14183, saving model to best.model\n",
      "0s - loss: 0.0823 - acc: 1.0000 - val_loss: 0.1418 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.14183 to 0.14142, saving model to best.model\n",
      "0s - loss: 0.1464 - acc: 0.9438 - val_loss: 0.1414 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.14142 to 0.14017, saving model to best.model\n",
      "0s - loss: 0.1728 - acc: 0.9438 - val_loss: 0.1402 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.14017 to 0.13857, saving model to best.model\n",
      "0s - loss: 0.1395 - acc: 0.9663 - val_loss: 0.1386 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.13857 to 0.13699, saving model to best.model\n",
      "0s - loss: 0.1621 - acc: 0.9551 - val_loss: 0.1370 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.13699 to 0.13590, saving model to best.model\n",
      "0s - loss: 0.1040 - acc: 0.9663 - val_loss: 0.1359 - val_acc: 0.9565\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.13590 to 0.13481, saving model to best.model\n",
      "0s - loss: 0.1559 - acc: 0.9551 - val_loss: 0.1348 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.13481 to 0.13335, saving model to best.model\n",
      "0s - loss: 0.1662 - acc: 0.9438 - val_loss: 0.1334 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.13335 to 0.13170, saving model to best.model\n",
      "0s - loss: 0.1181 - acc: 0.9775 - val_loss: 0.1317 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.13170 to 0.13057, saving model to best.model\n",
      "0s - loss: 0.1038 - acc: 0.9775 - val_loss: 0.1306 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.13057 to 0.13008, saving model to best.model\n",
      "0s - loss: 0.1130 - acc: 0.9663 - val_loss: 0.1301 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.13008 to 0.12923, saving model to best.model\n",
      "0s - loss: 0.1106 - acc: 0.9663 - val_loss: 0.1292 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.12923 to 0.12783, saving model to best.model\n",
      "0s - loss: 0.1425 - acc: 0.9438 - val_loss: 0.1278 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.12783 to 0.12592, saving model to best.model\n",
      "0s - loss: 0.1302 - acc: 0.9663 - val_loss: 0.1259 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.12592 to 0.12424, saving model to best.model\n",
      "0s - loss: 0.0954 - acc: 0.9888 - val_loss: 0.1242 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.12424 to 0.12252, saving model to best.model\n",
      "0s - loss: 0.0839 - acc: 1.0000 - val_loss: 0.1225 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.12252 to 0.12083, saving model to best.model\n",
      "0s - loss: 0.1432 - acc: 0.9551 - val_loss: 0.1208 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.12083 to 0.11867, saving model to best.model\n",
      "0s - loss: 0.1202 - acc: 0.9663 - val_loss: 0.1187 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.11867 to 0.11671, saving model to best.model\n",
      "0s - loss: 0.1476 - acc: 0.9663 - val_loss: 0.1167 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.11671 to 0.11472, saving model to best.model\n",
      "0s - loss: 0.1264 - acc: 0.9663 - val_loss: 0.1147 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.11472 to 0.11384, saving model to best.model\n",
      "0s - loss: 0.1210 - acc: 0.9551 - val_loss: 0.1138 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.11384 to 0.11322, saving model to best.model\n",
      "0s - loss: 0.0945 - acc: 0.9775 - val_loss: 0.1132 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.11322 to 0.11183, saving model to best.model\n",
      "0s - loss: 0.1001 - acc: 0.9551 - val_loss: 0.1118 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.11183 to 0.11090, saving model to best.model\n",
      "0s - loss: 0.1242 - acc: 0.9663 - val_loss: 0.1109 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.11090 to 0.10990, saving model to best.model\n",
      "0s - loss: 0.1361 - acc: 0.9438 - val_loss: 0.1099 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.10990 to 0.10946, saving model to best.model\n",
      "0s - loss: 0.1422 - acc: 0.9213 - val_loss: 0.1095 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.10946 to 0.10899, saving model to best.model\n",
      "0s - loss: 0.0822 - acc: 0.9663 - val_loss: 0.1090 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.10899 to 0.10874, saving model to best.model\n",
      "0s - loss: 0.0952 - acc: 0.9775 - val_loss: 0.1087 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.10874 to 0.10780, saving model to best.model\n",
      "0s - loss: 0.0895 - acc: 0.9775 - val_loss: 0.1078 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.10780 to 0.10679, saving model to best.model\n",
      "0s - loss: 0.0778 - acc: 0.9775 - val_loss: 0.1068 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.10679 to 0.10566, saving model to best.model\n",
      "0s - loss: 0.0804 - acc: 0.9775 - val_loss: 0.1057 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.10566 to 0.10471, saving model to best.model\n",
      "0s - loss: 0.1017 - acc: 0.9775 - val_loss: 0.1047 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.10471 to 0.10351, saving model to best.model\n",
      "0s - loss: 0.0691 - acc: 0.9888 - val_loss: 0.1035 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.10351 to 0.10223, saving model to best.model\n",
      "0s - loss: 0.0814 - acc: 0.9888 - val_loss: 0.1022 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.10223 to 0.10108, saving model to best.model\n",
      "0s - loss: 0.0662 - acc: 0.9888 - val_loss: 0.1011 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.10108 to 0.10017, saving model to best.model\n",
      "0s - loss: 0.0925 - acc: 0.9775 - val_loss: 0.1002 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.12463, saving model to best.model\n",
      "0s - loss: 1.2997 - acc: 0.3146 - val_loss: 1.1246 - val_acc: 0.3478\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.12463 to 1.10317, saving model to best.model\n",
      "0s - loss: 1.3638 - acc: 0.3034 - val_loss: 1.1032 - val_acc: 0.3478\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.10317 to 1.09322, saving model to best.model\n",
      "0s - loss: 1.2686 - acc: 0.4270 - val_loss: 1.0932 - val_acc: 0.3478\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.09322 to 1.09188, saving model to best.model\n",
      "0s - loss: 1.1625 - acc: 0.3708 - val_loss: 1.0919 - val_acc: 0.3478\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.3172 - acc: 0.3483 - val_loss: 1.0938 - val_acc: 0.3478\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1391 - acc: 0.4045 - val_loss: 1.0940 - val_acc: 0.3478\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.09188 to 1.09100, saving model to best.model\n",
      "0s - loss: 1.2959 - acc: 0.3146 - val_loss: 1.0910 - val_acc: 0.3478\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 1.09100 to 1.08598, saving model to best.model\n",
      "0s - loss: 1.2590 - acc: 0.3820 - val_loss: 1.0860 - val_acc: 0.3478\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 1.08598 to 1.08190, saving model to best.model\n",
      "0s - loss: 1.3055 - acc: 0.2921 - val_loss: 1.0819 - val_acc: 0.6522\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 1.08190 to 1.08047, saving model to best.model\n",
      "0s - loss: 1.2748 - acc: 0.3146 - val_loss: 1.0805 - val_acc: 0.3478\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2814 - acc: 0.3146 - val_loss: 1.0816 - val_acc: 0.3478\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2430 - acc: 0.3034 - val_loss: 1.0840 - val_acc: 0.3478\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2299 - acc: 0.3483 - val_loss: 1.0874 - val_acc: 0.3478\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.0986 - acc: 0.3820 - val_loss: 1.0904 - val_acc: 0.3478\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2399 - acc: 0.2697 - val_loss: 1.0922 - val_acc: 0.3478\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2117 - acc: 0.3596 - val_loss: 1.0923 - val_acc: 0.3478\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1737 - acc: 0.3820 - val_loss: 1.0909 - val_acc: 0.3478\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1931 - acc: 0.3596 - val_loss: 1.0876 - val_acc: 0.3478\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1384 - acc: 0.4607 - val_loss: 1.0830 - val_acc: 0.3478\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.08047 to 1.07804, saving model to best.model\n",
      "0s - loss: 1.2448 - acc: 0.2809 - val_loss: 1.0780 - val_acc: 0.3478\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.07804 to 1.07253, saving model to best.model\n",
      "0s - loss: 1.1819 - acc: 0.3820 - val_loss: 1.0725 - val_acc: 0.3478\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.07253 to 1.06744, saving model to best.model\n",
      "0s - loss: 1.1731 - acc: 0.3371 - val_loss: 1.0674 - val_acc: 0.3478\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.06744 to 1.06276, saving model to best.model\n",
      "0s - loss: 1.1981 - acc: 0.3708 - val_loss: 1.0628 - val_acc: 0.3478\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.06276 to 1.05917, saving model to best.model\n",
      "0s - loss: 1.2241 - acc: 0.3146 - val_loss: 1.0592 - val_acc: 0.3478\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.05917 to 1.05551, saving model to best.model\n",
      "0s - loss: 1.0924 - acc: 0.3708 - val_loss: 1.0555 - val_acc: 0.3478\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.05551 to 1.05211, saving model to best.model\n",
      "0s - loss: 1.1587 - acc: 0.3596 - val_loss: 1.0521 - val_acc: 0.3478\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.05211 to 1.04958, saving model to best.model\n",
      "0s - loss: 1.1888 - acc: 0.3258 - val_loss: 1.0496 - val_acc: 0.3478\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.04958 to 1.04709, saving model to best.model\n",
      "0s - loss: 1.1857 - acc: 0.2584 - val_loss: 1.0471 - val_acc: 0.3913\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.04709 to 1.04491, saving model to best.model\n",
      "0s - loss: 1.1365 - acc: 0.3933 - val_loss: 1.0449 - val_acc: 0.3913\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.04491 to 1.04268, saving model to best.model\n",
      "0s - loss: 1.0850 - acc: 0.4494 - val_loss: 1.0427 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.04268 to 1.04084, saving model to best.model\n",
      "0s - loss: 1.2462 - acc: 0.2921 - val_loss: 1.0408 - val_acc: 0.3478\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.04084 to 1.03901, saving model to best.model\n",
      "0s - loss: 1.1730 - acc: 0.3596 - val_loss: 1.0390 - val_acc: 0.3478\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.03901 to 1.03715, saving model to best.model\n",
      "0s - loss: 1.1944 - acc: 0.3371 - val_loss: 1.0371 - val_acc: 0.3478\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.03715 to 1.03564, saving model to best.model\n",
      "0s - loss: 1.2109 - acc: 0.3371 - val_loss: 1.0356 - val_acc: 0.3478\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.03564 to 1.03414, saving model to best.model\n",
      "0s - loss: 1.1791 - acc: 0.3820 - val_loss: 1.0341 - val_acc: 0.3478\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.03414 to 1.03247, saving model to best.model\n",
      "0s - loss: 1.2217 - acc: 0.3258 - val_loss: 1.0325 - val_acc: 0.3478\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.03247 to 1.03105, saving model to best.model\n",
      "0s - loss: 1.1570 - acc: 0.4045 - val_loss: 1.0311 - val_acc: 0.3478\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.03105 to 1.02910, saving model to best.model\n",
      "0s - loss: 1.1862 - acc: 0.2809 - val_loss: 1.0291 - val_acc: 0.3478\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.02910 to 1.02656, saving model to best.model\n",
      "0s - loss: 1.1257 - acc: 0.3933 - val_loss: 1.0266 - val_acc: 0.3478\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.02656 to 1.02388, saving model to best.model\n",
      "0s - loss: 1.1591 - acc: 0.3708 - val_loss: 1.0239 - val_acc: 0.3478\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.02388 to 1.02030, saving model to best.model\n",
      "0s - loss: 1.0379 - acc: 0.4270 - val_loss: 1.0203 - val_acc: 0.3478\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.02030 to 1.01702, saving model to best.model\n",
      "0s - loss: 1.1281 - acc: 0.3483 - val_loss: 1.0170 - val_acc: 0.3478\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.01702 to 1.01320, saving model to best.model\n",
      "0s - loss: 0.9970 - acc: 0.5281 - val_loss: 1.0132 - val_acc: 0.3478\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.01320 to 1.00892, saving model to best.model\n",
      "0s - loss: 1.1151 - acc: 0.3820 - val_loss: 1.0089 - val_acc: 0.3478\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.00892 to 1.00421, saving model to best.model\n",
      "0s - loss: 1.1334 - acc: 0.3596 - val_loss: 1.0042 - val_acc: 0.3478\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.00421 to 0.99919, saving model to best.model\n",
      "0s - loss: 1.0762 - acc: 0.4607 - val_loss: 0.9992 - val_acc: 0.3478\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.99919 to 0.99390, saving model to best.model\n",
      "0s - loss: 1.1220 - acc: 0.3708 - val_loss: 0.9939 - val_acc: 0.3913\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.99390 to 0.98842, saving model to best.model\n",
      "0s - loss: 1.0821 - acc: 0.4494 - val_loss: 0.9884 - val_acc: 0.4348\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.98842 to 0.98312, saving model to best.model\n",
      "0s - loss: 1.0896 - acc: 0.4157 - val_loss: 0.9831 - val_acc: 0.5652\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.98312 to 0.97825, saving model to best.model\n",
      "0s - loss: 1.0526 - acc: 0.4045 - val_loss: 0.9782 - val_acc: 0.6087\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.97825 to 0.97320, saving model to best.model\n",
      "0s - loss: 1.1111 - acc: 0.4045 - val_loss: 0.9732 - val_acc: 0.6087\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.97320 to 0.96813, saving model to best.model\n",
      "0s - loss: 1.0199 - acc: 0.4045 - val_loss: 0.9681 - val_acc: 0.6087\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.96813 to 0.96293, saving model to best.model\n",
      "0s - loss: 1.0783 - acc: 0.4270 - val_loss: 0.9629 - val_acc: 0.6957\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.96293 to 0.95754, saving model to best.model\n",
      "0s - loss: 0.9694 - acc: 0.5730 - val_loss: 0.9575 - val_acc: 0.6957\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.95754 to 0.95188, saving model to best.model\n",
      "0s - loss: 1.0595 - acc: 0.4270 - val_loss: 0.9519 - val_acc: 0.6957\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.95188 to 0.94618, saving model to best.model\n",
      "0s - loss: 1.0440 - acc: 0.5618 - val_loss: 0.9462 - val_acc: 0.7826\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.94618 to 0.94033, saving model to best.model\n",
      "0s - loss: 1.0492 - acc: 0.4382 - val_loss: 0.9403 - val_acc: 0.7826\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.94033 to 0.93451, saving model to best.model\n",
      "0s - loss: 1.0635 - acc: 0.4045 - val_loss: 0.9345 - val_acc: 0.8261\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.93451 to 0.92851, saving model to best.model\n",
      "0s - loss: 1.0256 - acc: 0.4382 - val_loss: 0.9285 - val_acc: 0.8261\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.92851 to 0.92231, saving model to best.model\n",
      "0s - loss: 0.9588 - acc: 0.5056 - val_loss: 0.9223 - val_acc: 0.8261\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.92231 to 0.91570, saving model to best.model\n",
      "0s - loss: 1.0183 - acc: 0.4607 - val_loss: 0.9157 - val_acc: 0.8261\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.91570 to 0.90916, saving model to best.model\n",
      "0s - loss: 1.0502 - acc: 0.4045 - val_loss: 0.9092 - val_acc: 0.8261\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.90916 to 0.90200, saving model to best.model\n",
      "0s - loss: 0.9397 - acc: 0.5393 - val_loss: 0.9020 - val_acc: 0.8261\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.90200 to 0.89440, saving model to best.model\n",
      "0s - loss: 0.9763 - acc: 0.5056 - val_loss: 0.8944 - val_acc: 0.9130\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.89440 to 0.88649, saving model to best.model\n",
      "0s - loss: 1.0454 - acc: 0.4382 - val_loss: 0.8865 - val_acc: 0.9130\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.88649 to 0.87806, saving model to best.model\n",
      "0s - loss: 0.9300 - acc: 0.6292 - val_loss: 0.8781 - val_acc: 0.9565\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.87806 to 0.86967, saving model to best.model\n",
      "0s - loss: 0.8813 - acc: 0.6517 - val_loss: 0.8697 - val_acc: 0.9565\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.86967 to 0.86113, saving model to best.model\n",
      "0s - loss: 0.9318 - acc: 0.5955 - val_loss: 0.8611 - val_acc: 0.9565\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.86113 to 0.85206, saving model to best.model\n",
      "0s - loss: 0.8780 - acc: 0.6067 - val_loss: 0.8521 - val_acc: 0.9565\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.85206 to 0.84299, saving model to best.model\n",
      "0s - loss: 0.8816 - acc: 0.5506 - val_loss: 0.8430 - val_acc: 0.9565\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.84299 to 0.83374, saving model to best.model\n",
      "0s - loss: 0.9606 - acc: 0.4944 - val_loss: 0.8337 - val_acc: 0.9565\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.83374 to 0.82435, saving model to best.model\n",
      "0s - loss: 1.0390 - acc: 0.4157 - val_loss: 0.8243 - val_acc: 0.9565\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.82435 to 0.81461, saving model to best.model\n",
      "0s - loss: 0.9084 - acc: 0.5281 - val_loss: 0.8146 - val_acc: 0.9565\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.81461 to 0.80458, saving model to best.model\n",
      "0s - loss: 0.8815 - acc: 0.6180 - val_loss: 0.8046 - val_acc: 0.9565\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.80458 to 0.79417, saving model to best.model\n",
      "0s - loss: 0.9105 - acc: 0.5955 - val_loss: 0.7942 - val_acc: 0.9565\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.79417 to 0.78373, saving model to best.model\n",
      "0s - loss: 0.8738 - acc: 0.5730 - val_loss: 0.7837 - val_acc: 0.9565\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.78373 to 0.77301, saving model to best.model\n",
      "0s - loss: 0.9018 - acc: 0.5393 - val_loss: 0.7730 - val_acc: 0.9565\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.77301 to 0.76182, saving model to best.model\n",
      "0s - loss: 0.8874 - acc: 0.5843 - val_loss: 0.7618 - val_acc: 1.0000\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.76182 to 0.75043, saving model to best.model\n",
      "0s - loss: 0.8293 - acc: 0.6067 - val_loss: 0.7504 - val_acc: 1.0000\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.75043 to 0.73915, saving model to best.model\n",
      "0s - loss: 0.8308 - acc: 0.5955 - val_loss: 0.7391 - val_acc: 1.0000\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.73915 to 0.72789, saving model to best.model\n",
      "0s - loss: 0.7933 - acc: 0.6517 - val_loss: 0.7279 - val_acc: 1.0000\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.72789 to 0.71653, saving model to best.model\n",
      "0s - loss: 0.8826 - acc: 0.6067 - val_loss: 0.7165 - val_acc: 1.0000\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.71653 to 0.70506, saving model to best.model\n",
      "0s - loss: 0.8442 - acc: 0.5393 - val_loss: 0.7051 - val_acc: 0.9565\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.70506 to 0.69345, saving model to best.model\n",
      "0s - loss: 0.7651 - acc: 0.6854 - val_loss: 0.6934 - val_acc: 0.9565\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69345 to 0.68208, saving model to best.model\n",
      "0s - loss: 0.7818 - acc: 0.5955 - val_loss: 0.6821 - val_acc: 0.9130\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68208 to 0.67104, saving model to best.model\n",
      "0s - loss: 0.8086 - acc: 0.5955 - val_loss: 0.6710 - val_acc: 0.9130\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67104 to 0.65994, saving model to best.model\n",
      "0s - loss: 0.7452 - acc: 0.6966 - val_loss: 0.6599 - val_acc: 0.9130\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.65994 to 0.64909, saving model to best.model\n",
      "0s - loss: 0.7107 - acc: 0.7079 - val_loss: 0.6491 - val_acc: 0.9130\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.64909 to 0.63874, saving model to best.model\n",
      "0s - loss: 0.7720 - acc: 0.6404 - val_loss: 0.6387 - val_acc: 0.8696\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.63874 to 0.62898, saving model to best.model\n",
      "0s - loss: 0.7507 - acc: 0.7079 - val_loss: 0.6290 - val_acc: 0.8696\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.62898 to 0.61934, saving model to best.model\n",
      "0s - loss: 0.7414 - acc: 0.6629 - val_loss: 0.6193 - val_acc: 0.8696\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.61934 to 0.60960, saving model to best.model\n",
      "0s - loss: 0.7173 - acc: 0.6517 - val_loss: 0.6096 - val_acc: 0.8696\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.60960 to 0.60136, saving model to best.model\n",
      "0s - loss: 0.7448 - acc: 0.6742 - val_loss: 0.6014 - val_acc: 0.8696\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.60136 to 0.59244, saving model to best.model\n",
      "0s - loss: 0.7594 - acc: 0.6292 - val_loss: 0.5924 - val_acc: 0.8696\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.59244 to 0.58339, saving model to best.model\n",
      "0s - loss: 0.6335 - acc: 0.7416 - val_loss: 0.5834 - val_acc: 0.9130\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.58339 to 0.57446, saving model to best.model\n",
      "0s - loss: 0.6858 - acc: 0.7079 - val_loss: 0.5745 - val_acc: 0.9130\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.57446 to 0.56549, saving model to best.model\n",
      "0s - loss: 0.6977 - acc: 0.7303 - val_loss: 0.5655 - val_acc: 0.9130\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.56549 to 0.55683, saving model to best.model\n",
      "0s - loss: 0.5822 - acc: 0.8090 - val_loss: 0.5568 - val_acc: 0.9130\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.55683 to 0.54713, saving model to best.model\n",
      "0s - loss: 0.6784 - acc: 0.6966 - val_loss: 0.5471 - val_acc: 0.9565\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.54713 to 0.53706, saving model to best.model\n",
      "0s - loss: 0.6341 - acc: 0.7865 - val_loss: 0.5371 - val_acc: 0.9565\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.53706 to 0.52666, saving model to best.model\n",
      "0s - loss: 0.7012 - acc: 0.7303 - val_loss: 0.5267 - val_acc: 0.9565\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.52666 to 0.51577, saving model to best.model\n",
      "0s - loss: 0.6691 - acc: 0.7079 - val_loss: 0.5158 - val_acc: 0.9130\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.51577 to 0.50537, saving model to best.model\n",
      "0s - loss: 0.6771 - acc: 0.6292 - val_loss: 0.5054 - val_acc: 0.8696\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.50537 to 0.49553, saving model to best.model\n",
      "0s - loss: 0.6443 - acc: 0.7528 - val_loss: 0.4955 - val_acc: 0.9130\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.49553 to 0.48639, saving model to best.model\n",
      "0s - loss: 0.6417 - acc: 0.6966 - val_loss: 0.4864 - val_acc: 0.9130\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.48639 to 0.47750, saving model to best.model\n",
      "0s - loss: 0.5867 - acc: 0.8202 - val_loss: 0.4775 - val_acc: 0.9130\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.47750 to 0.46920, saving model to best.model\n",
      "0s - loss: 0.6275 - acc: 0.7191 - val_loss: 0.4692 - val_acc: 0.9130\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.46920 to 0.46128, saving model to best.model\n",
      "0s - loss: 0.6188 - acc: 0.7191 - val_loss: 0.4613 - val_acc: 0.9130\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.46128 to 0.45364, saving model to best.model\n",
      "0s - loss: 0.6440 - acc: 0.7416 - val_loss: 0.4536 - val_acc: 0.9130\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.45364 to 0.44638, saving model to best.model\n",
      "0s - loss: 0.5818 - acc: 0.7303 - val_loss: 0.4464 - val_acc: 0.9130\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.44638 to 0.43947, saving model to best.model\n",
      "0s - loss: 0.5636 - acc: 0.7865 - val_loss: 0.4395 - val_acc: 0.9130\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.43947 to 0.43288, saving model to best.model\n",
      "0s - loss: 0.5042 - acc: 0.7865 - val_loss: 0.4329 - val_acc: 0.9130\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.43288 to 0.42634, saving model to best.model\n",
      "0s - loss: 0.6234 - acc: 0.7079 - val_loss: 0.4263 - val_acc: 0.9130\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.42634 to 0.41937, saving model to best.model\n",
      "0s - loss: 0.4895 - acc: 0.8652 - val_loss: 0.4194 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.41937 to 0.41219, saving model to best.model\n",
      "0s - loss: 0.5510 - acc: 0.7416 - val_loss: 0.4122 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.41219 to 0.40512, saving model to best.model\n",
      "0s - loss: 0.5469 - acc: 0.8202 - val_loss: 0.4051 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.40512 to 0.39842, saving model to best.model\n",
      "0s - loss: 0.5831 - acc: 0.7416 - val_loss: 0.3984 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.39842 to 0.39190, saving model to best.model\n",
      "0s - loss: 0.5187 - acc: 0.8090 - val_loss: 0.3919 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.39190 to 0.38518, saving model to best.model\n",
      "0s - loss: 0.5700 - acc: 0.7303 - val_loss: 0.3852 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.38518 to 0.37889, saving model to best.model\n",
      "0s - loss: 0.5190 - acc: 0.8090 - val_loss: 0.3789 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.37889 to 0.37293, saving model to best.model\n",
      "0s - loss: 0.5489 - acc: 0.7753 - val_loss: 0.3729 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.37293 to 0.36752, saving model to best.model\n",
      "0s - loss: 0.5793 - acc: 0.7416 - val_loss: 0.3675 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.36752 to 0.36205, saving model to best.model\n",
      "0s - loss: 0.4488 - acc: 0.8876 - val_loss: 0.3621 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.36205 to 0.35699, saving model to best.model\n",
      "0s - loss: 0.5062 - acc: 0.7416 - val_loss: 0.3570 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.35699 to 0.35209, saving model to best.model\n",
      "0s - loss: 0.5124 - acc: 0.7303 - val_loss: 0.3521 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.35209 to 0.34667, saving model to best.model\n",
      "0s - loss: 0.5297 - acc: 0.7528 - val_loss: 0.3467 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.34667 to 0.34098, saving model to best.model\n",
      "0s - loss: 0.5483 - acc: 0.7303 - val_loss: 0.3410 - val_acc: 0.9130\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.34098 to 0.33537, saving model to best.model\n",
      "0s - loss: 0.4330 - acc: 0.8539 - val_loss: 0.3354 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.33537 to 0.32966, saving model to best.model\n",
      "0s - loss: 0.4082 - acc: 0.8989 - val_loss: 0.3297 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.32966 to 0.32406, saving model to best.model\n",
      "0s - loss: 0.4921 - acc: 0.8539 - val_loss: 0.3241 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.32406 to 0.31900, saving model to best.model\n",
      "0s - loss: 0.4471 - acc: 0.8090 - val_loss: 0.3190 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.31900 to 0.31433, saving model to best.model\n",
      "0s - loss: 0.4297 - acc: 0.8764 - val_loss: 0.3143 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.31433 to 0.30932, saving model to best.model\n",
      "0s - loss: 0.4733 - acc: 0.8090 - val_loss: 0.3093 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.30932 to 0.30408, saving model to best.model\n",
      "0s - loss: 0.4369 - acc: 0.8764 - val_loss: 0.3041 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.30408 to 0.29895, saving model to best.model\n",
      "0s - loss: 0.4731 - acc: 0.7865 - val_loss: 0.2989 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.29895 to 0.29461, saving model to best.model\n",
      "0s - loss: 0.4425 - acc: 0.8539 - val_loss: 0.2946 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.29461 to 0.29066, saving model to best.model\n",
      "0s - loss: 0.4323 - acc: 0.8202 - val_loss: 0.2907 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.29066 to 0.28676, saving model to best.model\n",
      "0s - loss: 0.3945 - acc: 0.8315 - val_loss: 0.2868 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.28676 to 0.28244, saving model to best.model\n",
      "0s - loss: 0.4320 - acc: 0.8539 - val_loss: 0.2824 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.28244 to 0.27819, saving model to best.model\n",
      "0s - loss: 0.4387 - acc: 0.8315 - val_loss: 0.2782 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.27819 to 0.27363, saving model to best.model\n",
      "0s - loss: 0.4544 - acc: 0.7978 - val_loss: 0.2736 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.27363 to 0.26887, saving model to best.model\n",
      "0s - loss: 0.3691 - acc: 0.9101 - val_loss: 0.2689 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.26887 to 0.26429, saving model to best.model\n",
      "0s - loss: 0.3439 - acc: 0.8989 - val_loss: 0.2643 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.26429 to 0.26000, saving model to best.model\n",
      "0s - loss: 0.3660 - acc: 0.8652 - val_loss: 0.2600 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.26000 to 0.25561, saving model to best.model\n",
      "0s - loss: 0.4121 - acc: 0.8876 - val_loss: 0.2556 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.25561 to 0.25137, saving model to best.model\n",
      "0s - loss: 0.4235 - acc: 0.7640 - val_loss: 0.2514 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.25137 to 0.24746, saving model to best.model\n",
      "0s - loss: 0.3796 - acc: 0.8764 - val_loss: 0.2475 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.24746 to 0.24304, saving model to best.model\n",
      "0s - loss: 0.3630 - acc: 0.8876 - val_loss: 0.2430 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.24304 to 0.23869, saving model to best.model\n",
      "0s - loss: 0.3217 - acc: 0.8876 - val_loss: 0.2387 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.23869 to 0.23417, saving model to best.model\n",
      "0s - loss: 0.3628 - acc: 0.8652 - val_loss: 0.2342 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.23417 to 0.22987, saving model to best.model\n",
      "0s - loss: 0.3479 - acc: 0.8539 - val_loss: 0.2299 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.22987 to 0.22539, saving model to best.model\n",
      "0s - loss: 0.3323 - acc: 0.8989 - val_loss: 0.2254 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.22539 to 0.22094, saving model to best.model\n",
      "0s - loss: 0.2948 - acc: 0.9213 - val_loss: 0.2209 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.22094 to 0.21673, saving model to best.model\n",
      "0s - loss: 0.3066 - acc: 0.9101 - val_loss: 0.2167 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.21673 to 0.21272, saving model to best.model\n",
      "0s - loss: 0.3095 - acc: 0.8652 - val_loss: 0.2127 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.21272 to 0.20879, saving model to best.model\n",
      "0s - loss: 0.3234 - acc: 0.9213 - val_loss: 0.2088 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.20879 to 0.20496, saving model to best.model\n",
      "0s - loss: 0.2676 - acc: 0.9326 - val_loss: 0.2050 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.20496 to 0.20145, saving model to best.model\n",
      "0s - loss: 0.3046 - acc: 0.8876 - val_loss: 0.2015 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.20145 to 0.19746, saving model to best.model\n",
      "0s - loss: 0.2503 - acc: 0.8989 - val_loss: 0.1975 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.19746 to 0.19321, saving model to best.model\n",
      "0s - loss: 0.3011 - acc: 0.9213 - val_loss: 0.1932 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.19321 to 0.18916, saving model to best.model\n",
      "0s - loss: 0.2833 - acc: 0.9101 - val_loss: 0.1892 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.18916 to 0.18462, saving model to best.model\n",
      "0s - loss: 0.2494 - acc: 0.9551 - val_loss: 0.1846 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.18462 to 0.18031, saving model to best.model\n",
      "0s - loss: 0.2762 - acc: 0.8764 - val_loss: 0.1803 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.18031 to 0.17608, saving model to best.model\n",
      "0s - loss: 0.2723 - acc: 0.9213 - val_loss: 0.1761 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.17608 to 0.17159, saving model to best.model\n",
      "0s - loss: 0.3014 - acc: 0.8989 - val_loss: 0.1716 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.17159 to 0.16750, saving model to best.model\n",
      "0s - loss: 0.2508 - acc: 0.9213 - val_loss: 0.1675 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.16750 to 0.16374, saving model to best.model\n",
      "0s - loss: 0.2913 - acc: 0.8876 - val_loss: 0.1637 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.16374 to 0.16026, saving model to best.model\n",
      "0s - loss: 0.2764 - acc: 0.9101 - val_loss: 0.1603 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.16026 to 0.15703, saving model to best.model\n",
      "0s - loss: 0.3135 - acc: 0.8652 - val_loss: 0.1570 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.15703 to 0.15394, saving model to best.model\n",
      "0s - loss: 0.2856 - acc: 0.8989 - val_loss: 0.1539 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.15394 to 0.15096, saving model to best.model\n",
      "0s - loss: 0.2493 - acc: 0.9326 - val_loss: 0.1510 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.15096 to 0.14788, saving model to best.model\n",
      "0s - loss: 0.2289 - acc: 0.9213 - val_loss: 0.1479 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.14788 to 0.14454, saving model to best.model\n",
      "0s - loss: 0.2472 - acc: 0.9326 - val_loss: 0.1445 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.14454 to 0.14116, saving model to best.model\n",
      "0s - loss: 0.2269 - acc: 0.9326 - val_loss: 0.1412 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.14116 to 0.13762, saving model to best.model\n",
      "0s - loss: 0.2778 - acc: 0.9326 - val_loss: 0.1376 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.13762 to 0.13394, saving model to best.model\n",
      "0s - loss: 0.3020 - acc: 0.9101 - val_loss: 0.1339 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.13394 to 0.13035, saving model to best.model\n",
      "0s - loss: 0.2458 - acc: 0.9213 - val_loss: 0.1303 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.13035 to 0.12700, saving model to best.model\n",
      "0s - loss: 0.2417 - acc: 0.9101 - val_loss: 0.1270 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.12700 to 0.12363, saving model to best.model\n",
      "0s - loss: 0.2280 - acc: 0.9438 - val_loss: 0.1236 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.12363 to 0.12043, saving model to best.model\n",
      "0s - loss: 0.2172 - acc: 0.9551 - val_loss: 0.1204 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.12043 to 0.11731, saving model to best.model\n",
      "0s - loss: 0.2171 - acc: 0.9101 - val_loss: 0.1173 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.11731 to 0.11421, saving model to best.model\n",
      "0s - loss: 0.2140 - acc: 0.9438 - val_loss: 0.1142 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.11421 to 0.11141, saving model to best.model\n",
      "0s - loss: 0.2607 - acc: 0.8876 - val_loss: 0.1114 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.11141 to 0.10863, saving model to best.model\n",
      "0s - loss: 0.2159 - acc: 0.9213 - val_loss: 0.1086 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.10863 to 0.10590, saving model to best.model\n",
      "0s - loss: 0.2485 - acc: 0.9438 - val_loss: 0.1059 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.10590 to 0.10315, saving model to best.model\n",
      "0s - loss: 0.2344 - acc: 0.9438 - val_loss: 0.1032 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.10315 to 0.10057, saving model to best.model\n",
      "0s - loss: 0.2897 - acc: 0.8876 - val_loss: 0.1006 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.10057 to 0.09809, saving model to best.model\n",
      "0s - loss: 0.1388 - acc: 0.9888 - val_loss: 0.0981 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.09809 to 0.09560, saving model to best.model\n",
      "0s - loss: 0.1965 - acc: 0.9326 - val_loss: 0.0956 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.09560 to 0.09325, saving model to best.model\n",
      "0s - loss: 0.2107 - acc: 0.9326 - val_loss: 0.0932 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.09325 to 0.09090, saving model to best.model\n",
      "0s - loss: 0.1681 - acc: 0.9663 - val_loss: 0.0909 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.09090 to 0.08871, saving model to best.model\n",
      "0s - loss: 0.1849 - acc: 0.9326 - val_loss: 0.0887 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.08871 to 0.08664, saving model to best.model\n",
      "0s - loss: 0.1373 - acc: 0.9775 - val_loss: 0.0866 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.08664 to 0.08468, saving model to best.model\n",
      "0s - loss: 0.1503 - acc: 0.9438 - val_loss: 0.0847 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.08468 to 0.08272, saving model to best.model\n",
      "0s - loss: 0.1622 - acc: 0.9663 - val_loss: 0.0827 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.08272 to 0.08069, saving model to best.model\n",
      "0s - loss: 0.1714 - acc: 0.9551 - val_loss: 0.0807 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.08069 to 0.07859, saving model to best.model\n",
      "0s - loss: 0.1500 - acc: 0.9551 - val_loss: 0.0786 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.07859 to 0.07667, saving model to best.model\n",
      "0s - loss: 0.1715 - acc: 0.9438 - val_loss: 0.0767 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.07667 to 0.07459, saving model to best.model\n",
      "0s - loss: 0.1921 - acc: 0.9438 - val_loss: 0.0746 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.07459 to 0.07239, saving model to best.model\n",
      "0s - loss: 0.1600 - acc: 0.9775 - val_loss: 0.0724 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.06484, saving model to best.model\n",
      "0s - loss: 1.4852 - acc: 0.3146 - val_loss: 1.0648 - val_acc: 0.4348\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 1.3249 - acc: 0.4157 - val_loss: 1.0692 - val_acc: 0.3478\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.3122 - acc: 0.3596 - val_loss: 1.0951 - val_acc: 0.3478\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1395 - acc: 0.3708 - val_loss: 1.1358 - val_acc: 0.2174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2768 - acc: 0.3596 - val_loss: 1.1809 - val_acc: 0.2174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.3089 - acc: 0.2697 - val_loss: 1.2234 - val_acc: 0.2174\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1947 - acc: 0.4157 - val_loss: 1.2548 - val_acc: 0.2174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2108 - acc: 0.3708 - val_loss: 1.2709 - val_acc: 0.2174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1987 - acc: 0.3483 - val_loss: 1.2714 - val_acc: 0.2174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.4247 - acc: 0.2809 - val_loss: 1.2641 - val_acc: 0.2174\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2440 - acc: 0.4157 - val_loss: 1.2451 - val_acc: 0.2174\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2817 - acc: 0.3034 - val_loss: 1.2234 - val_acc: 0.2174\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2025 - acc: 0.3820 - val_loss: 1.2003 - val_acc: 0.2174\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.1796 - acc: 0.3596 - val_loss: 1.1779 - val_acc: 0.3478\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.1707 - acc: 0.4157 - val_loss: 1.1579 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2337 - acc: 0.3034 - val_loss: 1.1401 - val_acc: 0.3913\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1692 - acc: 0.3820 - val_loss: 1.1234 - val_acc: 0.3478\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2219 - acc: 0.3596 - val_loss: 1.1088 - val_acc: 0.3478\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.2258 - acc: 0.3933 - val_loss: 1.0971 - val_acc: 0.3478\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2234 - acc: 0.3258 - val_loss: 1.0884 - val_acc: 0.3478\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1631 - acc: 0.3596 - val_loss: 1.0807 - val_acc: 0.3478\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.2288 - acc: 0.2697 - val_loss: 1.0754 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1300 - acc: 0.3820 - val_loss: 1.0710 - val_acc: 0.4783\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1036 - acc: 0.4607 - val_loss: 1.0696 - val_acc: 0.4783\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.2786 - acc: 0.2697 - val_loss: 1.0701 - val_acc: 0.5652\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1830 - acc: 0.3596 - val_loss: 1.0727 - val_acc: 0.5652\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1237 - acc: 0.3933 - val_loss: 1.0774 - val_acc: 0.5652\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.03796, saving model to best.model\n",
      "0s - loss: 1.1634 - acc: 0.4157 - val_loss: 1.0380 - val_acc: 0.5217\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.03796 to 1.02482, saving model to best.model\n",
      "0s - loss: 1.2272 - acc: 0.3258 - val_loss: 1.0248 - val_acc: 0.5217\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.2577 - acc: 0.4607 - val_loss: 1.0282 - val_acc: 0.5217\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1004 - acc: 0.4157 - val_loss: 1.0402 - val_acc: 0.5217\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.1938 - acc: 0.3596 - val_loss: 1.0509 - val_acc: 0.5217\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.3022 - acc: 0.2921 - val_loss: 1.0548 - val_acc: 0.5217\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1659 - acc: 0.3483 - val_loss: 1.0531 - val_acc: 0.5217\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1957 - acc: 0.3483 - val_loss: 1.0457 - val_acc: 0.5217\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2433 - acc: 0.3034 - val_loss: 1.0353 - val_acc: 0.5217\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1363 - acc: 0.4382 - val_loss: 1.0249 - val_acc: 0.5217\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.02482 to 1.01561, saving model to best.model\n",
      "0s - loss: 1.1392 - acc: 0.3708 - val_loss: 1.0156 - val_acc: 0.5217\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.01561 to 1.00806, saving model to best.model\n",
      "0s - loss: 1.0978 - acc: 0.4944 - val_loss: 1.0081 - val_acc: 0.5217\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.00806 to 1.00328, saving model to best.model\n",
      "0s - loss: 1.1093 - acc: 0.3933 - val_loss: 1.0033 - val_acc: 0.5217\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.00328 to 1.00123, saving model to best.model\n",
      "0s - loss: 1.1756 - acc: 0.3820 - val_loss: 1.0012 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.00123 to 1.00103, saving model to best.model\n",
      "0s - loss: 1.1592 - acc: 0.4382 - val_loss: 1.0010 - val_acc: 0.5217\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.0976 - acc: 0.4270 - val_loss: 1.0011 - val_acc: 0.5217\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.00103 to 1.00076, saving model to best.model\n",
      "0s - loss: 1.0942 - acc: 0.4607 - val_loss: 1.0008 - val_acc: 0.5217\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.00076 to 1.00050, saving model to best.model\n",
      "0s - loss: 1.0775 - acc: 0.4494 - val_loss: 1.0005 - val_acc: 0.5217\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.00050 to 1.00030, saving model to best.model\n",
      "0s - loss: 1.1983 - acc: 0.4270 - val_loss: 1.0003 - val_acc: 0.5217\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.00030 to 0.99955, saving model to best.model\n",
      "0s - loss: 1.1397 - acc: 0.4270 - val_loss: 0.9995 - val_acc: 0.5217\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.99955 to 0.99862, saving model to best.model\n",
      "0s - loss: 1.1293 - acc: 0.4831 - val_loss: 0.9986 - val_acc: 0.5217\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.99862 to 0.99765, saving model to best.model\n",
      "0s - loss: 1.1029 - acc: 0.4157 - val_loss: 0.9976 - val_acc: 0.5217\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.99765 to 0.99709, saving model to best.model\n",
      "0s - loss: 1.2171 - acc: 0.3933 - val_loss: 0.9971 - val_acc: 0.5217\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.99709 to 0.99648, saving model to best.model\n",
      "0s - loss: 1.1520 - acc: 0.4157 - val_loss: 0.9965 - val_acc: 0.5217\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.99648 to 0.99573, saving model to best.model\n",
      "0s - loss: 1.1367 - acc: 0.4045 - val_loss: 0.9957 - val_acc: 0.5217\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.99573 to 0.99439, saving model to best.model\n",
      "0s - loss: 1.1278 - acc: 0.3933 - val_loss: 0.9944 - val_acc: 0.5217\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.99439 to 0.99273, saving model to best.model\n",
      "0s - loss: 1.1385 - acc: 0.3596 - val_loss: 0.9927 - val_acc: 0.5217\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.99273 to 0.99112, saving model to best.model\n",
      "0s - loss: 1.0483 - acc: 0.4382 - val_loss: 0.9911 - val_acc: 0.5217\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.99112 to 0.98876, saving model to best.model\n",
      "0s - loss: 1.0867 - acc: 0.3820 - val_loss: 0.9888 - val_acc: 0.5217\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.98876 to 0.98654, saving model to best.model\n",
      "0s - loss: 1.1094 - acc: 0.4157 - val_loss: 0.9865 - val_acc: 0.5217\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.98654 to 0.98365, saving model to best.model\n",
      "0s - loss: 1.0932 - acc: 0.3820 - val_loss: 0.9836 - val_acc: 0.5217\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.98365 to 0.98066, saving model to best.model\n",
      "0s - loss: 1.1015 - acc: 0.4607 - val_loss: 0.9807 - val_acc: 0.5217\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.98066 to 0.97693, saving model to best.model\n",
      "0s - loss: 1.0345 - acc: 0.4494 - val_loss: 0.9769 - val_acc: 0.5217\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.97693 to 0.97318, saving model to best.model\n",
      "0s - loss: 1.0970 - acc: 0.4831 - val_loss: 0.9732 - val_acc: 0.5217\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.97318 to 0.96975, saving model to best.model\n",
      "0s - loss: 1.1259 - acc: 0.3820 - val_loss: 0.9697 - val_acc: 0.5217\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.96975 to 0.96630, saving model to best.model\n",
      "0s - loss: 1.0883 - acc: 0.4270 - val_loss: 0.9663 - val_acc: 0.5217\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.96630 to 0.96325, saving model to best.model\n",
      "0s - loss: 1.0655 - acc: 0.4719 - val_loss: 0.9632 - val_acc: 0.5217\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.96325 to 0.96041, saving model to best.model\n",
      "0s - loss: 1.0290 - acc: 0.4382 - val_loss: 0.9604 - val_acc: 0.5217\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.96041 to 0.95760, saving model to best.model\n",
      "0s - loss: 1.0133 - acc: 0.4719 - val_loss: 0.9576 - val_acc: 0.5217\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.95760 to 0.95501, saving model to best.model\n",
      "0s - loss: 1.0601 - acc: 0.4270 - val_loss: 0.9550 - val_acc: 0.5217\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.95501 to 0.95232, saving model to best.model\n",
      "0s - loss: 1.0191 - acc: 0.4719 - val_loss: 0.9523 - val_acc: 0.5217\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.95232 to 0.94974, saving model to best.model\n",
      "0s - loss: 1.0961 - acc: 0.4831 - val_loss: 0.9497 - val_acc: 0.5217\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.94974 to 0.94703, saving model to best.model\n",
      "0s - loss: 1.0302 - acc: 0.5169 - val_loss: 0.9470 - val_acc: 0.5217\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.94703 to 0.94451, saving model to best.model\n",
      "0s - loss: 1.1879 - acc: 0.3820 - val_loss: 0.9445 - val_acc: 0.5217\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.94451 to 0.94243, saving model to best.model\n",
      "0s - loss: 1.0817 - acc: 0.4607 - val_loss: 0.9424 - val_acc: 0.5217\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.94243 to 0.94003, saving model to best.model\n",
      "0s - loss: 0.9965 - acc: 0.5730 - val_loss: 0.9400 - val_acc: 0.5217\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.94003 to 0.93750, saving model to best.model\n",
      "0s - loss: 1.0536 - acc: 0.4607 - val_loss: 0.9375 - val_acc: 0.5217\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.93750 to 0.93471, saving model to best.model\n",
      "0s - loss: 1.0668 - acc: 0.4719 - val_loss: 0.9347 - val_acc: 0.5217\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.93471 to 0.93161, saving model to best.model\n",
      "0s - loss: 1.0488 - acc: 0.4382 - val_loss: 0.9316 - val_acc: 0.5217\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.93161 to 0.92820, saving model to best.model\n",
      "0s - loss: 1.0199 - acc: 0.5281 - val_loss: 0.9282 - val_acc: 0.5217\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.92820 to 0.92430, saving model to best.model\n",
      "0s - loss: 1.0363 - acc: 0.5169 - val_loss: 0.9243 - val_acc: 0.5217\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.92430 to 0.92025, saving model to best.model\n",
      "0s - loss: 1.0428 - acc: 0.4607 - val_loss: 0.9203 - val_acc: 0.5217\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.92025 to 0.91599, saving model to best.model\n",
      "0s - loss: 1.0357 - acc: 0.4831 - val_loss: 0.9160 - val_acc: 0.5217\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.91599 to 0.91157, saving model to best.model\n",
      "0s - loss: 1.0483 - acc: 0.4607 - val_loss: 0.9116 - val_acc: 0.5217\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.91157 to 0.90705, saving model to best.model\n",
      "0s - loss: 1.0300 - acc: 0.5169 - val_loss: 0.9070 - val_acc: 0.5217\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.90705 to 0.90248, saving model to best.model\n",
      "0s - loss: 1.0130 - acc: 0.4270 - val_loss: 0.9025 - val_acc: 0.5217\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.90248 to 0.89815, saving model to best.model\n",
      "0s - loss: 1.0290 - acc: 0.4719 - val_loss: 0.8981 - val_acc: 0.5217\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.89815 to 0.89348, saving model to best.model\n",
      "0s - loss: 0.9988 - acc: 0.4831 - val_loss: 0.8935 - val_acc: 0.5217\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.89348 to 0.88849, saving model to best.model\n",
      "0s - loss: 0.9639 - acc: 0.4831 - val_loss: 0.8885 - val_acc: 0.5217\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.88849 to 0.88369, saving model to best.model\n",
      "0s - loss: 1.0282 - acc: 0.4831 - val_loss: 0.8837 - val_acc: 0.5217\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.88369 to 0.87861, saving model to best.model\n",
      "0s - loss: 0.9658 - acc: 0.5169 - val_loss: 0.8786 - val_acc: 0.5217\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.87861 to 0.87303, saving model to best.model\n",
      "0s - loss: 0.9520 - acc: 0.5281 - val_loss: 0.8730 - val_acc: 0.5217\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.87303 to 0.86709, saving model to best.model\n",
      "0s - loss: 0.9539 - acc: 0.5393 - val_loss: 0.8671 - val_acc: 0.5217\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.86709 to 0.86098, saving model to best.model\n",
      "0s - loss: 0.9949 - acc: 0.5618 - val_loss: 0.8610 - val_acc: 0.5217\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.86098 to 0.85461, saving model to best.model\n",
      "0s - loss: 0.9303 - acc: 0.6067 - val_loss: 0.8546 - val_acc: 0.5217\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.85461 to 0.84787, saving model to best.model\n",
      "0s - loss: 0.9519 - acc: 0.5169 - val_loss: 0.8479 - val_acc: 0.5217\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.84787 to 0.84099, saving model to best.model\n",
      "0s - loss: 0.9323 - acc: 0.5843 - val_loss: 0.8410 - val_acc: 0.5217\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.84099 to 0.83421, saving model to best.model\n",
      "0s - loss: 0.9128 - acc: 0.5843 - val_loss: 0.8342 - val_acc: 0.5217\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.83421 to 0.82747, saving model to best.model\n",
      "0s - loss: 0.9217 - acc: 0.5281 - val_loss: 0.8275 - val_acc: 0.5217\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.82747 to 0.82033, saving model to best.model\n",
      "0s - loss: 0.9060 - acc: 0.5955 - val_loss: 0.8203 - val_acc: 0.5217\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.82033 to 0.81294, saving model to best.model\n",
      "0s - loss: 0.9370 - acc: 0.5393 - val_loss: 0.8129 - val_acc: 0.5652\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.81294 to 0.80510, saving model to best.model\n",
      "0s - loss: 0.9611 - acc: 0.4607 - val_loss: 0.8051 - val_acc: 0.6087\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.80510 to 0.79719, saving model to best.model\n",
      "0s - loss: 0.8818 - acc: 0.5393 - val_loss: 0.7972 - val_acc: 0.6087\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.79719 to 0.78920, saving model to best.model\n",
      "0s - loss: 0.8806 - acc: 0.5730 - val_loss: 0.7892 - val_acc: 0.6522\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.78920 to 0.78091, saving model to best.model\n",
      "0s - loss: 0.9319 - acc: 0.5730 - val_loss: 0.7809 - val_acc: 0.6522\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.78091 to 0.77232, saving model to best.model\n",
      "0s - loss: 0.8519 - acc: 0.6742 - val_loss: 0.7723 - val_acc: 0.6522\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.77232 to 0.76344, saving model to best.model\n",
      "0s - loss: 0.8477 - acc: 0.6067 - val_loss: 0.7634 - val_acc: 0.6522\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.76344 to 0.75424, saving model to best.model\n",
      "0s - loss: 0.8346 - acc: 0.6292 - val_loss: 0.7542 - val_acc: 0.6522\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.75424 to 0.74492, saving model to best.model\n",
      "0s - loss: 0.9106 - acc: 0.5169 - val_loss: 0.7449 - val_acc: 0.6522\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.74492 to 0.73539, saving model to best.model\n",
      "0s - loss: 0.8733 - acc: 0.5169 - val_loss: 0.7354 - val_acc: 0.6522\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.73539 to 0.72587, saving model to best.model\n",
      "0s - loss: 0.8880 - acc: 0.5618 - val_loss: 0.7259 - val_acc: 0.6957\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.72587 to 0.71631, saving model to best.model\n",
      "0s - loss: 0.8506 - acc: 0.6180 - val_loss: 0.7163 - val_acc: 0.7391\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.71631 to 0.70654, saving model to best.model\n",
      "0s - loss: 0.8243 - acc: 0.6180 - val_loss: 0.7065 - val_acc: 0.8261\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.70654 to 0.69661, saving model to best.model\n",
      "0s - loss: 0.8357 - acc: 0.6067 - val_loss: 0.6966 - val_acc: 0.8261\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69661 to 0.68666, saving model to best.model\n",
      "0s - loss: 0.8084 - acc: 0.6854 - val_loss: 0.6867 - val_acc: 0.8261\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68666 to 0.67654, saving model to best.model\n",
      "0s - loss: 0.7943 - acc: 0.6517 - val_loss: 0.6765 - val_acc: 0.8261\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67654 to 0.66641, saving model to best.model\n",
      "0s - loss: 0.8023 - acc: 0.6404 - val_loss: 0.6664 - val_acc: 0.8261\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.66641 to 0.65629, saving model to best.model\n",
      "0s - loss: 0.8162 - acc: 0.6067 - val_loss: 0.6563 - val_acc: 0.8261\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.65629 to 0.64619, saving model to best.model\n",
      "0s - loss: 0.7674 - acc: 0.6517 - val_loss: 0.6462 - val_acc: 0.8261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.64619 to 0.63617, saving model to best.model\n",
      "0s - loss: 0.7944 - acc: 0.5955 - val_loss: 0.6362 - val_acc: 0.8696\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.63617 to 0.62556, saving model to best.model\n",
      "0s - loss: 0.7203 - acc: 0.7079 - val_loss: 0.6256 - val_acc: 0.8696\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.62556 to 0.61476, saving model to best.model\n",
      "0s - loss: 0.7356 - acc: 0.7191 - val_loss: 0.6148 - val_acc: 0.8696\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.61476 to 0.60358, saving model to best.model\n",
      "0s - loss: 0.7538 - acc: 0.6629 - val_loss: 0.6036 - val_acc: 0.8696\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.60358 to 0.59196, saving model to best.model\n",
      "0s - loss: 0.7271 - acc: 0.6854 - val_loss: 0.5920 - val_acc: 0.8696\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.59196 to 0.58037, saving model to best.model\n",
      "0s - loss: 0.7969 - acc: 0.6742 - val_loss: 0.5804 - val_acc: 0.8696\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.58037 to 0.56881, saving model to best.model\n",
      "0s - loss: 0.7184 - acc: 0.7079 - val_loss: 0.5688 - val_acc: 0.8696\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.56881 to 0.55751, saving model to best.model\n",
      "0s - loss: 0.7046 - acc: 0.7528 - val_loss: 0.5575 - val_acc: 0.8696\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.55751 to 0.54614, saving model to best.model\n",
      "0s - loss: 0.7028 - acc: 0.6966 - val_loss: 0.5461 - val_acc: 0.8696\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.54614 to 0.53502, saving model to best.model\n",
      "0s - loss: 0.6479 - acc: 0.7640 - val_loss: 0.5350 - val_acc: 0.8696\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.53502 to 0.52417, saving model to best.model\n",
      "0s - loss: 0.6709 - acc: 0.7079 - val_loss: 0.5242 - val_acc: 0.8696\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.52417 to 0.51362, saving model to best.model\n",
      "0s - loss: 0.5905 - acc: 0.7865 - val_loss: 0.5136 - val_acc: 0.8696\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.51362 to 0.50313, saving model to best.model\n",
      "0s - loss: 0.6474 - acc: 0.7079 - val_loss: 0.5031 - val_acc: 0.8696\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.50313 to 0.49262, saving model to best.model\n",
      "0s - loss: 0.6296 - acc: 0.7416 - val_loss: 0.4926 - val_acc: 0.8696\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.49262 to 0.48196, saving model to best.model\n",
      "0s - loss: 0.6789 - acc: 0.6292 - val_loss: 0.4820 - val_acc: 0.8696\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.48196 to 0.47104, saving model to best.model\n",
      "0s - loss: 0.6526 - acc: 0.7978 - val_loss: 0.4710 - val_acc: 0.8696\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.47104 to 0.46002, saving model to best.model\n",
      "0s - loss: 0.6108 - acc: 0.7978 - val_loss: 0.4600 - val_acc: 0.8696\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.46002 to 0.44905, saving model to best.model\n",
      "0s - loss: 0.6308 - acc: 0.7978 - val_loss: 0.4490 - val_acc: 0.8696\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.44905 to 0.43837, saving model to best.model\n",
      "0s - loss: 0.6003 - acc: 0.7416 - val_loss: 0.4384 - val_acc: 0.8696\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.43837 to 0.42795, saving model to best.model\n",
      "0s - loss: 0.5707 - acc: 0.7865 - val_loss: 0.4280 - val_acc: 0.9565\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.42795 to 0.41778, saving model to best.model\n",
      "0s - loss: 0.5293 - acc: 0.8652 - val_loss: 0.4178 - val_acc: 0.9565\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.41778 to 0.40784, saving model to best.model\n",
      "0s - loss: 0.5405 - acc: 0.8090 - val_loss: 0.4078 - val_acc: 0.9565\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.40784 to 0.39803, saving model to best.model\n",
      "0s - loss: 0.5371 - acc: 0.8090 - val_loss: 0.3980 - val_acc: 0.9565\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.39803 to 0.38843, saving model to best.model\n",
      "0s - loss: 0.5441 - acc: 0.8090 - val_loss: 0.3884 - val_acc: 0.9565\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.38843 to 0.37898, saving model to best.model\n",
      "0s - loss: 0.4812 - acc: 0.8652 - val_loss: 0.3790 - val_acc: 0.9565\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.37898 to 0.36972, saving model to best.model\n",
      "0s - loss: 0.5837 - acc: 0.7865 - val_loss: 0.3697 - val_acc: 0.9565\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.36972 to 0.36045, saving model to best.model\n",
      "0s - loss: 0.4856 - acc: 0.8202 - val_loss: 0.3605 - val_acc: 0.9565\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.36045 to 0.35135, saving model to best.model\n",
      "0s - loss: 0.4838 - acc: 0.8090 - val_loss: 0.3514 - val_acc: 0.9565\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.35135 to 0.34233, saving model to best.model\n",
      "0s - loss: 0.4744 - acc: 0.8315 - val_loss: 0.3423 - val_acc: 0.9565\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.34233 to 0.33347, saving model to best.model\n",
      "0s - loss: 0.4491 - acc: 0.8539 - val_loss: 0.3335 - val_acc: 0.9565\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.33347 to 0.32482, saving model to best.model\n",
      "0s - loss: 0.4718 - acc: 0.8764 - val_loss: 0.3248 - val_acc: 0.9565\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.32482 to 0.31615, saving model to best.model\n",
      "0s - loss: 0.3932 - acc: 0.8876 - val_loss: 0.3161 - val_acc: 0.9565\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.31615 to 0.30782, saving model to best.model\n",
      "0s - loss: 0.3677 - acc: 0.9326 - val_loss: 0.3078 - val_acc: 0.9565\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.30782 to 0.29968, saving model to best.model\n",
      "0s - loss: 0.4985 - acc: 0.8539 - val_loss: 0.2997 - val_acc: 0.9565\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.29968 to 0.29169, saving model to best.model\n",
      "0s - loss: 0.4683 - acc: 0.8427 - val_loss: 0.2917 - val_acc: 0.9565\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.29169 to 0.28403, saving model to best.model\n",
      "0s - loss: 0.4680 - acc: 0.8090 - val_loss: 0.2840 - val_acc: 0.9565\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.28403 to 0.27651, saving model to best.model\n",
      "0s - loss: 0.3890 - acc: 0.8764 - val_loss: 0.2765 - val_acc: 0.9565\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.27651 to 0.26910, saving model to best.model\n",
      "0s - loss: 0.4032 - acc: 0.8427 - val_loss: 0.2691 - val_acc: 0.9565\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.26910 to 0.26204, saving model to best.model\n",
      "0s - loss: 0.4572 - acc: 0.8427 - val_loss: 0.2620 - val_acc: 0.9565\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.26204 to 0.25497, saving model to best.model\n",
      "0s - loss: 0.4384 - acc: 0.8427 - val_loss: 0.2550 - val_acc: 0.9565\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.25497 to 0.24815, saving model to best.model\n",
      "0s - loss: 0.4241 - acc: 0.7978 - val_loss: 0.2482 - val_acc: 0.9565\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.24815 to 0.24183, saving model to best.model\n",
      "0s - loss: 0.3751 - acc: 0.8764 - val_loss: 0.2418 - val_acc: 0.9565\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.24183 to 0.23578, saving model to best.model\n",
      "0s - loss: 0.4121 - acc: 0.8652 - val_loss: 0.2358 - val_acc: 0.9565\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.23578 to 0.22980, saving model to best.model\n",
      "0s - loss: 0.3608 - acc: 0.8764 - val_loss: 0.2298 - val_acc: 0.9565\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.22980 to 0.22396, saving model to best.model\n",
      "0s - loss: 0.4157 - acc: 0.7978 - val_loss: 0.2240 - val_acc: 0.9565\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.22396 to 0.21825, saving model to best.model\n",
      "0s - loss: 0.4371 - acc: 0.8315 - val_loss: 0.2183 - val_acc: 0.9565\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.21825 to 0.21227, saving model to best.model\n",
      "0s - loss: 0.2986 - acc: 0.9551 - val_loss: 0.2123 - val_acc: 0.9565\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.21227 to 0.20636, saving model to best.model\n",
      "0s - loss: 0.3542 - acc: 0.8876 - val_loss: 0.2064 - val_acc: 0.9565\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.20636 to 0.20075, saving model to best.model\n",
      "0s - loss: 0.2489 - acc: 0.9438 - val_loss: 0.2008 - val_acc: 0.9565\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.20075 to 0.19539, saving model to best.model\n",
      "0s - loss: 0.3389 - acc: 0.8539 - val_loss: 0.1954 - val_acc: 0.9565\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.19539 to 0.19010, saving model to best.model\n",
      "0s - loss: 0.3017 - acc: 0.9326 - val_loss: 0.1901 - val_acc: 0.9565\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.19010 to 0.18467, saving model to best.model\n",
      "0s - loss: 0.3774 - acc: 0.8652 - val_loss: 0.1847 - val_acc: 0.9565\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.18467 to 0.17957, saving model to best.model\n",
      "0s - loss: 0.2763 - acc: 0.9438 - val_loss: 0.1796 - val_acc: 0.9565\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.17957 to 0.17486, saving model to best.model\n",
      "0s - loss: 0.3132 - acc: 0.9213 - val_loss: 0.1749 - val_acc: 0.9565\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.17486 to 0.17061, saving model to best.model\n",
      "0s - loss: 0.3001 - acc: 0.9101 - val_loss: 0.1706 - val_acc: 0.9565\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.17061 to 0.16671, saving model to best.model\n",
      "0s - loss: 0.2655 - acc: 0.9101 - val_loss: 0.1667 - val_acc: 0.9565\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.16671 to 0.16371, saving model to best.model\n",
      "0s - loss: 0.2924 - acc: 0.8876 - val_loss: 0.1637 - val_acc: 0.9565\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.16371 to 0.16127, saving model to best.model\n",
      "0s - loss: 0.2693 - acc: 0.9438 - val_loss: 0.1613 - val_acc: 0.9565\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.16127 to 0.15922, saving model to best.model\n",
      "0s - loss: 0.2605 - acc: 0.9438 - val_loss: 0.1592 - val_acc: 0.9565\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.15922 to 0.15735, saving model to best.model\n",
      "0s - loss: 0.2850 - acc: 0.8989 - val_loss: 0.1573 - val_acc: 0.9565\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.15735 to 0.15600, saving model to best.model\n",
      "0s - loss: 0.2442 - acc: 0.9438 - val_loss: 0.1560 - val_acc: 0.9565\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.15600 to 0.15488, saving model to best.model\n",
      "0s - loss: 0.2485 - acc: 0.9101 - val_loss: 0.1549 - val_acc: 0.9565\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.15488 to 0.15321, saving model to best.model\n",
      "0s - loss: 0.2202 - acc: 0.9326 - val_loss: 0.1532 - val_acc: 0.9565\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.15321 to 0.15073, saving model to best.model\n",
      "0s - loss: 0.2106 - acc: 0.9663 - val_loss: 0.1507 - val_acc: 0.9565\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.15073 to 0.14802, saving model to best.model\n",
      "0s - loss: 0.2230 - acc: 0.9438 - val_loss: 0.1480 - val_acc: 0.9565\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.14802 to 0.14358, saving model to best.model\n",
      "0s - loss: 0.3082 - acc: 0.8764 - val_loss: 0.1436 - val_acc: 0.9565\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.14358 to 0.13955, saving model to best.model\n",
      "0s - loss: 0.2675 - acc: 0.9326 - val_loss: 0.1395 - val_acc: 0.9565\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.13955 to 0.13586, saving model to best.model\n",
      "0s - loss: 0.2519 - acc: 0.9326 - val_loss: 0.1359 - val_acc: 0.9565\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.13586 to 0.13199, saving model to best.model\n",
      "0s - loss: 0.2203 - acc: 0.9438 - val_loss: 0.1320 - val_acc: 0.9565\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.13199 to 0.12909, saving model to best.model\n",
      "0s - loss: 0.1977 - acc: 0.9551 - val_loss: 0.1291 - val_acc: 0.9565\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.12909 to 0.12502, saving model to best.model\n",
      "0s - loss: 0.2716 - acc: 0.8989 - val_loss: 0.1250 - val_acc: 0.9565\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.12502 to 0.12095, saving model to best.model\n",
      "0s - loss: 0.2505 - acc: 0.9326 - val_loss: 0.1209 - val_acc: 0.9565\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.12095 to 0.11669, saving model to best.model\n",
      "0s - loss: 0.2160 - acc: 0.9213 - val_loss: 0.1167 - val_acc: 0.9565\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.11669 to 0.11313, saving model to best.model\n",
      "0s - loss: 0.2084 - acc: 0.9326 - val_loss: 0.1131 - val_acc: 0.9565\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.11313 to 0.11024, saving model to best.model\n",
      "0s - loss: 0.2140 - acc: 0.9326 - val_loss: 0.1102 - val_acc: 0.9565\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.11024 to 0.10819, saving model to best.model\n",
      "0s - loss: 0.2539 - acc: 0.9101 - val_loss: 0.1082 - val_acc: 0.9565\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.10819 to 0.10646, saving model to best.model\n",
      "0s - loss: 0.2100 - acc: 0.9326 - val_loss: 0.1065 - val_acc: 0.9565\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.10646 to 0.10438, saving model to best.model\n",
      "0s - loss: 0.2160 - acc: 0.9101 - val_loss: 0.1044 - val_acc: 0.9565\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.10438 to 0.10232, saving model to best.model\n",
      "0s - loss: 0.2352 - acc: 0.9326 - val_loss: 0.1023 - val_acc: 0.9565\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.10232 to 0.10023, saving model to best.model\n",
      "0s - loss: 0.1994 - acc: 0.9438 - val_loss: 0.1002 - val_acc: 0.9565\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.10023 to 0.09818, saving model to best.model\n",
      "0s - loss: 0.1959 - acc: 0.9326 - val_loss: 0.0982 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.09818 to 0.09597, saving model to best.model\n",
      "0s - loss: 0.1901 - acc: 0.9438 - val_loss: 0.0960 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.09597 to 0.09339, saving model to best.model\n",
      "0s - loss: 0.2065 - acc: 0.9326 - val_loss: 0.0934 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.09339 to 0.09085, saving model to best.model\n",
      "0s - loss: 0.1608 - acc: 0.9326 - val_loss: 0.0909 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.09085 to 0.08894, saving model to best.model\n",
      "0s - loss: 0.2133 - acc: 0.9213 - val_loss: 0.0889 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.08894 to 0.08752, saving model to best.model\n",
      "0s - loss: 0.1484 - acc: 0.9551 - val_loss: 0.0875 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.08752 to 0.08620, saving model to best.model\n",
      "0s - loss: 0.1719 - acc: 0.9663 - val_loss: 0.0862 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.08620 to 0.08479, saving model to best.model\n",
      "0s - loss: 0.2469 - acc: 0.9101 - val_loss: 0.0848 - val_acc: 1.0000\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.08479 to 0.08356, saving model to best.model\n",
      "0s - loss: 0.1441 - acc: 0.9775 - val_loss: 0.0836 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.08356 to 0.08235, saving model to best.model\n",
      "0s - loss: 0.1463 - acc: 0.9551 - val_loss: 0.0824 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.08235 to 0.08154, saving model to best.model\n",
      "0s - loss: 0.1331 - acc: 0.9663 - val_loss: 0.0815 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.08154 to 0.08006, saving model to best.model\n",
      "0s - loss: 0.2004 - acc: 0.9438 - val_loss: 0.0801 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.08006 to 0.07865, saving model to best.model\n",
      "0s - loss: 0.1641 - acc: 0.9775 - val_loss: 0.0786 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.07865 to 0.07803, saving model to best.model\n",
      "0s - loss: 0.1418 - acc: 0.9663 - val_loss: 0.0780 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.07803 to 0.07792, saving model to best.model\n",
      "0s - loss: 0.1791 - acc: 0.9326 - val_loss: 0.0779 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.07792 to 0.07777, saving model to best.model\n",
      "0s - loss: 0.2216 - acc: 0.9326 - val_loss: 0.0778 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.1568 - acc: 0.9438 - val_loss: 0.0781 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.07777 to 0.07763, saving model to best.model\n",
      "0s - loss: 0.1834 - acc: 0.9326 - val_loss: 0.0776 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.07763 to 0.07663, saving model to best.model\n",
      "0s - loss: 0.1325 - acc: 0.9663 - val_loss: 0.0766 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.07663 to 0.07467, saving model to best.model\n",
      "0s - loss: 0.2087 - acc: 0.9213 - val_loss: 0.0747 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.07467 to 0.07262, saving model to best.model\n",
      "0s - loss: 0.1054 - acc: 0.9775 - val_loss: 0.0726 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.07262 to 0.07044, saving model to best.model\n",
      "0s - loss: 0.1125 - acc: 0.9663 - val_loss: 0.0704 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.07044 to 0.06848, saving model to best.model\n",
      "0s - loss: 0.1248 - acc: 0.9551 - val_loss: 0.0685 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.06848 to 0.06656, saving model to best.model\n",
      "0s - loss: 0.1499 - acc: 0.9438 - val_loss: 0.0666 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.06656 to 0.06533, saving model to best.model\n",
      "0s - loss: 0.1544 - acc: 0.9326 - val_loss: 0.0653 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.06533 to 0.06456, saving model to best.model\n",
      "0s - loss: 0.1199 - acc: 0.9663 - val_loss: 0.0646 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.06456 to 0.06339, saving model to best.model\n",
      "0s - loss: 0.1124 - acc: 0.9663 - val_loss: 0.0634 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.06339 to 0.06187, saving model to best.model\n",
      "0s - loss: 0.1617 - acc: 0.9551 - val_loss: 0.0619 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.06187 to 0.06129, saving model to best.model\n",
      "0s - loss: 0.1552 - acc: 0.9326 - val_loss: 0.0613 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.06129 to 0.06102, saving model to best.model\n",
      "0s - loss: 0.1159 - acc: 0.9663 - val_loss: 0.0610 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.1390 - acc: 0.9663 - val_loss: 0.0618 - val_acc: 1.0000\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.07614, saving model to best.model\n",
      "0s - loss: 1.3754 - acc: 0.3371 - val_loss: 1.0761 - val_acc: 0.4348\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.07614 to 1.07593, saving model to best.model\n",
      "0s - loss: 1.3145 - acc: 0.3596 - val_loss: 1.0759 - val_acc: 0.4348\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.3013 - acc: 0.3146 - val_loss: 1.0979 - val_acc: 0.3043\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.2430 - acc: 0.3371 - val_loss: 1.1336 - val_acc: 0.3043\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.0862 - acc: 0.4831 - val_loss: 1.1784 - val_acc: 0.4783\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2288 - acc: 0.4270 - val_loss: 1.2214 - val_acc: 0.2609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.2339 - acc: 0.4045 - val_loss: 1.2581 - val_acc: 0.2609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.1777 - acc: 0.4045 - val_loss: 1.2838 - val_acc: 0.2609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1370 - acc: 0.3258 - val_loss: 1.2989 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.3862 - acc: 0.3596 - val_loss: 1.2994 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2630 - acc: 0.3258 - val_loss: 1.2911 - val_acc: 0.2609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2711 - acc: 0.4045 - val_loss: 1.2768 - val_acc: 0.2609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2532 - acc: 0.3820 - val_loss: 1.2568 - val_acc: 0.2609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2646 - acc: 0.3371 - val_loss: 1.2333 - val_acc: 0.5217\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2018 - acc: 0.3371 - val_loss: 1.2101 - val_acc: 0.4783\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2018 - acc: 0.3371 - val_loss: 1.1870 - val_acc: 0.4783\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2089 - acc: 0.3708 - val_loss: 1.1645 - val_acc: 0.4783\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.3611 - acc: 0.2697 - val_loss: 1.1451 - val_acc: 0.4783\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1473 - acc: 0.4157 - val_loss: 1.1283 - val_acc: 0.4783\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2049 - acc: 0.3820 - val_loss: 1.1141 - val_acc: 0.4783\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.0971 - acc: 0.4494 - val_loss: 1.1020 - val_acc: 0.4783\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.2708 - acc: 0.3146 - val_loss: 1.0928 - val_acc: 0.4783\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.0947 - acc: 0.4382 - val_loss: 1.0860 - val_acc: 0.4783\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1962 - acc: 0.4157 - val_loss: 1.0811 - val_acc: 0.5217\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1604 - acc: 0.4382 - val_loss: 1.0783 - val_acc: 0.5217\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1502 - acc: 0.4045 - val_loss: 1.0777 - val_acc: 0.5652\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1855 - acc: 0.3933 - val_loss: 1.0785 - val_acc: 0.5652\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1918 - acc: 0.3483 - val_loss: 1.0809 - val_acc: 0.5217\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.19562, saving model to best.model\n",
      "0s - loss: 1.3268 - acc: 0.3483 - val_loss: 1.1956 - val_acc: 0.3043\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.19562 to 1.13067, saving model to best.model\n",
      "0s - loss: 1.3523 - acc: 0.3708 - val_loss: 1.1307 - val_acc: 0.3043\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.13067 to 1.09735, saving model to best.model\n",
      "0s - loss: 1.4866 - acc: 0.2584 - val_loss: 1.0973 - val_acc: 0.3043\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.09735 to 1.08272, saving model to best.model\n",
      "0s - loss: 1.2368 - acc: 0.3596 - val_loss: 1.0827 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.08272 to 1.08134, saving model to best.model\n",
      "0s - loss: 1.3220 - acc: 0.3034 - val_loss: 1.0813 - val_acc: 0.4348\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.4223 - acc: 0.2697 - val_loss: 1.0865 - val_acc: 0.4348\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1598 - acc: 0.3708 - val_loss: 1.0923 - val_acc: 0.2609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.3690 - acc: 0.2921 - val_loss: 1.0969 - val_acc: 0.2609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2202 - acc: 0.3258 - val_loss: 1.0970 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2711 - acc: 0.3933 - val_loss: 1.0935 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.3155 - acc: 0.3146 - val_loss: 1.0869 - val_acc: 0.3043\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.08134 to 1.08008, saving model to best.model\n",
      "0s - loss: 1.2582 - acc: 0.3146 - val_loss: 1.0801 - val_acc: 0.4783\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.08008 to 1.07432, saving model to best.model\n",
      "0s - loss: 1.2027 - acc: 0.3371 - val_loss: 1.0743 - val_acc: 0.4783\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.07432 to 1.07125, saving model to best.model\n",
      "0s - loss: 1.2069 - acc: 0.3708 - val_loss: 1.0712 - val_acc: 0.6087\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.07125 to 1.07047, saving model to best.model\n",
      "0s - loss: 1.0940 - acc: 0.4045 - val_loss: 1.0705 - val_acc: 0.3043\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.07047 to 1.07035, saving model to best.model\n",
      "0s - loss: 1.2151 - acc: 0.3708 - val_loss: 1.0703 - val_acc: 0.3043\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.1760 - acc: 0.4382 - val_loss: 1.0708 - val_acc: 0.3043\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1763 - acc: 0.3933 - val_loss: 1.0726 - val_acc: 0.3043\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1827 - acc: 0.3820 - val_loss: 1.0741 - val_acc: 0.3043\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.2300 - acc: 0.3596 - val_loss: 1.0754 - val_acc: 0.3043\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1395 - acc: 0.4157 - val_loss: 1.0748 - val_acc: 0.3043\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.2494 - acc: 0.3820 - val_loss: 1.0731 - val_acc: 0.3043\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.07035 to 1.06993, saving model to best.model\n",
      "0s - loss: 1.0631 - acc: 0.4494 - val_loss: 1.0699 - val_acc: 0.3043\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.06993 to 1.06619, saving model to best.model\n",
      "0s - loss: 1.2396 - acc: 0.4157 - val_loss: 1.0662 - val_acc: 0.3043\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.06619 to 1.06274, saving model to best.model\n",
      "0s - loss: 1.2930 - acc: 0.3034 - val_loss: 1.0627 - val_acc: 0.3043\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.06274 to 1.05942, saving model to best.model\n",
      "0s - loss: 1.1450 - acc: 0.3708 - val_loss: 1.0594 - val_acc: 0.3043\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.05942 to 1.05647, saving model to best.model\n",
      "0s - loss: 1.1882 - acc: 0.3708 - val_loss: 1.0565 - val_acc: 0.3043\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.05647 to 1.05395, saving model to best.model\n",
      "0s - loss: 1.1900 - acc: 0.3258 - val_loss: 1.0539 - val_acc: 0.3043\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.05395 to 1.05180, saving model to best.model\n",
      "0s - loss: 1.1550 - acc: 0.3933 - val_loss: 1.0518 - val_acc: 0.3043\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.05180 to 1.05011, saving model to best.model\n",
      "0s - loss: 1.1396 - acc: 0.4045 - val_loss: 1.0501 - val_acc: 0.3913\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.05011 to 1.04840, saving model to best.model\n",
      "0s - loss: 1.1916 - acc: 0.3596 - val_loss: 1.0484 - val_acc: 0.4348\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.04840 to 1.04732, saving model to best.model\n",
      "0s - loss: 1.1714 - acc: 0.3034 - val_loss: 1.0473 - val_acc: 0.3913\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.04732 to 1.04553, saving model to best.model\n",
      "0s - loss: 1.1866 - acc: 0.3596 - val_loss: 1.0455 - val_acc: 0.4348\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.04553 to 1.04347, saving model to best.model\n",
      "0s - loss: 1.1669 - acc: 0.3596 - val_loss: 1.0435 - val_acc: 0.4348\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.04347 to 1.04127, saving model to best.model\n",
      "0s - loss: 1.1754 - acc: 0.3034 - val_loss: 1.0413 - val_acc: 0.4783\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.04127 to 1.03892, saving model to best.model\n",
      "0s - loss: 1.0893 - acc: 0.3708 - val_loss: 1.0389 - val_acc: 0.4783\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.03892 to 1.03706, saving model to best.model\n",
      "0s - loss: 1.1214 - acc: 0.3933 - val_loss: 1.0371 - val_acc: 0.3913\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.03706 to 1.03534, saving model to best.model\n",
      "0s - loss: 1.1626 - acc: 0.3596 - val_loss: 1.0353 - val_acc: 0.3478\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.03534 to 1.03392, saving model to best.model\n",
      "0s - loss: 1.1414 - acc: 0.3483 - val_loss: 1.0339 - val_acc: 0.3043\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.03392 to 1.03146, saving model to best.model\n",
      "0s - loss: 1.0995 - acc: 0.4270 - val_loss: 1.0315 - val_acc: 0.3478\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.03146 to 1.02927, saving model to best.model\n",
      "0s - loss: 1.1616 - acc: 0.4382 - val_loss: 1.0293 - val_acc: 0.3043\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.02927 to 1.02719, saving model to best.model\n",
      "0s - loss: 1.1211 - acc: 0.3708 - val_loss: 1.0272 - val_acc: 0.3043\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.02719 to 1.02502, saving model to best.model\n",
      "0s - loss: 1.1467 - acc: 0.3146 - val_loss: 1.0250 - val_acc: 0.3043\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.02502 to 1.02243, saving model to best.model\n",
      "0s - loss: 1.0857 - acc: 0.3933 - val_loss: 1.0224 - val_acc: 0.3043\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.02243 to 1.01999, saving model to best.model\n",
      "0s - loss: 1.0520 - acc: 0.3483 - val_loss: 1.0200 - val_acc: 0.3478\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.01999 to 1.01744, saving model to best.model\n",
      "0s - loss: 1.0452 - acc: 0.4944 - val_loss: 1.0174 - val_acc: 0.3478\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 1.01744 to 1.01459, saving model to best.model\n",
      "0s - loss: 1.0976 - acc: 0.3371 - val_loss: 1.0146 - val_acc: 0.3478\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 1.01459 to 1.01122, saving model to best.model\n",
      "0s - loss: 1.0695 - acc: 0.4270 - val_loss: 1.0112 - val_acc: 0.3478\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 1.01122 to 1.00735, saving model to best.model\n",
      "0s - loss: 1.0832 - acc: 0.4494 - val_loss: 1.0074 - val_acc: 0.3478\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 1.00735 to 1.00343, saving model to best.model\n",
      "0s - loss: 1.1414 - acc: 0.3596 - val_loss: 1.0034 - val_acc: 0.4348\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 1.00343 to 0.99996, saving model to best.model\n",
      "0s - loss: 1.0879 - acc: 0.3820 - val_loss: 1.0000 - val_acc: 0.4348\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.99996 to 0.99624, saving model to best.model\n",
      "0s - loss: 1.0570 - acc: 0.5169 - val_loss: 0.9962 - val_acc: 0.4783\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.99624 to 0.99290, saving model to best.model\n",
      "0s - loss: 1.0696 - acc: 0.3933 - val_loss: 0.9929 - val_acc: 0.4783\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.99290 to 0.98953, saving model to best.model\n",
      "0s - loss: 1.0348 - acc: 0.4719 - val_loss: 0.9895 - val_acc: 0.4783\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.98953 to 0.98671, saving model to best.model\n",
      "0s - loss: 1.0602 - acc: 0.4607 - val_loss: 0.9867 - val_acc: 0.4348\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.98671 to 0.98383, saving model to best.model\n",
      "0s - loss: 0.9465 - acc: 0.4944 - val_loss: 0.9838 - val_acc: 0.3913\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.98383 to 0.98029, saving model to best.model\n",
      "0s - loss: 1.0709 - acc: 0.3708 - val_loss: 0.9803 - val_acc: 0.3913\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.98029 to 0.97640, saving model to best.model\n",
      "0s - loss: 1.0737 - acc: 0.4270 - val_loss: 0.9764 - val_acc: 0.3913\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.97640 to 0.97202, saving model to best.model\n",
      "0s - loss: 1.0504 - acc: 0.4045 - val_loss: 0.9720 - val_acc: 0.4348\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.97202 to 0.96669, saving model to best.model\n",
      "0s - loss: 0.9549 - acc: 0.5506 - val_loss: 0.9667 - val_acc: 0.5652\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.96669 to 0.96141, saving model to best.model\n",
      "0s - loss: 1.0386 - acc: 0.4157 - val_loss: 0.9614 - val_acc: 0.6087\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.96141 to 0.95603, saving model to best.model\n",
      "0s - loss: 1.0958 - acc: 0.4382 - val_loss: 0.9560 - val_acc: 0.6087\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.95603 to 0.95057, saving model to best.model\n",
      "0s - loss: 1.0093 - acc: 0.5281 - val_loss: 0.9506 - val_acc: 0.6522\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.95057 to 0.94467, saving model to best.model\n",
      "0s - loss: 1.0050 - acc: 0.4944 - val_loss: 0.9447 - val_acc: 0.6957\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.94467 to 0.93868, saving model to best.model\n",
      "0s - loss: 1.0293 - acc: 0.4607 - val_loss: 0.9387 - val_acc: 0.7391\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.93868 to 0.93243, saving model to best.model\n",
      "0s - loss: 0.9779 - acc: 0.5169 - val_loss: 0.9324 - val_acc: 0.7391\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.93243 to 0.92580, saving model to best.model\n",
      "0s - loss: 1.0071 - acc: 0.4494 - val_loss: 0.9258 - val_acc: 0.7826\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.92580 to 0.91904, saving model to best.model\n",
      "0s - loss: 0.9691 - acc: 0.5618 - val_loss: 0.9190 - val_acc: 0.7826\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.91904 to 0.91229, saving model to best.model\n",
      "0s - loss: 0.8953 - acc: 0.5730 - val_loss: 0.9123 - val_acc: 0.7826\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.91229 to 0.90503, saving model to best.model\n",
      "0s - loss: 1.0665 - acc: 0.4494 - val_loss: 0.9050 - val_acc: 0.7826\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.90503 to 0.89797, saving model to best.model\n",
      "0s - loss: 0.9229 - acc: 0.5506 - val_loss: 0.8980 - val_acc: 0.7826\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.89797 to 0.89078, saving model to best.model\n",
      "0s - loss: 0.9144 - acc: 0.6067 - val_loss: 0.8908 - val_acc: 0.7826\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.89078 to 0.88370, saving model to best.model\n",
      "0s - loss: 0.9473 - acc: 0.5618 - val_loss: 0.8837 - val_acc: 0.7826\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.88370 to 0.87600, saving model to best.model\n",
      "0s - loss: 0.9144 - acc: 0.5618 - val_loss: 0.8760 - val_acc: 0.7826\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.87600 to 0.86868, saving model to best.model\n",
      "0s - loss: 0.9178 - acc: 0.5618 - val_loss: 0.8687 - val_acc: 0.7826\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.86868 to 0.86102, saving model to best.model\n",
      "0s - loss: 1.0163 - acc: 0.5169 - val_loss: 0.8610 - val_acc: 0.7391\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.86102 to 0.85282, saving model to best.model\n",
      "0s - loss: 0.8634 - acc: 0.6180 - val_loss: 0.8528 - val_acc: 0.7826\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.85282 to 0.84403, saving model to best.model\n",
      "0s - loss: 0.8983 - acc: 0.5730 - val_loss: 0.8440 - val_acc: 0.7826\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.84403 to 0.83528, saving model to best.model\n",
      "0s - loss: 0.9194 - acc: 0.6067 - val_loss: 0.8353 - val_acc: 0.7826\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.83528 to 0.82681, saving model to best.model\n",
      "0s - loss: 0.8570 - acc: 0.5843 - val_loss: 0.8268 - val_acc: 0.7826\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.82681 to 0.81794, saving model to best.model\n",
      "0s - loss: 0.8335 - acc: 0.6180 - val_loss: 0.8179 - val_acc: 0.8261\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.81794 to 0.80937, saving model to best.model\n",
      "0s - loss: 0.8194 - acc: 0.6629 - val_loss: 0.8094 - val_acc: 0.8261\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.80937 to 0.79983, saving model to best.model\n",
      "0s - loss: 0.8847 - acc: 0.5843 - val_loss: 0.7998 - val_acc: 0.8261\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.79983 to 0.79014, saving model to best.model\n",
      "0s - loss: 0.8094 - acc: 0.6854 - val_loss: 0.7901 - val_acc: 0.8261\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.79014 to 0.78063, saving model to best.model\n",
      "0s - loss: 0.8408 - acc: 0.6180 - val_loss: 0.7806 - val_acc: 0.8261\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.78063 to 0.77088, saving model to best.model\n",
      "0s - loss: 0.8348 - acc: 0.6966 - val_loss: 0.7709 - val_acc: 0.8261\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.77088 to 0.76105, saving model to best.model\n",
      "0s - loss: 0.8034 - acc: 0.6517 - val_loss: 0.7610 - val_acc: 0.8261\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.76105 to 0.75070, saving model to best.model\n",
      "0s - loss: 0.8428 - acc: 0.6404 - val_loss: 0.7507 - val_acc: 0.8261\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.75070 to 0.74059, saving model to best.model\n",
      "0s - loss: 0.8913 - acc: 0.5955 - val_loss: 0.7406 - val_acc: 0.8261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.74059 to 0.73027, saving model to best.model\n",
      "0s - loss: 0.8314 - acc: 0.6067 - val_loss: 0.7303 - val_acc: 0.8261\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.73027 to 0.71878, saving model to best.model\n",
      "0s - loss: 0.7821 - acc: 0.6629 - val_loss: 0.7188 - val_acc: 0.8261\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.71878 to 0.70775, saving model to best.model\n",
      "0s - loss: 0.7888 - acc: 0.6742 - val_loss: 0.7077 - val_acc: 0.8261\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.70775 to 0.69614, saving model to best.model\n",
      "0s - loss: 0.7471 - acc: 0.6854 - val_loss: 0.6961 - val_acc: 0.8261\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.69614 to 0.68482, saving model to best.model\n",
      "0s - loss: 0.7270 - acc: 0.6854 - val_loss: 0.6848 - val_acc: 0.8261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.68482 to 0.67295, saving model to best.model\n",
      "0s - loss: 0.7295 - acc: 0.7191 - val_loss: 0.6729 - val_acc: 0.8261\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67295 to 0.66223, saving model to best.model\n",
      "0s - loss: 0.7005 - acc: 0.7416 - val_loss: 0.6622 - val_acc: 0.8261\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66223 to 0.65195, saving model to best.model\n",
      "0s - loss: 0.7034 - acc: 0.7079 - val_loss: 0.6520 - val_acc: 0.8261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.65195 to 0.64184, saving model to best.model\n",
      "0s - loss: 0.7062 - acc: 0.7416 - val_loss: 0.6418 - val_acc: 0.8261\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.64184 to 0.63130, saving model to best.model\n",
      "0s - loss: 0.7562 - acc: 0.6854 - val_loss: 0.6313 - val_acc: 0.8261\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.63130 to 0.62246, saving model to best.model\n",
      "0s - loss: 0.6830 - acc: 0.7303 - val_loss: 0.6225 - val_acc: 0.8261\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.62246 to 0.61402, saving model to best.model\n",
      "0s - loss: 0.6938 - acc: 0.6742 - val_loss: 0.6140 - val_acc: 0.8261\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.61402 to 0.60719, saving model to best.model\n",
      "0s - loss: 0.6642 - acc: 0.7303 - val_loss: 0.6072 - val_acc: 0.8261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.60719 to 0.60082, saving model to best.model\n",
      "0s - loss: 0.6789 - acc: 0.7191 - val_loss: 0.6008 - val_acc: 0.8261\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.60082 to 0.59325, saving model to best.model\n",
      "0s - loss: 0.6318 - acc: 0.7640 - val_loss: 0.5933 - val_acc: 0.8261\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.59325 to 0.58487, saving model to best.model\n",
      "0s - loss: 0.6818 - acc: 0.6966 - val_loss: 0.5849 - val_acc: 0.8261\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.58487 to 0.57674, saving model to best.model\n",
      "0s - loss: 0.6083 - acc: 0.7191 - val_loss: 0.5767 - val_acc: 0.8261\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.57674 to 0.56891, saving model to best.model\n",
      "0s - loss: 0.6044 - acc: 0.7416 - val_loss: 0.5689 - val_acc: 0.8261\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.56891 to 0.56180, saving model to best.model\n",
      "0s - loss: 0.5932 - acc: 0.7640 - val_loss: 0.5618 - val_acc: 0.8261\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.56180 to 0.55366, saving model to best.model\n",
      "0s - loss: 0.5979 - acc: 0.7303 - val_loss: 0.5537 - val_acc: 0.8261\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.55366 to 0.54558, saving model to best.model\n",
      "0s - loss: 0.6418 - acc: 0.7079 - val_loss: 0.5456 - val_acc: 0.8261\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.54558 to 0.53649, saving model to best.model\n",
      "0s - loss: 0.5785 - acc: 0.7753 - val_loss: 0.5365 - val_acc: 0.8261\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.53649 to 0.52864, saving model to best.model\n",
      "0s - loss: 0.5722 - acc: 0.7978 - val_loss: 0.5286 - val_acc: 0.8261\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.52864 to 0.51974, saving model to best.model\n",
      "0s - loss: 0.5560 - acc: 0.7865 - val_loss: 0.5197 - val_acc: 0.8261\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.51974 to 0.51185, saving model to best.model\n",
      "0s - loss: 0.6251 - acc: 0.8090 - val_loss: 0.5119 - val_acc: 0.8261\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.51185 to 0.50629, saving model to best.model\n",
      "0s - loss: 0.5441 - acc: 0.7978 - val_loss: 0.5063 - val_acc: 0.8261\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.50629 to 0.49995, saving model to best.model\n",
      "0s - loss: 0.4705 - acc: 0.8090 - val_loss: 0.5000 - val_acc: 0.8261\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.49995 to 0.49313, saving model to best.model\n",
      "0s - loss: 0.5080 - acc: 0.7978 - val_loss: 0.4931 - val_acc: 0.8261\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.49313 to 0.48728, saving model to best.model\n",
      "0s - loss: 0.5109 - acc: 0.8427 - val_loss: 0.4873 - val_acc: 0.8261\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.48728 to 0.48231, saving model to best.model\n",
      "0s - loss: 0.5188 - acc: 0.7978 - val_loss: 0.4823 - val_acc: 0.8261\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.48231 to 0.47738, saving model to best.model\n",
      "0s - loss: 0.4814 - acc: 0.8090 - val_loss: 0.4774 - val_acc: 0.8261\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.47738 to 0.47205, saving model to best.model\n",
      "0s - loss: 0.5020 - acc: 0.8202 - val_loss: 0.4721 - val_acc: 0.8261\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.47205 to 0.46723, saving model to best.model\n",
      "0s - loss: 0.5419 - acc: 0.7865 - val_loss: 0.4672 - val_acc: 0.8261\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.46723 to 0.46249, saving model to best.model\n",
      "0s - loss: 0.4657 - acc: 0.8090 - val_loss: 0.4625 - val_acc: 0.8261\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.46249 to 0.45804, saving model to best.model\n",
      "0s - loss: 0.5006 - acc: 0.7640 - val_loss: 0.4580 - val_acc: 0.8261\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.45804 to 0.45248, saving model to best.model\n",
      "0s - loss: 0.4484 - acc: 0.7865 - val_loss: 0.4525 - val_acc: 0.8261\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.45248 to 0.44761, saving model to best.model\n",
      "0s - loss: 0.4473 - acc: 0.8315 - val_loss: 0.4476 - val_acc: 0.8261\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.44761 to 0.44132, saving model to best.model\n",
      "0s - loss: 0.4301 - acc: 0.8539 - val_loss: 0.4413 - val_acc: 0.8261\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.44132 to 0.43543, saving model to best.model\n",
      "0s - loss: 0.4231 - acc: 0.8539 - val_loss: 0.4354 - val_acc: 0.8261\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.43543 to 0.42982, saving model to best.model\n",
      "0s - loss: 0.4242 - acc: 0.8539 - val_loss: 0.4298 - val_acc: 0.8261\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.42982 to 0.42470, saving model to best.model\n",
      "0s - loss: 0.4384 - acc: 0.8315 - val_loss: 0.4247 - val_acc: 0.8261\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.42470 to 0.42000, saving model to best.model\n",
      "0s - loss: 0.4686 - acc: 0.8427 - val_loss: 0.4200 - val_acc: 0.8261\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.42000 to 0.41673, saving model to best.model\n",
      "0s - loss: 0.4442 - acc: 0.8315 - val_loss: 0.4167 - val_acc: 0.8261\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.41673 to 0.41357, saving model to best.model\n",
      "0s - loss: 0.3621 - acc: 0.9101 - val_loss: 0.4136 - val_acc: 0.8261\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.41357 to 0.40937, saving model to best.model\n",
      "0s - loss: 0.4120 - acc: 0.8539 - val_loss: 0.4094 - val_acc: 0.8261\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.40937 to 0.40449, saving model to best.model\n",
      "0s - loss: 0.3903 - acc: 0.8764 - val_loss: 0.4045 - val_acc: 0.8261\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.40449 to 0.39896, saving model to best.model\n",
      "0s - loss: 0.4761 - acc: 0.8090 - val_loss: 0.3990 - val_acc: 0.8261\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.39896 to 0.39506, saving model to best.model\n",
      "0s - loss: 0.3906 - acc: 0.8652 - val_loss: 0.3951 - val_acc: 0.8261\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.39506 to 0.39301, saving model to best.model\n",
      "0s - loss: 0.3468 - acc: 0.9101 - val_loss: 0.3930 - val_acc: 0.8261\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.39301 to 0.39121, saving model to best.model\n",
      "0s - loss: 0.3502 - acc: 0.8539 - val_loss: 0.3912 - val_acc: 0.8261\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.39121 to 0.38876, saving model to best.model\n",
      "0s - loss: 0.3624 - acc: 0.8764 - val_loss: 0.3888 - val_acc: 0.8261\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.38876 to 0.38585, saving model to best.model\n",
      "0s - loss: 0.3367 - acc: 0.8989 - val_loss: 0.3859 - val_acc: 0.8261\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.38585 to 0.38100, saving model to best.model\n",
      "0s - loss: 0.3931 - acc: 0.8539 - val_loss: 0.3810 - val_acc: 0.8261\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.38100 to 0.37621, saving model to best.model\n",
      "0s - loss: 0.3891 - acc: 0.8652 - val_loss: 0.3762 - val_acc: 0.8261\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.37621 to 0.37166, saving model to best.model\n",
      "0s - loss: 0.3121 - acc: 0.8989 - val_loss: 0.3717 - val_acc: 0.8261\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.37166 to 0.36870, saving model to best.model\n",
      "0s - loss: 0.3507 - acc: 0.8539 - val_loss: 0.3687 - val_acc: 0.8261\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.36870 to 0.36366, saving model to best.model\n",
      "0s - loss: 0.3014 - acc: 0.8989 - val_loss: 0.3637 - val_acc: 0.8261\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.36366 to 0.35857, saving model to best.model\n",
      "0s - loss: 0.3685 - acc: 0.8315 - val_loss: 0.3586 - val_acc: 0.8261\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.35857 to 0.35257, saving model to best.model\n",
      "0s - loss: 0.3608 - acc: 0.8427 - val_loss: 0.3526 - val_acc: 0.8261\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.35257 to 0.34965, saving model to best.model\n",
      "0s - loss: 0.3668 - acc: 0.8652 - val_loss: 0.3497 - val_acc: 0.8261\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.34965 to 0.34657, saving model to best.model\n",
      "0s - loss: 0.3488 - acc: 0.8876 - val_loss: 0.3466 - val_acc: 0.8261\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.34657 to 0.34478, saving model to best.model\n",
      "0s - loss: 0.3237 - acc: 0.9101 - val_loss: 0.3448 - val_acc: 0.8261\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.34478 to 0.34073, saving model to best.model\n",
      "0s - loss: 0.3314 - acc: 0.8315 - val_loss: 0.3407 - val_acc: 0.8261\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.34073 to 0.33464, saving model to best.model\n",
      "0s - loss: 0.3247 - acc: 0.8989 - val_loss: 0.3346 - val_acc: 0.8261\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.33464 to 0.32751, saving model to best.model\n",
      "0s - loss: 0.3117 - acc: 0.9101 - val_loss: 0.3275 - val_acc: 0.8261\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.32751 to 0.31875, saving model to best.model\n",
      "0s - loss: 0.4058 - acc: 0.8315 - val_loss: 0.3188 - val_acc: 0.8261\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.31875 to 0.31094, saving model to best.model\n",
      "0s - loss: 0.3137 - acc: 0.8989 - val_loss: 0.3109 - val_acc: 0.8261\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.31094 to 0.30138, saving model to best.model\n",
      "0s - loss: 0.3377 - acc: 0.8764 - val_loss: 0.3014 - val_acc: 0.8261\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.30138 to 0.29286, saving model to best.model\n",
      "0s - loss: 0.3366 - acc: 0.9101 - val_loss: 0.2929 - val_acc: 0.8696\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.29286 to 0.28510, saving model to best.model\n",
      "0s - loss: 0.3068 - acc: 0.8876 - val_loss: 0.2851 - val_acc: 0.8696\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.28510 to 0.27981, saving model to best.model\n",
      "0s - loss: 0.2546 - acc: 0.9213 - val_loss: 0.2798 - val_acc: 0.8696\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.27981 to 0.27488, saving model to best.model\n",
      "0s - loss: 0.3058 - acc: 0.8652 - val_loss: 0.2749 - val_acc: 0.8696\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.27488 to 0.27125, saving model to best.model\n",
      "0s - loss: 0.3697 - acc: 0.8652 - val_loss: 0.2712 - val_acc: 0.8696\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.27125 to 0.26748, saving model to best.model\n",
      "0s - loss: 0.2754 - acc: 0.9438 - val_loss: 0.2675 - val_acc: 0.8696\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.26748 to 0.26478, saving model to best.model\n",
      "0s - loss: 0.2576 - acc: 0.9326 - val_loss: 0.2648 - val_acc: 0.8696\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.26478 to 0.26391, saving model to best.model\n",
      "0s - loss: 0.2334 - acc: 0.9101 - val_loss: 0.2639 - val_acc: 0.8696\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.26391 to 0.26211, saving model to best.model\n",
      "0s - loss: 0.3350 - acc: 0.8652 - val_loss: 0.2621 - val_acc: 0.8696\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.26211 to 0.25784, saving model to best.model\n",
      "0s - loss: 0.2811 - acc: 0.9101 - val_loss: 0.2578 - val_acc: 0.8696\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.25784 to 0.25586, saving model to best.model\n",
      "0s - loss: 0.3009 - acc: 0.8989 - val_loss: 0.2559 - val_acc: 0.8696\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.25586 to 0.25118, saving model to best.model\n",
      "0s - loss: 0.2839 - acc: 0.8989 - val_loss: 0.2512 - val_acc: 0.8696\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.25118 to 0.24499, saving model to best.model\n",
      "0s - loss: 0.2469 - acc: 0.8876 - val_loss: 0.2450 - val_acc: 0.8696\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.24499 to 0.23980, saving model to best.model\n",
      "0s - loss: 0.2026 - acc: 0.9438 - val_loss: 0.2398 - val_acc: 0.8696\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.23980 to 0.23328, saving model to best.model\n",
      "0s - loss: 0.3020 - acc: 0.8989 - val_loss: 0.2333 - val_acc: 0.8696\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.23328 to 0.22690, saving model to best.model\n",
      "0s - loss: 0.2234 - acc: 0.9213 - val_loss: 0.2269 - val_acc: 0.8696\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.22690 to 0.22091, saving model to best.model\n",
      "0s - loss: 0.1845 - acc: 0.9775 - val_loss: 0.2209 - val_acc: 0.9130\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.22091 to 0.21677, saving model to best.model\n",
      "0s - loss: 0.1856 - acc: 0.9326 - val_loss: 0.2168 - val_acc: 0.9130\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.21677 to 0.21360, saving model to best.model\n",
      "0s - loss: 0.2344 - acc: 0.9326 - val_loss: 0.2136 - val_acc: 0.9130\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.21360 to 0.21084, saving model to best.model\n",
      "0s - loss: 0.1947 - acc: 0.9438 - val_loss: 0.2108 - val_acc: 0.9130\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.21084 to 0.20694, saving model to best.model\n",
      "0s - loss: 0.2398 - acc: 0.9101 - val_loss: 0.2069 - val_acc: 0.9130\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.20694 to 0.20327, saving model to best.model\n",
      "0s - loss: 0.2069 - acc: 0.9663 - val_loss: 0.2033 - val_acc: 0.9130\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.20327 to 0.19957, saving model to best.model\n",
      "0s - loss: 0.2348 - acc: 0.8989 - val_loss: 0.1996 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.19957 to 0.19571, saving model to best.model\n",
      "0s - loss: 0.1889 - acc: 0.9213 - val_loss: 0.1957 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.19571 to 0.19189, saving model to best.model\n",
      "0s - loss: 0.2638 - acc: 0.9101 - val_loss: 0.1919 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.19189 to 0.18755, saving model to best.model\n",
      "0s - loss: 0.2068 - acc: 0.9438 - val_loss: 0.1875 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.18755 to 0.18516, saving model to best.model\n",
      "0s - loss: 0.2701 - acc: 0.8989 - val_loss: 0.1852 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.18516 to 0.18505, saving model to best.model\n",
      "0s - loss: 0.2775 - acc: 0.8539 - val_loss: 0.1851 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.2640 - acc: 0.9101 - val_loss: 0.1859 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.2116 - acc: 0.9438 - val_loss: 0.1867 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.1551 - acc: 0.9551 - val_loss: 0.1874 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.1739 - acc: 0.9438 - val_loss: 0.1872 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.2100 - acc: 0.9438 - val_loss: 0.1868 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.18505 to 0.18359, saving model to best.model\n",
      "0s - loss: 0.2334 - acc: 0.9213 - val_loss: 0.1836 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.18359 to 0.18118, saving model to best.model\n",
      "0s - loss: 0.1486 - acc: 0.9775 - val_loss: 0.1812 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.18118 to 0.17718, saving model to best.model\n",
      "0s - loss: 0.1848 - acc: 0.9101 - val_loss: 0.1772 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.17718 to 0.17328, saving model to best.model\n",
      "0s - loss: 0.2375 - acc: 0.9101 - val_loss: 0.1733 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.17328 to 0.17057, saving model to best.model\n",
      "0s - loss: 0.2263 - acc: 0.9213 - val_loss: 0.1706 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.17057 to 0.16807, saving model to best.model\n",
      "0s - loss: 0.1678 - acc: 0.9438 - val_loss: 0.1681 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.16807 to 0.16515, saving model to best.model\n",
      "0s - loss: 0.1493 - acc: 0.9663 - val_loss: 0.1652 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.16515 to 0.16188, saving model to best.model\n",
      "0s - loss: 0.1467 - acc: 0.9438 - val_loss: 0.1619 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.16188 to 0.15945, saving model to best.model\n",
      "0s - loss: 0.1853 - acc: 0.9326 - val_loss: 0.1595 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.15945 to 0.15537, saving model to best.model\n",
      "0s - loss: 0.2267 - acc: 0.8876 - val_loss: 0.1554 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.80757, saving model to best.model\n",
      "0s - loss: 2.0372 - acc: 0.2921 - val_loss: 1.8076 - val_acc: 0.2609\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.80757 to 1.57305, saving model to best.model\n",
      "0s - loss: 1.8852 - acc: 0.2921 - val_loss: 1.5730 - val_acc: 0.2609\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.57305 to 1.37373, saving model to best.model\n",
      "0s - loss: 1.7195 - acc: 0.2809 - val_loss: 1.3737 - val_acc: 0.2609\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.37373 to 1.21644, saving model to best.model\n",
      "0s - loss: 1.6336 - acc: 0.2697 - val_loss: 1.2164 - val_acc: 0.2609\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.21644 to 1.09950, saving model to best.model\n",
      "0s - loss: 1.3308 - acc: 0.3258 - val_loss: 1.0995 - val_acc: 0.2609\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.09950 to 1.02185, saving model to best.model\n",
      "0s - loss: 1.2806 - acc: 0.3371 - val_loss: 1.0218 - val_acc: 0.2609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 1.02185 to 0.97774, saving model to best.model\n",
      "0s - loss: 1.2447 - acc: 0.3820 - val_loss: 0.9777 - val_acc: 0.6087\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.97774 to 0.96038, saving model to best.model\n",
      "0s - loss: 1.2692 - acc: 0.3708 - val_loss: 0.9604 - val_acc: 0.6087\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.2811 - acc: 0.3258 - val_loss: 0.9607 - val_acc: 0.6087\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.1612 - acc: 0.4270 - val_loss: 0.9722 - val_acc: 0.6087\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2749 - acc: 0.3708 - val_loss: 0.9888 - val_acc: 0.6087\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2263 - acc: 0.4270 - val_loss: 1.0081 - val_acc: 0.6087\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.2075 - acc: 0.4157 - val_loss: 1.0281 - val_acc: 0.6087\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2177 - acc: 0.4494 - val_loss: 1.0479 - val_acc: 0.6087\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2954 - acc: 0.3820 - val_loss: 1.0648 - val_acc: 0.6087\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.3019 - acc: 0.3933 - val_loss: 1.0794 - val_acc: 0.6087\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2507 - acc: 0.4045 - val_loss: 1.0911 - val_acc: 0.6087\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.2405 - acc: 0.4494 - val_loss: 1.0991 - val_acc: 0.6087\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.3527 - acc: 0.2921 - val_loss: 1.0998 - val_acc: 0.6087\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.4000 - acc: 0.3034 - val_loss: 1.0950 - val_acc: 0.6087\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.2737 - acc: 0.3596 - val_loss: 1.0854 - val_acc: 0.6087\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.3666 - acc: 0.3258 - val_loss: 1.0734 - val_acc: 0.6087\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.2286 - acc: 0.3596 - val_loss: 1.0610 - val_acc: 0.6087\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.3222 - acc: 0.2584 - val_loss: 1.0470 - val_acc: 0.6087\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.3606 - acc: 0.2921 - val_loss: 1.0301 - val_acc: 0.6087\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1479 - acc: 0.3596 - val_loss: 1.0152 - val_acc: 0.6087\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1203 - acc: 0.4831 - val_loss: 1.0021 - val_acc: 0.6087\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.1855 - acc: 0.3371 - val_loss: 0.9920 - val_acc: 0.6087\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.2701 - acc: 0.3596 - val_loss: 0.9834 - val_acc: 0.6087\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 1.0176 - acc: 0.4831 - val_loss: 0.9769 - val_acc: 0.6087\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 1.1667 - acc: 0.3933 - val_loss: 0.9718 - val_acc: 0.6087\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 1.0855 - acc: 0.4382 - val_loss: 0.9679 - val_acc: 0.6087\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 1.0558 - acc: 0.4607 - val_loss: 0.9650 - val_acc: 0.6087\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 1.2543 - acc: 0.2921 - val_loss: 0.9615 - val_acc: 0.6087\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.08666, saving model to best.model\n",
      "0s - loss: 1.2342 - acc: 0.4157 - val_loss: 1.0867 - val_acc: 0.3913\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.08666 to 1.08437, saving model to best.model\n",
      "0s - loss: 1.2512 - acc: 0.3371 - val_loss: 1.0844 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 1.2935 - acc: 0.4382 - val_loss: 1.0880 - val_acc: 0.3913\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.1613 - acc: 0.3371 - val_loss: 1.0923 - val_acc: 0.3913\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.2123 - acc: 0.3483 - val_loss: 1.0953 - val_acc: 0.3913\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.1957 - acc: 0.3708 - val_loss: 1.0968 - val_acc: 0.3913\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.3216 - acc: 0.3258 - val_loss: 1.0970 - val_acc: 0.3913\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.3468 - acc: 0.3146 - val_loss: 1.0950 - val_acc: 0.5652\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.1919 - acc: 0.3933 - val_loss: 1.0931 - val_acc: 0.3478\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.3345 - acc: 0.3708 - val_loss: 1.0894 - val_acc: 0.3478\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 1.08437 to 1.08413, saving model to best.model\n",
      "0s - loss: 1.2906 - acc: 0.3483 - val_loss: 1.0841 - val_acc: 0.3478\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 1.08413 to 1.07917, saving model to best.model\n",
      "0s - loss: 1.3094 - acc: 0.2584 - val_loss: 1.0792 - val_acc: 0.3478\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 1.07917 to 1.07430, saving model to best.model\n",
      "0s - loss: 1.2267 - acc: 0.3820 - val_loss: 1.0743 - val_acc: 0.3478\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 1.07430 to 1.06907, saving model to best.model\n",
      "0s - loss: 1.1946 - acc: 0.3034 - val_loss: 1.0691 - val_acc: 0.3478\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 1.06907 to 1.06507, saving model to best.model\n",
      "0s - loss: 1.3295 - acc: 0.2697 - val_loss: 1.0651 - val_acc: 0.4348\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 1.06507 to 1.06196, saving model to best.model\n",
      "0s - loss: 1.1540 - acc: 0.3596 - val_loss: 1.0620 - val_acc: 0.6957\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 1.06196 to 1.05907, saving model to best.model\n",
      "0s - loss: 1.2757 - acc: 0.3483 - val_loss: 1.0591 - val_acc: 0.6087\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 1.05907 to 1.05648, saving model to best.model\n",
      "0s - loss: 1.2315 - acc: 0.3258 - val_loss: 1.0565 - val_acc: 0.4783\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 1.05648 to 1.05438, saving model to best.model\n",
      "0s - loss: 1.1622 - acc: 0.3933 - val_loss: 1.0544 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 1.05438 to 1.05255, saving model to best.model\n",
      "0s - loss: 1.1048 - acc: 0.4157 - val_loss: 1.0526 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 1.05255 to 1.05095, saving model to best.model\n",
      "0s - loss: 1.2102 - acc: 0.3034 - val_loss: 1.0509 - val_acc: 0.3913\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 1.05095 to 1.04922, saving model to best.model\n",
      "0s - loss: 1.2293 - acc: 0.3146 - val_loss: 1.0492 - val_acc: 0.3913\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 1.04922 to 1.04744, saving model to best.model\n",
      "0s - loss: 1.1408 - acc: 0.4045 - val_loss: 1.0474 - val_acc: 0.3913\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 1.04744 to 1.04571, saving model to best.model\n",
      "0s - loss: 1.1543 - acc: 0.4157 - val_loss: 1.0457 - val_acc: 0.3913\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 1.04571 to 1.04408, saving model to best.model\n",
      "0s - loss: 1.2481 - acc: 0.2809 - val_loss: 1.0441 - val_acc: 0.5217\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 1.04408 to 1.04276, saving model to best.model\n",
      "0s - loss: 1.1857 - acc: 0.3371 - val_loss: 1.0428 - val_acc: 0.5217\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 1.04276 to 1.04176, saving model to best.model\n",
      "0s - loss: 1.1157 - acc: 0.4944 - val_loss: 1.0418 - val_acc: 0.6087\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 1.04176 to 1.04071, saving model to best.model\n",
      "0s - loss: 1.0985 - acc: 0.4157 - val_loss: 1.0407 - val_acc: 0.6522\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 1.04071 to 1.03979, saving model to best.model\n",
      "0s - loss: 1.1127 - acc: 0.4607 - val_loss: 1.0398 - val_acc: 0.6957\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 1.03979 to 1.03869, saving model to best.model\n",
      "0s - loss: 1.0338 - acc: 0.4270 - val_loss: 1.0387 - val_acc: 0.6957\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 1.03869 to 1.03738, saving model to best.model\n",
      "0s - loss: 1.1311 - acc: 0.3596 - val_loss: 1.0374 - val_acc: 0.6522\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 1.03738 to 1.03568, saving model to best.model\n",
      "0s - loss: 1.2426 - acc: 0.4157 - val_loss: 1.0357 - val_acc: 0.6522\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 1.03568 to 1.03376, saving model to best.model\n",
      "0s - loss: 1.1002 - acc: 0.4382 - val_loss: 1.0338 - val_acc: 0.6957\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 1.03376 to 1.03170, saving model to best.model\n",
      "0s - loss: 1.1156 - acc: 0.4270 - val_loss: 1.0317 - val_acc: 0.6957\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 1.03170 to 1.02942, saving model to best.model\n",
      "0s - loss: 1.1183 - acc: 0.3708 - val_loss: 1.0294 - val_acc: 0.6957\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 1.02942 to 1.02680, saving model to best.model\n",
      "0s - loss: 1.1220 - acc: 0.3708 - val_loss: 1.0268 - val_acc: 0.6957\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 1.02680 to 1.02424, saving model to best.model\n",
      "0s - loss: 1.0402 - acc: 0.4494 - val_loss: 1.0242 - val_acc: 0.6087\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 1.02424 to 1.02132, saving model to best.model\n",
      "0s - loss: 1.0644 - acc: 0.4831 - val_loss: 1.0213 - val_acc: 0.6957\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 1.02132 to 1.01804, saving model to best.model\n",
      "0s - loss: 1.0741 - acc: 0.4157 - val_loss: 1.0180 - val_acc: 0.6957\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 1.01804 to 1.01496, saving model to best.model\n",
      "0s - loss: 1.1042 - acc: 0.4719 - val_loss: 1.0150 - val_acc: 0.6957\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 1.01496 to 1.01213, saving model to best.model\n",
      "0s - loss: 1.0073 - acc: 0.4607 - val_loss: 1.0121 - val_acc: 0.6957\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 1.01213 to 1.00931, saving model to best.model\n",
      "0s - loss: 1.1230 - acc: 0.4270 - val_loss: 1.0093 - val_acc: 0.6957\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 1.00931 to 1.00640, saving model to best.model\n",
      "0s - loss: 1.0436 - acc: 0.4831 - val_loss: 1.0064 - val_acc: 0.6957\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 1.00640 to 1.00364, saving model to best.model\n",
      "0s - loss: 1.0289 - acc: 0.4719 - val_loss: 1.0036 - val_acc: 0.6957\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 1.00364 to 1.00092, saving model to best.model\n",
      "0s - loss: 1.1360 - acc: 0.2472 - val_loss: 1.0009 - val_acc: 0.6957\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 1.00092 to 0.99768, saving model to best.model\n",
      "0s - loss: 1.1004 - acc: 0.3820 - val_loss: 0.9977 - val_acc: 0.6522\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.99768 to 0.99400, saving model to best.model\n",
      "0s - loss: 1.1539 - acc: 0.4270 - val_loss: 0.9940 - val_acc: 0.6522\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.99400 to 0.98968, saving model to best.model\n",
      "0s - loss: 1.0794 - acc: 0.4045 - val_loss: 0.9897 - val_acc: 0.6957\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.98968 to 0.98519, saving model to best.model\n",
      "0s - loss: 1.0881 - acc: 0.4494 - val_loss: 0.9852 - val_acc: 0.6957\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.98519 to 0.98049, saving model to best.model\n",
      "0s - loss: 1.0808 - acc: 0.4270 - val_loss: 0.9805 - val_acc: 0.6957\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.98049 to 0.97557, saving model to best.model\n",
      "0s - loss: 1.0108 - acc: 0.5056 - val_loss: 0.9756 - val_acc: 0.6957\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.97557 to 0.97062, saving model to best.model\n",
      "0s - loss: 1.0528 - acc: 0.4382 - val_loss: 0.9706 - val_acc: 0.6957\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.97062 to 0.96558, saving model to best.model\n",
      "0s - loss: 1.0962 - acc: 0.3933 - val_loss: 0.9656 - val_acc: 0.6957\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.96558 to 0.96034, saving model to best.model\n",
      "0s - loss: 1.0026 - acc: 0.4944 - val_loss: 0.9603 - val_acc: 0.6957\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.96034 to 0.95493, saving model to best.model\n",
      "0s - loss: 1.0376 - acc: 0.4607 - val_loss: 0.9549 - val_acc: 0.6957\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.95493 to 0.94951, saving model to best.model\n",
      "0s - loss: 1.0536 - acc: 0.4382 - val_loss: 0.9495 - val_acc: 0.6957\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.94951 to 0.94385, saving model to best.model\n",
      "0s - loss: 0.9821 - acc: 0.5730 - val_loss: 0.9439 - val_acc: 0.6957\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.94385 to 0.93816, saving model to best.model\n",
      "0s - loss: 0.9566 - acc: 0.5618 - val_loss: 0.9382 - val_acc: 0.6957\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.93816 to 0.93214, saving model to best.model\n",
      "0s - loss: 0.9905 - acc: 0.5281 - val_loss: 0.9321 - val_acc: 0.6957\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.93214 to 0.92624, saving model to best.model\n",
      "0s - loss: 1.0004 - acc: 0.4944 - val_loss: 0.9262 - val_acc: 0.6957\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.92624 to 0.91998, saving model to best.model\n",
      "0s - loss: 0.9800 - acc: 0.5169 - val_loss: 0.9200 - val_acc: 0.6957\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.91998 to 0.91347, saving model to best.model\n",
      "0s - loss: 1.0703 - acc: 0.4382 - val_loss: 0.9135 - val_acc: 0.6957\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.91347 to 0.90649, saving model to best.model\n",
      "0s - loss: 0.9982 - acc: 0.5056 - val_loss: 0.9065 - val_acc: 0.6957\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.90649 to 0.89926, saving model to best.model\n",
      "0s - loss: 1.0078 - acc: 0.5281 - val_loss: 0.8993 - val_acc: 0.6957\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.89926 to 0.89174, saving model to best.model\n",
      "0s - loss: 1.0229 - acc: 0.5169 - val_loss: 0.8917 - val_acc: 0.6957\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.89174 to 0.88422, saving model to best.model\n",
      "0s - loss: 0.9657 - acc: 0.5618 - val_loss: 0.8842 - val_acc: 0.6957\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.88422 to 0.87639, saving model to best.model\n",
      "0s - loss: 0.9154 - acc: 0.5618 - val_loss: 0.8764 - val_acc: 0.6957\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.87639 to 0.86842, saving model to best.model\n",
      "0s - loss: 0.9488 - acc: 0.5169 - val_loss: 0.8684 - val_acc: 0.6957\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.86842 to 0.86007, saving model to best.model\n",
      "0s - loss: 0.9636 - acc: 0.5506 - val_loss: 0.8601 - val_acc: 0.6957\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.86007 to 0.85146, saving model to best.model\n",
      "0s - loss: 0.9179 - acc: 0.5730 - val_loss: 0.8515 - val_acc: 0.6957\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.85146 to 0.84251, saving model to best.model\n",
      "0s - loss: 0.9134 - acc: 0.5843 - val_loss: 0.8425 - val_acc: 0.6957\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.84251 to 0.83294, saving model to best.model\n",
      "0s - loss: 0.8720 - acc: 0.6292 - val_loss: 0.8329 - val_acc: 0.6957\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.83294 to 0.82320, saving model to best.model\n",
      "0s - loss: 0.8601 - acc: 0.6404 - val_loss: 0.8232 - val_acc: 0.6957\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.82320 to 0.81354, saving model to best.model\n",
      "0s - loss: 0.8900 - acc: 0.5843 - val_loss: 0.8135 - val_acc: 0.6957\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.81354 to 0.80356, saving model to best.model\n",
      "0s - loss: 0.8608 - acc: 0.5843 - val_loss: 0.8036 - val_acc: 0.6957\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.80356 to 0.79375, saving model to best.model\n",
      "0s - loss: 0.8745 - acc: 0.5955 - val_loss: 0.7937 - val_acc: 0.6957\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.79375 to 0.78323, saving model to best.model\n",
      "0s - loss: 0.8286 - acc: 0.6292 - val_loss: 0.7832 - val_acc: 0.6957\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.78323 to 0.77292, saving model to best.model\n",
      "0s - loss: 0.8718 - acc: 0.6292 - val_loss: 0.7729 - val_acc: 0.6957\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.77292 to 0.76238, saving model to best.model\n",
      "0s - loss: 0.8166 - acc: 0.6067 - val_loss: 0.7624 - val_acc: 0.6957\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.76238 to 0.75155, saving model to best.model\n",
      "0s - loss: 0.8357 - acc: 0.5955 - val_loss: 0.7515 - val_acc: 0.6957\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.75155 to 0.74033, saving model to best.model\n",
      "0s - loss: 0.7968 - acc: 0.7079 - val_loss: 0.7403 - val_acc: 0.6957\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.74033 to 0.72904, saving model to best.model\n",
      "0s - loss: 0.7724 - acc: 0.6854 - val_loss: 0.7290 - val_acc: 0.6957\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.72904 to 0.71799, saving model to best.model\n",
      "0s - loss: 0.8196 - acc: 0.6067 - val_loss: 0.7180 - val_acc: 0.6957\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.71799 to 0.70643, saving model to best.model\n",
      "0s - loss: 0.7664 - acc: 0.6629 - val_loss: 0.7064 - val_acc: 0.7391\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.70643 to 0.69483, saving model to best.model\n",
      "0s - loss: 0.7891 - acc: 0.6517 - val_loss: 0.6948 - val_acc: 0.7391\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69483 to 0.68353, saving model to best.model\n",
      "0s - loss: 0.7790 - acc: 0.6742 - val_loss: 0.6835 - val_acc: 0.7391\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68353 to 0.67226, saving model to best.model\n",
      "0s - loss: 0.7081 - acc: 0.7079 - val_loss: 0.6723 - val_acc: 0.7826\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67226 to 0.66125, saving model to best.model\n",
      "0s - loss: 0.7557 - acc: 0.6517 - val_loss: 0.6613 - val_acc: 0.7826\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.66125 to 0.65086, saving model to best.model\n",
      "0s - loss: 0.7031 - acc: 0.6854 - val_loss: 0.6509 - val_acc: 0.8261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.65086 to 0.64083, saving model to best.model\n",
      "0s - loss: 0.7188 - acc: 0.6629 - val_loss: 0.6408 - val_acc: 0.7826\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.64083 to 0.63137, saving model to best.model\n",
      "0s - loss: 0.6723 - acc: 0.7753 - val_loss: 0.6314 - val_acc: 0.7826\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.63137 to 0.62146, saving model to best.model\n",
      "0s - loss: 0.7158 - acc: 0.7079 - val_loss: 0.6215 - val_acc: 0.7826\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.62146 to 0.61183, saving model to best.model\n",
      "0s - loss: 0.6331 - acc: 0.7079 - val_loss: 0.6118 - val_acc: 0.7826\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.61183 to 0.60267, saving model to best.model\n",
      "0s - loss: 0.6806 - acc: 0.7191 - val_loss: 0.6027 - val_acc: 0.8261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.60267 to 0.59431, saving model to best.model\n",
      "0s - loss: 0.6819 - acc: 0.7191 - val_loss: 0.5943 - val_acc: 0.8261\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.59431 to 0.58589, saving model to best.model\n",
      "0s - loss: 0.6439 - acc: 0.7303 - val_loss: 0.5859 - val_acc: 0.8261\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.58589 to 0.57779, saving model to best.model\n",
      "0s - loss: 0.6509 - acc: 0.7303 - val_loss: 0.5778 - val_acc: 0.8261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.57779 to 0.57003, saving model to best.model\n",
      "0s - loss: 0.6382 - acc: 0.7191 - val_loss: 0.5700 - val_acc: 0.8261\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.57003 to 0.56217, saving model to best.model\n",
      "0s - loss: 0.6008 - acc: 0.7640 - val_loss: 0.5622 - val_acc: 0.8261\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.56217 to 0.55460, saving model to best.model\n",
      "0s - loss: 0.5246 - acc: 0.8202 - val_loss: 0.5546 - val_acc: 0.8261\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.55460 to 0.54651, saving model to best.model\n",
      "0s - loss: 0.5934 - acc: 0.6966 - val_loss: 0.5465 - val_acc: 0.8261\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.54651 to 0.53853, saving model to best.model\n",
      "0s - loss: 0.6123 - acc: 0.7303 - val_loss: 0.5385 - val_acc: 0.8261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.53853 to 0.53023, saving model to best.model\n",
      "0s - loss: 0.6169 - acc: 0.7079 - val_loss: 0.5302 - val_acc: 0.8261\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.53023 to 0.52225, saving model to best.model\n",
      "0s - loss: 0.5157 - acc: 0.8427 - val_loss: 0.5222 - val_acc: 0.8261\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.52225 to 0.51418, saving model to best.model\n",
      "0s - loss: 0.5353 - acc: 0.7528 - val_loss: 0.5142 - val_acc: 0.8261\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.51418 to 0.50565, saving model to best.model\n",
      "0s - loss: 0.5764 - acc: 0.7753 - val_loss: 0.5057 - val_acc: 0.8261\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.50565 to 0.49720, saving model to best.model\n",
      "0s - loss: 0.5073 - acc: 0.8090 - val_loss: 0.4972 - val_acc: 0.8261\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.49720 to 0.48886, saving model to best.model\n",
      "0s - loss: 0.5241 - acc: 0.8090 - val_loss: 0.4889 - val_acc: 0.8261\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.48886 to 0.47954, saving model to best.model\n",
      "0s - loss: 0.4832 - acc: 0.8202 - val_loss: 0.4795 - val_acc: 0.8261\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.47954 to 0.47098, saving model to best.model\n",
      "0s - loss: 0.4617 - acc: 0.8202 - val_loss: 0.4710 - val_acc: 0.8696\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.47098 to 0.46364, saving model to best.model\n",
      "0s - loss: 0.4835 - acc: 0.7978 - val_loss: 0.4636 - val_acc: 0.8696\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.46364 to 0.45649, saving model to best.model\n",
      "0s - loss: 0.5034 - acc: 0.8427 - val_loss: 0.4565 - val_acc: 0.8696\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.45649 to 0.44938, saving model to best.model\n",
      "0s - loss: 0.5214 - acc: 0.7528 - val_loss: 0.4494 - val_acc: 0.8696\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.44938 to 0.44265, saving model to best.model\n",
      "0s - loss: 0.4445 - acc: 0.8539 - val_loss: 0.4427 - val_acc: 0.9130\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.44265 to 0.43619, saving model to best.model\n",
      "0s - loss: 0.5042 - acc: 0.7640 - val_loss: 0.4362 - val_acc: 0.9130\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.43619 to 0.43073, saving model to best.model\n",
      "0s - loss: 0.4184 - acc: 0.8539 - val_loss: 0.4307 - val_acc: 0.9130\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.43073 to 0.42568, saving model to best.model\n",
      "0s - loss: 0.4610 - acc: 0.8315 - val_loss: 0.4257 - val_acc: 0.9130\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.42568 to 0.42169, saving model to best.model\n",
      "0s - loss: 0.4213 - acc: 0.8427 - val_loss: 0.4217 - val_acc: 0.9130\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.42169 to 0.41952, saving model to best.model\n",
      "0s - loss: 0.4663 - acc: 0.8539 - val_loss: 0.4195 - val_acc: 0.9130\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.41952 to 0.41731, saving model to best.model\n",
      "0s - loss: 0.4236 - acc: 0.8652 - val_loss: 0.4173 - val_acc: 0.9130\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.41731 to 0.41446, saving model to best.model\n",
      "0s - loss: 0.4780 - acc: 0.7865 - val_loss: 0.4145 - val_acc: 0.9130\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.41446 to 0.41076, saving model to best.model\n",
      "0s - loss: 0.3663 - acc: 0.8876 - val_loss: 0.4108 - val_acc: 0.9130\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.41076 to 0.40714, saving model to best.model\n",
      "0s - loss: 0.4178 - acc: 0.8539 - val_loss: 0.4071 - val_acc: 0.9130\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.40714 to 0.40356, saving model to best.model\n",
      "0s - loss: 0.4498 - acc: 0.8315 - val_loss: 0.4036 - val_acc: 0.9130\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.40356 to 0.39947, saving model to best.model\n",
      "0s - loss: 0.4406 - acc: 0.8315 - val_loss: 0.3995 - val_acc: 0.9130\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.39947 to 0.39557, saving model to best.model\n",
      "0s - loss: 0.3699 - acc: 0.8876 - val_loss: 0.3956 - val_acc: 0.9130\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.39557 to 0.39124, saving model to best.model\n",
      "0s - loss: 0.3509 - acc: 0.8989 - val_loss: 0.3912 - val_acc: 0.8696\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.39124 to 0.38552, saving model to best.model\n",
      "0s - loss: 0.3921 - acc: 0.8652 - val_loss: 0.3855 - val_acc: 0.9130\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.38552 to 0.38027, saving model to best.model\n",
      "0s - loss: 0.4144 - acc: 0.8764 - val_loss: 0.3803 - val_acc: 0.9130\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.38027 to 0.37502, saving model to best.model\n",
      "0s - loss: 0.3273 - acc: 0.9101 - val_loss: 0.3750 - val_acc: 0.9130\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.37502 to 0.36977, saving model to best.model\n",
      "0s - loss: 0.3302 - acc: 0.9101 - val_loss: 0.3698 - val_acc: 0.9130\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.36977 to 0.36424, saving model to best.model\n",
      "0s - loss: 0.3571 - acc: 0.8652 - val_loss: 0.3642 - val_acc: 0.9130\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.36424 to 0.35855, saving model to best.model\n",
      "0s - loss: 0.3176 - acc: 0.8876 - val_loss: 0.3585 - val_acc: 0.9130\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.35855 to 0.35295, saving model to best.model\n",
      "0s - loss: 0.3395 - acc: 0.9101 - val_loss: 0.3529 - val_acc: 0.9130\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.35295 to 0.34764, saving model to best.model\n",
      "0s - loss: 0.3359 - acc: 0.8539 - val_loss: 0.3476 - val_acc: 0.9130\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.34764 to 0.34287, saving model to best.model\n",
      "0s - loss: 0.3155 - acc: 0.8876 - val_loss: 0.3429 - val_acc: 0.9130\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.34287 to 0.33881, saving model to best.model\n",
      "0s - loss: 0.2906 - acc: 0.9213 - val_loss: 0.3388 - val_acc: 0.9130\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.33881 to 0.33398, saving model to best.model\n",
      "0s - loss: 0.3044 - acc: 0.8652 - val_loss: 0.3340 - val_acc: 0.9130\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.33398 to 0.32842, saving model to best.model\n",
      "0s - loss: 0.2800 - acc: 0.9101 - val_loss: 0.3284 - val_acc: 0.9130\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.32842 to 0.32266, saving model to best.model\n",
      "0s - loss: 0.3035 - acc: 0.8876 - val_loss: 0.3227 - val_acc: 0.9130\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.32266 to 0.31716, saving model to best.model\n",
      "0s - loss: 0.2559 - acc: 0.9326 - val_loss: 0.3172 - val_acc: 0.9130\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.31716 to 0.31256, saving model to best.model\n",
      "0s - loss: 0.2976 - acc: 0.8764 - val_loss: 0.3126 - val_acc: 0.9130\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.31256 to 0.30694, saving model to best.model\n",
      "0s - loss: 0.2618 - acc: 0.9438 - val_loss: 0.3069 - val_acc: 0.9130\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.30694 to 0.30240, saving model to best.model\n",
      "0s - loss: 0.2899 - acc: 0.8989 - val_loss: 0.3024 - val_acc: 0.9130\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.30240 to 0.29930, saving model to best.model\n",
      "0s - loss: 0.3337 - acc: 0.8876 - val_loss: 0.2993 - val_acc: 0.9130\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.29930 to 0.29622, saving model to best.model\n",
      "0s - loss: 0.2717 - acc: 0.8989 - val_loss: 0.2962 - val_acc: 0.9130\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.29622 to 0.29336, saving model to best.model\n",
      "0s - loss: 0.2804 - acc: 0.8989 - val_loss: 0.2934 - val_acc: 0.9130\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.29336 to 0.29032, saving model to best.model\n",
      "0s - loss: 0.2614 - acc: 0.9213 - val_loss: 0.2903 - val_acc: 0.9130\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.29032 to 0.28707, saving model to best.model\n",
      "0s - loss: 0.2991 - acc: 0.8652 - val_loss: 0.2871 - val_acc: 0.9130\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.28707 to 0.28544, saving model to best.model\n",
      "0s - loss: 0.2809 - acc: 0.8764 - val_loss: 0.2854 - val_acc: 0.9130\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.28544 to 0.28313, saving model to best.model\n",
      "0s - loss: 0.2936 - acc: 0.8989 - val_loss: 0.2831 - val_acc: 0.9130\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.28313 to 0.28116, saving model to best.model\n",
      "0s - loss: 0.2523 - acc: 0.9101 - val_loss: 0.2812 - val_acc: 0.9130\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.28116 to 0.27864, saving model to best.model\n",
      "0s - loss: 0.2608 - acc: 0.8876 - val_loss: 0.2786 - val_acc: 0.9130\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.27864 to 0.27665, saving model to best.model\n",
      "0s - loss: 0.2592 - acc: 0.9438 - val_loss: 0.2766 - val_acc: 0.9130\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.27665 to 0.27429, saving model to best.model\n",
      "0s - loss: 0.2073 - acc: 0.9326 - val_loss: 0.2743 - val_acc: 0.9130\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.27429 to 0.27164, saving model to best.model\n",
      "0s - loss: 0.2211 - acc: 0.9551 - val_loss: 0.2716 - val_acc: 0.9130\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.27164 to 0.26725, saving model to best.model\n",
      "0s - loss: 0.2097 - acc: 0.9438 - val_loss: 0.2672 - val_acc: 0.9130\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.26725 to 0.26308, saving model to best.model\n",
      "0s - loss: 0.1886 - acc: 0.9326 - val_loss: 0.2631 - val_acc: 0.9130\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.26308 to 0.25800, saving model to best.model\n",
      "0s - loss: 0.2309 - acc: 0.9326 - val_loss: 0.2580 - val_acc: 0.9130\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.25800 to 0.25331, saving model to best.model\n",
      "0s - loss: 0.1584 - acc: 0.9663 - val_loss: 0.2533 - val_acc: 0.9130\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.25331 to 0.24875, saving model to best.model\n",
      "0s - loss: 0.1683 - acc: 0.9663 - val_loss: 0.2488 - val_acc: 0.9130\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.24875 to 0.24507, saving model to best.model\n",
      "0s - loss: 0.1673 - acc: 0.9438 - val_loss: 0.2451 - val_acc: 0.9130\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.24507 to 0.24139, saving model to best.model\n",
      "0s - loss: 0.1465 - acc: 0.9775 - val_loss: 0.2414 - val_acc: 0.9130\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.24139 to 0.23744, saving model to best.model\n",
      "0s - loss: 0.1884 - acc: 0.9775 - val_loss: 0.2374 - val_acc: 0.9130\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.23744 to 0.23383, saving model to best.model\n",
      "0s - loss: 0.1942 - acc: 0.9438 - val_loss: 0.2338 - val_acc: 0.9130\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.23383 to 0.22985, saving model to best.model\n",
      "0s - loss: 0.1625 - acc: 0.9663 - val_loss: 0.2298 - val_acc: 0.9130\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.22985 to 0.22569, saving model to best.model\n",
      "0s - loss: 0.2036 - acc: 0.9213 - val_loss: 0.2257 - val_acc: 0.9130\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.22569 to 0.22125, saving model to best.model\n",
      "0s - loss: 0.1927 - acc: 0.9326 - val_loss: 0.2213 - val_acc: 0.9130\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.22125 to 0.21684, saving model to best.model\n",
      "0s - loss: 0.1498 - acc: 0.9438 - val_loss: 0.2168 - val_acc: 0.9130\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.21684 to 0.21286, saving model to best.model\n",
      "0s - loss: 0.1580 - acc: 0.9888 - val_loss: 0.2129 - val_acc: 0.9130\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.21286 to 0.20961, saving model to best.model\n",
      "0s - loss: 0.1648 - acc: 0.9663 - val_loss: 0.2096 - val_acc: 0.9565\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.20961 to 0.20581, saving model to best.model\n",
      "0s - loss: 0.1791 - acc: 0.9663 - val_loss: 0.2058 - val_acc: 0.9565\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.20581 to 0.20229, saving model to best.model\n",
      "0s - loss: 0.1608 - acc: 0.9663 - val_loss: 0.2023 - val_acc: 0.9565\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.20229 to 0.19991, saving model to best.model\n",
      "0s - loss: 0.1387 - acc: 0.9775 - val_loss: 0.1999 - val_acc: 0.9565\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.19991 to 0.19876, saving model to best.model\n",
      "0s - loss: 0.2025 - acc: 0.9326 - val_loss: 0.1988 - val_acc: 0.9565\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.19876 to 0.19750, saving model to best.model\n",
      "0s - loss: 0.1416 - acc: 0.9551 - val_loss: 0.1975 - val_acc: 0.9565\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.19750 to 0.19619, saving model to best.model\n",
      "0s - loss: 0.1392 - acc: 0.9551 - val_loss: 0.1962 - val_acc: 0.9565\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.19619 to 0.19442, saving model to best.model\n",
      "0s - loss: 0.1749 - acc: 0.9213 - val_loss: 0.1944 - val_acc: 0.9565\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.19442 to 0.19185, saving model to best.model\n",
      "0s - loss: 0.1431 - acc: 0.9663 - val_loss: 0.1918 - val_acc: 0.9565\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.19185 to 0.18965, saving model to best.model\n",
      "0s - loss: 0.1448 - acc: 0.9775 - val_loss: 0.1896 - val_acc: 0.9565\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.18965 to 0.18630, saving model to best.model\n",
      "0s - loss: 0.1566 - acc: 0.9438 - val_loss: 0.1863 - val_acc: 0.9565\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.18630 to 0.18347, saving model to best.model\n",
      "0s - loss: 0.1421 - acc: 0.9663 - val_loss: 0.1835 - val_acc: 0.9565\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.18347 to 0.18038, saving model to best.model\n",
      "0s - loss: 0.1558 - acc: 0.9663 - val_loss: 0.1804 - val_acc: 0.9565\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.18038 to 0.17838, saving model to best.model\n",
      "0s - loss: 0.1349 - acc: 0.9663 - val_loss: 0.1784 - val_acc: 0.9565\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.17838 to 0.17754, saving model to best.model\n",
      "0s - loss: 0.1124 - acc: 0.9888 - val_loss: 0.1775 - val_acc: 0.9565\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.17754 to 0.17713, saving model to best.model\n",
      "0s - loss: 0.1840 - acc: 0.9213 - val_loss: 0.1771 - val_acc: 0.9565\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.17713 to 0.17630, saving model to best.model\n",
      "0s - loss: 0.1443 - acc: 0.9551 - val_loss: 0.1763 - val_acc: 0.9565\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.17630 to 0.17495, saving model to best.model\n",
      "0s - loss: 0.1563 - acc: 0.9438 - val_loss: 0.1750 - val_acc: 0.9565\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.17495 to 0.17442, saving model to best.model\n",
      "0s - loss: 0.0957 - acc: 0.9888 - val_loss: 0.1744 - val_acc: 0.9565\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.17442 to 0.17401, saving model to best.model\n",
      "0s - loss: 0.0721 - acc: 1.0000 - val_loss: 0.1740 - val_acc: 0.9565\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.1148 - acc: 0.9663 - val_loss: 0.1741 - val_acc: 0.9565\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.0999 - acc: 0.9775 - val_loss: 0.1741 - val_acc: 0.9565\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.17401 to 0.17336, saving model to best.model\n",
      "0s - loss: 0.2025 - acc: 0.9101 - val_loss: 0.1734 - val_acc: 0.9565\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.1151 - acc: 0.9663 - val_loss: 0.1736 - val_acc: 0.9565\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.1102 - acc: 0.9663 - val_loss: 0.1752 - val_acc: 0.9565\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.1204 - acc: 0.9663 - val_loss: 0.1769 - val_acc: 0.9565\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.0972 - acc: 0.9888 - val_loss: 0.1789 - val_acc: 0.9565\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.1244 - acc: 0.9551 - val_loss: 0.1816 - val_acc: 0.9565\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.0993 - acc: 0.9663 - val_loss: 0.1836 - val_acc: 0.9565\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.1074 - acc: 0.9551 - val_loss: 0.1859 - val_acc: 0.9565\n",
      "Train on 89 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.11765, saving model to best.model\n",
      "0s - loss: 1.5929 - acc: 0.3258 - val_loss: 1.1176 - val_acc: 0.4783\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.11765 to 1.06441, saving model to best.model\n",
      "0s - loss: 1.5814 - acc: 0.3146 - val_loss: 1.0644 - val_acc: 0.4783\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.06441 to 1.05554, saving model to best.model\n",
      "0s - loss: 1.2802 - acc: 0.3258 - val_loss: 1.0555 - val_acc: 0.4783\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 1.3888 - acc: 0.3146 - val_loss: 1.0837 - val_acc: 0.2174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 1.3831 - acc: 0.2921 - val_loss: 1.1366 - val_acc: 0.2609\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 1.2600 - acc: 0.3708 - val_loss: 1.2001 - val_acc: 0.2609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 1.1420 - acc: 0.3708 - val_loss: 1.2649 - val_acc: 0.2609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 1.2432 - acc: 0.3034 - val_loss: 1.3151 - val_acc: 0.2609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 1.3788 - acc: 0.3146 - val_loss: 1.3543 - val_acc: 0.2609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 1.2430 - acc: 0.4382 - val_loss: 1.3796 - val_acc: 0.2609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 1.2474 - acc: 0.4382 - val_loss: 1.3894 - val_acc: 0.2609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 1.2673 - acc: 0.3820 - val_loss: 1.3838 - val_acc: 0.2609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 1.1412 - acc: 0.4382 - val_loss: 1.3673 - val_acc: 0.2609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 1.2671 - acc: 0.3933 - val_loss: 1.3429 - val_acc: 0.2609\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 1.2243 - acc: 0.3596 - val_loss: 1.3132 - val_acc: 0.2609\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 1.2766 - acc: 0.3483 - val_loss: 1.2804 - val_acc: 0.2609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 1.2348 - acc: 0.3483 - val_loss: 1.2455 - val_acc: 0.2609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 1.1811 - acc: 0.4494 - val_loss: 1.2130 - val_acc: 0.2609\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 1.1191 - acc: 0.3933 - val_loss: 1.1827 - val_acc: 0.2609\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 1.1986 - acc: 0.3933 - val_loss: 1.1571 - val_acc: 0.2609\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 1.1731 - acc: 0.4157 - val_loss: 1.1346 - val_acc: 0.2609\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 1.1376 - acc: 0.3933 - val_loss: 1.1167 - val_acc: 0.2609\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 1.1724 - acc: 0.3708 - val_loss: 1.0998 - val_acc: 0.2609\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 1.1657 - acc: 0.3933 - val_loss: 1.0857 - val_acc: 0.2609\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 1.1287 - acc: 0.3708 - val_loss: 1.0747 - val_acc: 0.2609\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 1.1525 - acc: 0.4382 - val_loss: 1.0665 - val_acc: 0.2609\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 1.1433 - acc: 0.3596 - val_loss: 1.0614 - val_acc: 0.2609\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 1.2428 - acc: 0.3483 - val_loss: 1.0580 - val_acc: 0.2609\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 1.1474 - acc: 0.4045 - val_loss: 1.0561 - val_acc: 0.2609\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for i in range(50):\n",
    "    y_pred=train_nn_simple(train,X_val,y_val)\n",
    "    result.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result=np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1,\n",
       "       2, 2, 2, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=[]\n",
    "for i in range(45):\n",
    "    m=result[:,i].tolist()\n",
    "    if m.count(1)>m.count(2) and m.count(1)>m.count(3):\n",
    "        pred.append(1)\n",
    "    elif m.count(2)>m.count(1) and m.count(2)>m.count(3):\n",
    "        pred.append(2)\n",
    "    else:\n",
    "        pred.append(3)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 13]\n",
      " [ 0  0 23]\n",
      " [ 0  0  9]]\n",
      "20.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00        23\n",
      "          3       0.20      1.00      0.33         9\n",
      "\n",
      "avg / total       0.04      0.20      0.07        45\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(y_val, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_val, y_pred) * 100) \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4AAAAIoCAYAAAA4DIVgAAAAAXNSR0IArs4c6QAAQABJREFUeAHs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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "from IPython.display import Image  \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=X_train.columns, \n",
    "                         class_names=['0','1','2'],\n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
