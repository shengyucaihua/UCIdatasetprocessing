{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_list=['a1','a2','a3','a4','a5','a6',\n",
    "          'b1','b2','b3','b4','b5','b6',\n",
    "           'c1','c2','c3','c4','c5','c6',\n",
    "           'd1','d2','d3','d4','d5','d6',\n",
    "           'e1','e2','e3','e4','e5','e6',\n",
    "           'f1','f2','f3','f4','f5','f6',\n",
    "           'g1','g2','g3','g4','g5','g6','Class']\n",
    "train=pd.read_csv(\"/Users/bruce/Downloads/connect-4.data\",names=name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>a6</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>...</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>g5</th>\n",
       "      <th>g6</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>...</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  a1 a2 a3 a4 a5 a6 b1 b2 b3 b4  ...  f4 f5 f6 g1 g2 g3 g4 g5 g6 Class\n",
       "0  b  b  b  b  b  b  b  b  b  b  ...   b  b  b  b  b  b  b  b  b   win\n",
       "1  b  b  b  b  b  b  b  b  b  b  ...   b  b  b  b  b  b  b  b  b   win\n",
       "2  b  b  b  b  b  b  o  b  b  b  ...   b  b  b  b  b  b  b  b  b   win\n",
       "3  b  b  b  b  b  b  b  b  b  b  ...   b  b  b  b  b  b  b  b  b   win\n",
       "4  o  b  b  b  b  b  b  b  b  b  ...   b  b  b  b  b  b  b  b  b   win\n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in train.columns:\n",
    "    train[i]=le.fit_transform(train[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>a6</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>...</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>g5</th>\n",
       "      <th>g6</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a1  a2  a3  a4  a5  a6  b1  b2  b3  b4  ...    f4  f5  f6  g1  g2  g3  g4  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0  ...     0   0   0   0   0   0   0   \n",
       "1   0   0   0   0   0   0   0   0   0   0  ...     0   0   0   0   0   0   0   \n",
       "2   0   0   0   0   0   0   1   0   0   0  ...     0   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0   0   0   0   0   0  ...     0   0   0   0   0   0   0   \n",
       "4   1   0   0   0   0   0   0   0   0   0  ...     0   0   0   0   0   0   0   \n",
       "\n",
       "   g5  g6  Class  \n",
       "0   0   0      2  \n",
       "1   0   0      2  \n",
       "2   0   0      2  \n",
       "3   0   0      2  \n",
       "4   0   0      2  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>a6</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>...</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>g5</th>\n",
       "      <th>g6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a1  a2  a3  a4  a5  a6  b1  b2  b3  b4 ...  f3  f4  f5  f6  g1  g2  g3  g4  \\\n",
       "0   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   0   \n",
       "1   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   0   \n",
       "2   0   0   0   0   0   0   1   0   0   0 ...   0   0   0   0   0   0   0   0   \n",
       "3   0   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   0   \n",
       "4   1   0   0   0   0   0   0   0   0   0 ...   0   0   0   0   0   0   0   0   \n",
       "\n",
       "   g5  g6  \n",
       "0   0   0  \n",
       "1   0   0  \n",
       "2   0   0  \n",
       "3   0   0  \n",
       "4   0   0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=train.drop(\"Class\",axis=1)\n",
    "outcomes=train[\"Class\"].values\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, outcomes, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranom Forest         75.72 (+/-) 0.75 \n"
     ]
    }
   ],
   "source": [
    "model=RandomForestClassifier(n_estimators=5)\n",
    "kfold = KFold(n_splits=10, random_state=0)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "results=[\"Ranom Forest\",cv_result.mean(),cv_result.std()]\n",
    "\n",
    "print('{:20s} {:2.2f} (+/-) {:2.2f} '.format(results[0] , results[1] * 100, results[2] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0  1550]\n",
      " [    0    20  4172]\n",
      " [    0     0 11148]]\n",
      "66.1219656602\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1550\n",
      "          1       1.00      0.00      0.01      4192\n",
      "          2       0.66      1.00      0.80     11148\n",
      "\n",
      "avg / total       0.68      0.66      0.53     16890\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=100,max_features='auto',bootstrap=True,oob_score=True,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0   155  1395]\n",
      " [    0   912  3280]\n",
      " [    0   601 10547]]\n",
      "67.8448786264\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1550\n",
      "          1       0.55      0.22      0.31      4192\n",
      "          2       0.69      0.95      0.80     11148\n",
      "\n",
      "avg / total       0.59      0.68      0.61     16890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=1,max_features=None,bootstrap=False,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  278   422   850]\n",
      " [  135  3262   795]\n",
      " [   84   480 10584]]\n",
      "83.6234458259\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.18      0.27      1550\n",
      "          1       0.78      0.78      0.78      4192\n",
      "          2       0.87      0.95      0.91     11148\n",
      "\n",
      "avg / total       0.82      0.84      0.82     16890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=500,num_boost_round=1,max_depth=6,subsample=0.632,colsample_bytree=0.4)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1   212  1337]\n",
      " [    3  1346  2843]\n",
      " [    0   716 10432]]\n",
      "69.739490823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.00      0.00      1550\n",
      "          1       0.59      0.32      0.42      4192\n",
      "          2       0.71      0.94      0.81     11148\n",
      "\n",
      "avg / total       0.64      0.70      0.64     16890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=1,num_boost_round=1,max_depth=6,subsample=1,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  348   438   764]\n",
      " [  172  3353   667]\n",
      " [  126   433 10589]]\n",
      "84.6062759029\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.22      0.32      1550\n",
      "          1       0.79      0.80      0.80      4192\n",
      "          2       0.88      0.95      0.91     11148\n",
      "\n",
      "avg / total       0.83      0.85      0.83     16890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=500,num_boost_round=1,max_depth=6,subsample=0.632,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History\n",
    "from keras.layers import Dense, Activation, Dropout, Input\n",
    "from keras import optimizers\n",
    "history=History()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standard feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(len(np.unique(Y_train)), activation='softmax'))\n",
    "    \n",
    "m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45600 samples, validate on 5067 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.85618, saving model to best.model\n",
      "1s - loss: 0.8987 - acc: 0.6409 - val_loss: 0.8562 - val_acc: 0.6493\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8562 - acc: 0.6587 - val_loss: 0.8568 - val_acc: 0.6493\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.85618 to 0.85362, saving model to best.model\n",
      "1s - loss: 0.8511 - acc: 0.6587 - val_loss: 0.8536 - val_acc: 0.6493\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.85362 to 0.84547, saving model to best.model\n",
      "1s - loss: 0.8443 - acc: 0.6587 - val_loss: 0.8455 - val_acc: 0.6493\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84547 to 0.84110, saving model to best.model\n",
      "1s - loss: 0.8371 - acc: 0.6587 - val_loss: 0.8411 - val_acc: 0.6493\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.84110 to 0.83885, saving model to best.model\n",
      "1s - loss: 0.8344 - acc: 0.6586 - val_loss: 0.8389 - val_acc: 0.6493\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83885 to 0.83839, saving model to best.model\n",
      "1s - loss: 0.8310 - acc: 0.6587 - val_loss: 0.8384 - val_acc: 0.6493\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "1s - loss: 0.8306 - acc: 0.6587 - val_loss: 0.8397 - val_acc: 0.6493\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.83839 to 0.83735, saving model to best.model\n",
      "1s - loss: 0.8288 - acc: 0.6587 - val_loss: 0.8374 - val_acc: 0.6493\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.83735 to 0.83710, saving model to best.model\n",
      "1s - loss: 0.8263 - acc: 0.6586 - val_loss: 0.8371 - val_acc: 0.6493\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8280 - acc: 0.6586 - val_loss: 0.8372 - val_acc: 0.6493\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.83710 to 0.83605, saving model to best.model\n",
      "1s - loss: 0.8263 - acc: 0.6586 - val_loss: 0.8360 - val_acc: 0.6493\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.83605 to 0.83562, saving model to best.model\n",
      "1s - loss: 0.8258 - acc: 0.6587 - val_loss: 0.8356 - val_acc: 0.6493\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.83562 to 0.83459, saving model to best.model\n",
      "1s - loss: 0.8236 - acc: 0.6586 - val_loss: 0.8346 - val_acc: 0.6493\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.83459 to 0.83395, saving model to best.model\n",
      "1s - loss: 0.8240 - acc: 0.6587 - val_loss: 0.8340 - val_acc: 0.6493\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.83395 to 0.83269, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6584 - val_loss: 0.8327 - val_acc: 0.6493\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.83269 to 0.83204, saving model to best.model\n",
      "1s - loss: 0.8212 - acc: 0.6588 - val_loss: 0.8320 - val_acc: 0.6493\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.83204 to 0.83038, saving model to best.model\n",
      "1s - loss: 0.8202 - acc: 0.6587 - val_loss: 0.8304 - val_acc: 0.6493\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.83038 to 0.82859, saving model to best.model\n",
      "1s - loss: 0.8190 - acc: 0.6595 - val_loss: 0.8286 - val_acc: 0.6499\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82859 to 0.82803, saving model to best.model\n",
      "1s - loss: 0.8172 - acc: 0.6603 - val_loss: 0.8280 - val_acc: 0.6501\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82803 to 0.82507, saving model to best.model\n",
      "1s - loss: 0.8162 - acc: 0.6601 - val_loss: 0.8251 - val_acc: 0.6562\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82507 to 0.82308, saving model to best.model\n",
      "1s - loss: 0.8148 - acc: 0.6603 - val_loss: 0.8231 - val_acc: 0.6582\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.82308 to 0.82159, saving model to best.model\n",
      "1s - loss: 0.8142 - acc: 0.6630 - val_loss: 0.8216 - val_acc: 0.6578\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.82159 to 0.82025, saving model to best.model\n",
      "1s - loss: 0.8125 - acc: 0.6626 - val_loss: 0.8202 - val_acc: 0.6594\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.82025 to 0.81750, saving model to best.model\n",
      "1s - loss: 0.8102 - acc: 0.6632 - val_loss: 0.8175 - val_acc: 0.6602\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81750 to 0.81441, saving model to best.model\n",
      "1s - loss: 0.8085 - acc: 0.6646 - val_loss: 0.8144 - val_acc: 0.6607\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81441 to 0.81247, saving model to best.model\n",
      "1s - loss: 0.8067 - acc: 0.6640 - val_loss: 0.8125 - val_acc: 0.6602\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81247 to 0.81102, saving model to best.model\n",
      "1s - loss: 0.8052 - acc: 0.6654 - val_loss: 0.8110 - val_acc: 0.6596\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81102 to 0.81019, saving model to best.model\n",
      "1s - loss: 0.8036 - acc: 0.6656 - val_loss: 0.8102 - val_acc: 0.6596\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81019 to 0.80591, saving model to best.model\n",
      "1s - loss: 0.8021 - acc: 0.6651 - val_loss: 0.8059 - val_acc: 0.6635\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80591 to 0.80402, saving model to best.model\n",
      "1s - loss: 0.8001 - acc: 0.6680 - val_loss: 0.8040 - val_acc: 0.6619\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80402 to 0.79966, saving model to best.model\n",
      "1s - loss: 0.7983 - acc: 0.6680 - val_loss: 0.7997 - val_acc: 0.6643\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79966 to 0.79838, saving model to best.model\n",
      "1s - loss: 0.7968 - acc: 0.6671 - val_loss: 0.7984 - val_acc: 0.6661\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79838 to 0.79628, saving model to best.model\n",
      "1s - loss: 0.7945 - acc: 0.6707 - val_loss: 0.7963 - val_acc: 0.6651\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79628 to 0.79454, saving model to best.model\n",
      "1s - loss: 0.7940 - acc: 0.6702 - val_loss: 0.7945 - val_acc: 0.6682\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79454 to 0.79020, saving model to best.model\n",
      "1s - loss: 0.7902 - acc: 0.6720 - val_loss: 0.7902 - val_acc: 0.6728\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79020 to 0.78676, saving model to best.model\n",
      "1s - loss: 0.7889 - acc: 0.6728 - val_loss: 0.7868 - val_acc: 0.6730\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78676 to 0.78156, saving model to best.model\n",
      "1s - loss: 0.7852 - acc: 0.6749 - val_loss: 0.7816 - val_acc: 0.6757\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78156 to 0.78024, saving model to best.model\n",
      "1s - loss: 0.7841 - acc: 0.6750 - val_loss: 0.7802 - val_acc: 0.6759\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78024 to 0.77907, saving model to best.model\n",
      "1s - loss: 0.7834 - acc: 0.6752 - val_loss: 0.7791 - val_acc: 0.6769\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77907 to 0.77538, saving model to best.model\n",
      "1s - loss: 0.7794 - acc: 0.6755 - val_loss: 0.7754 - val_acc: 0.6813\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77538 to 0.77065, saving model to best.model\n",
      "1s - loss: 0.7784 - acc: 0.6771 - val_loss: 0.7706 - val_acc: 0.6823\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77065 to 0.76879, saving model to best.model\n",
      "1s - loss: 0.7774 - acc: 0.6767 - val_loss: 0.7688 - val_acc: 0.6811\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.76879 to 0.76402, saving model to best.model\n",
      "1s - loss: 0.7736 - acc: 0.6786 - val_loss: 0.7640 - val_acc: 0.6830\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76402 to 0.76092, saving model to best.model\n",
      "1s - loss: 0.7722 - acc: 0.6796 - val_loss: 0.7609 - val_acc: 0.6856\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76092 to 0.75816, saving model to best.model\n",
      "1s - loss: 0.7707 - acc: 0.6788 - val_loss: 0.7582 - val_acc: 0.6868\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.75816 to 0.75145, saving model to best.model\n",
      "1s - loss: 0.7650 - acc: 0.6842 - val_loss: 0.7515 - val_acc: 0.6900\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.75145 to 0.74903, saving model to best.model\n",
      "1s - loss: 0.7662 - acc: 0.6825 - val_loss: 0.7490 - val_acc: 0.6915\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.74903 to 0.74471, saving model to best.model\n",
      "1s - loss: 0.7587 - acc: 0.6859 - val_loss: 0.7447 - val_acc: 0.6905\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "1s - loss: 0.7589 - acc: 0.6859 - val_loss: 0.7455 - val_acc: 0.6880\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.74471 to 0.74019, saving model to best.model\n",
      "1s - loss: 0.7560 - acc: 0.6868 - val_loss: 0.7402 - val_acc: 0.6900\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.74019 to 0.73719, saving model to best.model\n",
      "1s - loss: 0.7559 - acc: 0.6872 - val_loss: 0.7372 - val_acc: 0.6902\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.73719 to 0.73234, saving model to best.model\n",
      "1s - loss: 0.7522 - acc: 0.6898 - val_loss: 0.7323 - val_acc: 0.6947\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.73234 to 0.72644, saving model to best.model\n",
      "1s - loss: 0.7497 - acc: 0.6898 - val_loss: 0.7264 - val_acc: 0.6969\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.72644 to 0.72146, saving model to best.model\n",
      "1s - loss: 0.7478 - acc: 0.6921 - val_loss: 0.7215 - val_acc: 0.7010\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "1s - loss: 0.7449 - acc: 0.6928 - val_loss: 0.7231 - val_acc: 0.6969\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.72146 to 0.71982, saving model to best.model\n",
      "1s - loss: 0.7444 - acc: 0.6927 - val_loss: 0.7198 - val_acc: 0.6986\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.71982 to 0.71627, saving model to best.model\n",
      "1s - loss: 0.7399 - acc: 0.6952 - val_loss: 0.7163 - val_acc: 0.6996\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.71627 to 0.71314, saving model to best.model\n",
      "1s - loss: 0.7411 - acc: 0.6932 - val_loss: 0.7131 - val_acc: 0.7024\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.71314 to 0.71024, saving model to best.model\n",
      "1s - loss: 0.7375 - acc: 0.6974 - val_loss: 0.7102 - val_acc: 0.7020\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.71024 to 0.70565, saving model to best.model\n",
      "1s - loss: 0.7343 - acc: 0.6996 - val_loss: 0.7057 - val_acc: 0.7071\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.70565 to 0.70285, saving model to best.model\n",
      "1s - loss: 0.7329 - acc: 0.6962 - val_loss: 0.7028 - val_acc: 0.7081\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "1s - loss: 0.7313 - acc: 0.6984 - val_loss: 0.7047 - val_acc: 0.7065\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.70285 to 0.70062, saving model to best.model\n",
      "1s - loss: 0.7306 - acc: 0.7003 - val_loss: 0.7006 - val_acc: 0.7069\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.70062 to 0.69830, saving model to best.model\n",
      "1s - loss: 0.7279 - acc: 0.6995 - val_loss: 0.6983 - val_acc: 0.7103\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.69830 to 0.69617, saving model to best.model\n",
      "1s - loss: 0.7264 - acc: 0.7020 - val_loss: 0.6962 - val_acc: 0.7073\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.69617 to 0.69571, saving model to best.model\n",
      "1s - loss: 0.7248 - acc: 0.7017 - val_loss: 0.6957 - val_acc: 0.7107\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.69571 to 0.69050, saving model to best.model\n",
      "1s - loss: 0.7242 - acc: 0.7028 - val_loss: 0.6905 - val_acc: 0.7148\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "1s - loss: 0.7214 - acc: 0.7037 - val_loss: 0.6916 - val_acc: 0.7156\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.69050 to 0.68690, saving model to best.model\n",
      "1s - loss: 0.7206 - acc: 0.7036 - val_loss: 0.6869 - val_acc: 0.7160\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.68690 to 0.68346, saving model to best.model\n",
      "1s - loss: 0.7175 - acc: 0.7055 - val_loss: 0.6835 - val_acc: 0.7178\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.68346 to 0.68195, saving model to best.model\n",
      "1s - loss: 0.7181 - acc: 0.7041 - val_loss: 0.6820 - val_acc: 0.7227\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.68195 to 0.68153, saving model to best.model\n",
      "1s - loss: 0.7161 - acc: 0.7080 - val_loss: 0.6815 - val_acc: 0.7188\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.7141 - acc: 0.7065 - val_loss: 0.6833 - val_acc: 0.7201\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.68153 to 0.67741, saving model to best.model\n",
      "1s - loss: 0.7135 - acc: 0.7089 - val_loss: 0.6774 - val_acc: 0.7201\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "1s - loss: 0.7108 - acc: 0.7102 - val_loss: 0.6792 - val_acc: 0.7178\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7106 - acc: 0.7085 - val_loss: 0.6779 - val_acc: 0.7176\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.67741 to 0.67421, saving model to best.model\n",
      "1s - loss: 0.7108 - acc: 0.7099 - val_loss: 0.6742 - val_acc: 0.7217\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7073 - acc: 0.7112 - val_loss: 0.6749 - val_acc: 0.7237\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.67421 to 0.67170, saving model to best.model\n",
      "1s - loss: 0.7046 - acc: 0.7109 - val_loss: 0.6717 - val_acc: 0.7247\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.67170 to 0.66852, saving model to best.model\n",
      "1s - loss: 0.7045 - acc: 0.7126 - val_loss: 0.6685 - val_acc: 0.7241\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7047 - acc: 0.7111 - val_loss: 0.6695 - val_acc: 0.7257\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "1s - loss: 0.7035 - acc: 0.7125 - val_loss: 0.6706 - val_acc: 0.7188\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.66852 to 0.66658, saving model to best.model\n",
      "1s - loss: 0.7023 - acc: 0.7114 - val_loss: 0.6666 - val_acc: 0.7261\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.66658 to 0.66270, saving model to best.model\n",
      "1s - loss: 0.7012 - acc: 0.7137 - val_loss: 0.6627 - val_acc: 0.7312\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.66270 to 0.66191, saving model to best.model\n",
      "1s - loss: 0.6987 - acc: 0.7140 - val_loss: 0.6619 - val_acc: 0.7296\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.66191 to 0.66139, saving model to best.model\n",
      "1s - loss: 0.6999 - acc: 0.7137 - val_loss: 0.6614 - val_acc: 0.7320\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.66139 to 0.66094, saving model to best.model\n",
      "1s - loss: 0.6981 - acc: 0.7150 - val_loss: 0.6609 - val_acc: 0.7306\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.6965 - acc: 0.7148 - val_loss: 0.6623 - val_acc: 0.7273\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "1s - loss: 0.6986 - acc: 0.7134 - val_loss: 0.6615 - val_acc: 0.7257\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.66094 to 0.65826, saving model to best.model\n",
      "1s - loss: 0.6950 - acc: 0.7148 - val_loss: 0.6583 - val_acc: 0.7330\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.65826 to 0.65613, saving model to best.model\n",
      "1s - loss: 0.6924 - acc: 0.7155 - val_loss: 0.6561 - val_acc: 0.7294\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.65613 to 0.65554, saving model to best.model\n",
      "1s - loss: 0.6952 - acc: 0.7155 - val_loss: 0.6555 - val_acc: 0.7284\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.6924 - acc: 0.7181 - val_loss: 0.6586 - val_acc: 0.7251\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.6917 - acc: 0.7179 - val_loss: 0.6563 - val_acc: 0.7276\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.65554 to 0.65476, saving model to best.model\n",
      "1s - loss: 0.6904 - acc: 0.7174 - val_loss: 0.6548 - val_acc: 0.7304\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.65476 to 0.65239, saving model to best.model\n",
      "1s - loss: 0.6882 - acc: 0.7165 - val_loss: 0.6524 - val_acc: 0.7322\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.65239 to 0.65134, saving model to best.model\n",
      "1s - loss: 0.6871 - acc: 0.7201 - val_loss: 0.6513 - val_acc: 0.7338\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.65134 to 0.64939, saving model to best.model\n",
      "1s - loss: 0.6914 - acc: 0.7169 - val_loss: 0.6494 - val_acc: 0.7355\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.6894 - acc: 0.7177 - val_loss: 0.6501 - val_acc: 0.7344\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.6888 - acc: 0.7177 - val_loss: 0.6512 - val_acc: 0.7330\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.64939 to 0.64658, saving model to best.model\n",
      "1s - loss: 0.6856 - acc: 0.7197 - val_loss: 0.6466 - val_acc: 0.7367\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.6862 - acc: 0.7196 - val_loss: 0.6470 - val_acc: 0.7344\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.6872 - acc: 0.7194 - val_loss: 0.6478 - val_acc: 0.7342\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.64658 to 0.64452, saving model to best.model\n",
      "1s - loss: 0.6873 - acc: 0.7190 - val_loss: 0.6445 - val_acc: 0.7373\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.6824 - acc: 0.7230 - val_loss: 0.6465 - val_acc: 0.7336\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.64452 to 0.64025, saving model to best.model\n",
      "1s - loss: 0.6818 - acc: 0.7188 - val_loss: 0.6403 - val_acc: 0.7389\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6802 - acc: 0.7214 - val_loss: 0.6424 - val_acc: 0.7361\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6828 - acc: 0.7188 - val_loss: 0.6430 - val_acc: 0.7385\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6790 - acc: 0.7231 - val_loss: 0.6422 - val_acc: 0.7389\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6793 - acc: 0.7232 - val_loss: 0.6414 - val_acc: 0.7383\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.64025 to 0.63908, saving model to best.model\n",
      "1s - loss: 0.6776 - acc: 0.7213 - val_loss: 0.6391 - val_acc: 0.7379\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6779 - acc: 0.7223 - val_loss: 0.6430 - val_acc: 0.7332\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6774 - acc: 0.7229 - val_loss: 0.6393 - val_acc: 0.7361\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.63908 to 0.63715, saving model to best.model\n",
      "1s - loss: 0.6764 - acc: 0.7220 - val_loss: 0.6371 - val_acc: 0.7393\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.63715 to 0.63691, saving model to best.model\n",
      "1s - loss: 0.6775 - acc: 0.7219 - val_loss: 0.6369 - val_acc: 0.7411\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.63691 to 0.63630, saving model to best.model\n",
      "1s - loss: 0.6787 - acc: 0.7212 - val_loss: 0.6363 - val_acc: 0.7373\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.63630 to 0.63425, saving model to best.model\n",
      "1s - loss: 0.6755 - acc: 0.7240 - val_loss: 0.6342 - val_acc: 0.7389\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.63425 to 0.63311, saving model to best.model\n",
      "1s - loss: 0.6756 - acc: 0.7243 - val_loss: 0.6331 - val_acc: 0.7411\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6741 - acc: 0.7226 - val_loss: 0.6347 - val_acc: 0.7383\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.63311 to 0.63196, saving model to best.model\n",
      "1s - loss: 0.6739 - acc: 0.7248 - val_loss: 0.6320 - val_acc: 0.7403\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6730 - acc: 0.7245 - val_loss: 0.6329 - val_acc: 0.7385\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6727 - acc: 0.7254 - val_loss: 0.6340 - val_acc: 0.7383\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "1s - loss: 0.6743 - acc: 0.7232 - val_loss: 0.6341 - val_acc: 0.7393\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.63196 to 0.63124, saving model to best.model\n",
      "1s - loss: 0.6691 - acc: 0.7267 - val_loss: 0.6312 - val_acc: 0.7425\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63124 to 0.62972, saving model to best.model\n",
      "1s - loss: 0.6709 - acc: 0.7250 - val_loss: 0.6297 - val_acc: 0.7446\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62972 to 0.62846, saving model to best.model\n",
      "1s - loss: 0.6704 - acc: 0.7256 - val_loss: 0.6285 - val_acc: 0.7428\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6705 - acc: 0.7251 - val_loss: 0.6300 - val_acc: 0.7395\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6677 - acc: 0.7280 - val_loss: 0.6323 - val_acc: 0.7385\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6698 - acc: 0.7264 - val_loss: 0.6293 - val_acc: 0.7405\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.62846 to 0.62628, saving model to best.model\n",
      "1s - loss: 0.6659 - acc: 0.7275 - val_loss: 0.6263 - val_acc: 0.7462\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6677 - acc: 0.7271 - val_loss: 0.6282 - val_acc: 0.7415\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.62628 to 0.62563, saving model to best.model\n",
      "1s - loss: 0.6683 - acc: 0.7269 - val_loss: 0.6256 - val_acc: 0.7436\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6672 - acc: 0.7260 - val_loss: 0.6263 - val_acc: 0.7430\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6654 - acc: 0.7266 - val_loss: 0.6284 - val_acc: 0.7415\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.62563 to 0.62500, saving model to best.model\n",
      "1s - loss: 0.6649 - acc: 0.7282 - val_loss: 0.6250 - val_acc: 0.7442\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.62500 to 0.62381, saving model to best.model\n",
      "1s - loss: 0.6646 - acc: 0.7282 - val_loss: 0.6238 - val_acc: 0.7444\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6653 - acc: 0.7293 - val_loss: 0.6248 - val_acc: 0.7438\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62381 to 0.62293, saving model to best.model\n",
      "1s - loss: 0.6645 - acc: 0.7291 - val_loss: 0.6229 - val_acc: 0.7458\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.62293 to 0.62226, saving model to best.model\n",
      "1s - loss: 0.6651 - acc: 0.7264 - val_loss: 0.6223 - val_acc: 0.7466\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6601 - acc: 0.7300 - val_loss: 0.6234 - val_acc: 0.7464\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.62226 to 0.61975, saving model to best.model\n",
      "1s - loss: 0.6638 - acc: 0.7263 - val_loss: 0.6198 - val_acc: 0.7452\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6611 - acc: 0.7295 - val_loss: 0.6221 - val_acc: 0.7452\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6610 - acc: 0.7285 - val_loss: 0.6213 - val_acc: 0.7482\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6614 - acc: 0.7283 - val_loss: 0.6224 - val_acc: 0.7458\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.61975 to 0.61952, saving model to best.model\n",
      "1s - loss: 0.6598 - acc: 0.7293 - val_loss: 0.6195 - val_acc: 0.7452\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6604 - acc: 0.7299 - val_loss: 0.6210 - val_acc: 0.7438\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6580 - acc: 0.7313 - val_loss: 0.6219 - val_acc: 0.7458\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6559 - acc: 0.7336 - val_loss: 0.6198 - val_acc: 0.7458\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6562 - acc: 0.7304 - val_loss: 0.6198 - val_acc: 0.7454\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.61952 to 0.61919, saving model to best.model\n",
      "1s - loss: 0.6582 - acc: 0.7307 - val_loss: 0.6192 - val_acc: 0.7464\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6547 - acc: 0.7330 - val_loss: 0.6204 - val_acc: 0.7432\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.61919 to 0.61621, saving model to best.model\n",
      "1s - loss: 0.6541 - acc: 0.7335 - val_loss: 0.6162 - val_acc: 0.7462\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.61621 to 0.61565, saving model to best.model\n",
      "1s - loss: 0.6559 - acc: 0.7295 - val_loss: 0.6156 - val_acc: 0.7472\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6565 - acc: 0.7305 - val_loss: 0.6163 - val_acc: 0.7472\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6562 - acc: 0.7311 - val_loss: 0.6158 - val_acc: 0.7472\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6559 - acc: 0.7312 - val_loss: 0.6178 - val_acc: 0.7474\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.61565 to 0.61423, saving model to best.model\n",
      "1s - loss: 0.6548 - acc: 0.7318 - val_loss: 0.6142 - val_acc: 0.7486\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61423 to 0.61032, saving model to best.model\n",
      "1s - loss: 0.6504 - acc: 0.7327 - val_loss: 0.6103 - val_acc: 0.7505\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6543 - acc: 0.7315 - val_loss: 0.6137 - val_acc: 0.7494\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6543 - acc: 0.7319 - val_loss: 0.6120 - val_acc: 0.7503\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6535 - acc: 0.7338 - val_loss: 0.6117 - val_acc: 0.7503\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6523 - acc: 0.7320 - val_loss: 0.6167 - val_acc: 0.7466\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6533 - acc: 0.7329 - val_loss: 0.6170 - val_acc: 0.7460\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6516 - acc: 0.7342 - val_loss: 0.6138 - val_acc: 0.7492\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6510 - acc: 0.7325 - val_loss: 0.6112 - val_acc: 0.7509\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6499 - acc: 0.7327 - val_loss: 0.6112 - val_acc: 0.7500\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61032 to 0.60986, saving model to best.model\n",
      "1s - loss: 0.6502 - acc: 0.7341 - val_loss: 0.6099 - val_acc: 0.7503\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.60986 to 0.60892, saving model to best.model\n",
      "1s - loss: 0.6492 - acc: 0.7341 - val_loss: 0.6089 - val_acc: 0.7484\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6491 - acc: 0.7342 - val_loss: 0.6094 - val_acc: 0.7500\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6492 - acc: 0.7344 - val_loss: 0.6112 - val_acc: 0.7494\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6510 - acc: 0.7343 - val_loss: 0.6095 - val_acc: 0.7511\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6500 - acc: 0.7349 - val_loss: 0.6101 - val_acc: 0.7513\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.60892 to 0.60759, saving model to best.model\n",
      "1s - loss: 0.6503 - acc: 0.7338 - val_loss: 0.6076 - val_acc: 0.7519\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6469 - acc: 0.7353 - val_loss: 0.6086 - val_acc: 0.7519\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.60759 to 0.60718, saving model to best.model\n",
      "1s - loss: 0.6458 - acc: 0.7366 - val_loss: 0.6072 - val_acc: 0.7523\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6489 - acc: 0.7338 - val_loss: 0.6078 - val_acc: 0.7503\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6496 - acc: 0.7333 - val_loss: 0.6108 - val_acc: 0.7501\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.60718 to 0.60615, saving model to best.model\n",
      "1s - loss: 0.6474 - acc: 0.7347 - val_loss: 0.6062 - val_acc: 0.7519\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6481 - acc: 0.7352 - val_loss: 0.6062 - val_acc: 0.7519\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6454 - acc: 0.7355 - val_loss: 0.6070 - val_acc: 0.7511\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60615 to 0.60517, saving model to best.model\n",
      "1s - loss: 0.6437 - acc: 0.7366 - val_loss: 0.6052 - val_acc: 0.7519\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6435 - acc: 0.7370 - val_loss: 0.6061 - val_acc: 0.7511\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60517 to 0.60515, saving model to best.model\n",
      "1s - loss: 0.6450 - acc: 0.7367 - val_loss: 0.6052 - val_acc: 0.7537\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60515 to 0.60396, saving model to best.model\n",
      "1s - loss: 0.6422 - acc: 0.7364 - val_loss: 0.6040 - val_acc: 0.7539\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6445 - acc: 0.7369 - val_loss: 0.6044 - val_acc: 0.7527\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60396 to 0.60195, saving model to best.model\n",
      "1s - loss: 0.6439 - acc: 0.7380 - val_loss: 0.6020 - val_acc: 0.7563\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6459 - acc: 0.7341 - val_loss: 0.6061 - val_acc: 0.7500\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60195 to 0.60181, saving model to best.model\n",
      "1s - loss: 0.6421 - acc: 0.7371 - val_loss: 0.6018 - val_acc: 0.7580\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60181 to 0.60083, saving model to best.model\n",
      "1s - loss: 0.6405 - acc: 0.7364 - val_loss: 0.6008 - val_acc: 0.7573\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6439 - acc: 0.7370 - val_loss: 0.6046 - val_acc: 0.7555\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6416 - acc: 0.7362 - val_loss: 0.6011 - val_acc: 0.7569\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60083 to 0.60022, saving model to best.model\n",
      "1s - loss: 0.6426 - acc: 0.7382 - val_loss: 0.6002 - val_acc: 0.7551\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6412 - acc: 0.7377 - val_loss: 0.6018 - val_acc: 0.7543\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60022 to 0.59740, saving model to best.model\n",
      "1s - loss: 0.6417 - acc: 0.7383 - val_loss: 0.5974 - val_acc: 0.7600\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6407 - acc: 0.7373 - val_loss: 0.5984 - val_acc: 0.7580\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6395 - acc: 0.7362 - val_loss: 0.6020 - val_acc: 0.7545\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6420 - acc: 0.7369 - val_loss: 0.6022 - val_acc: 0.7576\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6385 - acc: 0.7390 - val_loss: 0.6013 - val_acc: 0.7549\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6437 - acc: 0.7359 - val_loss: 0.5986 - val_acc: 0.7594\n"
     ]
    }
   ],
   "source": [
    "hist=m.fit(\n",
    "    # Feature matrix\n",
    "    X_train.values, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=125),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    "    batch_size=32, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f61b950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1   410  1139]\n",
      " [    1  2341  1850]\n",
      " [    0   570 10578]]\n",
      "76.4949674364\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.00      0.00      1550\n",
      "          1       0.70      0.56      0.62      4192\n",
      "          2       0.78      0.95      0.86     11148\n",
      "\n",
      "avg / total       0.74      0.76      0.72     16890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m.load_weights(\"best.model\")\n",
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in m.predict(X_test.values).argmax(axis=1)]\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45600 samples, validate on 5067 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.85794, saving model to best.model\n",
      "2s - loss: 0.8543 - acc: 0.6587 - val_loss: 0.8579 - val_acc: 0.6493\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.85794 to 0.85725, saving model to best.model\n",
      "2s - loss: 0.8426 - acc: 0.6587 - val_loss: 0.8573 - val_acc: 0.6493\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "2s - loss: 0.8371 - acc: 0.6587 - val_loss: 0.8598 - val_acc: 0.6493\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.85725 to 0.84258, saving model to best.model\n",
      "2s - loss: 0.8320 - acc: 0.6586 - val_loss: 0.8426 - val_acc: 0.6493\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "2s - loss: 0.8312 - acc: 0.6585 - val_loss: 0.8437 - val_acc: 0.6493\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "2s - loss: 0.8317 - acc: 0.6588 - val_loss: 0.8450 - val_acc: 0.6493\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "2s - loss: 0.8288 - acc: 0.6588 - val_loss: 0.8520 - val_acc: 0.6491\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.84258 to 0.83646, saving model to best.model\n",
      "2s - loss: 0.8285 - acc: 0.6589 - val_loss: 0.8365 - val_acc: 0.6495\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "2s - loss: 0.8259 - acc: 0.6589 - val_loss: 0.8373 - val_acc: 0.6493\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "2s - loss: 0.8255 - acc: 0.6591 - val_loss: 0.8381 - val_acc: 0.6493\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "2s - loss: 0.8241 - acc: 0.6591 - val_loss: 0.8376 - val_acc: 0.6538\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.83646 to 0.83400, saving model to best.model\n",
      "2s - loss: 0.8243 - acc: 0.6591 - val_loss: 0.8340 - val_acc: 0.6521\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "2s - loss: 0.8256 - acc: 0.6598 - val_loss: 0.8390 - val_acc: 0.6495\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.83400 to 0.83318, saving model to best.model\n",
      "2s - loss: 0.8221 - acc: 0.6598 - val_loss: 0.8332 - val_acc: 0.6550\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.83318 to 0.83284, saving model to best.model\n",
      "2s - loss: 0.8202 - acc: 0.6614 - val_loss: 0.8328 - val_acc: 0.6572\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "2s - loss: 0.8192 - acc: 0.6624 - val_loss: 0.8366 - val_acc: 0.6495\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "2s - loss: 0.8178 - acc: 0.6621 - val_loss: 0.8368 - val_acc: 0.6598\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.83284 to 0.82873, saving model to best.model\n",
      "2s - loss: 0.8191 - acc: 0.6618 - val_loss: 0.8287 - val_acc: 0.6578\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "2s - loss: 0.8162 - acc: 0.6629 - val_loss: 0.8312 - val_acc: 0.6499\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82873 to 0.82583, saving model to best.model\n",
      "2s - loss: 0.8159 - acc: 0.6625 - val_loss: 0.8258 - val_acc: 0.6607\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "2s - loss: 0.8161 - acc: 0.6620 - val_loss: 0.8342 - val_acc: 0.6517\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "2s - loss: 0.8117 - acc: 0.6649 - val_loss: 0.8331 - val_acc: 0.6507\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.82583 to 0.82397, saving model to best.model\n",
      "2s - loss: 0.8081 - acc: 0.6669 - val_loss: 0.8240 - val_acc: 0.6546\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.82397 to 0.81586, saving model to best.model\n",
      "2s - loss: 0.8041 - acc: 0.6682 - val_loss: 0.8159 - val_acc: 0.6657\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "2s - loss: 0.8040 - acc: 0.6672 - val_loss: 0.8363 - val_acc: 0.6521\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "2s - loss: 0.7982 - acc: 0.6693 - val_loss: 0.8174 - val_acc: 0.6645\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81586 to 0.80449, saving model to best.model\n",
      "2s - loss: 0.7912 - acc: 0.6720 - val_loss: 0.8045 - val_acc: 0.6655\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80449 to 0.80062, saving model to best.model\n",
      "3s - loss: 0.7840 - acc: 0.6754 - val_loss: 0.8006 - val_acc: 0.6675\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80062 to 0.78576, saving model to best.model\n",
      "3s - loss: 0.7809 - acc: 0.6786 - val_loss: 0.7858 - val_acc: 0.6696\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "3s - loss: 0.7692 - acc: 0.6837 - val_loss: 0.7974 - val_acc: 0.6617\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.78576 to 0.77606, saving model to best.model\n",
      "3s - loss: 0.7622 - acc: 0.6850 - val_loss: 0.7761 - val_acc: 0.6759\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.77606 to 0.77483, saving model to best.model\n",
      "3s - loss: 0.7556 - acc: 0.6883 - val_loss: 0.7748 - val_acc: 0.6704\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.77483 to 0.76757, saving model to best.model\n",
      "3s - loss: 0.7509 - acc: 0.6896 - val_loss: 0.7676 - val_acc: 0.6730\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.76757 to 0.75325, saving model to best.model\n",
      "3s - loss: 0.7444 - acc: 0.6945 - val_loss: 0.7532 - val_acc: 0.6828\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "2s - loss: 0.7358 - acc: 0.6961 - val_loss: 0.7621 - val_acc: 0.6757\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.75325 to 0.74501, saving model to best.model\n",
      "3s - loss: 0.7305 - acc: 0.6993 - val_loss: 0.7450 - val_acc: 0.6923\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.74501 to 0.73269, saving model to best.model\n",
      "3s - loss: 0.7217 - acc: 0.7029 - val_loss: 0.7327 - val_acc: 0.7010\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.73269 to 0.72219, saving model to best.model\n",
      "3s - loss: 0.7183 - acc: 0.7042 - val_loss: 0.7222 - val_acc: 0.6980\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.72219 to 0.71707, saving model to best.model\n",
      "2s - loss: 0.7120 - acc: 0.7068 - val_loss: 0.7171 - val_acc: 0.7050\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.71707 to 0.71484, saving model to best.model\n",
      "2s - loss: 0.7061 - acc: 0.7096 - val_loss: 0.7148 - val_acc: 0.7059\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "2s - loss: 0.6986 - acc: 0.7137 - val_loss: 0.7245 - val_acc: 0.6949\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "2s - loss: 0.6956 - acc: 0.7124 - val_loss: 0.7261 - val_acc: 0.7048\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.71484 to 0.70687, saving model to best.model\n",
      "2s - loss: 0.6972 - acc: 0.7142 - val_loss: 0.7069 - val_acc: 0.7136\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.70687 to 0.70102, saving model to best.model\n",
      "3s - loss: 0.6861 - acc: 0.7164 - val_loss: 0.7010 - val_acc: 0.7038\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.70102 to 0.68998, saving model to best.model\n",
      "3s - loss: 0.6803 - acc: 0.7218 - val_loss: 0.6900 - val_acc: 0.7123\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.68998 to 0.68197, saving model to best.model\n",
      "3s - loss: 0.6739 - acc: 0.7236 - val_loss: 0.6820 - val_acc: 0.7201\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "3s - loss: 0.6681 - acc: 0.7272 - val_loss: 0.7033 - val_acc: 0.7053\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.68197 to 0.67902, saving model to best.model\n",
      "3s - loss: 0.6700 - acc: 0.7255 - val_loss: 0.6790 - val_acc: 0.7196\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "3s - loss: 0.6693 - acc: 0.7268 - val_loss: 0.6925 - val_acc: 0.7079\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.67902 to 0.66735, saving model to best.model\n",
      "3s - loss: 0.6582 - acc: 0.7316 - val_loss: 0.6673 - val_acc: 0.7275\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "3s - loss: 0.6567 - acc: 0.7324 - val_loss: 0.6746 - val_acc: 0.7198\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "3s - loss: 0.6549 - acc: 0.7331 - val_loss: 0.7775 - val_acc: 0.6649\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.66735 to 0.65811, saving model to best.model\n",
      "3s - loss: 0.6522 - acc: 0.7336 - val_loss: 0.6581 - val_acc: 0.7276\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "3s - loss: 0.6503 - acc: 0.7336 - val_loss: 0.6599 - val_acc: 0.7324\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "3s - loss: 0.6429 - acc: 0.7387 - val_loss: 0.6614 - val_acc: 0.7231\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "3s - loss: 0.6425 - acc: 0.7387 - val_loss: 0.6826 - val_acc: 0.7229\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.65811 to 0.65633, saving model to best.model\n",
      "4s - loss: 0.6383 - acc: 0.7401 - val_loss: 0.6563 - val_acc: 0.7371\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.65633 to 0.64657, saving model to best.model\n",
      "3s - loss: 0.6332 - acc: 0.7408 - val_loss: 0.6466 - val_acc: 0.7421\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.64657 to 0.64633, saving model to best.model\n",
      "3s - loss: 0.6351 - acc: 0.7414 - val_loss: 0.6463 - val_acc: 0.7379\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "3s - loss: 0.6297 - acc: 0.7441 - val_loss: 0.6514 - val_acc: 0.7385\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.64633 to 0.63951, saving model to best.model\n",
      "4s - loss: 0.6313 - acc: 0.7440 - val_loss: 0.6395 - val_acc: 0.7444\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.63951 to 0.63301, saving model to best.model\n",
      "3s - loss: 0.6229 - acc: 0.7472 - val_loss: 0.6330 - val_acc: 0.7413\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "3s - loss: 0.6267 - acc: 0.7449 - val_loss: 0.6424 - val_acc: 0.7425\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "2s - loss: 0.6203 - acc: 0.7488 - val_loss: 0.6636 - val_acc: 0.7330\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.63301 to 0.62876, saving model to best.model\n",
      "2s - loss: 0.6214 - acc: 0.7488 - val_loss: 0.6288 - val_acc: 0.7413\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.62876 to 0.62456, saving model to best.model\n",
      "2s - loss: 0.6137 - acc: 0.7518 - val_loss: 0.6246 - val_acc: 0.7474\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "2s - loss: 0.6133 - acc: 0.7508 - val_loss: 0.6295 - val_acc: 0.7415\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.62456 to 0.62092, saving model to best.model\n",
      "2s - loss: 0.6076 - acc: 0.7551 - val_loss: 0.6209 - val_acc: 0.7444\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.62092 to 0.61636, saving model to best.model\n",
      "2s - loss: 0.6086 - acc: 0.7553 - val_loss: 0.6164 - val_acc: 0.7496\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "2s - loss: 0.6072 - acc: 0.7542 - val_loss: 0.6810 - val_acc: 0.7186\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "2s - loss: 0.6079 - acc: 0.7547 - val_loss: 0.6415 - val_acc: 0.7353\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.61636 to 0.61428, saving model to best.model\n",
      "2s - loss: 0.6042 - acc: 0.7553 - val_loss: 0.6143 - val_acc: 0.7549\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.61428 to 0.61024, saving model to best.model\n",
      "2s - loss: 0.6001 - acc: 0.7583 - val_loss: 0.6102 - val_acc: 0.7535\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "2s - loss: 0.5983 - acc: 0.7583 - val_loss: 0.6124 - val_acc: 0.7503\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.61024 to 0.60397, saving model to best.model\n",
      "2s - loss: 0.5984 - acc: 0.7582 - val_loss: 0.6040 - val_acc: 0.7584\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.60397 to 0.59918, saving model to best.model\n",
      "2s - loss: 0.5977 - acc: 0.7587 - val_loss: 0.5992 - val_acc: 0.7596\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "2s - loss: 0.5933 - acc: 0.7602 - val_loss: 0.6061 - val_acc: 0.7553\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "2s - loss: 0.5935 - acc: 0.7620 - val_loss: 0.6032 - val_acc: 0.7586\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "2s - loss: 0.5871 - acc: 0.7636 - val_loss: 0.6201 - val_acc: 0.7501\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "2s - loss: 0.5923 - acc: 0.7602 - val_loss: 0.6403 - val_acc: 0.7395\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "2s - loss: 0.5919 - acc: 0.7608 - val_loss: 0.5998 - val_acc: 0.7590\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "2s - loss: 0.5835 - acc: 0.7650 - val_loss: 0.6034 - val_acc: 0.7535\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.59918 to 0.59096, saving model to best.model\n",
      "2s - loss: 0.5862 - acc: 0.7629 - val_loss: 0.5910 - val_acc: 0.7648\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "2s - loss: 0.5810 - acc: 0.7657 - val_loss: 0.5999 - val_acc: 0.7636\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.59096 to 0.58783, saving model to best.model\n",
      "2s - loss: 0.5813 - acc: 0.7674 - val_loss: 0.5878 - val_acc: 0.7671\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "2s - loss: 0.5807 - acc: 0.7662 - val_loss: 0.6013 - val_acc: 0.7563\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "2s - loss: 0.5782 - acc: 0.7677 - val_loss: 0.5920 - val_acc: 0.7632\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "3s - loss: 0.5753 - acc: 0.7706 - val_loss: 0.6354 - val_acc: 0.7409\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "2s - loss: 0.5752 - acc: 0.7694 - val_loss: 0.6220 - val_acc: 0.7484\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.58783 to 0.58458, saving model to best.model\n",
      "3s - loss: 0.5758 - acc: 0.7674 - val_loss: 0.5846 - val_acc: 0.7669\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.58458 to 0.58347, saving model to best.model\n",
      "3s - loss: 0.5705 - acc: 0.7709 - val_loss: 0.5835 - val_acc: 0.7669\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "3s - loss: 0.5792 - acc: 0.7660 - val_loss: 0.5905 - val_acc: 0.7655\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.58347 to 0.57803, saving model to best.model\n",
      "3s - loss: 0.5690 - acc: 0.7712 - val_loss: 0.5780 - val_acc: 0.7691\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "2s - loss: 0.5716 - acc: 0.7704 - val_loss: 0.5900 - val_acc: 0.7669\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.57803 to 0.57657, saving model to best.model\n",
      "2s - loss: 0.5659 - acc: 0.7721 - val_loss: 0.5766 - val_acc: 0.7701\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "2s - loss: 0.5629 - acc: 0.7745 - val_loss: 0.5806 - val_acc: 0.7665\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "2s - loss: 0.5646 - acc: 0.7730 - val_loss: 0.5889 - val_acc: 0.7602\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "2s - loss: 0.5634 - acc: 0.7746 - val_loss: 0.5929 - val_acc: 0.7634\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "2s - loss: 0.5619 - acc: 0.7744 - val_loss: 0.5850 - val_acc: 0.7646\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "2s - loss: 0.5635 - acc: 0.7737 - val_loss: 0.5905 - val_acc: 0.7644\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.57657 to 0.57085, saving model to best.model\n",
      "2s - loss: 0.5608 - acc: 0.7757 - val_loss: 0.5709 - val_acc: 0.7728\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.57085 to 0.56874, saving model to best.model\n",
      "2s - loss: 0.5579 - acc: 0.7767 - val_loss: 0.5687 - val_acc: 0.7754\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.56874 to 0.56564, saving model to best.model\n",
      "2s - loss: 0.5568 - acc: 0.7757 - val_loss: 0.5656 - val_acc: 0.7748\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "2s - loss: 0.5575 - acc: 0.7761 - val_loss: 0.6026 - val_acc: 0.7586\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "2s - loss: 0.5591 - acc: 0.7774 - val_loss: 0.5700 - val_acc: 0.7756\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "2s - loss: 0.5534 - acc: 0.7799 - val_loss: 0.5698 - val_acc: 0.7750\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "2s - loss: 0.5540 - acc: 0.7785 - val_loss: 0.5991 - val_acc: 0.7622\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.56564 to 0.56128, saving model to best.model\n",
      "2s - loss: 0.5534 - acc: 0.7782 - val_loss: 0.5613 - val_acc: 0.7788\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "2s - loss: 0.5501 - acc: 0.7804 - val_loss: 0.5646 - val_acc: 0.7748\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "2s - loss: 0.5556 - acc: 0.7772 - val_loss: 0.5661 - val_acc: 0.7732\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "2s - loss: 0.5547 - acc: 0.7778 - val_loss: 0.5881 - val_acc: 0.7665\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "2s - loss: 0.5475 - acc: 0.7814 - val_loss: 0.5950 - val_acc: 0.7608\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.56128 to 0.56031, saving model to best.model\n",
      "2s - loss: 0.5433 - acc: 0.7820 - val_loss: 0.5603 - val_acc: 0.7752\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "2s - loss: 0.5452 - acc: 0.7840 - val_loss: 0.5891 - val_acc: 0.7691\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "2s - loss: 0.5485 - acc: 0.7796 - val_loss: 0.5938 - val_acc: 0.7653\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "2s - loss: 0.5451 - acc: 0.7822 - val_loss: 0.6323 - val_acc: 0.7462\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "2s - loss: 0.5441 - acc: 0.7834 - val_loss: 0.5883 - val_acc: 0.7642\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "2s - loss: 0.5396 - acc: 0.7847 - val_loss: 0.5927 - val_acc: 0.7614\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.56031 to 0.55796, saving model to best.model\n",
      "2s - loss: 0.5475 - acc: 0.7795 - val_loss: 0.5580 - val_acc: 0.7799\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.55796 to 0.55090, saving model to best.model\n",
      "2s - loss: 0.5362 - acc: 0.7863 - val_loss: 0.5509 - val_acc: 0.7813\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "2s - loss: 0.5375 - acc: 0.7851 - val_loss: 0.5559 - val_acc: 0.7805\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "2s - loss: 0.5371 - acc: 0.7861 - val_loss: 0.5702 - val_acc: 0.7744\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "2s - loss: 0.5391 - acc: 0.7858 - val_loss: 0.5545 - val_acc: 0.7786\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.55090 to 0.54667, saving model to best.model\n",
      "2s - loss: 0.5372 - acc: 0.7861 - val_loss: 0.5467 - val_acc: 0.7873\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "2s - loss: 0.5337 - acc: 0.7866 - val_loss: 0.5535 - val_acc: 0.7809\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.54667 to 0.54593, saving model to best.model\n",
      "2s - loss: 0.5328 - acc: 0.7885 - val_loss: 0.5459 - val_acc: 0.7865\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "2s - loss: 0.5338 - acc: 0.7882 - val_loss: 0.5592 - val_acc: 0.7760\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "2s - loss: 0.5355 - acc: 0.7867 - val_loss: 0.5484 - val_acc: 0.7823\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "2s - loss: 0.5274 - acc: 0.7894 - val_loss: 0.5485 - val_acc: 0.7821\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "2s - loss: 0.5344 - acc: 0.7870 - val_loss: 0.5482 - val_acc: 0.7807\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.54593 to 0.54445, saving model to best.model\n",
      "2s - loss: 0.5306 - acc: 0.7893 - val_loss: 0.5444 - val_acc: 0.7847\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.54445 to 0.54358, saving model to best.model\n",
      "2s - loss: 0.5275 - acc: 0.7906 - val_loss: 0.5436 - val_acc: 0.7865\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.54358 to 0.54180, saving model to best.model\n",
      "2s - loss: 0.5266 - acc: 0.7899 - val_loss: 0.5418 - val_acc: 0.7886\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "2s - loss: 0.5241 - acc: 0.7920 - val_loss: 0.5560 - val_acc: 0.7770\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "2s - loss: 0.5278 - acc: 0.7903 - val_loss: 0.5532 - val_acc: 0.7815\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "2s - loss: 0.5276 - acc: 0.7913 - val_loss: 0.5700 - val_acc: 0.7721\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.54180 to 0.53815, saving model to best.model\n",
      "2s - loss: 0.5224 - acc: 0.7916 - val_loss: 0.5381 - val_acc: 0.7910\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.53815 to 0.53609, saving model to best.model\n",
      "2s - loss: 0.5203 - acc: 0.7927 - val_loss: 0.5361 - val_acc: 0.7898\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "2s - loss: 0.5194 - acc: 0.7930 - val_loss: 0.5399 - val_acc: 0.7896\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "2s - loss: 0.5261 - acc: 0.7899 - val_loss: 0.5403 - val_acc: 0.7886\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "2s - loss: 0.5196 - acc: 0.7941 - val_loss: 0.5592 - val_acc: 0.7776\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "2s - loss: 0.5207 - acc: 0.7921 - val_loss: 0.5558 - val_acc: 0.7758\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "2s - loss: 0.5176 - acc: 0.7944 - val_loss: 0.5378 - val_acc: 0.7851\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "2s - loss: 0.5175 - acc: 0.7930 - val_loss: 0.5852 - val_acc: 0.7630\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "2s - loss: 0.5151 - acc: 0.7945 - val_loss: 0.5400 - val_acc: 0.7847\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.53609 to 0.53286, saving model to best.model\n",
      "2s - loss: 0.5140 - acc: 0.7961 - val_loss: 0.5329 - val_acc: 0.7912\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "2s - loss: 0.5177 - acc: 0.7942 - val_loss: 0.5340 - val_acc: 0.7918\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "2s - loss: 0.5137 - acc: 0.7957 - val_loss: 0.5380 - val_acc: 0.7910\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.53286 to 0.52871, saving model to best.model\n",
      "2s - loss: 0.5101 - acc: 0.7973 - val_loss: 0.5287 - val_acc: 0.7924\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.52871 to 0.52560, saving model to best.model\n",
      "2s - loss: 0.5131 - acc: 0.7973 - val_loss: 0.5256 - val_acc: 0.7975\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "2s - loss: 0.5192 - acc: 0.7919 - val_loss: 0.5283 - val_acc: 0.7898\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "2s - loss: 0.5086 - acc: 0.7981 - val_loss: 0.5411 - val_acc: 0.7843\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.52560 to 0.52382, saving model to best.model\n",
      "2s - loss: 0.5134 - acc: 0.7959 - val_loss: 0.5238 - val_acc: 0.7936\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "2s - loss: 0.5127 - acc: 0.7972 - val_loss: 0.5252 - val_acc: 0.7926\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "2s - loss: 0.5072 - acc: 0.7982 - val_loss: 0.5686 - val_acc: 0.7705\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "2s - loss: 0.5080 - acc: 0.7973 - val_loss: 0.5270 - val_acc: 0.7894\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.52382 to 0.52356, saving model to best.model\n",
      "2s - loss: 0.5087 - acc: 0.7966 - val_loss: 0.5236 - val_acc: 0.7953\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "2s - loss: 0.5071 - acc: 0.7997 - val_loss: 0.5359 - val_acc: 0.7890\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "2s - loss: 0.5055 - acc: 0.7984 - val_loss: 0.5895 - val_acc: 0.7659\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "2s - loss: 0.5049 - acc: 0.8003 - val_loss: 0.5274 - val_acc: 0.7910\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.52356 to 0.52208, saving model to best.model\n",
      "2s - loss: 0.5063 - acc: 0.7992 - val_loss: 0.5221 - val_acc: 0.7991\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "2s - loss: 0.5008 - acc: 0.8014 - val_loss: 0.5295 - val_acc: 0.7924\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "2s - loss: 0.5010 - acc: 0.8009 - val_loss: 0.5227 - val_acc: 0.7948\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.52208 to 0.51991, saving model to best.model\n",
      "2s - loss: 0.5047 - acc: 0.7994 - val_loss: 0.5199 - val_acc: 0.7957\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "2s - loss: 0.5029 - acc: 0.8006 - val_loss: 0.5328 - val_acc: 0.7902\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "2s - loss: 0.5042 - acc: 0.7981 - val_loss: 0.5388 - val_acc: 0.7920\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "2s - loss: 0.4986 - acc: 0.8009 - val_loss: 0.5328 - val_acc: 0.7884\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "2s - loss: 0.5007 - acc: 0.8003 - val_loss: 0.5202 - val_acc: 0.7961\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.51991 to 0.51622, saving model to best.model\n",
      "2s - loss: 0.4979 - acc: 0.8021 - val_loss: 0.5162 - val_acc: 0.7965\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.51622 to 0.51570, saving model to best.model\n",
      "2s - loss: 0.4964 - acc: 0.8030 - val_loss: 0.5157 - val_acc: 0.7993\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.51570 to 0.51295, saving model to best.model\n",
      "2s - loss: 0.4952 - acc: 0.8046 - val_loss: 0.5129 - val_acc: 0.8007\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "2s - loss: 0.4964 - acc: 0.8038 - val_loss: 0.5411 - val_acc: 0.7869\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "2s - loss: 0.4958 - acc: 0.8036 - val_loss: 0.5213 - val_acc: 0.7930\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.51295 to 0.51159, saving model to best.model\n",
      "2s - loss: 0.4956 - acc: 0.8037 - val_loss: 0.5116 - val_acc: 0.8013\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "2s - loss: 0.4918 - acc: 0.8048 - val_loss: 0.5385 - val_acc: 0.7882\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "2s - loss: 0.4903 - acc: 0.8023 - val_loss: 0.5294 - val_acc: 0.7942\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "2s - loss: 0.4970 - acc: 0.8014 - val_loss: 0.5188 - val_acc: 0.7959\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "2s - loss: 0.4908 - acc: 0.8052 - val_loss: 0.5181 - val_acc: 0.7938\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.51159 to 0.51007, saving model to best.model\n",
      "2s - loss: 0.4894 - acc: 0.8057 - val_loss: 0.5101 - val_acc: 0.8009\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "2s - loss: 0.4895 - acc: 0.8058 - val_loss: 0.5169 - val_acc: 0.7955\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "2s - loss: 0.4909 - acc: 0.8048 - val_loss: 0.5358 - val_acc: 0.7894\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.51007 to 0.50753, saving model to best.model\n",
      "2s - loss: 0.4919 - acc: 0.8047 - val_loss: 0.5075 - val_acc: 0.8009\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "2s - loss: 0.4867 - acc: 0.8062 - val_loss: 0.5253 - val_acc: 0.7916\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "2s - loss: 0.4908 - acc: 0.8041 - val_loss: 0.5077 - val_acc: 0.8009\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "2s - loss: 0.4876 - acc: 0.8065 - val_loss: 0.5088 - val_acc: 0.8013\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "2s - loss: 0.4836 - acc: 0.8084 - val_loss: 0.5306 - val_acc: 0.7869\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "2s - loss: 0.4869 - acc: 0.8054 - val_loss: 0.5103 - val_acc: 0.8019\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "2s - loss: 0.4829 - acc: 0.8073 - val_loss: 0.5130 - val_acc: 0.7993\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "2s - loss: 0.4851 - acc: 0.8077 - val_loss: 0.5559 - val_acc: 0.7794\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "2s - loss: 0.4888 - acc: 0.8053 - val_loss: 0.5546 - val_acc: 0.7774\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.50753 to 0.50534, saving model to best.model\n",
      "2s - loss: 0.4829 - acc: 0.8075 - val_loss: 0.5053 - val_acc: 0.7985\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "2s - loss: 0.4813 - acc: 0.8093 - val_loss: 0.5181 - val_acc: 0.7949\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "2s - loss: 0.4851 - acc: 0.8075 - val_loss: 0.5068 - val_acc: 0.8022\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "3s - loss: 0.4824 - acc: 0.8077 - val_loss: 0.5176 - val_acc: 0.7953\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "3s - loss: 0.4830 - acc: 0.8077 - val_loss: 0.5362 - val_acc: 0.7878\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.50534 to 0.50502, saving model to best.model\n",
      "3s - loss: 0.4897 - acc: 0.8073 - val_loss: 0.5050 - val_acc: 0.8017\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "2s - loss: 0.4801 - acc: 0.8091 - val_loss: 0.5173 - val_acc: 0.7959\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "2s - loss: 0.4793 - acc: 0.8096 - val_loss: 0.5073 - val_acc: 0.8030\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.50502 to 0.50350, saving model to best.model\n",
      "3s - loss: 0.4762 - acc: 0.8110 - val_loss: 0.5035 - val_acc: 0.8005\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "3s - loss: 0.4798 - acc: 0.8089 - val_loss: 0.5238 - val_acc: 0.7908\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,History\n",
    "from keras.models import Model\n",
    "import keras\n",
    "history = History()\n",
    "\n",
    "\n",
    "input_1 = Input(shape=(1,))\n",
    "input_2 = Input(shape=(1,))\n",
    "input_3 = Input(shape=(1,))\n",
    "input_4 = Input(shape=(1,))\n",
    "input_5 = Input(shape=(1,))\n",
    "input_6 = Input(shape=(1,))\n",
    "input_7 = Input(shape=(1,))\n",
    "input_8 = Input(shape=(1,))\n",
    "input_9 = Input(shape=(1,))\n",
    "input_10 = Input(shape=(1,))\n",
    "input_11= Input(shape=(1,))\n",
    "input_12= Input(shape=(1,))\n",
    "input_13= Input(shape=(1,))\n",
    "input_14= Input(shape=(1,))\n",
    "input_15= Input(shape=(1,))\n",
    "input_16= Input(shape=(1,))\n",
    "\n",
    "input_17= Input(shape=(1,))\n",
    "input_18= Input(shape=(1,))\n",
    "input_19= Input(shape=(1,))\n",
    "input_20= Input(shape=(1,))\n",
    "input_21= Input(shape=(1,))\n",
    "input_22 = Input(shape=(1,))\n",
    "input_23= Input(shape=(1,))\n",
    "input_24= Input(shape=(1,))\n",
    "\n",
    "input_25= Input(shape=(1,))\n",
    "input_26= Input(shape=(1,))\n",
    "input_27= Input(shape=(1,))\n",
    "input_28= Input(shape=(1,))\n",
    "input_29= Input(shape=(1,))\n",
    "input_30= Input(shape=(1,))\n",
    "input_31= Input(shape=(1,))\n",
    "input_32= Input(shape=(1,))\n",
    "\n",
    "input_33= Input(shape=(1,))\n",
    "input_34= Input(shape=(1,))\n",
    "input_35= Input(shape=(1,))\n",
    "input_36= Input(shape=(1,))\n",
    "input_37= Input(shape=(1,))\n",
    "input_38= Input(shape=(1,))\n",
    "input_39 = Input(shape=(1,))\n",
    "input_40= Input(shape=(1,))\n",
    "\n",
    "input_41 = Input(shape=(1,))\n",
    "input_42 = Input(shape=(1,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden_1 = Dense(32, activation='sigmoid')(input_1)\n",
    "hidden_2 = Dense(32, activation='sigmoid')(input_2)\n",
    "hidden_3 = Dense(32, activation='sigmoid')(input_3)\n",
    "hidden_4 = Dense(32, activation='sigmoid')(input_4)\n",
    "hidden_5 = Dense(32, activation='sigmoid')(input_5)\n",
    "hidden_6 = Dense(32, activation='sigmoid')(input_6)\n",
    "hidden_7 = Dense(32, activation='sigmoid')(input_7)\n",
    "hidden_8 = Dense(32, activation='sigmoid')(input_8)\n",
    "hidden_9 = Dense(32, activation='sigmoid')(input_9)\n",
    "hidden_10 = Dense(32, activation='sigmoid')(input_10)\n",
    "hidden_11= Dense(32, activation='sigmoid')(input_11)\n",
    "hidden_12 = Dense(32, activation='sigmoid')(input_12)\n",
    "hidden_13 = Dense(32, activation='sigmoid')(input_13)\n",
    "hidden_14 = Dense(32, activation='sigmoid')(input_14)\n",
    "hidden_15 = Dense(32, activation='sigmoid')(input_15)\n",
    "hidden_16 = Dense(32, activation='sigmoid')(input_16)\n",
    "hidden_17 = Dense(32, activation='sigmoid')(input_17)\n",
    "hidden_18 = Dense(32, activation='sigmoid')(input_18)\n",
    "hidden_19 = Dense(32, activation='sigmoid')(input_19)\n",
    "hidden_20 = Dense(32, activation='sigmoid')(input_20)\n",
    "\n",
    "hidden_21 = Dense(32, activation='sigmoid')(input_21)\n",
    "hidden_22 = Dense(32, activation='sigmoid')(input_22)\n",
    "hidden_23 = Dense(32, activation='sigmoid')(input_23)\n",
    "hidden_24 = Dense(32, activation='sigmoid')(input_24)\n",
    "hidden_25 = Dense(32, activation='sigmoid')(input_25)\n",
    "hidden_26 = Dense(32, activation='sigmoid')(input_26)\n",
    "hidden_27 = Dense(32, activation='sigmoid')(input_27)\n",
    "hidden_28 = Dense(32, activation='sigmoid')(input_28)\n",
    "hidden_29 = Dense(32, activation='sigmoid')(input_29)\n",
    "hidden_30 = Dense(32, activation='sigmoid')(input_30)\n",
    "hidden_31 = Dense(32, activation='sigmoid')(input_31)\n",
    "hidden_32 = Dense(32, activation='sigmoid')(input_32)\n",
    "hidden_33 = Dense(32, activation='sigmoid')(input_33)\n",
    "hidden_34 = Dense(32, activation='sigmoid')(input_34)\n",
    "hidden_35 = Dense(32, activation='sigmoid')(input_35)\n",
    "hidden_36 = Dense(32, activation='sigmoid')(input_36)\n",
    "hidden_37 = Dense(32, activation='sigmoid')(input_37)\n",
    "hidden_38 = Dense(32, activation='sigmoid')(input_38)\n",
    "hidden_39 = Dense(32, activation='sigmoid')(input_39)\n",
    "hidden_40 = Dense(32, activation='sigmoid')(input_40)\n",
    "hidden_41 = Dense(32, activation='sigmoid')(input_41)\n",
    "hidden_42 = Dense(32, activation='sigmoid')(input_42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "value_list=[X_train[['a1']].values,\n",
    "            X_train[['a2']].values,\n",
    "            X_train[['a3']].values,\n",
    "            X_train[['a4']].values,\n",
    "            X_train[['a5']].values,\n",
    "            X_train[['a6']].values,\n",
    "            X_train[['b1']].values,\n",
    "            X_train[['b2']].values,\n",
    "            X_train[['b3']].values,\n",
    "            X_train[['b4']].values,\n",
    "            X_train[['b5']].values,\n",
    "            X_train[['b6']].values,\n",
    "            X_train[['c1']].values,\n",
    "            X_train[['c2']].values,\n",
    "            X_train[['c3']].values,\n",
    "            X_train[['c4']].values,\n",
    "            X_train[['c5']].values,\n",
    "            X_train[['c6']].values,\n",
    "            X_train[['d1']].values,\n",
    "            X_train[['d2']].values,\n",
    "            X_train[['d3']].values,\n",
    "            X_train[['d4']].values,\n",
    "            X_train[['d5']].values,\n",
    "            X_train[['d6']].values,\n",
    "            X_train[['e1']].values,\n",
    "            X_train[['e2']].values,\n",
    "            X_train[['e3']].values,\n",
    "            X_train[['e4']].values,\n",
    "            X_train[['e5']].values,\n",
    "            X_train[['e6']].values,\n",
    "            X_train[['f1']].values,\n",
    "            X_train[['f2']].values,\n",
    "            X_train[['f3']].values,\n",
    "            X_train[['f4']].values,\n",
    "            X_train[['f5']].values,\n",
    "            X_train[['f6']].values,\n",
    "            X_train[['g1']].values,\n",
    "            X_train[['g2']].values,\n",
    "            X_train[['g3']].values,\n",
    "            X_train[['g4']].values,\n",
    "            X_train[['g5']].values,\n",
    "            X_train[['g6']].values,\n",
    "            \n",
    "           ]\n",
    "\n",
    "value_list_test=[X_test[['a1']].values,\n",
    "            X_test[['a2']].values,\n",
    "            X_test[['a3']].values,\n",
    "            X_test[['a4']].values,\n",
    "            X_test[['a5']].values,\n",
    "            X_test[['a6']].values,\n",
    "            X_test[['b1']].values,\n",
    "            X_test[['b2']].values,\n",
    "            X_test[['b3']].values,\n",
    "            X_test[['b4']].values,\n",
    "            X_test[['b5']].values,\n",
    "            X_test[['b6']].values,\n",
    "            X_test[['c1']].values,\n",
    "            X_test[['c2']].values,\n",
    "            X_test[['c3']].values,\n",
    "            X_test[['c4']].values,\n",
    "            X_test[['c5']].values,\n",
    "            X_test[['c6']].values,\n",
    "            X_test[['d1']].values,\n",
    "            X_test[['d2']].values,\n",
    "            X_test[['d3']].values,\n",
    "            X_test[['d4']].values,\n",
    "            X_test[['d5']].values,\n",
    "            X_test[['d6']].values,\n",
    "            X_test[['e1']].values,\n",
    "            X_test[['e2']].values,\n",
    "            X_test[['e3']].values,\n",
    "            X_test[['e4']].values,\n",
    "            X_test[['e5']].values,\n",
    "            X_test[['e6']].values,\n",
    "            X_test[['f1']].values,\n",
    "            X_test[['f2']].values,\n",
    "            X_test[['f3']].values,\n",
    "            X_test[['f4']].values,\n",
    "            X_test[['f5']].values,\n",
    "            X_test[['f6']].values,\n",
    "            X_test[['g1']].values,\n",
    "            X_test[['g2']].values,\n",
    "            X_test[['g3']].values,\n",
    "            X_test[['g4']].values,\n",
    "            X_test[['g5']].values,\n",
    "            X_test[['g6']].values,\n",
    "           ]\n",
    "\n",
    "x = keras.layers.concatenate([hidden_1,hidden_2,hidden_3,hidden_4,hidden_5,hidden_6,hidden_7,hidden_8,\n",
    "                             hidden_9,hidden_10,hidden_11,hidden_12,hidden_13,hidden_14,hidden_15,hidden_16,\n",
    "                             hidden_17,hidden_18,hidden_19,hidden_20,hidden_21,hidden_22,hidden_23,hidden_24,\n",
    "                             hidden_25,hidden_26,hidden_27,hidden_28,hidden_29,hidden_30,hidden_31,hidden_32,\n",
    "                             hidden_33,hidden_34,hidden_35,hidden_36,hidden_37,hidden_38,hidden_39,hidden_40,\n",
    "                             hidden_41,hidden_42])\n",
    "\n",
    "x = Dense(96, activation='sigmoid')(x)\n",
    "output = Dense(len(np.unique(Y_train)), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[input_1,input_2,input_3,input_4,input_5,input_6,input_7,input_8,\n",
    "                     input_9,input_10,input_11,input_12,input_13,input_14,input_15,input_16,\n",
    "                     input_17,input_18,input_19,input_20,input_21,input_22,input_23,input_24,\n",
    "                     input_25,input_26,input_27,input_28,input_29,input_30,input_31,input_32,\n",
    "                     input_33,input_34,input_35,input_36,input_37,input_38,input_39,input_40,\n",
    "                     input_41,input_42], outputs=[output])\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "hist=model.fit(\n",
    "    # Feature matrix\n",
    "    value_list, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    "    batch_size=256, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nq2TdPBezAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e5d67d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.model\")\n",
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in model.predict(value_list_test).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   86   465   999]\n",
      " [   89  3006  1097]\n",
      " [   52   609 10487]]\n",
      "80.3966844287\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.06      0.10      1550\n",
      "          1       0.74      0.72      0.73      4192\n",
      "          2       0.83      0.94      0.88     11148\n",
      "\n",
      "avg / total       0.77      0.80      0.77     16890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train,data_val=train_test_split(train,test_size=0.25, random_state=10)\n",
    "X_val=data_val.drop(['Class'], axis=1).values\n",
    "y_val=data_val['Class'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn_simple(data_train,X_val,y_val):\n",
    "    \n",
    "\n",
    "    data_train_new=data_train.sample(frac=0.632,replace=True)\n",
    "    X_train=data_train_new.drop(['Class'], axis=1).values\n",
    "    y_train=data_train_new['Class'].ravel()\n",
    "    \n",
    "    m = Sequential()\n",
    "    m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "    \n",
    "    m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    m.fit(\n",
    "    # Feature matrix\n",
    "    X_train, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=256, \n",
    "    )\n",
    "    m.load_weights(\"best.model\")\n",
    "    mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    "    )\n",
    "    y_pred = [mapping[pred] for pred in m.predict(X_val).argmax(axis=1)]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83848, saving model to best.model\n",
      "1s - loss: 0.9256 - acc: 0.6192 - val_loss: 0.8385 - val_acc: 0.6623\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.83848 to 0.83654, saving model to best.model\n",
      "1s - loss: 0.8620 - acc: 0.6572 - val_loss: 0.8365 - val_acc: 0.6623\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.83654 to 0.83602, saving model to best.model\n",
      "0s - loss: 0.8529 - acc: 0.6575 - val_loss: 0.8360 - val_acc: 0.6623\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "1s - loss: 0.8495 - acc: 0.6575 - val_loss: 0.8364 - val_acc: 0.6623\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83602 to 0.83428, saving model to best.model\n",
      "1s - loss: 0.8462 - acc: 0.6575 - val_loss: 0.8343 - val_acc: 0.6623\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83428 to 0.82971, saving model to best.model\n",
      "1s - loss: 0.8441 - acc: 0.6575 - val_loss: 0.8297 - val_acc: 0.6623\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82971 to 0.82378, saving model to best.model\n",
      "1s - loss: 0.8392 - acc: 0.6575 - val_loss: 0.8238 - val_acc: 0.6623\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82378 to 0.82109, saving model to best.model\n",
      "1s - loss: 0.8340 - acc: 0.6575 - val_loss: 0.8211 - val_acc: 0.6623\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82109 to 0.81998, saving model to best.model\n",
      "1s - loss: 0.8322 - acc: 0.6575 - val_loss: 0.8200 - val_acc: 0.6623\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81998 to 0.81886, saving model to best.model\n",
      "1s - loss: 0.8296 - acc: 0.6575 - val_loss: 0.8189 - val_acc: 0.6623\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81886 to 0.81842, saving model to best.model\n",
      "1s - loss: 0.8294 - acc: 0.6574 - val_loss: 0.8184 - val_acc: 0.6623\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81842 to 0.81772, saving model to best.model\n",
      "1s - loss: 0.8276 - acc: 0.6574 - val_loss: 0.8177 - val_acc: 0.6623\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8263 - acc: 0.6575 - val_loss: 0.8182 - val_acc: 0.6623\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8249 - acc: 0.6574 - val_loss: 0.8179 - val_acc: 0.6623\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81772 to 0.81680, saving model to best.model\n",
      "0s - loss: 0.8253 - acc: 0.6577 - val_loss: 0.8168 - val_acc: 0.6623\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8232 - acc: 0.6576 - val_loss: 0.8190 - val_acc: 0.6623\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81680 to 0.81588, saving model to best.model\n",
      "1s - loss: 0.8236 - acc: 0.6574 - val_loss: 0.8159 - val_acc: 0.6623\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81588 to 0.81549, saving model to best.model\n",
      "1s - loss: 0.8220 - acc: 0.6571 - val_loss: 0.8155 - val_acc: 0.6623\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "1s - loss: 0.8223 - acc: 0.6574 - val_loss: 0.8168 - val_acc: 0.6621\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81549 to 0.81431, saving model to best.model\n",
      "1s - loss: 0.8231 - acc: 0.6576 - val_loss: 0.8143 - val_acc: 0.6623\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81431 to 0.81305, saving model to best.model\n",
      "1s - loss: 0.8214 - acc: 0.6577 - val_loss: 0.8131 - val_acc: 0.6623\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81305 to 0.81242, saving model to best.model\n",
      "0s - loss: 0.8200 - acc: 0.6585 - val_loss: 0.8124 - val_acc: 0.6623\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81242 to 0.81224, saving model to best.model\n",
      "1s - loss: 0.8196 - acc: 0.6589 - val_loss: 0.8122 - val_acc: 0.6628\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81224 to 0.81091, saving model to best.model\n",
      "1s - loss: 0.8176 - acc: 0.6588 - val_loss: 0.8109 - val_acc: 0.6638\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "1s - loss: 0.8173 - acc: 0.6597 - val_loss: 0.8135 - val_acc: 0.6635\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81091 to 0.80911, saving model to best.model\n",
      "1s - loss: 0.8170 - acc: 0.6596 - val_loss: 0.8091 - val_acc: 0.6666\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80911 to 0.80893, saving model to best.model\n",
      "1s - loss: 0.8151 - acc: 0.6599 - val_loss: 0.8089 - val_acc: 0.6636\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80893 to 0.80726, saving model to best.model\n",
      "1s - loss: 0.8139 - acc: 0.6605 - val_loss: 0.8073 - val_acc: 0.6667\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80726 to 0.80588, saving model to best.model\n",
      "1s - loss: 0.8138 - acc: 0.6602 - val_loss: 0.8059 - val_acc: 0.6673\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80588 to 0.80584, saving model to best.model\n",
      "1s - loss: 0.8121 - acc: 0.6614 - val_loss: 0.8058 - val_acc: 0.6676\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80584 to 0.80477, saving model to best.model\n",
      "1s - loss: 0.8112 - acc: 0.6630 - val_loss: 0.8048 - val_acc: 0.6680\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80477 to 0.80373, saving model to best.model\n",
      "1s - loss: 0.8113 - acc: 0.6618 - val_loss: 0.8037 - val_acc: 0.6673\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80373 to 0.80250, saving model to best.model\n",
      "1s - loss: 0.8088 - acc: 0.6621 - val_loss: 0.8025 - val_acc: 0.6667\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80250 to 0.80067, saving model to best.model\n",
      "1s - loss: 0.8088 - acc: 0.6622 - val_loss: 0.8007 - val_acc: 0.6681\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80067 to 0.79942, saving model to best.model\n",
      "0s - loss: 0.8076 - acc: 0.6628 - val_loss: 0.7994 - val_acc: 0.6673\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79942 to 0.79760, saving model to best.model\n",
      "1s - loss: 0.8068 - acc: 0.6630 - val_loss: 0.7976 - val_acc: 0.6715\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79760 to 0.79694, saving model to best.model\n",
      "1s - loss: 0.8045 - acc: 0.6652 - val_loss: 0.7969 - val_acc: 0.6705\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79694 to 0.79470, saving model to best.model\n",
      "1s - loss: 0.8040 - acc: 0.6633 - val_loss: 0.7947 - val_acc: 0.6728\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79470 to 0.79393, saving model to best.model\n",
      "1s - loss: 0.8031 - acc: 0.6645 - val_loss: 0.7939 - val_acc: 0.6696\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79393 to 0.79186, saving model to best.model\n",
      "1s - loss: 0.8014 - acc: 0.6655 - val_loss: 0.7919 - val_acc: 0.6724\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79186 to 0.79043, saving model to best.model\n",
      "1s - loss: 0.8015 - acc: 0.6667 - val_loss: 0.7904 - val_acc: 0.6724\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.79043 to 0.78818, saving model to best.model\n",
      "1s - loss: 0.7974 - acc: 0.6695 - val_loss: 0.7882 - val_acc: 0.6740\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78818 to 0.78565, saving model to best.model\n",
      "1s - loss: 0.7959 - acc: 0.6686 - val_loss: 0.7857 - val_acc: 0.6747\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78565 to 0.78361, saving model to best.model\n",
      "1s - loss: 0.7957 - acc: 0.6684 - val_loss: 0.7836 - val_acc: 0.6747\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78361 to 0.78148, saving model to best.model\n",
      "0s - loss: 0.7921 - acc: 0.6719 - val_loss: 0.7815 - val_acc: 0.6762\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78148 to 0.77808, saving model to best.model\n",
      "1s - loss: 0.7900 - acc: 0.6697 - val_loss: 0.7781 - val_acc: 0.6780\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77808 to 0.77534, saving model to best.model\n",
      "1s - loss: 0.7884 - acc: 0.6727 - val_loss: 0.7753 - val_acc: 0.6801\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "1s - loss: 0.7888 - acc: 0.6720 - val_loss: 0.7756 - val_acc: 0.6766\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77534 to 0.77291, saving model to best.model\n",
      "1s - loss: 0.7879 - acc: 0.6730 - val_loss: 0.7729 - val_acc: 0.6778\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "1s - loss: 0.7863 - acc: 0.6756 - val_loss: 0.7733 - val_acc: 0.6815\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77291 to 0.76763, saving model to best.model\n",
      "1s - loss: 0.7815 - acc: 0.6761 - val_loss: 0.7676 - val_acc: 0.6849\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76763 to 0.76655, saving model to best.model\n",
      "1s - loss: 0.7807 - acc: 0.6776 - val_loss: 0.7665 - val_acc: 0.6817\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76655 to 0.76290, saving model to best.model\n",
      "1s - loss: 0.7820 - acc: 0.6758 - val_loss: 0.7629 - val_acc: 0.6895\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76290 to 0.76213, saving model to best.model\n",
      "1s - loss: 0.7786 - acc: 0.6775 - val_loss: 0.7621 - val_acc: 0.6876\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76213 to 0.76109, saving model to best.model\n",
      "1s - loss: 0.7793 - acc: 0.6768 - val_loss: 0.7611 - val_acc: 0.6872\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76109 to 0.75702, saving model to best.model\n",
      "1s - loss: 0.7745 - acc: 0.6791 - val_loss: 0.7570 - val_acc: 0.6905\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75702 to 0.75473, saving model to best.model\n",
      "0s - loss: 0.7728 - acc: 0.6805 - val_loss: 0.7547 - val_acc: 0.6920\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75473 to 0.75443, saving model to best.model\n",
      "1s - loss: 0.7725 - acc: 0.6820 - val_loss: 0.7544 - val_acc: 0.6910\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75443 to 0.74967, saving model to best.model\n",
      "1s - loss: 0.7718 - acc: 0.6806 - val_loss: 0.7497 - val_acc: 0.6938\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "1s - loss: 0.7686 - acc: 0.6807 - val_loss: 0.7508 - val_acc: 0.6918\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74967 to 0.74654, saving model to best.model\n",
      "0s - loss: 0.7681 - acc: 0.6817 - val_loss: 0.7465 - val_acc: 0.6966\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74654 to 0.74525, saving model to best.model\n",
      "1s - loss: 0.7654 - acc: 0.6821 - val_loss: 0.7452 - val_acc: 0.6968\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74525 to 0.74072, saving model to best.model\n",
      "0s - loss: 0.7638 - acc: 0.6849 - val_loss: 0.7407 - val_acc: 0.6987\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.74072 to 0.73863, saving model to best.model\n",
      "1s - loss: 0.7611 - acc: 0.6866 - val_loss: 0.7386 - val_acc: 0.6993\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.7602 - acc: 0.6854 - val_loss: 0.7387 - val_acc: 0.6967\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73863 to 0.73747, saving model to best.model\n",
      "0s - loss: 0.7619 - acc: 0.6857 - val_loss: 0.7375 - val_acc: 0.6952\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73747 to 0.73139, saving model to best.model\n",
      "1s - loss: 0.7566 - acc: 0.6864 - val_loss: 0.7314 - val_acc: 0.7042\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73139 to 0.72947, saving model to best.model\n",
      "0s - loss: 0.7571 - acc: 0.6873 - val_loss: 0.7295 - val_acc: 0.7042\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72947 to 0.72740, saving model to best.model\n",
      "0s - loss: 0.7528 - acc: 0.6898 - val_loss: 0.7274 - val_acc: 0.7042\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72740 to 0.72348, saving model to best.model\n",
      "0s - loss: 0.7529 - acc: 0.6906 - val_loss: 0.7235 - val_acc: 0.7060\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72348 to 0.72342, saving model to best.model\n",
      "1s - loss: 0.7466 - acc: 0.6925 - val_loss: 0.7234 - val_acc: 0.7050\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72342 to 0.72133, saving model to best.model\n",
      "0s - loss: 0.7494 - acc: 0.6937 - val_loss: 0.7213 - val_acc: 0.7061\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.72133 to 0.72116, saving model to best.model\n",
      "0s - loss: 0.7478 - acc: 0.6938 - val_loss: 0.7212 - val_acc: 0.7097\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.72116 to 0.71540, saving model to best.model\n",
      "0s - loss: 0.7435 - acc: 0.6930 - val_loss: 0.7154 - val_acc: 0.7112\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71540 to 0.71409, saving model to best.model\n",
      "0s - loss: 0.7440 - acc: 0.6931 - val_loss: 0.7141 - val_acc: 0.7125\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71409 to 0.70793, saving model to best.model\n",
      "0s - loss: 0.7417 - acc: 0.6949 - val_loss: 0.7079 - val_acc: 0.7176\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.7402 - acc: 0.6957 - val_loss: 0.7081 - val_acc: 0.7144\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70793 to 0.70489, saving model to best.model\n",
      "0s - loss: 0.7369 - acc: 0.6966 - val_loss: 0.7049 - val_acc: 0.7191\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.7401 - acc: 0.6977 - val_loss: 0.7077 - val_acc: 0.7152\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70489 to 0.70360, saving model to best.model\n",
      "0s - loss: 0.7339 - acc: 0.6998 - val_loss: 0.7036 - val_acc: 0.7151\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70360 to 0.70130, saving model to best.model\n",
      "0s - loss: 0.7339 - acc: 0.6986 - val_loss: 0.7013 - val_acc: 0.7193\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.70130 to 0.70009, saving model to best.model\n",
      "0s - loss: 0.7342 - acc: 0.6991 - val_loss: 0.7001 - val_acc: 0.7192\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.70009 to 0.69650, saving model to best.model\n",
      "0s - loss: 0.7300 - acc: 0.7014 - val_loss: 0.6965 - val_acc: 0.7211\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69650 to 0.69510, saving model to best.model\n",
      "0s - loss: 0.7312 - acc: 0.7010 - val_loss: 0.6951 - val_acc: 0.7220\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69510 to 0.69390, saving model to best.model\n",
      "0s - loss: 0.7275 - acc: 0.7025 - val_loss: 0.6939 - val_acc: 0.7201\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69390 to 0.69250, saving model to best.model\n",
      "0s - loss: 0.7277 - acc: 0.6989 - val_loss: 0.6925 - val_acc: 0.7206\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.69250 to 0.69172, saving model to best.model\n",
      "0s - loss: 0.7253 - acc: 0.7015 - val_loss: 0.6917 - val_acc: 0.7239\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.69172 to 0.68943, saving model to best.model\n",
      "0s - loss: 0.7240 - acc: 0.7043 - val_loss: 0.6894 - val_acc: 0.7207\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68943 to 0.68870, saving model to best.model\n",
      "0s - loss: 0.7223 - acc: 0.7043 - val_loss: 0.6887 - val_acc: 0.7206\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68870 to 0.68798, saving model to best.model\n",
      "0s - loss: 0.7222 - acc: 0.7032 - val_loss: 0.6880 - val_acc: 0.7225\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68798 to 0.68396, saving model to best.model\n",
      "0s - loss: 0.7206 - acc: 0.7047 - val_loss: 0.6840 - val_acc: 0.7280\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68396 to 0.68227, saving model to best.model\n",
      "0s - loss: 0.7229 - acc: 0.7025 - val_loss: 0.6823 - val_acc: 0.7278\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.7179 - acc: 0.7062 - val_loss: 0.6830 - val_acc: 0.7230\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68227 to 0.67631, saving model to best.model\n",
      "0s - loss: 0.7165 - acc: 0.7060 - val_loss: 0.6763 - val_acc: 0.7295\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.7158 - acc: 0.7055 - val_loss: 0.6781 - val_acc: 0.7249\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67631 to 0.67441, saving model to best.model\n",
      "0s - loss: 0.7143 - acc: 0.7083 - val_loss: 0.6744 - val_acc: 0.7289\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.7129 - acc: 0.7094 - val_loss: 0.6753 - val_acc: 0.7282\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.7117 - acc: 0.7077 - val_loss: 0.6751 - val_acc: 0.7293\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67441 to 0.67392, saving model to best.model\n",
      "0s - loss: 0.7107 - acc: 0.7087 - val_loss: 0.6739 - val_acc: 0.7278\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7099 - acc: 0.7084 - val_loss: 0.6743 - val_acc: 0.7270\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.67392 to 0.67124, saving model to best.model\n",
      "0s - loss: 0.7062 - acc: 0.7090 - val_loss: 0.6712 - val_acc: 0.7299\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67124 to 0.66765, saving model to best.model\n",
      "0s - loss: 0.7044 - acc: 0.7111 - val_loss: 0.6676 - val_acc: 0.7319\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7084 - acc: 0.7110 - val_loss: 0.6683 - val_acc: 0.7307\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.7059 - acc: 0.7101 - val_loss: 0.6692 - val_acc: 0.7303\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66765 to 0.66411, saving model to best.model\n",
      "0s - loss: 0.7041 - acc: 0.7118 - val_loss: 0.6641 - val_acc: 0.7329\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7065 - acc: 0.7111 - val_loss: 0.6685 - val_acc: 0.7272\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66411 to 0.66203, saving model to best.model\n",
      "0s - loss: 0.7025 - acc: 0.7122 - val_loss: 0.6620 - val_acc: 0.7336\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.7005 - acc: 0.7136 - val_loss: 0.6638 - val_acc: 0.7304\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66203 to 0.66059, saving model to best.model\n",
      "0s - loss: 0.7009 - acc: 0.7124 - val_loss: 0.6606 - val_acc: 0.7331\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66059 to 0.66054, saving model to best.model\n",
      "0s - loss: 0.6976 - acc: 0.7154 - val_loss: 0.6605 - val_acc: 0.7329\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.66054 to 0.66026, saving model to best.model\n",
      "0s - loss: 0.6971 - acc: 0.7132 - val_loss: 0.6603 - val_acc: 0.7323\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.66026 to 0.65722, saving model to best.model\n",
      "0s - loss: 0.6966 - acc: 0.7150 - val_loss: 0.6572 - val_acc: 0.7326\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6972 - acc: 0.7146 - val_loss: 0.6594 - val_acc: 0.7328\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.6975 - acc: 0.7145 - val_loss: 0.6582 - val_acc: 0.7330\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.65722 to 0.65517, saving model to best.model\n",
      "0s - loss: 0.6960 - acc: 0.7169 - val_loss: 0.6552 - val_acc: 0.7333\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65517 to 0.65361, saving model to best.model\n",
      "0s - loss: 0.6928 - acc: 0.7176 - val_loss: 0.6536 - val_acc: 0.7341\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.6963 - acc: 0.7151 - val_loss: 0.6551 - val_acc: 0.7345\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65361 to 0.65317, saving model to best.model\n",
      "0s - loss: 0.6921 - acc: 0.7171 - val_loss: 0.6532 - val_acc: 0.7362\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6932 - acc: 0.7154 - val_loss: 0.6550 - val_acc: 0.7321\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65317 to 0.65024, saving model to best.model\n",
      "0s - loss: 0.6893 - acc: 0.7164 - val_loss: 0.6502 - val_acc: 0.7367\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65024 to 0.64955, saving model to best.model\n",
      "0s - loss: 0.6888 - acc: 0.7170 - val_loss: 0.6496 - val_acc: 0.7357\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6900 - acc: 0.7184 - val_loss: 0.6500 - val_acc: 0.7362\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64955 to 0.64834, saving model to best.model\n",
      "0s - loss: 0.6892 - acc: 0.7184 - val_loss: 0.6483 - val_acc: 0.7358\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6896 - acc: 0.7193 - val_loss: 0.6502 - val_acc: 0.7356\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6866 - acc: 0.7198 - val_loss: 0.6499 - val_acc: 0.7350\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.64834 to 0.64606, saving model to best.model\n",
      "0s - loss: 0.6863 - acc: 0.7178 - val_loss: 0.6461 - val_acc: 0.7369\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.64606 to 0.64499, saving model to best.model\n",
      "0s - loss: 0.6858 - acc: 0.7188 - val_loss: 0.6450 - val_acc: 0.7390\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6824 - acc: 0.7229 - val_loss: 0.6468 - val_acc: 0.7363\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6840 - acc: 0.7209 - val_loss: 0.6467 - val_acc: 0.7355\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64499 to 0.64364, saving model to best.model\n",
      "0s - loss: 0.6823 - acc: 0.7219 - val_loss: 0.6436 - val_acc: 0.7381\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64364 to 0.64209, saving model to best.model\n",
      "0s - loss: 0.6803 - acc: 0.7222 - val_loss: 0.6421 - val_acc: 0.7374\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6821 - acc: 0.7212 - val_loss: 0.6430 - val_acc: 0.7383\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6814 - acc: 0.7200 - val_loss: 0.6426 - val_acc: 0.7378\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.64209 to 0.64141, saving model to best.model\n",
      "0s - loss: 0.6798 - acc: 0.7214 - val_loss: 0.6414 - val_acc: 0.7383\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.64141 to 0.64079, saving model to best.model\n",
      "0s - loss: 0.6779 - acc: 0.7230 - val_loss: 0.6408 - val_acc: 0.7393\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64079 to 0.64021, saving model to best.model\n",
      "0s - loss: 0.6786 - acc: 0.7228 - val_loss: 0.6402 - val_acc: 0.7384\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6771 - acc: 0.7240 - val_loss: 0.6410 - val_acc: 0.7382\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6771 - acc: 0.7231 - val_loss: 0.6411 - val_acc: 0.7374\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.64021 to 0.63778, saving model to best.model\n",
      "0s - loss: 0.6769 - acc: 0.7216 - val_loss: 0.6378 - val_acc: 0.7392\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63778 to 0.63534, saving model to best.model\n",
      "0s - loss: 0.6743 - acc: 0.7245 - val_loss: 0.6353 - val_acc: 0.7384\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.6739 - acc: 0.7239 - val_loss: 0.6362 - val_acc: 0.7413\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6722 - acc: 0.7253 - val_loss: 0.6356 - val_acc: 0.7393\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63534 to 0.63515, saving model to best.model\n",
      "0s - loss: 0.6755 - acc: 0.7237 - val_loss: 0.6351 - val_acc: 0.7398\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6758 - acc: 0.7232 - val_loss: 0.6357 - val_acc: 0.7393\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63515 to 0.63389, saving model to best.model\n",
      "0s - loss: 0.6762 - acc: 0.7240 - val_loss: 0.6339 - val_acc: 0.7404\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6741 - acc: 0.7253 - val_loss: 0.6343 - val_acc: 0.7396\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63389 to 0.62956, saving model to best.model\n",
      "0s - loss: 0.6692 - acc: 0.7283 - val_loss: 0.6296 - val_acc: 0.7437\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6725 - acc: 0.7242 - val_loss: 0.6354 - val_acc: 0.7393\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6707 - acc: 0.7240 - val_loss: 0.6313 - val_acc: 0.7420\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62956 to 0.62895, saving model to best.model\n",
      "0s - loss: 0.6698 - acc: 0.7265 - val_loss: 0.6289 - val_acc: 0.7433\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6697 - acc: 0.7260 - val_loss: 0.6315 - val_acc: 0.7419\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6699 - acc: 0.7253 - val_loss: 0.6298 - val_acc: 0.7433\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6692 - acc: 0.7277 - val_loss: 0.6321 - val_acc: 0.7407\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62895 to 0.62875, saving model to best.model\n",
      "0s - loss: 0.6692 - acc: 0.7265 - val_loss: 0.6287 - val_acc: 0.7431\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6670 - acc: 0.7269 - val_loss: 0.6295 - val_acc: 0.7424\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62875 to 0.62564, saving model to best.model\n",
      "0s - loss: 0.6645 - acc: 0.7314 - val_loss: 0.6256 - val_acc: 0.7436\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6677 - acc: 0.7273 - val_loss: 0.6267 - val_acc: 0.7427\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6655 - acc: 0.7277 - val_loss: 0.6262 - val_acc: 0.7424\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62564 to 0.62531, saving model to best.model\n",
      "1s - loss: 0.6650 - acc: 0.7276 - val_loss: 0.6253 - val_acc: 0.7434\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6640 - acc: 0.7298 - val_loss: 0.6264 - val_acc: 0.7422\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6629 - acc: 0.7300 - val_loss: 0.6260 - val_acc: 0.7439\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62531 to 0.62409, saving model to best.model\n",
      "1s - loss: 0.6649 - acc: 0.7284 - val_loss: 0.6241 - val_acc: 0.7438\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62409 to 0.62232, saving model to best.model\n",
      "1s - loss: 0.6623 - acc: 0.7278 - val_loss: 0.6223 - val_acc: 0.7441\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7304 - val_loss: 0.6234 - val_acc: 0.7438\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6627 - acc: 0.7279 - val_loss: 0.6242 - val_acc: 0.7430\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6613 - acc: 0.7335 - val_loss: 0.6248 - val_acc: 0.7430\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6628 - acc: 0.7296 - val_loss: 0.6224 - val_acc: 0.7441\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.62232 to 0.62098, saving model to best.model\n",
      "1s - loss: 0.6626 - acc: 0.7306 - val_loss: 0.6210 - val_acc: 0.7459\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.62098 to 0.62064, saving model to best.model\n",
      "1s - loss: 0.6581 - acc: 0.7310 - val_loss: 0.6206 - val_acc: 0.7468\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.62064 to 0.61929, saving model to best.model\n",
      "1s - loss: 0.6605 - acc: 0.7298 - val_loss: 0.6193 - val_acc: 0.7459\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61929 to 0.61896, saving model to best.model\n",
      "1s - loss: 0.6548 - acc: 0.7341 - val_loss: 0.6190 - val_acc: 0.7466\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6594 - acc: 0.7307 - val_loss: 0.6204 - val_acc: 0.7457\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61896 to 0.61892, saving model to best.model\n",
      "1s - loss: 0.6548 - acc: 0.7326 - val_loss: 0.6189 - val_acc: 0.7466\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6580 - acc: 0.7327 - val_loss: 0.6195 - val_acc: 0.7453\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6569 - acc: 0.7311 - val_loss: 0.6197 - val_acc: 0.7458\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.61892 to 0.61716, saving model to best.model\n",
      "1s - loss: 0.6562 - acc: 0.7296 - val_loss: 0.6172 - val_acc: 0.7460\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61716 to 0.61661, saving model to best.model\n",
      "1s - loss: 0.6541 - acc: 0.7344 - val_loss: 0.6166 - val_acc: 0.7460\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6580 - acc: 0.7297 - val_loss: 0.6218 - val_acc: 0.7433\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61661 to 0.61592, saving model to best.model\n",
      "1s - loss: 0.6549 - acc: 0.7319 - val_loss: 0.6159 - val_acc: 0.7467\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6537 - acc: 0.7340 - val_loss: 0.6160 - val_acc: 0.7481\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.61592 to 0.61405, saving model to best.model\n",
      "1s - loss: 0.6530 - acc: 0.7328 - val_loss: 0.6140 - val_acc: 0.7491\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6555 - acc: 0.7314 - val_loss: 0.6156 - val_acc: 0.7467\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6536 - acc: 0.7327 - val_loss: 0.6184 - val_acc: 0.7453\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6568 - acc: 0.7331 - val_loss: 0.6150 - val_acc: 0.7472\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61405 to 0.61320, saving model to best.model\n",
      "0s - loss: 0.6549 - acc: 0.7336 - val_loss: 0.6132 - val_acc: 0.7484\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6551 - acc: 0.7332 - val_loss: 0.6150 - val_acc: 0.7473\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.61320 to 0.61272, saving model to best.model\n",
      "1s - loss: 0.6515 - acc: 0.7344 - val_loss: 0.6127 - val_acc: 0.7479\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6538 - acc: 0.7327 - val_loss: 0.6139 - val_acc: 0.7475\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.61272 to 0.61190, saving model to best.model\n",
      "1s - loss: 0.6514 - acc: 0.7336 - val_loss: 0.6119 - val_acc: 0.7486\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.61190 to 0.61176, saving model to best.model\n",
      "0s - loss: 0.6531 - acc: 0.7316 - val_loss: 0.6118 - val_acc: 0.7494\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.61176 to 0.61127, saving model to best.model\n",
      "0s - loss: 0.6496 - acc: 0.7334 - val_loss: 0.6113 - val_acc: 0.7496\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6497 - acc: 0.7364 - val_loss: 0.6116 - val_acc: 0.7486\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6476 - acc: 0.7356 - val_loss: 0.6115 - val_acc: 0.7481\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6519 - acc: 0.7337 - val_loss: 0.6115 - val_acc: 0.7493\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.61127 to 0.60875, saving model to best.model\n",
      "1s - loss: 0.6493 - acc: 0.7369 - val_loss: 0.6087 - val_acc: 0.7502\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6487 - acc: 0.7355 - val_loss: 0.6104 - val_acc: 0.7484\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6501 - acc: 0.7342 - val_loss: 0.6102 - val_acc: 0.7494\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60875 to 0.60631, saving model to best.model\n",
      "1s - loss: 0.6472 - acc: 0.7364 - val_loss: 0.6063 - val_acc: 0.7506\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.6439 - acc: 0.7389 - val_loss: 0.6068 - val_acc: 0.7505\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6437 - acc: 0.7371 - val_loss: 0.6093 - val_acc: 0.7488\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84609, saving model to best.model\n",
      "1s - loss: 0.9104 - acc: 0.6345 - val_loss: 0.8461 - val_acc: 0.6564\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8564 - acc: 0.6619 - val_loss: 0.8462 - val_acc: 0.6564\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84609 to 0.84494, saving model to best.model\n",
      "1s - loss: 0.8484 - acc: 0.6623 - val_loss: 0.8449 - val_acc: 0.6564\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84494 to 0.83894, saving model to best.model\n",
      "0s - loss: 0.8455 - acc: 0.6623 - val_loss: 0.8389 - val_acc: 0.6564\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83894 to 0.83311, saving model to best.model\n",
      "1s - loss: 0.8397 - acc: 0.6623 - val_loss: 0.8331 - val_acc: 0.6564\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83311 to 0.82804, saving model to best.model\n",
      "1s - loss: 0.8350 - acc: 0.6623 - val_loss: 0.8280 - val_acc: 0.6564\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82804 to 0.82559, saving model to best.model\n",
      "1s - loss: 0.8321 - acc: 0.6623 - val_loss: 0.8256 - val_acc: 0.6564\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82559 to 0.82389, saving model to best.model\n",
      "1s - loss: 0.8288 - acc: 0.6623 - val_loss: 0.8239 - val_acc: 0.6564\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82389 to 0.82164, saving model to best.model\n",
      "1s - loss: 0.8268 - acc: 0.6623 - val_loss: 0.8216 - val_acc: 0.6564\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8264 - acc: 0.6623 - val_loss: 0.8217 - val_acc: 0.6564\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8257 - acc: 0.6623 - val_loss: 0.8219 - val_acc: 0.6564\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82164 to 0.82014, saving model to best.model\n",
      "1s - loss: 0.8237 - acc: 0.6623 - val_loss: 0.8201 - val_acc: 0.6564\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8246 - acc: 0.6623 - val_loss: 0.8225 - val_acc: 0.6564\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82014 to 0.81954, saving model to best.model\n",
      "1s - loss: 0.8237 - acc: 0.6623 - val_loss: 0.8195 - val_acc: 0.6564\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81954 to 0.81953, saving model to best.model\n",
      "1s - loss: 0.8227 - acc: 0.6623 - val_loss: 0.8195 - val_acc: 0.6564\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81953 to 0.81892, saving model to best.model\n",
      "1s - loss: 0.8224 - acc: 0.6623 - val_loss: 0.8189 - val_acc: 0.6564\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81892 to 0.81806, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6623 - val_loss: 0.8181 - val_acc: 0.6564\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81806 to 0.81733, saving model to best.model\n",
      "1s - loss: 0.8207 - acc: 0.6623 - val_loss: 0.8173 - val_acc: 0.6564\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "1s - loss: 0.8205 - acc: 0.6623 - val_loss: 0.8181 - val_acc: 0.6564\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "1s - loss: 0.8208 - acc: 0.6623 - val_loss: 0.8179 - val_acc: 0.6564\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81733 to 0.81614, saving model to best.model\n",
      "1s - loss: 0.8207 - acc: 0.6620 - val_loss: 0.8161 - val_acc: 0.6564\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81614 to 0.81436, saving model to best.model\n",
      "1s - loss: 0.8193 - acc: 0.6621 - val_loss: 0.8144 - val_acc: 0.6564\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "1s - loss: 0.8187 - acc: 0.6626 - val_loss: 0.8145 - val_acc: 0.6564\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "1s - loss: 0.8179 - acc: 0.6625 - val_loss: 0.8159 - val_acc: 0.6564\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81436 to 0.81297, saving model to best.model\n",
      "1s - loss: 0.8170 - acc: 0.6630 - val_loss: 0.8130 - val_acc: 0.6564\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81297 to 0.81065, saving model to best.model\n",
      "1s - loss: 0.8145 - acc: 0.6641 - val_loss: 0.8106 - val_acc: 0.6574\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "1s - loss: 0.8152 - acc: 0.6635 - val_loss: 0.8108 - val_acc: 0.6563\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81065 to 0.80871, saving model to best.model\n",
      "1s - loss: 0.8140 - acc: 0.6642 - val_loss: 0.8087 - val_acc: 0.6595\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.8128 - acc: 0.6646 - val_loss: 0.8101 - val_acc: 0.6570\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80871 to 0.80625, saving model to best.model\n",
      "0s - loss: 0.8119 - acc: 0.6659 - val_loss: 0.8063 - val_acc: 0.6607\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80625 to 0.80503, saving model to best.model\n",
      "0s - loss: 0.8097 - acc: 0.6668 - val_loss: 0.8050 - val_acc: 0.6617\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80503 to 0.80375, saving model to best.model\n",
      "1s - loss: 0.8089 - acc: 0.6657 - val_loss: 0.8038 - val_acc: 0.6618\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80375 to 0.80050, saving model to best.model\n",
      "1s - loss: 0.8071 - acc: 0.6678 - val_loss: 0.8005 - val_acc: 0.6635\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80050 to 0.79999, saving model to best.model\n",
      "0s - loss: 0.8043 - acc: 0.6698 - val_loss: 0.8000 - val_acc: 0.6631\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79999 to 0.79823, saving model to best.model\n",
      "1s - loss: 0.8037 - acc: 0.6678 - val_loss: 0.7982 - val_acc: 0.6643\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79823 to 0.79535, saving model to best.model\n",
      "1s - loss: 0.8018 - acc: 0.6697 - val_loss: 0.7954 - val_acc: 0.6632\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79535 to 0.79511, saving model to best.model\n",
      "1s - loss: 0.8000 - acc: 0.6689 - val_loss: 0.7951 - val_acc: 0.6622\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79511 to 0.79144, saving model to best.model\n",
      "0s - loss: 0.7992 - acc: 0.6704 - val_loss: 0.7914 - val_acc: 0.6646\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79144 to 0.78970, saving model to best.model\n",
      "0s - loss: 0.7972 - acc: 0.6702 - val_loss: 0.7897 - val_acc: 0.6639\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78970 to 0.78762, saving model to best.model\n",
      "0s - loss: 0.7954 - acc: 0.6718 - val_loss: 0.7876 - val_acc: 0.6693\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78762 to 0.78422, saving model to best.model\n",
      "0s - loss: 0.7941 - acc: 0.6734 - val_loss: 0.7842 - val_acc: 0.6731\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78422 to 0.78254, saving model to best.model\n",
      "1s - loss: 0.7916 - acc: 0.6731 - val_loss: 0.7825 - val_acc: 0.6701\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78254 to 0.78162, saving model to best.model\n",
      "0s - loss: 0.7909 - acc: 0.6727 - val_loss: 0.7816 - val_acc: 0.6725\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78162 to 0.77936, saving model to best.model\n",
      "1s - loss: 0.7880 - acc: 0.6736 - val_loss: 0.7794 - val_acc: 0.6710\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77936 to 0.77880, saving model to best.model\n",
      "1s - loss: 0.7896 - acc: 0.6741 - val_loss: 0.7788 - val_acc: 0.6707\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77880 to 0.77759, saving model to best.model\n",
      "1s - loss: 0.7868 - acc: 0.6758 - val_loss: 0.7776 - val_acc: 0.6747\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77759 to 0.77484, saving model to best.model\n",
      "1s - loss: 0.7873 - acc: 0.6757 - val_loss: 0.7748 - val_acc: 0.6745\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77484 to 0.77420, saving model to best.model\n",
      "0s - loss: 0.7847 - acc: 0.6766 - val_loss: 0.7742 - val_acc: 0.6804\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77420 to 0.77018, saving model to best.model\n",
      "1s - loss: 0.7817 - acc: 0.6803 - val_loss: 0.7702 - val_acc: 0.6836\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77018 to 0.76841, saving model to best.model\n",
      "1s - loss: 0.7840 - acc: 0.6778 - val_loss: 0.7684 - val_acc: 0.6828\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76841 to 0.76606, saving model to best.model\n",
      "0s - loss: 0.7804 - acc: 0.6769 - val_loss: 0.7661 - val_acc: 0.6843\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76606 to 0.76437, saving model to best.model\n",
      "0s - loss: 0.7766 - acc: 0.6813 - val_loss: 0.7644 - val_acc: 0.6845\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.7784 - acc: 0.6802 - val_loss: 0.7649 - val_acc: 0.6819\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76437 to 0.76095, saving model to best.model\n",
      "0s - loss: 0.7777 - acc: 0.6808 - val_loss: 0.7609 - val_acc: 0.6878\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76095 to 0.75640, saving model to best.model\n",
      "1s - loss: 0.7749 - acc: 0.6805 - val_loss: 0.7564 - val_acc: 0.6902\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.7726 - acc: 0.6801 - val_loss: 0.7571 - val_acc: 0.6882\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75640 to 0.75518, saving model to best.model\n",
      "0s - loss: 0.7715 - acc: 0.6822 - val_loss: 0.7552 - val_acc: 0.6888\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75518 to 0.75261, saving model to best.model\n",
      "1s - loss: 0.7686 - acc: 0.6834 - val_loss: 0.7526 - val_acc: 0.6888\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75261 to 0.74992, saving model to best.model\n",
      "0s - loss: 0.7677 - acc: 0.6830 - val_loss: 0.7499 - val_acc: 0.6902\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74992 to 0.74920, saving model to best.model\n",
      "0s - loss: 0.7682 - acc: 0.6839 - val_loss: 0.7492 - val_acc: 0.6924\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74920 to 0.74704, saving model to best.model\n",
      "1s - loss: 0.7652 - acc: 0.6848 - val_loss: 0.7470 - val_acc: 0.6932\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74704 to 0.74088, saving model to best.model\n",
      "0s - loss: 0.7622 - acc: 0.6856 - val_loss: 0.7409 - val_acc: 0.6939\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74088 to 0.73936, saving model to best.model\n",
      "0s - loss: 0.7598 - acc: 0.6848 - val_loss: 0.7394 - val_acc: 0.6961\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "1s - loss: 0.7596 - acc: 0.6887 - val_loss: 0.7399 - val_acc: 0.6963\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73936 to 0.73391, saving model to best.model\n",
      "1s - loss: 0.7589 - acc: 0.6886 - val_loss: 0.7339 - val_acc: 0.6967\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "1s - loss: 0.7575 - acc: 0.6902 - val_loss: 0.7340 - val_acc: 0.6979\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73391 to 0.73355, saving model to best.model\n",
      "1s - loss: 0.7553 - acc: 0.6885 - val_loss: 0.7336 - val_acc: 0.6989\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73355 to 0.72976, saving model to best.model\n",
      "1s - loss: 0.7515 - acc: 0.6899 - val_loss: 0.7298 - val_acc: 0.6999\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72976 to 0.72826, saving model to best.model\n",
      "1s - loss: 0.7534 - acc: 0.6902 - val_loss: 0.7283 - val_acc: 0.7004\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72826 to 0.72781, saving model to best.model\n",
      "1s - loss: 0.7512 - acc: 0.6897 - val_loss: 0.7278 - val_acc: 0.7016\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72781 to 0.72453, saving model to best.model\n",
      "1s - loss: 0.7481 - acc: 0.6915 - val_loss: 0.7245 - val_acc: 0.7006\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72453 to 0.71924, saving model to best.model\n",
      "1s - loss: 0.7468 - acc: 0.6952 - val_loss: 0.7192 - val_acc: 0.7033\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71924 to 0.71732, saving model to best.model\n",
      "1s - loss: 0.7431 - acc: 0.6942 - val_loss: 0.7173 - val_acc: 0.7061\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71732 to 0.71609, saving model to best.model\n",
      "1s - loss: 0.7414 - acc: 0.6957 - val_loss: 0.7161 - val_acc: 0.7063\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71609 to 0.71253, saving model to best.model\n",
      "1s - loss: 0.7428 - acc: 0.6939 - val_loss: 0.7125 - val_acc: 0.7059\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71253 to 0.71063, saving model to best.model\n",
      "1s - loss: 0.7392 - acc: 0.6975 - val_loss: 0.7106 - val_acc: 0.7063\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.71063 to 0.70834, saving model to best.model\n",
      "1s - loss: 0.7386 - acc: 0.6954 - val_loss: 0.7083 - val_acc: 0.7068\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "1s - loss: 0.7370 - acc: 0.6961 - val_loss: 0.7109 - val_acc: 0.7050\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70834 to 0.70448, saving model to best.model\n",
      "0s - loss: 0.7359 - acc: 0.6991 - val_loss: 0.7045 - val_acc: 0.7078\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7361 - acc: 0.6989 - val_loss: 0.7075 - val_acc: 0.7053\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70448 to 0.69976, saving model to best.model\n",
      "0s - loss: 0.7335 - acc: 0.6975 - val_loss: 0.6998 - val_acc: 0.7123\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7309 - acc: 0.7020 - val_loss: 0.6999 - val_acc: 0.7081\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69976 to 0.69186, saving model to best.model\n",
      "1s - loss: 0.7290 - acc: 0.7014 - val_loss: 0.6919 - val_acc: 0.7139\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7257 - acc: 0.7020 - val_loss: 0.6939 - val_acc: 0.7152\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7291 - acc: 0.7000 - val_loss: 0.6954 - val_acc: 0.7138\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.7274 - acc: 0.7017 - val_loss: 0.6923 - val_acc: 0.7141\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.69186 to 0.68873, saving model to best.model\n",
      "0s - loss: 0.7239 - acc: 0.7031 - val_loss: 0.6887 - val_acc: 0.7159\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68873 to 0.68677, saving model to best.model\n",
      "0s - loss: 0.7204 - acc: 0.7050 - val_loss: 0.6868 - val_acc: 0.7194\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68677 to 0.68599, saving model to best.model\n",
      "0s - loss: 0.7212 - acc: 0.7041 - val_loss: 0.6860 - val_acc: 0.7178\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.7242 - acc: 0.7025 - val_loss: 0.6863 - val_acc: 0.7193\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68599 to 0.68483, saving model to best.model\n",
      "0s - loss: 0.7210 - acc: 0.7060 - val_loss: 0.6848 - val_acc: 0.7155\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68483 to 0.68247, saving model to best.model\n",
      "0s - loss: 0.7176 - acc: 0.7088 - val_loss: 0.6825 - val_acc: 0.7204\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68247 to 0.68079, saving model to best.model\n",
      "0s - loss: 0.7213 - acc: 0.7052 - val_loss: 0.6808 - val_acc: 0.7211\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68079 to 0.67798, saving model to best.model\n",
      "0s - loss: 0.7158 - acc: 0.7076 - val_loss: 0.6780 - val_acc: 0.7228\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7160 - acc: 0.7077 - val_loss: 0.6789 - val_acc: 0.7170\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67798 to 0.67450, saving model to best.model\n",
      "1s - loss: 0.7118 - acc: 0.7088 - val_loss: 0.6745 - val_acc: 0.7248\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7124 - acc: 0.7110 - val_loss: 0.6753 - val_acc: 0.7218\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67450 to 0.66973, saving model to best.model\n",
      "1s - loss: 0.7081 - acc: 0.7106 - val_loss: 0.6697 - val_acc: 0.7231\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7130 - acc: 0.7083 - val_loss: 0.6729 - val_acc: 0.7247\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66973 to 0.66857, saving model to best.model\n",
      "1s - loss: 0.7117 - acc: 0.7092 - val_loss: 0.6686 - val_acc: 0.7265\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66857 to 0.66832, saving model to best.model\n",
      "1s - loss: 0.7088 - acc: 0.7087 - val_loss: 0.6683 - val_acc: 0.7235\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7102 - acc: 0.7082 - val_loss: 0.6703 - val_acc: 0.7227\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66832 to 0.66809, saving model to best.model\n",
      "1s - loss: 0.7086 - acc: 0.7084 - val_loss: 0.6681 - val_acc: 0.7251\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66809 to 0.66521, saving model to best.model\n",
      "1s - loss: 0.7068 - acc: 0.7112 - val_loss: 0.6652 - val_acc: 0.7256\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66521 to 0.66478, saving model to best.model\n",
      "1s - loss: 0.7058 - acc: 0.7105 - val_loss: 0.6648 - val_acc: 0.7310\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.66478 to 0.66141, saving model to best.model\n",
      "1s - loss: 0.7067 - acc: 0.7102 - val_loss: 0.6614 - val_acc: 0.7321\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66141 to 0.65913, saving model to best.model\n",
      "0s - loss: 0.7044 - acc: 0.7135 - val_loss: 0.6591 - val_acc: 0.7288\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7001 - acc: 0.7138 - val_loss: 0.6598 - val_acc: 0.7347\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65913 to 0.65514, saving model to best.model\n",
      "1s - loss: 0.7019 - acc: 0.7113 - val_loss: 0.6551 - val_acc: 0.7356\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.7009 - acc: 0.7134 - val_loss: 0.6585 - val_acc: 0.7300\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6989 - acc: 0.7154 - val_loss: 0.6592 - val_acc: 0.7281\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65514 to 0.65309, saving model to best.model\n",
      "1s - loss: 0.6962 - acc: 0.7148 - val_loss: 0.6531 - val_acc: 0.7317\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6938 - acc: 0.7195 - val_loss: 0.6544 - val_acc: 0.7336\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6960 - acc: 0.7152 - val_loss: 0.6544 - val_acc: 0.7317\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6948 - acc: 0.7183 - val_loss: 0.6570 - val_acc: 0.7315\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65309 to 0.65018, saving model to best.model\n",
      "1s - loss: 0.6944 - acc: 0.7165 - val_loss: 0.6502 - val_acc: 0.7340\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6919 - acc: 0.7186 - val_loss: 0.6511 - val_acc: 0.7330\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6928 - acc: 0.7183 - val_loss: 0.6509 - val_acc: 0.7354\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.65018 to 0.64751, saving model to best.model\n",
      "1s - loss: 0.6919 - acc: 0.7165 - val_loss: 0.6475 - val_acc: 0.7363\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64751 to 0.64526, saving model to best.model\n",
      "1s - loss: 0.6898 - acc: 0.7187 - val_loss: 0.6453 - val_acc: 0.7381\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6894 - acc: 0.7183 - val_loss: 0.6461 - val_acc: 0.7368\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6887 - acc: 0.7191 - val_loss: 0.6468 - val_acc: 0.7361\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64526 to 0.64329, saving model to best.model\n",
      "1s - loss: 0.6874 - acc: 0.7198 - val_loss: 0.6433 - val_acc: 0.7388\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64329 to 0.64184, saving model to best.model\n",
      "1s - loss: 0.6873 - acc: 0.7198 - val_loss: 0.6418 - val_acc: 0.7388\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6884 - acc: 0.7189 - val_loss: 0.6446 - val_acc: 0.7374\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6843 - acc: 0.7203 - val_loss: 0.6428 - val_acc: 0.7392\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6847 - acc: 0.7203 - val_loss: 0.6430 - val_acc: 0.7393\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64184 to 0.63854, saving model to best.model\n",
      "1s - loss: 0.6832 - acc: 0.7196 - val_loss: 0.6385 - val_acc: 0.7396\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6853 - acc: 0.7224 - val_loss: 0.6407 - val_acc: 0.7348\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6837 - acc: 0.7206 - val_loss: 0.6413 - val_acc: 0.7333\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.63854 to 0.63618, saving model to best.model\n",
      "1s - loss: 0.6818 - acc: 0.7238 - val_loss: 0.6362 - val_acc: 0.7409\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.63618 to 0.63366, saving model to best.model\n",
      "1s - loss: 0.6826 - acc: 0.7224 - val_loss: 0.6337 - val_acc: 0.7431\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6802 - acc: 0.7241 - val_loss: 0.6363 - val_acc: 0.7403\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6778 - acc: 0.7229 - val_loss: 0.6357 - val_acc: 0.7397\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6801 - acc: 0.7210 - val_loss: 0.6398 - val_acc: 0.7383\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6775 - acc: 0.7244 - val_loss: 0.6349 - val_acc: 0.7433\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63366 to 0.63164, saving model to best.model\n",
      "1s - loss: 0.6779 - acc: 0.7245 - val_loss: 0.6316 - val_acc: 0.7433\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6801 - acc: 0.7238 - val_loss: 0.6336 - val_acc: 0.7423\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6737 - acc: 0.7260 - val_loss: 0.6325 - val_acc: 0.7404\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63164 to 0.63078, saving model to best.model\n",
      "1s - loss: 0.6795 - acc: 0.7227 - val_loss: 0.6308 - val_acc: 0.7440\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.6787 - acc: 0.7235 - val_loss: 0.6331 - val_acc: 0.7395\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.63078 to 0.62747, saving model to best.model\n",
      "1s - loss: 0.6746 - acc: 0.7244 - val_loss: 0.6275 - val_acc: 0.7454\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6726 - acc: 0.7255 - val_loss: 0.6301 - val_acc: 0.7395\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62747 to 0.62598, saving model to best.model\n",
      "1s - loss: 0.6716 - acc: 0.7265 - val_loss: 0.6260 - val_acc: 0.7446\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6706 - acc: 0.7264 - val_loss: 0.6271 - val_acc: 0.7431\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62598 to 0.62538, saving model to best.model\n",
      "1s - loss: 0.6700 - acc: 0.7244 - val_loss: 0.6254 - val_acc: 0.7466\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7265 - val_loss: 0.6293 - val_acc: 0.7429\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62538 to 0.62345, saving model to best.model\n",
      "1s - loss: 0.6698 - acc: 0.7269 - val_loss: 0.6234 - val_acc: 0.7452\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.62345 to 0.62339, saving model to best.model\n",
      "0s - loss: 0.6677 - acc: 0.7272 - val_loss: 0.6234 - val_acc: 0.7459\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62339 to 0.62124, saving model to best.model\n",
      "0s - loss: 0.6679 - acc: 0.7278 - val_loss: 0.6212 - val_acc: 0.7458\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6698 - acc: 0.7288 - val_loss: 0.6229 - val_acc: 0.7464\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6680 - acc: 0.7261 - val_loss: 0.6214 - val_acc: 0.7458\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6657 - acc: 0.7274 - val_loss: 0.6247 - val_acc: 0.7453\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.6649 - acc: 0.7271 - val_loss: 0.6238 - val_acc: 0.7440\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62124 to 0.61890, saving model to best.model\n",
      "0s - loss: 0.6639 - acc: 0.7301 - val_loss: 0.6189 - val_acc: 0.7482\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6643 - acc: 0.7290 - val_loss: 0.6212 - val_acc: 0.7456\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6657 - acc: 0.7280 - val_loss: 0.6225 - val_acc: 0.7436\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6631 - acc: 0.7303 - val_loss: 0.6199 - val_acc: 0.7461\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61890 to 0.61716, saving model to best.model\n",
      "0s - loss: 0.6627 - acc: 0.7289 - val_loss: 0.6172 - val_acc: 0.7485\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6649 - acc: 0.7292 - val_loss: 0.6185 - val_acc: 0.7485\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.61716 to 0.61582, saving model to best.model\n",
      "1s - loss: 0.6578 - acc: 0.7310 - val_loss: 0.6158 - val_acc: 0.7498\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6623 - acc: 0.7299 - val_loss: 0.6173 - val_acc: 0.7484\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7306 - val_loss: 0.6163 - val_acc: 0.7485\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7287 - val_loss: 0.6191 - val_acc: 0.7447\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.61582 to 0.61480, saving model to best.model\n",
      "1s - loss: 0.6620 - acc: 0.7326 - val_loss: 0.6148 - val_acc: 0.7499\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6602 - acc: 0.7294 - val_loss: 0.6184 - val_acc: 0.7456\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6639 - acc: 0.7311 - val_loss: 0.6178 - val_acc: 0.7450\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61480 to 0.61311, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7328 - val_loss: 0.6131 - val_acc: 0.7494\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6601 - acc: 0.7301 - val_loss: 0.6152 - val_acc: 0.7479\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6571 - acc: 0.7301 - val_loss: 0.6150 - val_acc: 0.7458\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61311 to 0.61192, saving model to best.model\n",
      "1s - loss: 0.6597 - acc: 0.7296 - val_loss: 0.6119 - val_acc: 0.7509\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7319 - val_loss: 0.6125 - val_acc: 0.7495\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6586 - acc: 0.7312 - val_loss: 0.6121 - val_acc: 0.7514\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6593 - acc: 0.7316 - val_loss: 0.6123 - val_acc: 0.7488\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61192 to 0.61038, saving model to best.model\n",
      "1s - loss: 0.6589 - acc: 0.7307 - val_loss: 0.6104 - val_acc: 0.7504\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.61038 to 0.60858, saving model to best.model\n",
      "1s - loss: 0.6523 - acc: 0.7358 - val_loss: 0.6086 - val_acc: 0.7509\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6536 - acc: 0.7339 - val_loss: 0.6091 - val_acc: 0.7501\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.6552 - acc: 0.7307 - val_loss: 0.6108 - val_acc: 0.7500\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6517 - acc: 0.7348 - val_loss: 0.6090 - val_acc: 0.7514\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6531 - acc: 0.7332 - val_loss: 0.6125 - val_acc: 0.7464\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60858 to 0.60765, saving model to best.model\n",
      "1s - loss: 0.6534 - acc: 0.7340 - val_loss: 0.6076 - val_acc: 0.7532\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6529 - acc: 0.7349 - val_loss: 0.6113 - val_acc: 0.7482\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60765 to 0.60705, saving model to best.model\n",
      "0s - loss: 0.6533 - acc: 0.7350 - val_loss: 0.6071 - val_acc: 0.7528\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6518 - acc: 0.7356 - val_loss: 0.6093 - val_acc: 0.7491\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60705 to 0.60528, saving model to best.model\n",
      "1s - loss: 0.6517 - acc: 0.7361 - val_loss: 0.6053 - val_acc: 0.7526\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6511 - acc: 0.7359 - val_loss: 0.6094 - val_acc: 0.7500\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60528 to 0.60309, saving model to best.model\n",
      "0s - loss: 0.6448 - acc: 0.7387 - val_loss: 0.6031 - val_acc: 0.7569\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6486 - acc: 0.7356 - val_loss: 0.6038 - val_acc: 0.7574\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6502 - acc: 0.7355 - val_loss: 0.6068 - val_acc: 0.7555\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6523 - acc: 0.7337 - val_loss: 0.6045 - val_acc: 0.7553\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6473 - acc: 0.7336 - val_loss: 0.6049 - val_acc: 0.7529\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6507 - acc: 0.7349 - val_loss: 0.6039 - val_acc: 0.7540\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60309 to 0.60133, saving model to best.model\n",
      "0s - loss: 0.6494 - acc: 0.7344 - val_loss: 0.6013 - val_acc: 0.7583\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6467 - acc: 0.7368 - val_loss: 0.6031 - val_acc: 0.7552\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6451 - acc: 0.7385 - val_loss: 0.6040 - val_acc: 0.7525\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6485 - acc: 0.7360 - val_loss: 0.6025 - val_acc: 0.7560\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60133 to 0.60008, saving model to best.model\n",
      "1s - loss: 0.6428 - acc: 0.7358 - val_loss: 0.6001 - val_acc: 0.7573\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6456 - acc: 0.7366 - val_loss: 0.6036 - val_acc: 0.7515\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6447 - acc: 0.7359 - val_loss: 0.6040 - val_acc: 0.7528\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6419 - acc: 0.7386 - val_loss: 0.6028 - val_acc: 0.7518\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84761, saving model to best.model\n",
      "1s - loss: 0.9417 - acc: 0.6145 - val_loss: 0.8476 - val_acc: 0.6553\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84761 to 0.84633, saving model to best.model\n",
      "1s - loss: 0.8660 - acc: 0.6554 - val_loss: 0.8463 - val_acc: 0.6553\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84633 to 0.84573, saving model to best.model\n",
      "1s - loss: 0.8570 - acc: 0.6564 - val_loss: 0.8457 - val_acc: 0.6553\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84573 to 0.84316, saving model to best.model\n",
      "0s - loss: 0.8522 - acc: 0.6564 - val_loss: 0.8432 - val_acc: 0.6553\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84316 to 0.83548, saving model to best.model\n",
      "1s - loss: 0.8475 - acc: 0.6564 - val_loss: 0.8355 - val_acc: 0.6553\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83548 to 0.83011, saving model to best.model\n",
      "1s - loss: 0.8420 - acc: 0.6564 - val_loss: 0.8301 - val_acc: 0.6553\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83011 to 0.82519, saving model to best.model\n",
      "1s - loss: 0.8368 - acc: 0.6564 - val_loss: 0.8252 - val_acc: 0.6553\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82519 to 0.82245, saving model to best.model\n",
      "1s - loss: 0.8335 - acc: 0.6564 - val_loss: 0.8224 - val_acc: 0.6553\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82245 to 0.82240, saving model to best.model\n",
      "1s - loss: 0.8317 - acc: 0.6563 - val_loss: 0.8224 - val_acc: 0.6553\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82240 to 0.82027, saving model to best.model\n",
      "1s - loss: 0.8298 - acc: 0.6563 - val_loss: 0.8203 - val_acc: 0.6553\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8298 - acc: 0.6563 - val_loss: 0.8236 - val_acc: 0.6553\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82027 to 0.82020, saving model to best.model\n",
      "1s - loss: 0.8287 - acc: 0.6568 - val_loss: 0.8202 - val_acc: 0.6553\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82020 to 0.81991, saving model to best.model\n",
      "1s - loss: 0.8272 - acc: 0.6562 - val_loss: 0.8199 - val_acc: 0.6553\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81991 to 0.81893, saving model to best.model\n",
      "1s - loss: 0.8265 - acc: 0.6567 - val_loss: 0.8189 - val_acc: 0.6553\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8260 - acc: 0.6566 - val_loss: 0.8190 - val_acc: 0.6553\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81893 to 0.81750, saving model to best.model\n",
      "0s - loss: 0.8246 - acc: 0.6559 - val_loss: 0.8175 - val_acc: 0.6553\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81750 to 0.81745, saving model to best.model\n",
      "1s - loss: 0.8252 - acc: 0.6565 - val_loss: 0.8175 - val_acc: 0.6553\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.8246 - acc: 0.6565 - val_loss: 0.8176 - val_acc: 0.6553\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81745 to 0.81640, saving model to best.model\n",
      "0s - loss: 0.8238 - acc: 0.6563 - val_loss: 0.8164 - val_acc: 0.6553\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.8238 - acc: 0.6565 - val_loss: 0.8175 - val_acc: 0.6553\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81640 to 0.81484, saving model to best.model\n",
      "1s - loss: 0.8229 - acc: 0.6565 - val_loss: 0.8148 - val_acc: 0.6553\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81484 to 0.81448, saving model to best.model\n",
      "1s - loss: 0.8210 - acc: 0.6568 - val_loss: 0.8145 - val_acc: 0.6553\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81448 to 0.81396, saving model to best.model\n",
      "1s - loss: 0.8198 - acc: 0.6565 - val_loss: 0.8140 - val_acc: 0.6581\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81396 to 0.81220, saving model to best.model\n",
      "1s - loss: 0.8203 - acc: 0.6561 - val_loss: 0.8122 - val_acc: 0.6553\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81220 to 0.81129, saving model to best.model\n",
      "1s - loss: 0.8171 - acc: 0.6579 - val_loss: 0.8113 - val_acc: 0.6567\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81129 to 0.81070, saving model to best.model\n",
      "1s - loss: 0.8168 - acc: 0.6581 - val_loss: 0.8107 - val_acc: 0.6569\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81070 to 0.80998, saving model to best.model\n",
      "0s - loss: 0.8173 - acc: 0.6582 - val_loss: 0.8100 - val_acc: 0.6568\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80998 to 0.80692, saving model to best.model\n",
      "1s - loss: 0.8140 - acc: 0.6589 - val_loss: 0.8069 - val_acc: 0.6607\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.8144 - acc: 0.6597 - val_loss: 0.8072 - val_acc: 0.6591\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80692 to 0.80619, saving model to best.model\n",
      "1s - loss: 0.8119 - acc: 0.6609 - val_loss: 0.8062 - val_acc: 0.6596\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80619 to 0.80263, saving model to best.model\n",
      "1s - loss: 0.8110 - acc: 0.6611 - val_loss: 0.8026 - val_acc: 0.6651\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "1s - loss: 0.8105 - acc: 0.6607 - val_loss: 0.8031 - val_acc: 0.6656\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80263 to 0.80090, saving model to best.model\n",
      "1s - loss: 0.8076 - acc: 0.6609 - val_loss: 0.8009 - val_acc: 0.6652\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80090 to 0.79916, saving model to best.model\n",
      "1s - loss: 0.8064 - acc: 0.6624 - val_loss: 0.7992 - val_acc: 0.6641\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79916 to 0.79441, saving model to best.model\n",
      "1s - loss: 0.8039 - acc: 0.6633 - val_loss: 0.7944 - val_acc: 0.6697\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "1s - loss: 0.8029 - acc: 0.6642 - val_loss: 0.7964 - val_acc: 0.6650\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79441 to 0.79142, saving model to best.model\n",
      "0s - loss: 0.8017 - acc: 0.6638 - val_loss: 0.7914 - val_acc: 0.6701\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79142 to 0.79097, saving model to best.model\n",
      "1s - loss: 0.8023 - acc: 0.6650 - val_loss: 0.7910 - val_acc: 0.6703\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79097 to 0.78798, saving model to best.model\n",
      "1s - loss: 0.7991 - acc: 0.6649 - val_loss: 0.7880 - val_acc: 0.6715\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78798 to 0.78690, saving model to best.model\n",
      "1s - loss: 0.7974 - acc: 0.6646 - val_loss: 0.7869 - val_acc: 0.6683\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78690 to 0.78448, saving model to best.model\n",
      "1s - loss: 0.7952 - acc: 0.6672 - val_loss: 0.7845 - val_acc: 0.6684\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78448 to 0.78266, saving model to best.model\n",
      "0s - loss: 0.7932 - acc: 0.6662 - val_loss: 0.7827 - val_acc: 0.6741\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78266 to 0.78034, saving model to best.model\n",
      "0s - loss: 0.7914 - acc: 0.6674 - val_loss: 0.7803 - val_acc: 0.6749\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78034 to 0.77705, saving model to best.model\n",
      "1s - loss: 0.7914 - acc: 0.6673 - val_loss: 0.7771 - val_acc: 0.6765\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77705 to 0.77690, saving model to best.model\n",
      "0s - loss: 0.7889 - acc: 0.6705 - val_loss: 0.7769 - val_acc: 0.6756\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77690 to 0.77410, saving model to best.model\n",
      "1s - loss: 0.7875 - acc: 0.6702 - val_loss: 0.7741 - val_acc: 0.6778\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.7864 - acc: 0.6705 - val_loss: 0.7752 - val_acc: 0.6731\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77410 to 0.77018, saving model to best.model\n",
      "0s - loss: 0.7844 - acc: 0.6708 - val_loss: 0.7702 - val_acc: 0.6809\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77018 to 0.76874, saving model to best.model\n",
      "0s - loss: 0.7804 - acc: 0.6741 - val_loss: 0.7687 - val_acc: 0.6799\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76874 to 0.76821, saving model to best.model\n",
      "1s - loss: 0.7824 - acc: 0.6727 - val_loss: 0.7682 - val_acc: 0.6813\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76821 to 0.76308, saving model to best.model\n",
      "0s - loss: 0.7822 - acc: 0.6725 - val_loss: 0.7631 - val_acc: 0.6835\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76308 to 0.75979, saving model to best.model\n",
      "1s - loss: 0.7757 - acc: 0.6741 - val_loss: 0.7598 - val_acc: 0.6858\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75979 to 0.75704, saving model to best.model\n",
      "1s - loss: 0.7729 - acc: 0.6770 - val_loss: 0.7570 - val_acc: 0.6872\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75704 to 0.75510, saving model to best.model\n",
      "0s - loss: 0.7730 - acc: 0.6762 - val_loss: 0.7551 - val_acc: 0.6896\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.7699 - acc: 0.6774 - val_loss: 0.7556 - val_acc: 0.6852\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75510 to 0.75097, saving model to best.model\n",
      "1s - loss: 0.7693 - acc: 0.6783 - val_loss: 0.7510 - val_acc: 0.6872\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75097 to 0.74632, saving model to best.model\n",
      "1s - loss: 0.7662 - acc: 0.6810 - val_loss: 0.7463 - val_acc: 0.6929\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "1s - loss: 0.7674 - acc: 0.6811 - val_loss: 0.7470 - val_acc: 0.6896\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74632 to 0.74302, saving model to best.model\n",
      "1s - loss: 0.7613 - acc: 0.6849 - val_loss: 0.7430 - val_acc: 0.6961\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74302 to 0.73829, saving model to best.model\n",
      "1s - loss: 0.7625 - acc: 0.6851 - val_loss: 0.7383 - val_acc: 0.6978\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73829 to 0.73515, saving model to best.model\n",
      "1s - loss: 0.7599 - acc: 0.6853 - val_loss: 0.7351 - val_acc: 0.6981\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73515 to 0.73305, saving model to best.model\n",
      "1s - loss: 0.7581 - acc: 0.6876 - val_loss: 0.7330 - val_acc: 0.6985\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "1s - loss: 0.7585 - acc: 0.6834 - val_loss: 0.7330 - val_acc: 0.6977\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73305 to 0.72864, saving model to best.model\n",
      "1s - loss: 0.7533 - acc: 0.6876 - val_loss: 0.7286 - val_acc: 0.6971\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72864 to 0.72646, saving model to best.model\n",
      "0s - loss: 0.7507 - acc: 0.6896 - val_loss: 0.7265 - val_acc: 0.7018\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72646 to 0.71990, saving model to best.model\n",
      "0s - loss: 0.7493 - acc: 0.6899 - val_loss: 0.7199 - val_acc: 0.7063\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.7499 - acc: 0.6900 - val_loss: 0.7251 - val_acc: 0.6968\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71990 to 0.71860, saving model to best.model\n",
      "1s - loss: 0.7477 - acc: 0.6890 - val_loss: 0.7186 - val_acc: 0.7034\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71860 to 0.71521, saving model to best.model\n",
      "1s - loss: 0.7449 - acc: 0.6929 - val_loss: 0.7152 - val_acc: 0.7097\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71521 to 0.71508, saving model to best.model\n",
      "1s - loss: 0.7427 - acc: 0.6945 - val_loss: 0.7151 - val_acc: 0.7049\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71508 to 0.70724, saving model to best.model\n",
      "1s - loss: 0.7404 - acc: 0.6947 - val_loss: 0.7072 - val_acc: 0.7126\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.7417 - acc: 0.6952 - val_loss: 0.7102 - val_acc: 0.7063\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7394 - acc: 0.6972 - val_loss: 0.7081 - val_acc: 0.7122\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70724 to 0.70221, saving model to best.model\n",
      "0s - loss: 0.7350 - acc: 0.6973 - val_loss: 0.7022 - val_acc: 0.7135\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.7372 - acc: 0.6963 - val_loss: 0.7059 - val_acc: 0.7091\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "1s - loss: 0.7353 - acc: 0.6970 - val_loss: 0.7049 - val_acc: 0.7052\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70221 to 0.69607, saving model to best.model\n",
      "0s - loss: 0.7342 - acc: 0.6965 - val_loss: 0.6961 - val_acc: 0.7151\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.7300 - acc: 0.6986 - val_loss: 0.6974 - val_acc: 0.7122\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69607 to 0.69171, saving model to best.model\n",
      "1s - loss: 0.7284 - acc: 0.7008 - val_loss: 0.6917 - val_acc: 0.7166\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7302 - acc: 0.6986 - val_loss: 0.6945 - val_acc: 0.7157\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7293 - acc: 0.7005 - val_loss: 0.6919 - val_acc: 0.7210\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69171 to 0.68996, saving model to best.model\n",
      "1s - loss: 0.7242 - acc: 0.7021 - val_loss: 0.6900 - val_acc: 0.7192\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68996 to 0.68866, saving model to best.model\n",
      "1s - loss: 0.7265 - acc: 0.7023 - val_loss: 0.6887 - val_acc: 0.7203\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68866 to 0.68610, saving model to best.model\n",
      "1s - loss: 0.7241 - acc: 0.7036 - val_loss: 0.6861 - val_acc: 0.7199\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7226 - acc: 0.7057 - val_loss: 0.6868 - val_acc: 0.7212\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7221 - acc: 0.7034 - val_loss: 0.6871 - val_acc: 0.7198\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68610 to 0.68260, saving model to best.model\n",
      "1s - loss: 0.7209 - acc: 0.7054 - val_loss: 0.6826 - val_acc: 0.7211\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68260 to 0.67928, saving model to best.model\n",
      "1s - loss: 0.7200 - acc: 0.7070 - val_loss: 0.6793 - val_acc: 0.7242\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67928 to 0.67792, saving model to best.model\n",
      "1s - loss: 0.7188 - acc: 0.7064 - val_loss: 0.6779 - val_acc: 0.7261\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67792 to 0.67676, saving model to best.model\n",
      "1s - loss: 0.7169 - acc: 0.7059 - val_loss: 0.6768 - val_acc: 0.7261\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67676 to 0.67536, saving model to best.model\n",
      "1s - loss: 0.7169 - acc: 0.7042 - val_loss: 0.6754 - val_acc: 0.7248\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67536 to 0.67247, saving model to best.model\n",
      "1s - loss: 0.7124 - acc: 0.7079 - val_loss: 0.6725 - val_acc: 0.7254\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7109 - acc: 0.7105 - val_loss: 0.6760 - val_acc: 0.7258\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67247 to 0.67032, saving model to best.model\n",
      "1s - loss: 0.7086 - acc: 0.7103 - val_loss: 0.6703 - val_acc: 0.7296\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7121 - acc: 0.7098 - val_loss: 0.6704 - val_acc: 0.7317\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67032 to 0.66806, saving model to best.model\n",
      "1s - loss: 0.7089 - acc: 0.7100 - val_loss: 0.6681 - val_acc: 0.7270\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7102 - acc: 0.7101 - val_loss: 0.6698 - val_acc: 0.7278\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.66806 to 0.66802, saving model to best.model\n",
      "1s - loss: 0.7101 - acc: 0.7107 - val_loss: 0.6680 - val_acc: 0.7321\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66802 to 0.66486, saving model to best.model\n",
      "1s - loss: 0.7067 - acc: 0.7096 - val_loss: 0.6649 - val_acc: 0.7316\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7077 - acc: 0.7102 - val_loss: 0.6661 - val_acc: 0.7303\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66486 to 0.66218, saving model to best.model\n",
      "1s - loss: 0.7081 - acc: 0.7103 - val_loss: 0.6622 - val_acc: 0.7324\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7065 - acc: 0.7102 - val_loss: 0.6626 - val_acc: 0.7302\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66218 to 0.66194, saving model to best.model\n",
      "1s - loss: 0.7053 - acc: 0.7123 - val_loss: 0.6619 - val_acc: 0.7338\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66194 to 0.65701, saving model to best.model\n",
      "1s - loss: 0.6997 - acc: 0.7155 - val_loss: 0.6570 - val_acc: 0.7365\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7034 - acc: 0.7114 - val_loss: 0.6572 - val_acc: 0.7347\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65701 to 0.65634, saving model to best.model\n",
      "1s - loss: 0.7004 - acc: 0.7135 - val_loss: 0.6563 - val_acc: 0.7356\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.7009 - acc: 0.7151 - val_loss: 0.6568 - val_acc: 0.7368\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65634 to 0.65495, saving model to best.model\n",
      "1s - loss: 0.6965 - acc: 0.7163 - val_loss: 0.6550 - val_acc: 0.7374\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.6988 - acc: 0.7140 - val_loss: 0.6571 - val_acc: 0.7342\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65495 to 0.65018, saving model to best.model\n",
      "1s - loss: 0.6964 - acc: 0.7169 - val_loss: 0.6502 - val_acc: 0.7385\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6941 - acc: 0.7170 - val_loss: 0.6513 - val_acc: 0.7382\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.6956 - acc: 0.7168 - val_loss: 0.6517 - val_acc: 0.7369\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6969 - acc: 0.7174 - val_loss: 0.6512 - val_acc: 0.7354\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6956 - acc: 0.7150 - val_loss: 0.6535 - val_acc: 0.7336\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.65018 to 0.65003, saving model to best.model\n",
      "1s - loss: 0.6944 - acc: 0.7157 - val_loss: 0.6500 - val_acc: 0.7349\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65003 to 0.64873, saving model to best.model\n",
      "0s - loss: 0.6933 - acc: 0.7170 - val_loss: 0.6487 - val_acc: 0.7352\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64873 to 0.64608, saving model to best.model\n",
      "0s - loss: 0.6905 - acc: 0.7182 - val_loss: 0.6461 - val_acc: 0.7388\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.6923 - acc: 0.7182 - val_loss: 0.6513 - val_acc: 0.7345\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6913 - acc: 0.7196 - val_loss: 0.6481 - val_acc: 0.7415\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64608 to 0.64547, saving model to best.model\n",
      "0s - loss: 0.6909 - acc: 0.7179 - val_loss: 0.6455 - val_acc: 0.7365\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64547 to 0.64448, saving model to best.model\n",
      "0s - loss: 0.6891 - acc: 0.7193 - val_loss: 0.6445 - val_acc: 0.7385\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.64448 to 0.64134, saving model to best.model\n",
      "0s - loss: 0.6867 - acc: 0.7208 - val_loss: 0.6413 - val_acc: 0.7384\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6876 - acc: 0.7215 - val_loss: 0.6427 - val_acc: 0.7388\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64134 to 0.63949, saving model to best.model\n",
      "0s - loss: 0.6864 - acc: 0.7201 - val_loss: 0.6395 - val_acc: 0.7395\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.63949 to 0.63716, saving model to best.model\n",
      "1s - loss: 0.6860 - acc: 0.7193 - val_loss: 0.6372 - val_acc: 0.7436\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6829 - acc: 0.7242 - val_loss: 0.6401 - val_acc: 0.7381\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6849 - acc: 0.7229 - val_loss: 0.6412 - val_acc: 0.7396\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6833 - acc: 0.7220 - val_loss: 0.6380 - val_acc: 0.7412\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6827 - acc: 0.7226 - val_loss: 0.6388 - val_acc: 0.7369\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6830 - acc: 0.7241 - val_loss: 0.6375 - val_acc: 0.7381\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6839 - acc: 0.7215 - val_loss: 0.6391 - val_acc: 0.7404\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6817 - acc: 0.7230 - val_loss: 0.6393 - val_acc: 0.7398\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63716 to 0.63570, saving model to best.model\n",
      "1s - loss: 0.6800 - acc: 0.7203 - val_loss: 0.6357 - val_acc: 0.7403\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63570 to 0.63408, saving model to best.model\n",
      "0s - loss: 0.6791 - acc: 0.7211 - val_loss: 0.6341 - val_acc: 0.7403\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6773 - acc: 0.7259 - val_loss: 0.6346 - val_acc: 0.7419\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63408 to 0.63083, saving model to best.model\n",
      "1s - loss: 0.6771 - acc: 0.7247 - val_loss: 0.6308 - val_acc: 0.7433\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6772 - acc: 0.7240 - val_loss: 0.6311 - val_acc: 0.7404\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6786 - acc: 0.7232 - val_loss: 0.6333 - val_acc: 0.7438\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6788 - acc: 0.7232 - val_loss: 0.6316 - val_acc: 0.7446\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6791 - acc: 0.7227 - val_loss: 0.6337 - val_acc: 0.7406\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6806 - acc: 0.7222 - val_loss: 0.6325 - val_acc: 0.7418\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6756 - acc: 0.7249 - val_loss: 0.6327 - val_acc: 0.7399\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63083 to 0.63050, saving model to best.model\n",
      "0s - loss: 0.6733 - acc: 0.7252 - val_loss: 0.6305 - val_acc: 0.7434\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63050 to 0.62785, saving model to best.model\n",
      "0s - loss: 0.6719 - acc: 0.7255 - val_loss: 0.6278 - val_acc: 0.7458\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.62785 to 0.62577, saving model to best.model\n",
      "1s - loss: 0.6722 - acc: 0.7261 - val_loss: 0.6258 - val_acc: 0.7454\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6702 - acc: 0.7289 - val_loss: 0.6258 - val_acc: 0.7437\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.62577 to 0.62336, saving model to best.model\n",
      "1s - loss: 0.6745 - acc: 0.7261 - val_loss: 0.6234 - val_acc: 0.7479\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6721 - acc: 0.7248 - val_loss: 0.6249 - val_acc: 0.7441\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6736 - acc: 0.7276 - val_loss: 0.6248 - val_acc: 0.7439\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6671 - acc: 0.7307 - val_loss: 0.6234 - val_acc: 0.7470\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6726 - acc: 0.7254 - val_loss: 0.6257 - val_acc: 0.7443\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62336 to 0.62327, saving model to best.model\n",
      "1s - loss: 0.6720 - acc: 0.7275 - val_loss: 0.6233 - val_acc: 0.7444\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62327 to 0.62214, saving model to best.model\n",
      "1s - loss: 0.6693 - acc: 0.7276 - val_loss: 0.6221 - val_acc: 0.7473\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6712 - acc: 0.7258 - val_loss: 0.6241 - val_acc: 0.7433\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6706 - acc: 0.7279 - val_loss: 0.6223 - val_acc: 0.7474\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62214 to 0.62140, saving model to best.model\n",
      "1s - loss: 0.6696 - acc: 0.7278 - val_loss: 0.6214 - val_acc: 0.7467\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.62140 to 0.62025, saving model to best.model\n",
      "1s - loss: 0.6676 - acc: 0.7296 - val_loss: 0.6203 - val_acc: 0.7493\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.62025 to 0.61817, saving model to best.model\n",
      "1s - loss: 0.6677 - acc: 0.7274 - val_loss: 0.6182 - val_acc: 0.7496\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6669 - acc: 0.7289 - val_loss: 0.6195 - val_acc: 0.7498\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6678 - acc: 0.7275 - val_loss: 0.6188 - val_acc: 0.7496\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6673 - acc: 0.7284 - val_loss: 0.6230 - val_acc: 0.7463\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61817 to 0.61753, saving model to best.model\n",
      "1s - loss: 0.6662 - acc: 0.7291 - val_loss: 0.6175 - val_acc: 0.7481\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6662 - acc: 0.7296 - val_loss: 0.6178 - val_acc: 0.7495\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61753 to 0.61627, saving model to best.model\n",
      "1s - loss: 0.6653 - acc: 0.7304 - val_loss: 0.6163 - val_acc: 0.7498\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.61627 to 0.61523, saving model to best.model\n",
      "1s - loss: 0.6665 - acc: 0.7307 - val_loss: 0.6152 - val_acc: 0.7513\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61523 to 0.61503, saving model to best.model\n",
      "1s - loss: 0.6617 - acc: 0.7318 - val_loss: 0.6150 - val_acc: 0.7501\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6653 - acc: 0.7281 - val_loss: 0.6166 - val_acc: 0.7486\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61503 to 0.61271, saving model to best.model\n",
      "1s - loss: 0.6621 - acc: 0.7330 - val_loss: 0.6127 - val_acc: 0.7500\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6604 - acc: 0.7304 - val_loss: 0.6132 - val_acc: 0.7499\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6631 - acc: 0.7296 - val_loss: 0.6170 - val_acc: 0.7498\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6609 - acc: 0.7296 - val_loss: 0.6138 - val_acc: 0.7506\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61271 to 0.61115, saving model to best.model\n",
      "0s - loss: 0.6613 - acc: 0.7310 - val_loss: 0.6111 - val_acc: 0.7527\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61115 to 0.61026, saving model to best.model\n",
      "1s - loss: 0.6585 - acc: 0.7315 - val_loss: 0.6103 - val_acc: 0.7511\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6598 - acc: 0.7321 - val_loss: 0.6106 - val_acc: 0.7506\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6573 - acc: 0.7319 - val_loss: 0.6109 - val_acc: 0.7500\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6592 - acc: 0.7305 - val_loss: 0.6130 - val_acc: 0.7479\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6603 - acc: 0.7309 - val_loss: 0.6109 - val_acc: 0.7526\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61026 to 0.60848, saving model to best.model\n",
      "0s - loss: 0.6560 - acc: 0.7311 - val_loss: 0.6085 - val_acc: 0.7516\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6589 - acc: 0.7329 - val_loss: 0.6102 - val_acc: 0.7514\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.6580 - acc: 0.7312 - val_loss: 0.6102 - val_acc: 0.7501\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60848 to 0.60650, saving model to best.model\n",
      "0s - loss: 0.6543 - acc: 0.7330 - val_loss: 0.6065 - val_acc: 0.7533\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6572 - acc: 0.7328 - val_loss: 0.6089 - val_acc: 0.7519\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6549 - acc: 0.7344 - val_loss: 0.6083 - val_acc: 0.7532\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6540 - acc: 0.7342 - val_loss: 0.6075 - val_acc: 0.7513\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6554 - acc: 0.7341 - val_loss: 0.6076 - val_acc: 0.7514\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.60650 to 0.60631, saving model to best.model\n",
      "0s - loss: 0.6582 - acc: 0.7330 - val_loss: 0.6063 - val_acc: 0.7539\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60631 to 0.60458, saving model to best.model\n",
      "1s - loss: 0.6568 - acc: 0.7326 - val_loss: 0.6046 - val_acc: 0.7547\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6575 - acc: 0.7323 - val_loss: 0.6052 - val_acc: 0.7552\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60458 to 0.60439, saving model to best.model\n",
      "1s - loss: 0.6519 - acc: 0.7331 - val_loss: 0.6044 - val_acc: 0.7559\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60439 to 0.60351, saving model to best.model\n",
      "1s - loss: 0.6535 - acc: 0.7354 - val_loss: 0.6035 - val_acc: 0.7541\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60351 to 0.60013, saving model to best.model\n",
      "1s - loss: 0.6502 - acc: 0.7367 - val_loss: 0.6001 - val_acc: 0.7583\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6541 - acc: 0.7345 - val_loss: 0.6052 - val_acc: 0.7546\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60013 to 0.59971, saving model to best.model\n",
      "1s - loss: 0.6516 - acc: 0.7353 - val_loss: 0.5997 - val_acc: 0.7574\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6519 - acc: 0.7350 - val_loss: 0.6024 - val_acc: 0.7550\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6531 - acc: 0.7336 - val_loss: 0.6010 - val_acc: 0.7553\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6520 - acc: 0.7352 - val_loss: 0.6011 - val_acc: 0.7554\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.59971 to 0.59947, saving model to best.model\n",
      "0s - loss: 0.6507 - acc: 0.7342 - val_loss: 0.5995 - val_acc: 0.7564\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6526 - acc: 0.7345 - val_loss: 0.6000 - val_acc: 0.7563\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.59947 to 0.59900, saving model to best.model\n",
      "0s - loss: 0.6497 - acc: 0.7353 - val_loss: 0.5990 - val_acc: 0.7564\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6491 - acc: 0.7347 - val_loss: 0.6011 - val_acc: 0.7577\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83947, saving model to best.model\n",
      "1s - loss: 0.9113 - acc: 0.6296 - val_loss: 0.8395 - val_acc: 0.6633\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8585 - acc: 0.6569 - val_loss: 0.8399 - val_acc: 0.6633\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "1s - loss: 0.8522 - acc: 0.6570 - val_loss: 0.8395 - val_acc: 0.6633\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83947 to 0.83416, saving model to best.model\n",
      "0s - loss: 0.8493 - acc: 0.6570 - val_loss: 0.8342 - val_acc: 0.6633\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83416 to 0.82805, saving model to best.model\n",
      "1s - loss: 0.8424 - acc: 0.6570 - val_loss: 0.8280 - val_acc: 0.6633\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82805 to 0.82419, saving model to best.model\n",
      "1s - loss: 0.8373 - acc: 0.6570 - val_loss: 0.8242 - val_acc: 0.6633\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82419 to 0.82146, saving model to best.model\n",
      "1s - loss: 0.8349 - acc: 0.6570 - val_loss: 0.8215 - val_acc: 0.6633\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82146 to 0.81883, saving model to best.model\n",
      "1s - loss: 0.8316 - acc: 0.6570 - val_loss: 0.8188 - val_acc: 0.6633\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "1s - loss: 0.8313 - acc: 0.6569 - val_loss: 0.8192 - val_acc: 0.6633\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81883 to 0.81818, saving model to best.model\n",
      "0s - loss: 0.8295 - acc: 0.6570 - val_loss: 0.8182 - val_acc: 0.6633\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81818 to 0.81685, saving model to best.model\n",
      "0s - loss: 0.8284 - acc: 0.6570 - val_loss: 0.8168 - val_acc: 0.6633\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81685 to 0.81680, saving model to best.model\n",
      "0s - loss: 0.8290 - acc: 0.6569 - val_loss: 0.8168 - val_acc: 0.6633\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.8280 - acc: 0.6570 - val_loss: 0.8171 - val_acc: 0.6633\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81680 to 0.81647, saving model to best.model\n",
      "0s - loss: 0.8279 - acc: 0.6569 - val_loss: 0.8165 - val_acc: 0.6633\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81647 to 0.81582, saving model to best.model\n",
      "0s - loss: 0.8266 - acc: 0.6569 - val_loss: 0.8158 - val_acc: 0.6633\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8250 - acc: 0.6570 - val_loss: 0.8186 - val_acc: 0.6633\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81582 to 0.81493, saving model to best.model\n",
      "1s - loss: 0.8246 - acc: 0.6570 - val_loss: 0.8149 - val_acc: 0.6633\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81493 to 0.81418, saving model to best.model\n",
      "0s - loss: 0.8239 - acc: 0.6569 - val_loss: 0.8142 - val_acc: 0.6633\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81418 to 0.81245, saving model to best.model\n",
      "0s - loss: 0.8214 - acc: 0.6570 - val_loss: 0.8125 - val_acc: 0.6633\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81245 to 0.81217, saving model to best.model\n",
      "0s - loss: 0.8230 - acc: 0.6567 - val_loss: 0.8122 - val_acc: 0.6633\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81217 to 0.81043, saving model to best.model\n",
      "0s - loss: 0.8221 - acc: 0.6567 - val_loss: 0.8104 - val_acc: 0.6633\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.8203 - acc: 0.6574 - val_loss: 0.8114 - val_acc: 0.6632\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81043 to 0.80837, saving model to best.model\n",
      "0s - loss: 0.8201 - acc: 0.6575 - val_loss: 0.8084 - val_acc: 0.6628\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80837 to 0.80687, saving model to best.model\n",
      "0s - loss: 0.8177 - acc: 0.6576 - val_loss: 0.8069 - val_acc: 0.6628\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.8165 - acc: 0.6574 - val_loss: 0.8071 - val_acc: 0.6667\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80687 to 0.80503, saving model to best.model\n",
      "1s - loss: 0.8159 - acc: 0.6595 - val_loss: 0.8050 - val_acc: 0.6636\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80503 to 0.80335, saving model to best.model\n",
      "1s - loss: 0.8154 - acc: 0.6589 - val_loss: 0.8033 - val_acc: 0.6658\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "1s - loss: 0.8146 - acc: 0.6593 - val_loss: 0.8037 - val_acc: 0.6672\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80335 to 0.79961, saving model to best.model\n",
      "1s - loss: 0.8128 - acc: 0.6609 - val_loss: 0.7996 - val_acc: 0.6697\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.79961 to 0.79946, saving model to best.model\n",
      "1s - loss: 0.8123 - acc: 0.6608 - val_loss: 0.7995 - val_acc: 0.6685\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79946 to 0.79724, saving model to best.model\n",
      "0s - loss: 0.8099 - acc: 0.6611 - val_loss: 0.7972 - val_acc: 0.6678\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79724 to 0.79454, saving model to best.model\n",
      "0s - loss: 0.8070 - acc: 0.6639 - val_loss: 0.7945 - val_acc: 0.6693\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79454 to 0.79226, saving model to best.model\n",
      "0s - loss: 0.8068 - acc: 0.6630 - val_loss: 0.7923 - val_acc: 0.6740\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79226 to 0.78912, saving model to best.model\n",
      "0s - loss: 0.8045 - acc: 0.6645 - val_loss: 0.7891 - val_acc: 0.6755\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.78912 to 0.78720, saving model to best.model\n",
      "0s - loss: 0.8035 - acc: 0.6648 - val_loss: 0.7872 - val_acc: 0.6772\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.78720 to 0.78675, saving model to best.model\n",
      "0s - loss: 0.8019 - acc: 0.6666 - val_loss: 0.7867 - val_acc: 0.6737\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78675 to 0.78511, saving model to best.model\n",
      "0s - loss: 0.8013 - acc: 0.6665 - val_loss: 0.7851 - val_acc: 0.6748\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78511 to 0.78138, saving model to best.model\n",
      "0s - loss: 0.7983 - acc: 0.6652 - val_loss: 0.7814 - val_acc: 0.6819\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78138 to 0.78085, saving model to best.model\n",
      "0s - loss: 0.7973 - acc: 0.6676 - val_loss: 0.7808 - val_acc: 0.6768\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78085 to 0.77854, saving model to best.model\n",
      "0s - loss: 0.7962 - acc: 0.6691 - val_loss: 0.7785 - val_acc: 0.6779\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77854 to 0.77662, saving model to best.model\n",
      "0s - loss: 0.7941 - acc: 0.6690 - val_loss: 0.7766 - val_acc: 0.6843\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77662 to 0.77528, saving model to best.model\n",
      "0s - loss: 0.7925 - acc: 0.6691 - val_loss: 0.7753 - val_acc: 0.6878\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77528 to 0.77419, saving model to best.model\n",
      "0s - loss: 0.7914 - acc: 0.6701 - val_loss: 0.7742 - val_acc: 0.6875\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77419 to 0.77151, saving model to best.model\n",
      "0s - loss: 0.7905 - acc: 0.6722 - val_loss: 0.7715 - val_acc: 0.6884\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77151 to 0.76977, saving model to best.model\n",
      "0s - loss: 0.7867 - acc: 0.6727 - val_loss: 0.7698 - val_acc: 0.6907\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76977 to 0.76663, saving model to best.model\n",
      "0s - loss: 0.7852 - acc: 0.6731 - val_loss: 0.7666 - val_acc: 0.6877\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76663 to 0.76307, saving model to best.model\n",
      "1s - loss: 0.7849 - acc: 0.6736 - val_loss: 0.7631 - val_acc: 0.6922\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "1s - loss: 0.7826 - acc: 0.6761 - val_loss: 0.7638 - val_acc: 0.6909\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76307 to 0.76108, saving model to best.model\n",
      "1s - loss: 0.7806 - acc: 0.6761 - val_loss: 0.7611 - val_acc: 0.6952\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76108 to 0.75951, saving model to best.model\n",
      "0s - loss: 0.7802 - acc: 0.6737 - val_loss: 0.7595 - val_acc: 0.6899\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75951 to 0.75638, saving model to best.model\n",
      "0s - loss: 0.7777 - acc: 0.6782 - val_loss: 0.7564 - val_acc: 0.6913\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75638 to 0.75149, saving model to best.model\n",
      "1s - loss: 0.7743 - acc: 0.6802 - val_loss: 0.7515 - val_acc: 0.6987\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "1s - loss: 0.7752 - acc: 0.6789 - val_loss: 0.7541 - val_acc: 0.6941\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75149 to 0.74713, saving model to best.model\n",
      "1s - loss: 0.7728 - acc: 0.6791 - val_loss: 0.7471 - val_acc: 0.6989\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74713 to 0.74480, saving model to best.model\n",
      "1s - loss: 0.7709 - acc: 0.6834 - val_loss: 0.7448 - val_acc: 0.7013\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74480 to 0.74347, saving model to best.model\n",
      "1s - loss: 0.7695 - acc: 0.6810 - val_loss: 0.7435 - val_acc: 0.7032\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74347 to 0.74003, saving model to best.model\n",
      "1s - loss: 0.7666 - acc: 0.6842 - val_loss: 0.7400 - val_acc: 0.7034\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74003 to 0.73806, saving model to best.model\n",
      "1s - loss: 0.7651 - acc: 0.6854 - val_loss: 0.7381 - val_acc: 0.7037\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73806 to 0.73476, saving model to best.model\n",
      "1s - loss: 0.7655 - acc: 0.6843 - val_loss: 0.7348 - val_acc: 0.7029\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73476 to 0.73088, saving model to best.model\n",
      "1s - loss: 0.7626 - acc: 0.6840 - val_loss: 0.7309 - val_acc: 0.7057\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73088 to 0.72983, saving model to best.model\n",
      "1s - loss: 0.7586 - acc: 0.6879 - val_loss: 0.7298 - val_acc: 0.7046\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72983 to 0.72912, saving model to best.model\n",
      "1s - loss: 0.7562 - acc: 0.6883 - val_loss: 0.7291 - val_acc: 0.7078\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.72912 to 0.72267, saving model to best.model\n",
      "1s - loss: 0.7561 - acc: 0.6870 - val_loss: 0.7227 - val_acc: 0.7069\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72267 to 0.72221, saving model to best.model\n",
      "1s - loss: 0.7556 - acc: 0.6886 - val_loss: 0.7222 - val_acc: 0.7074\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72221 to 0.71801, saving model to best.model\n",
      "1s - loss: 0.7533 - acc: 0.6892 - val_loss: 0.7180 - val_acc: 0.7132\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71801 to 0.71515, saving model to best.model\n",
      "1s - loss: 0.7514 - acc: 0.6904 - val_loss: 0.7152 - val_acc: 0.7119\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.71515 to 0.71371, saving model to best.model\n",
      "1s - loss: 0.7475 - acc: 0.6926 - val_loss: 0.7137 - val_acc: 0.7114\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71371 to 0.71197, saving model to best.model\n",
      "1s - loss: 0.7458 - acc: 0.6937 - val_loss: 0.7120 - val_acc: 0.7151\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71197 to 0.71019, saving model to best.model\n",
      "1s - loss: 0.7462 - acc: 0.6917 - val_loss: 0.7102 - val_acc: 0.7108\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71019 to 0.70794, saving model to best.model\n",
      "1s - loss: 0.7436 - acc: 0.6959 - val_loss: 0.7079 - val_acc: 0.7126\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70794 to 0.70600, saving model to best.model\n",
      "1s - loss: 0.7416 - acc: 0.6936 - val_loss: 0.7060 - val_acc: 0.7155\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70600 to 0.70219, saving model to best.model\n",
      "1s - loss: 0.7389 - acc: 0.6955 - val_loss: 0.7022 - val_acc: 0.7150\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70219 to 0.69711, saving model to best.model\n",
      "1s - loss: 0.7366 - acc: 0.6989 - val_loss: 0.6971 - val_acc: 0.7237\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.7374 - acc: 0.6977 - val_loss: 0.6973 - val_acc: 0.7261\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69711 to 0.69635, saving model to best.model\n",
      "1s - loss: 0.7349 - acc: 0.6969 - val_loss: 0.6963 - val_acc: 0.7246\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69635 to 0.69575, saving model to best.model\n",
      "1s - loss: 0.7358 - acc: 0.6957 - val_loss: 0.6958 - val_acc: 0.7235\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69575 to 0.69295, saving model to best.model\n",
      "1s - loss: 0.7297 - acc: 0.7002 - val_loss: 0.6930 - val_acc: 0.7206\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69295 to 0.68650, saving model to best.model\n",
      "1s - loss: 0.7287 - acc: 0.7007 - val_loss: 0.6865 - val_acc: 0.7275\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7302 - acc: 0.6981 - val_loss: 0.6874 - val_acc: 0.7286\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.68650 to 0.68046, saving model to best.model\n",
      "1s - loss: 0.7283 - acc: 0.7011 - val_loss: 0.6805 - val_acc: 0.7287\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68046 to 0.67942, saving model to best.model\n",
      "1s - loss: 0.7247 - acc: 0.7012 - val_loss: 0.6794 - val_acc: 0.7300\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7241 - acc: 0.7046 - val_loss: 0.6843 - val_acc: 0.7282\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.67942 to 0.67852, saving model to best.model\n",
      "1s - loss: 0.7237 - acc: 0.7031 - val_loss: 0.6785 - val_acc: 0.7331\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67852 to 0.67568, saving model to best.model\n",
      "1s - loss: 0.7228 - acc: 0.7030 - val_loss: 0.6757 - val_acc: 0.7315\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7210 - acc: 0.7071 - val_loss: 0.6760 - val_acc: 0.7316\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67568 to 0.67524, saving model to best.model\n",
      "1s - loss: 0.7229 - acc: 0.7028 - val_loss: 0.6752 - val_acc: 0.7304\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67524 to 0.67185, saving model to best.model\n",
      "1s - loss: 0.7137 - acc: 0.7094 - val_loss: 0.6718 - val_acc: 0.7327\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7173 - acc: 0.7071 - val_loss: 0.6724 - val_acc: 0.7343\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67185 to 0.67087, saving model to best.model\n",
      "1s - loss: 0.7143 - acc: 0.7070 - val_loss: 0.6709 - val_acc: 0.7357\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67087 to 0.66784, saving model to best.model\n",
      "1s - loss: 0.7131 - acc: 0.7094 - val_loss: 0.6678 - val_acc: 0.7361\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.66784 to 0.66638, saving model to best.model\n",
      "0s - loss: 0.7148 - acc: 0.7077 - val_loss: 0.6664 - val_acc: 0.7358\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66638 to 0.66631, saving model to best.model\n",
      "1s - loss: 0.7080 - acc: 0.7121 - val_loss: 0.6663 - val_acc: 0.7352\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66631 to 0.66552, saving model to best.model\n",
      "1s - loss: 0.7090 - acc: 0.7111 - val_loss: 0.6655 - val_acc: 0.7317\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66552 to 0.66477, saving model to best.model\n",
      "0s - loss: 0.7106 - acc: 0.7095 - val_loss: 0.6648 - val_acc: 0.7358\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66477 to 0.65791, saving model to best.model\n",
      "1s - loss: 0.7073 - acc: 0.7109 - val_loss: 0.6579 - val_acc: 0.7372\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7070 - acc: 0.7103 - val_loss: 0.6612 - val_acc: 0.7390\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7056 - acc: 0.7114 - val_loss: 0.6588 - val_acc: 0.7396\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7066 - acc: 0.7107 - val_loss: 0.6599 - val_acc: 0.7388\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.65791 to 0.65693, saving model to best.model\n",
      "1s - loss: 0.7035 - acc: 0.7111 - val_loss: 0.6569 - val_acc: 0.7385\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.65693 to 0.65231, saving model to best.model\n",
      "0s - loss: 0.7031 - acc: 0.7125 - val_loss: 0.6523 - val_acc: 0.7403\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7061 - acc: 0.7123 - val_loss: 0.6574 - val_acc: 0.7343\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7002 - acc: 0.7133 - val_loss: 0.6548 - val_acc: 0.7376\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.65231 to 0.65209, saving model to best.model\n",
      "0s - loss: 0.6993 - acc: 0.7146 - val_loss: 0.6521 - val_acc: 0.7393\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65209 to 0.64812, saving model to best.model\n",
      "0s - loss: 0.6981 - acc: 0.7147 - val_loss: 0.6481 - val_acc: 0.7410\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.6996 - acc: 0.7129 - val_loss: 0.6511 - val_acc: 0.7390\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.6973 - acc: 0.7143 - val_loss: 0.6486 - val_acc: 0.7418\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.64812 to 0.64483, saving model to best.model\n",
      "1s - loss: 0.6940 - acc: 0.7168 - val_loss: 0.6448 - val_acc: 0.7448\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6949 - acc: 0.7167 - val_loss: 0.6455 - val_acc: 0.7418\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64483 to 0.64302, saving model to best.model\n",
      "1s - loss: 0.6925 - acc: 0.7175 - val_loss: 0.6430 - val_acc: 0.7436\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.6924 - acc: 0.7146 - val_loss: 0.6438 - val_acc: 0.7416\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.64302 to 0.64180, saving model to best.model\n",
      "1s - loss: 0.6935 - acc: 0.7155 - val_loss: 0.6418 - val_acc: 0.7427\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6925 - acc: 0.7183 - val_loss: 0.6424 - val_acc: 0.7452\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6907 - acc: 0.7164 - val_loss: 0.6419 - val_acc: 0.7434\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.64180 to 0.63993, saving model to best.model\n",
      "1s - loss: 0.6902 - acc: 0.7194 - val_loss: 0.6399 - val_acc: 0.7437\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.63993 to 0.63885, saving model to best.model\n",
      "1s - loss: 0.6885 - acc: 0.7187 - val_loss: 0.6389 - val_acc: 0.7437\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.63885 to 0.63802, saving model to best.model\n",
      "0s - loss: 0.6858 - acc: 0.7194 - val_loss: 0.6380 - val_acc: 0.7436\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.63802 to 0.63525, saving model to best.model\n",
      "0s - loss: 0.6889 - acc: 0.7198 - val_loss: 0.6352 - val_acc: 0.7461\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.63525 to 0.63436, saving model to best.model\n",
      "1s - loss: 0.6846 - acc: 0.7203 - val_loss: 0.6344 - val_acc: 0.7453\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6861 - acc: 0.7197 - val_loss: 0.6373 - val_acc: 0.7444\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.63436 to 0.63245, saving model to best.model\n",
      "1s - loss: 0.6842 - acc: 0.7215 - val_loss: 0.6325 - val_acc: 0.7475\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.63245 to 0.63124, saving model to best.model\n",
      "1s - loss: 0.6831 - acc: 0.7188 - val_loss: 0.6312 - val_acc: 0.7479\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6837 - acc: 0.7202 - val_loss: 0.6333 - val_acc: 0.7475\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6811 - acc: 0.7216 - val_loss: 0.6320 - val_acc: 0.7485\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6843 - acc: 0.7227 - val_loss: 0.6329 - val_acc: 0.7474\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6834 - acc: 0.7209 - val_loss: 0.6334 - val_acc: 0.7434\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63124 to 0.62878, saving model to best.model\n",
      "0s - loss: 0.6829 - acc: 0.7205 - val_loss: 0.6288 - val_acc: 0.7471\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62878 to 0.62860, saving model to best.model\n",
      "0s - loss: 0.6819 - acc: 0.7228 - val_loss: 0.6286 - val_acc: 0.7479\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6777 - acc: 0.7228 - val_loss: 0.6297 - val_acc: 0.7493\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.62860 to 0.62667, saving model to best.model\n",
      "1s - loss: 0.6762 - acc: 0.7219 - val_loss: 0.6267 - val_acc: 0.7481\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6827 - acc: 0.7193 - val_loss: 0.6297 - val_acc: 0.7493\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6770 - acc: 0.7224 - val_loss: 0.6279 - val_acc: 0.7485\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6796 - acc: 0.7232 - val_loss: 0.6277 - val_acc: 0.7474\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.62667 to 0.62477, saving model to best.model\n",
      "0s - loss: 0.6757 - acc: 0.7239 - val_loss: 0.6248 - val_acc: 0.7491\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6761 - acc: 0.7259 - val_loss: 0.6248 - val_acc: 0.7506\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6755 - acc: 0.7237 - val_loss: 0.6261 - val_acc: 0.7472\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.62477 to 0.62282, saving model to best.model\n",
      "1s - loss: 0.6758 - acc: 0.7221 - val_loss: 0.6228 - val_acc: 0.7496\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6727 - acc: 0.7248 - val_loss: 0.6239 - val_acc: 0.7486\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62282 to 0.62100, saving model to best.model\n",
      "0s - loss: 0.6725 - acc: 0.7249 - val_loss: 0.6210 - val_acc: 0.7509\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6726 - acc: 0.7265 - val_loss: 0.6219 - val_acc: 0.7482\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.62100 to 0.61648, saving model to best.model\n",
      "1s - loss: 0.6713 - acc: 0.7248 - val_loss: 0.6165 - val_acc: 0.7522\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6698 - acc: 0.7250 - val_loss: 0.6194 - val_acc: 0.7514\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6731 - acc: 0.7248 - val_loss: 0.6191 - val_acc: 0.7507\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6697 - acc: 0.7259 - val_loss: 0.6180 - val_acc: 0.7492\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6716 - acc: 0.7259 - val_loss: 0.6168 - val_acc: 0.7521\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6714 - acc: 0.7260 - val_loss: 0.6168 - val_acc: 0.7521\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.61648 to 0.61495, saving model to best.model\n",
      "1s - loss: 0.6637 - acc: 0.7298 - val_loss: 0.6150 - val_acc: 0.7532\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.61495 to 0.61299, saving model to best.model\n",
      "1s - loss: 0.6673 - acc: 0.7271 - val_loss: 0.6130 - val_acc: 0.7542\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6682 - acc: 0.7268 - val_loss: 0.6132 - val_acc: 0.7540\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6666 - acc: 0.7271 - val_loss: 0.6137 - val_acc: 0.7527\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.61299 to 0.61248, saving model to best.model\n",
      "1s - loss: 0.6644 - acc: 0.7298 - val_loss: 0.6125 - val_acc: 0.7508\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.61248 to 0.61123, saving model to best.model\n",
      "1s - loss: 0.6675 - acc: 0.7262 - val_loss: 0.6112 - val_acc: 0.7553\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6679 - acc: 0.7271 - val_loss: 0.6124 - val_acc: 0.7536\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6668 - acc: 0.7248 - val_loss: 0.6153 - val_acc: 0.7556\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.61123 to 0.61008, saving model to best.model\n",
      "1s - loss: 0.6630 - acc: 0.7299 - val_loss: 0.6101 - val_acc: 0.7539\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.61008 to 0.60776, saving model to best.model\n",
      "1s - loss: 0.6641 - acc: 0.7275 - val_loss: 0.6078 - val_acc: 0.7562\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6611 - acc: 0.7289 - val_loss: 0.6095 - val_acc: 0.7546\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.60776 to 0.60769, saving model to best.model\n",
      "1s - loss: 0.6599 - acc: 0.7277 - val_loss: 0.6077 - val_acc: 0.7560\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6603 - acc: 0.7301 - val_loss: 0.6083 - val_acc: 0.7543\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.60769 to 0.60675, saving model to best.model\n",
      "1s - loss: 0.6596 - acc: 0.7306 - val_loss: 0.6067 - val_acc: 0.7568\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6653 - acc: 0.7290 - val_loss: 0.6094 - val_acc: 0.7555\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6602 - acc: 0.7307 - val_loss: 0.6080 - val_acc: 0.7541\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.60675 to 0.60631, saving model to best.model\n",
      "1s - loss: 0.6550 - acc: 0.7314 - val_loss: 0.6063 - val_acc: 0.7563\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.60631 to 0.60494, saving model to best.model\n",
      "1s - loss: 0.6590 - acc: 0.7307 - val_loss: 0.6049 - val_acc: 0.7571\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.60494 to 0.60291, saving model to best.model\n",
      "1s - loss: 0.6615 - acc: 0.7288 - val_loss: 0.6029 - val_acc: 0.7573\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6570 - acc: 0.7315 - val_loss: 0.6055 - val_acc: 0.7562\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.60291 to 0.60234, saving model to best.model\n",
      "1s - loss: 0.6582 - acc: 0.7296 - val_loss: 0.6023 - val_acc: 0.7571\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6598 - acc: 0.7305 - val_loss: 0.6052 - val_acc: 0.7539\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.60234 to 0.60206, saving model to best.model\n",
      "0s - loss: 0.6577 - acc: 0.7318 - val_loss: 0.6021 - val_acc: 0.7576\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.60206 to 0.60038, saving model to best.model\n",
      "0s - loss: 0.6552 - acc: 0.7320 - val_loss: 0.6004 - val_acc: 0.7568\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6588 - acc: 0.7292 - val_loss: 0.6052 - val_acc: 0.7534\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6579 - acc: 0.7315 - val_loss: 0.6038 - val_acc: 0.7577\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6565 - acc: 0.7313 - val_loss: 0.6048 - val_acc: 0.7566\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6547 - acc: 0.7332 - val_loss: 0.6028 - val_acc: 0.7597\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6548 - acc: 0.7316 - val_loss: 0.6010 - val_acc: 0.7574\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.60038 to 0.59939, saving model to best.model\n",
      "0s - loss: 0.6558 - acc: 0.7310 - val_loss: 0.5994 - val_acc: 0.7580\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.59939 to 0.59780, saving model to best.model\n",
      "0s - loss: 0.6528 - acc: 0.7342 - val_loss: 0.5978 - val_acc: 0.7595\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.59780 to 0.59520, saving model to best.model\n",
      "1s - loss: 0.6507 - acc: 0.7330 - val_loss: 0.5952 - val_acc: 0.7605\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6524 - acc: 0.7324 - val_loss: 0.5980 - val_acc: 0.7602\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6518 - acc: 0.7323 - val_loss: 0.5972 - val_acc: 0.7582\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6480 - acc: 0.7352 - val_loss: 0.5967 - val_acc: 0.7603\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6514 - acc: 0.7356 - val_loss: 0.5991 - val_acc: 0.7598\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6498 - acc: 0.7359 - val_loss: 0.5961 - val_acc: 0.7595\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.59520 to 0.59433, saving model to best.model\n",
      "1s - loss: 0.6482 - acc: 0.7343 - val_loss: 0.5943 - val_acc: 0.7604\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6450 - acc: 0.7377 - val_loss: 0.5948 - val_acc: 0.7607\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.59433 to 0.59388, saving model to best.model\n",
      "1s - loss: 0.6488 - acc: 0.7335 - val_loss: 0.5939 - val_acc: 0.7608\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6500 - acc: 0.7350 - val_loss: 0.5941 - val_acc: 0.7615\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59388 to 0.59143, saving model to best.model\n",
      "1s - loss: 0.6442 - acc: 0.7370 - val_loss: 0.5914 - val_acc: 0.7611\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6478 - acc: 0.7349 - val_loss: 0.5952 - val_acc: 0.7591\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6481 - acc: 0.7335 - val_loss: 0.5934 - val_acc: 0.7596\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6457 - acc: 0.7372 - val_loss: 0.5945 - val_acc: 0.7632\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6465 - acc: 0.7355 - val_loss: 0.5949 - val_acc: 0.7573\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.59143 to 0.58985, saving model to best.model\n",
      "1s - loss: 0.6462 - acc: 0.7364 - val_loss: 0.5899 - val_acc: 0.7629\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6436 - acc: 0.7369 - val_loss: 0.5905 - val_acc: 0.7630\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.58985 to 0.58913, saving model to best.model\n",
      "1s - loss: 0.6446 - acc: 0.7352 - val_loss: 0.5891 - val_acc: 0.7616\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6453 - acc: 0.7355 - val_loss: 0.5926 - val_acc: 0.7643\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6432 - acc: 0.7380 - val_loss: 0.5902 - val_acc: 0.7643\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6446 - acc: 0.7360 - val_loss: 0.5892 - val_acc: 0.7630\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.58913 to 0.58810, saving model to best.model\n",
      "1s - loss: 0.6442 - acc: 0.7351 - val_loss: 0.5881 - val_acc: 0.7628\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.58810 to 0.58624, saving model to best.model\n",
      "1s - loss: 0.6386 - acc: 0.7389 - val_loss: 0.5862 - val_acc: 0.7648\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6423 - acc: 0.7377 - val_loss: 0.5866 - val_acc: 0.7651\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84537, saving model to best.model\n",
      "1s - loss: 0.9043 - acc: 0.6357 - val_loss: 0.8454 - val_acc: 0.6547\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84537 to 0.84429, saving model to best.model\n",
      "1s - loss: 0.8624 - acc: 0.6527 - val_loss: 0.8443 - val_acc: 0.6547\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84429 to 0.84056, saving model to best.model\n",
      "1s - loss: 0.8573 - acc: 0.6528 - val_loss: 0.8406 - val_acc: 0.6547\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84056 to 0.83143, saving model to best.model\n",
      "0s - loss: 0.8505 - acc: 0.6528 - val_loss: 0.8314 - val_acc: 0.6547\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83143 to 0.82879, saving model to best.model\n",
      "1s - loss: 0.8459 - acc: 0.6528 - val_loss: 0.8288 - val_acc: 0.6547\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82879 to 0.82549, saving model to best.model\n",
      "0s - loss: 0.8418 - acc: 0.6528 - val_loss: 0.8255 - val_acc: 0.6547\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82549 to 0.82507, saving model to best.model\n",
      "1s - loss: 0.8387 - acc: 0.6528 - val_loss: 0.8251 - val_acc: 0.6547\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82507 to 0.82337, saving model to best.model\n",
      "1s - loss: 0.8374 - acc: 0.6528 - val_loss: 0.8234 - val_acc: 0.6547\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82337 to 0.82265, saving model to best.model\n",
      "1s - loss: 0.8355 - acc: 0.6529 - val_loss: 0.8227 - val_acc: 0.6547\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82265 to 0.82151, saving model to best.model\n",
      "1s - loss: 0.8343 - acc: 0.6528 - val_loss: 0.8215 - val_acc: 0.6547\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8339 - acc: 0.6528 - val_loss: 0.8226 - val_acc: 0.6547\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8333 - acc: 0.6528 - val_loss: 0.8221 - val_acc: 0.6547\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8323 - acc: 0.6528 - val_loss: 0.8215 - val_acc: 0.6547\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8321 - acc: 0.6528 - val_loss: 0.8223 - val_acc: 0.6547\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82151 to 0.81911, saving model to best.model\n",
      "1s - loss: 0.8309 - acc: 0.6528 - val_loss: 0.8191 - val_acc: 0.6547\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8305 - acc: 0.6528 - val_loss: 0.8205 - val_acc: 0.6547\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81911 to 0.81788, saving model to best.model\n",
      "1s - loss: 0.8310 - acc: 0.6528 - val_loss: 0.8179 - val_acc: 0.6547\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "1s - loss: 0.8289 - acc: 0.6528 - val_loss: 0.8194 - val_acc: 0.6547\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81788 to 0.81670, saving model to best.model\n",
      "1s - loss: 0.8277 - acc: 0.6529 - val_loss: 0.8167 - val_acc: 0.6547\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81670 to 0.81522, saving model to best.model\n",
      "1s - loss: 0.8285 - acc: 0.6529 - val_loss: 0.8152 - val_acc: 0.6547\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81522 to 0.81509, saving model to best.model\n",
      "1s - loss: 0.8261 - acc: 0.6527 - val_loss: 0.8151 - val_acc: 0.6547\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81509 to 0.81362, saving model to best.model\n",
      "1s - loss: 0.8268 - acc: 0.6531 - val_loss: 0.8136 - val_acc: 0.6547\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81362 to 0.81293, saving model to best.model\n",
      "1s - loss: 0.8272 - acc: 0.6528 - val_loss: 0.8129 - val_acc: 0.6547\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81293 to 0.81263, saving model to best.model\n",
      "1s - loss: 0.8244 - acc: 0.6534 - val_loss: 0.8126 - val_acc: 0.6550\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81263 to 0.81130, saving model to best.model\n",
      "1s - loss: 0.8240 - acc: 0.6546 - val_loss: 0.8113 - val_acc: 0.6549\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81130 to 0.80915, saving model to best.model\n",
      "1s - loss: 0.8233 - acc: 0.6535 - val_loss: 0.8092 - val_acc: 0.6562\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "1s - loss: 0.8222 - acc: 0.6558 - val_loss: 0.8098 - val_acc: 0.6563\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80915 to 0.80771, saving model to best.model\n",
      "1s - loss: 0.8216 - acc: 0.6556 - val_loss: 0.8077 - val_acc: 0.6568\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80771 to 0.80700, saving model to best.model\n",
      "1s - loss: 0.8199 - acc: 0.6560 - val_loss: 0.8070 - val_acc: 0.6576\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80700 to 0.80505, saving model to best.model\n",
      "1s - loss: 0.8188 - acc: 0.6578 - val_loss: 0.8050 - val_acc: 0.6605\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80505 to 0.80255, saving model to best.model\n",
      "1s - loss: 0.8164 - acc: 0.6581 - val_loss: 0.8025 - val_acc: 0.6650\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80255 to 0.80135, saving model to best.model\n",
      "1s - loss: 0.8156 - acc: 0.6583 - val_loss: 0.8013 - val_acc: 0.6633\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80135 to 0.80010, saving model to best.model\n",
      "0s - loss: 0.8145 - acc: 0.6586 - val_loss: 0.8001 - val_acc: 0.6615\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80010 to 0.79627, saving model to best.model\n",
      "1s - loss: 0.8135 - acc: 0.6604 - val_loss: 0.7963 - val_acc: 0.6693\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79627 to 0.79516, saving model to best.model\n",
      "0s - loss: 0.8121 - acc: 0.6584 - val_loss: 0.7952 - val_acc: 0.6653\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79516 to 0.79479, saving model to best.model\n",
      "0s - loss: 0.8108 - acc: 0.6598 - val_loss: 0.7948 - val_acc: 0.6644\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79479 to 0.79160, saving model to best.model\n",
      "1s - loss: 0.8088 - acc: 0.6603 - val_loss: 0.7916 - val_acc: 0.6664\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.8089 - acc: 0.6616 - val_loss: 0.7920 - val_acc: 0.6689\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79160 to 0.78822, saving model to best.model\n",
      "0s - loss: 0.8054 - acc: 0.6632 - val_loss: 0.7882 - val_acc: 0.6655\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78822 to 0.78715, saving model to best.model\n",
      "0s - loss: 0.8053 - acc: 0.6627 - val_loss: 0.7871 - val_acc: 0.6681\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78715 to 0.78549, saving model to best.model\n",
      "0s - loss: 0.8027 - acc: 0.6634 - val_loss: 0.7855 - val_acc: 0.6733\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78549 to 0.78212, saving model to best.model\n",
      "0s - loss: 0.8028 - acc: 0.6630 - val_loss: 0.7821 - val_acc: 0.6726\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78212 to 0.78035, saving model to best.model\n",
      "1s - loss: 0.8006 - acc: 0.6639 - val_loss: 0.7803 - val_acc: 0.6741\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78035 to 0.77779, saving model to best.model\n",
      "1s - loss: 0.7983 - acc: 0.6646 - val_loss: 0.7778 - val_acc: 0.6747\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77779 to 0.77698, saving model to best.model\n",
      "1s - loss: 0.7975 - acc: 0.6650 - val_loss: 0.7770 - val_acc: 0.6745\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77698 to 0.77453, saving model to best.model\n",
      "1s - loss: 0.7976 - acc: 0.6660 - val_loss: 0.7745 - val_acc: 0.6746\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77453 to 0.77283, saving model to best.model\n",
      "1s - loss: 0.7944 - acc: 0.6655 - val_loss: 0.7728 - val_acc: 0.6782\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77283 to 0.77238, saving model to best.model\n",
      "1s - loss: 0.7939 - acc: 0.6681 - val_loss: 0.7724 - val_acc: 0.6737\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77238 to 0.77108, saving model to best.model\n",
      "0s - loss: 0.7929 - acc: 0.6690 - val_loss: 0.7711 - val_acc: 0.6785\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77108 to 0.76754, saving model to best.model\n",
      "0s - loss: 0.7910 - acc: 0.6688 - val_loss: 0.7675 - val_acc: 0.6794\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76754 to 0.76732, saving model to best.model\n",
      "1s - loss: 0.7916 - acc: 0.6687 - val_loss: 0.7673 - val_acc: 0.6830\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76732 to 0.76321, saving model to best.model\n",
      "1s - loss: 0.7863 - acc: 0.6710 - val_loss: 0.7632 - val_acc: 0.6828\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76321 to 0.76093, saving model to best.model\n",
      "1s - loss: 0.7866 - acc: 0.6702 - val_loss: 0.7609 - val_acc: 0.6843\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76093 to 0.76070, saving model to best.model\n",
      "1s - loss: 0.7850 - acc: 0.6711 - val_loss: 0.7607 - val_acc: 0.6811\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76070 to 0.75540, saving model to best.model\n",
      "1s - loss: 0.7846 - acc: 0.6710 - val_loss: 0.7554 - val_acc: 0.6859\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75540 to 0.75315, saving model to best.model\n",
      "1s - loss: 0.7802 - acc: 0.6734 - val_loss: 0.7532 - val_acc: 0.6870\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75315 to 0.75085, saving model to best.model\n",
      "1s - loss: 0.7787 - acc: 0.6748 - val_loss: 0.7508 - val_acc: 0.6883\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75085 to 0.74768, saving model to best.model\n",
      "1s - loss: 0.7770 - acc: 0.6741 - val_loss: 0.7477 - val_acc: 0.6897\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74768 to 0.74300, saving model to best.model\n",
      "1s - loss: 0.7734 - acc: 0.6764 - val_loss: 0.7430 - val_acc: 0.6897\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "1s - loss: 0.7734 - acc: 0.6787 - val_loss: 0.7431 - val_acc: 0.6933\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74300 to 0.73711, saving model to best.model\n",
      "1s - loss: 0.7701 - acc: 0.6792 - val_loss: 0.7371 - val_acc: 0.6954\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73711 to 0.73687, saving model to best.model\n",
      "1s - loss: 0.7707 - acc: 0.6788 - val_loss: 0.7369 - val_acc: 0.6904\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73687 to 0.73446, saving model to best.model\n",
      "1s - loss: 0.7678 - acc: 0.6809 - val_loss: 0.7345 - val_acc: 0.6944\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73446 to 0.73178, saving model to best.model\n",
      "1s - loss: 0.7650 - acc: 0.6799 - val_loss: 0.7318 - val_acc: 0.6917\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73178 to 0.73097, saving model to best.model\n",
      "1s - loss: 0.7643 - acc: 0.6809 - val_loss: 0.7310 - val_acc: 0.6897\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73097 to 0.72761, saving model to best.model\n",
      "1s - loss: 0.7625 - acc: 0.6824 - val_loss: 0.7276 - val_acc: 0.6974\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72761 to 0.72250, saving model to best.model\n",
      "1s - loss: 0.7581 - acc: 0.6851 - val_loss: 0.7225 - val_acc: 0.6977\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72250 to 0.72012, saving model to best.model\n",
      "1s - loss: 0.7581 - acc: 0.6847 - val_loss: 0.7201 - val_acc: 0.6996\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72012 to 0.71877, saving model to best.model\n",
      "1s - loss: 0.7568 - acc: 0.6836 - val_loss: 0.7188 - val_acc: 0.6994\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71877 to 0.71367, saving model to best.model\n",
      "1s - loss: 0.7545 - acc: 0.6869 - val_loss: 0.7137 - val_acc: 0.6998\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.7490 - acc: 0.6894 - val_loss: 0.7148 - val_acc: 0.6964\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71367 to 0.70932, saving model to best.model\n",
      "1s - loss: 0.7501 - acc: 0.6904 - val_loss: 0.7093 - val_acc: 0.7008\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70932 to 0.70867, saving model to best.model\n",
      "1s - loss: 0.7475 - acc: 0.6900 - val_loss: 0.7087 - val_acc: 0.7018\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70867 to 0.70418, saving model to best.model\n",
      "0s - loss: 0.7477 - acc: 0.6893 - val_loss: 0.7042 - val_acc: 0.7036\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70418 to 0.70046, saving model to best.model\n",
      "1s - loss: 0.7432 - acc: 0.6918 - val_loss: 0.7005 - val_acc: 0.7067\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70046 to 0.69906, saving model to best.model\n",
      "1s - loss: 0.7430 - acc: 0.6929 - val_loss: 0.6991 - val_acc: 0.7040\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69906 to 0.69461, saving model to best.model\n",
      "1s - loss: 0.7422 - acc: 0.6924 - val_loss: 0.6946 - val_acc: 0.7070\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "1s - loss: 0.7400 - acc: 0.6920 - val_loss: 0.6969 - val_acc: 0.7048\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69461 to 0.69135, saving model to best.model\n",
      "1s - loss: 0.7375 - acc: 0.6958 - val_loss: 0.6913 - val_acc: 0.7093\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.7390 - acc: 0.6934 - val_loss: 0.6915 - val_acc: 0.7094\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69135 to 0.68850, saving model to best.model\n",
      "1s - loss: 0.7348 - acc: 0.6948 - val_loss: 0.6885 - val_acc: 0.7136\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.7318 - acc: 0.6981 - val_loss: 0.6888 - val_acc: 0.7126\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68850 to 0.68456, saving model to best.model\n",
      "0s - loss: 0.7326 - acc: 0.6966 - val_loss: 0.6846 - val_acc: 0.7116\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7320 - acc: 0.6952 - val_loss: 0.6848 - val_acc: 0.7090\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.68456 to 0.68325, saving model to best.model\n",
      "1s - loss: 0.7303 - acc: 0.6966 - val_loss: 0.6833 - val_acc: 0.7101\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68325 to 0.67619, saving model to best.model\n",
      "0s - loss: 0.7264 - acc: 0.6981 - val_loss: 0.6762 - val_acc: 0.7156\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7254 - acc: 0.7016 - val_loss: 0.6766 - val_acc: 0.7208\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7227 - acc: 0.7027 - val_loss: 0.6764 - val_acc: 0.7152\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67619 to 0.67397, saving model to best.model\n",
      "1s - loss: 0.7205 - acc: 0.7023 - val_loss: 0.6740 - val_acc: 0.7178\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67397 to 0.67366, saving model to best.model\n",
      "1s - loss: 0.7203 - acc: 0.7029 - val_loss: 0.6737 - val_acc: 0.7155\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67366 to 0.66978, saving model to best.model\n",
      "1s - loss: 0.7211 - acc: 0.7030 - val_loss: 0.6698 - val_acc: 0.7208\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66978 to 0.66740, saving model to best.model\n",
      "1s - loss: 0.7185 - acc: 0.7024 - val_loss: 0.6674 - val_acc: 0.7217\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7202 - acc: 0.7029 - val_loss: 0.6728 - val_acc: 0.7217\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7157 - acc: 0.7044 - val_loss: 0.6677 - val_acc: 0.7191\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66740 to 0.66679, saving model to best.model\n",
      "1s - loss: 0.7162 - acc: 0.7044 - val_loss: 0.6668 - val_acc: 0.7225\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.66679 to 0.66422, saving model to best.model\n",
      "1s - loss: 0.7151 - acc: 0.7065 - val_loss: 0.6642 - val_acc: 0.7244\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66422 to 0.66306, saving model to best.model\n",
      "1s - loss: 0.7149 - acc: 0.7052 - val_loss: 0.6631 - val_acc: 0.7244\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7117 - acc: 0.7048 - val_loss: 0.6652 - val_acc: 0.7192\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66306 to 0.66202, saving model to best.model\n",
      "1s - loss: 0.7102 - acc: 0.7072 - val_loss: 0.6620 - val_acc: 0.7206\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66202 to 0.66077, saving model to best.model\n",
      "1s - loss: 0.7076 - acc: 0.7092 - val_loss: 0.6608 - val_acc: 0.7261\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7096 - acc: 0.7076 - val_loss: 0.6614 - val_acc: 0.7252\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66077 to 0.65993, saving model to best.model\n",
      "1s - loss: 0.7083 - acc: 0.7067 - val_loss: 0.6599 - val_acc: 0.7222\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7092 - acc: 0.7061 - val_loss: 0.6602 - val_acc: 0.7220\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65993 to 0.65719, saving model to best.model\n",
      "1s - loss: 0.7065 - acc: 0.7126 - val_loss: 0.6572 - val_acc: 0.7265\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65719 to 0.65606, saving model to best.model\n",
      "1s - loss: 0.7042 - acc: 0.7116 - val_loss: 0.6561 - val_acc: 0.7285\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65606 to 0.65386, saving model to best.model\n",
      "0s - loss: 0.7057 - acc: 0.7094 - val_loss: 0.6539 - val_acc: 0.7249\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65386 to 0.65270, saving model to best.model\n",
      "1s - loss: 0.7012 - acc: 0.7132 - val_loss: 0.6527 - val_acc: 0.7245\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7035 - acc: 0.7124 - val_loss: 0.6534 - val_acc: 0.7272\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.7036 - acc: 0.7092 - val_loss: 0.6540 - val_acc: 0.7293\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65270 to 0.65223, saving model to best.model\n",
      "1s - loss: 0.7054 - acc: 0.7092 - val_loss: 0.6522 - val_acc: 0.7307\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.65223 to 0.65158, saving model to best.model\n",
      "1s - loss: 0.7011 - acc: 0.7107 - val_loss: 0.6516 - val_acc: 0.7294\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65158 to 0.65141, saving model to best.model\n",
      "1s - loss: 0.7029 - acc: 0.7116 - val_loss: 0.6514 - val_acc: 0.7246\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65141 to 0.64636, saving model to best.model\n",
      "0s - loss: 0.6987 - acc: 0.7141 - val_loss: 0.6464 - val_acc: 0.7322\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6984 - acc: 0.7132 - val_loss: 0.6525 - val_acc: 0.7224\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6993 - acc: 0.7116 - val_loss: 0.6491 - val_acc: 0.7278\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.6957 - acc: 0.7147 - val_loss: 0.6471 - val_acc: 0.7280\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64636 to 0.64379, saving model to best.model\n",
      "1s - loss: 0.6979 - acc: 0.7131 - val_loss: 0.6438 - val_acc: 0.7310\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6968 - acc: 0.7131 - val_loss: 0.6475 - val_acc: 0.7258\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64379 to 0.64155, saving model to best.model\n",
      "1s - loss: 0.6930 - acc: 0.7142 - val_loss: 0.6416 - val_acc: 0.7362\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6949 - acc: 0.7142 - val_loss: 0.6429 - val_acc: 0.7306\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6908 - acc: 0.7140 - val_loss: 0.6431 - val_acc: 0.7288\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6908 - acc: 0.7166 - val_loss: 0.6429 - val_acc: 0.7319\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6915 - acc: 0.7156 - val_loss: 0.6423 - val_acc: 0.7293\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64155 to 0.64084, saving model to best.model\n",
      "1s - loss: 0.6911 - acc: 0.7172 - val_loss: 0.6408 - val_acc: 0.7323\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64084 to 0.63745, saving model to best.model\n",
      "1s - loss: 0.6880 - acc: 0.7203 - val_loss: 0.6374 - val_acc: 0.7344\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6885 - acc: 0.7174 - val_loss: 0.6380 - val_acc: 0.7342\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.63745 to 0.63740, saving model to best.model\n",
      "1s - loss: 0.6877 - acc: 0.7178 - val_loss: 0.6374 - val_acc: 0.7327\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6858 - acc: 0.7179 - val_loss: 0.6378 - val_acc: 0.7315\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.63740 to 0.63464, saving model to best.model\n",
      "0s - loss: 0.6849 - acc: 0.7169 - val_loss: 0.6346 - val_acc: 0.7328\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6862 - acc: 0.7174 - val_loss: 0.6371 - val_acc: 0.7307\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6867 - acc: 0.7183 - val_loss: 0.6360 - val_acc: 0.7388\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6856 - acc: 0.7168 - val_loss: 0.6356 - val_acc: 0.7314\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63464 to 0.63202, saving model to best.model\n",
      "1s - loss: 0.6832 - acc: 0.7193 - val_loss: 0.6320 - val_acc: 0.7393\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63202 to 0.63195, saving model to best.model\n",
      "1s - loss: 0.6796 - acc: 0.7211 - val_loss: 0.6320 - val_acc: 0.7354\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6807 - acc: 0.7200 - val_loss: 0.6335 - val_acc: 0.7329\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63195 to 0.62999, saving model to best.model\n",
      "0s - loss: 0.6812 - acc: 0.7196 - val_loss: 0.6300 - val_acc: 0.7343\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6849 - acc: 0.7172 - val_loss: 0.6325 - val_acc: 0.7363\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6818 - acc: 0.7209 - val_loss: 0.6311 - val_acc: 0.7344\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6792 - acc: 0.7189 - val_loss: 0.6303 - val_acc: 0.7386\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.62999 to 0.62585, saving model to best.model\n",
      "1s - loss: 0.6770 - acc: 0.7233 - val_loss: 0.6258 - val_acc: 0.7417\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6784 - acc: 0.7233 - val_loss: 0.6289 - val_acc: 0.7369\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6792 - acc: 0.7209 - val_loss: 0.6293 - val_acc: 0.7348\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6776 - acc: 0.7225 - val_loss: 0.6269 - val_acc: 0.7381\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62585 to 0.62570, saving model to best.model\n",
      "1s - loss: 0.6720 - acc: 0.7246 - val_loss: 0.6257 - val_acc: 0.7384\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.62570 to 0.62539, saving model to best.model\n",
      "1s - loss: 0.6782 - acc: 0.7231 - val_loss: 0.6254 - val_acc: 0.7386\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6746 - acc: 0.7224 - val_loss: 0.6262 - val_acc: 0.7420\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.62539 to 0.62363, saving model to best.model\n",
      "1s - loss: 0.6734 - acc: 0.7256 - val_loss: 0.6236 - val_acc: 0.7398\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62363 to 0.62183, saving model to best.model\n",
      "0s - loss: 0.6732 - acc: 0.7223 - val_loss: 0.6218 - val_acc: 0.7402\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6736 - acc: 0.7251 - val_loss: 0.6225 - val_acc: 0.7405\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62183 to 0.62169, saving model to best.model\n",
      "1s - loss: 0.6718 - acc: 0.7251 - val_loss: 0.6217 - val_acc: 0.7425\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6739 - acc: 0.7231 - val_loss: 0.6238 - val_acc: 0.7391\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6724 - acc: 0.7236 - val_loss: 0.6222 - val_acc: 0.7439\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62169 to 0.62066, saving model to best.model\n",
      "0s - loss: 0.6711 - acc: 0.7251 - val_loss: 0.6207 - val_acc: 0.7434\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.6697 - acc: 0.7262 - val_loss: 0.6208 - val_acc: 0.7438\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6703 - acc: 0.7260 - val_loss: 0.6226 - val_acc: 0.7410\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62066 to 0.61911, saving model to best.model\n",
      "1s - loss: 0.6671 - acc: 0.7265 - val_loss: 0.6191 - val_acc: 0.7400\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61911 to 0.61753, saving model to best.model\n",
      "1s - loss: 0.6669 - acc: 0.7263 - val_loss: 0.6175 - val_acc: 0.7453\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6706 - acc: 0.7268 - val_loss: 0.6208 - val_acc: 0.7378\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6696 - acc: 0.7231 - val_loss: 0.6184 - val_acc: 0.7445\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6669 - acc: 0.7237 - val_loss: 0.6177 - val_acc: 0.7445\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6663 - acc: 0.7281 - val_loss: 0.6178 - val_acc: 0.7452\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61753 to 0.61677, saving model to best.model\n",
      "1s - loss: 0.6662 - acc: 0.7245 - val_loss: 0.6168 - val_acc: 0.7441\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.61677 to 0.61612, saving model to best.model\n",
      "0s - loss: 0.6652 - acc: 0.7257 - val_loss: 0.6161 - val_acc: 0.7470\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61612 to 0.61343, saving model to best.model\n",
      "1s - loss: 0.6654 - acc: 0.7264 - val_loss: 0.6134 - val_acc: 0.7464\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6641 - acc: 0.7282 - val_loss: 0.6140 - val_acc: 0.7459\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6609 - acc: 0.7303 - val_loss: 0.6181 - val_acc: 0.7438\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6650 - acc: 0.7261 - val_loss: 0.6151 - val_acc: 0.7471\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6651 - acc: 0.7270 - val_loss: 0.6151 - val_acc: 0.7459\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6643 - acc: 0.7286 - val_loss: 0.6140 - val_acc: 0.7443\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61343 to 0.61086, saving model to best.model\n",
      "1s - loss: 0.6647 - acc: 0.7271 - val_loss: 0.6109 - val_acc: 0.7484\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6631 - acc: 0.7282 - val_loss: 0.6110 - val_acc: 0.7472\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6651 - acc: 0.7269 - val_loss: 0.6139 - val_acc: 0.7460\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6623 - acc: 0.7299 - val_loss: 0.6115 - val_acc: 0.7491\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.61086 to 0.60926, saving model to best.model\n",
      "0s - loss: 0.6598 - acc: 0.7292 - val_loss: 0.6093 - val_acc: 0.7485\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6608 - acc: 0.7318 - val_loss: 0.6123 - val_acc: 0.7443\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.60926 to 0.60672, saving model to best.model\n",
      "1s - loss: 0.6594 - acc: 0.7279 - val_loss: 0.6067 - val_acc: 0.7512\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6599 - acc: 0.7297 - val_loss: 0.6068 - val_acc: 0.7514\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6582 - acc: 0.7304 - val_loss: 0.6091 - val_acc: 0.7496\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6618 - acc: 0.7266 - val_loss: 0.6084 - val_acc: 0.7518\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6586 - acc: 0.7307 - val_loss: 0.6095 - val_acc: 0.7463\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6589 - acc: 0.7311 - val_loss: 0.6080 - val_acc: 0.7508\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6575 - acc: 0.7319 - val_loss: 0.6068 - val_acc: 0.7478\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6594 - acc: 0.7306 - val_loss: 0.6069 - val_acc: 0.7514\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60672 to 0.60521, saving model to best.model\n",
      "1s - loss: 0.6556 - acc: 0.7304 - val_loss: 0.6052 - val_acc: 0.7511\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6568 - acc: 0.7309 - val_loss: 0.6060 - val_acc: 0.7479\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.60521 to 0.60338, saving model to best.model\n",
      "1s - loss: 0.6541 - acc: 0.7303 - val_loss: 0.6034 - val_acc: 0.7522\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60338 to 0.60314, saving model to best.model\n",
      "1s - loss: 0.6520 - acc: 0.7318 - val_loss: 0.6031 - val_acc: 0.7514\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60314 to 0.60291, saving model to best.model\n",
      "1s - loss: 0.6555 - acc: 0.7296 - val_loss: 0.6029 - val_acc: 0.7527\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60291 to 0.60174, saving model to best.model\n",
      "1s - loss: 0.6547 - acc: 0.7334 - val_loss: 0.6017 - val_acc: 0.7526\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6539 - acc: 0.7328 - val_loss: 0.6019 - val_acc: 0.7512\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6550 - acc: 0.7305 - val_loss: 0.6031 - val_acc: 0.7502\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.60174 to 0.60164, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7316 - val_loss: 0.6016 - val_acc: 0.7529\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6535 - acc: 0.7332 - val_loss: 0.6024 - val_acc: 0.7537\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60164 to 0.60001, saving model to best.model\n",
      "1s - loss: 0.6541 - acc: 0.7317 - val_loss: 0.6000 - val_acc: 0.7530\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6562 - acc: 0.7311 - val_loss: 0.6034 - val_acc: 0.7508\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6524 - acc: 0.7310 - val_loss: 0.6021 - val_acc: 0.7511\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6503 - acc: 0.7321 - val_loss: 0.6006 - val_acc: 0.7515\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6506 - acc: 0.7321 - val_loss: 0.6003 - val_acc: 0.7512\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60001 to 0.59781, saving model to best.model\n",
      "1s - loss: 0.6498 - acc: 0.7331 - val_loss: 0.5978 - val_acc: 0.7544\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.59781 to 0.59702, saving model to best.model\n",
      "1s - loss: 0.6476 - acc: 0.7347 - val_loss: 0.5970 - val_acc: 0.7553\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84556, saving model to best.model\n",
      "0s - loss: 0.9027 - acc: 0.6350 - val_loss: 0.8456 - val_acc: 0.6570\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8524 - acc: 0.6617 - val_loss: 0.8460 - val_acc: 0.6570\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84556 to 0.84357, saving model to best.model\n",
      "0s - loss: 0.8458 - acc: 0.6622 - val_loss: 0.8436 - val_acc: 0.6570\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84357 to 0.83955, saving model to best.model\n",
      "1s - loss: 0.8442 - acc: 0.6622 - val_loss: 0.8396 - val_acc: 0.6570\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83955 to 0.83182, saving model to best.model\n",
      "1s - loss: 0.8382 - acc: 0.6622 - val_loss: 0.8318 - val_acc: 0.6570\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83182 to 0.82607, saving model to best.model\n",
      "1s - loss: 0.8334 - acc: 0.6622 - val_loss: 0.8261 - val_acc: 0.6570\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82607 to 0.82338, saving model to best.model\n",
      "1s - loss: 0.8299 - acc: 0.6622 - val_loss: 0.8234 - val_acc: 0.6570\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82338 to 0.82094, saving model to best.model\n",
      "1s - loss: 0.8264 - acc: 0.6622 - val_loss: 0.8209 - val_acc: 0.6570\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82094 to 0.82064, saving model to best.model\n",
      "1s - loss: 0.8247 - acc: 0.6623 - val_loss: 0.8206 - val_acc: 0.6570\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82064 to 0.81945, saving model to best.model\n",
      "1s - loss: 0.8242 - acc: 0.6622 - val_loss: 0.8195 - val_acc: 0.6570\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8205 - acc: 0.6622 - val_loss: 0.8215 - val_acc: 0.6570\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81945 to 0.81867, saving model to best.model\n",
      "1s - loss: 0.8230 - acc: 0.6621 - val_loss: 0.8187 - val_acc: 0.6570\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8210 - acc: 0.6621 - val_loss: 0.8187 - val_acc: 0.6570\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81867 to 0.81768, saving model to best.model\n",
      "1s - loss: 0.8202 - acc: 0.6623 - val_loss: 0.8177 - val_acc: 0.6570\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81768 to 0.81765, saving model to best.model\n",
      "1s - loss: 0.8197 - acc: 0.6621 - val_loss: 0.8177 - val_acc: 0.6570\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81765 to 0.81705, saving model to best.model\n",
      "1s - loss: 0.8183 - acc: 0.6624 - val_loss: 0.8171 - val_acc: 0.6570\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81705 to 0.81631, saving model to best.model\n",
      "1s - loss: 0.8166 - acc: 0.6623 - val_loss: 0.8163 - val_acc: 0.6570\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81631 to 0.81566, saving model to best.model\n",
      "1s - loss: 0.8173 - acc: 0.6623 - val_loss: 0.8157 - val_acc: 0.6570\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81566 to 0.81478, saving model to best.model\n",
      "1s - loss: 0.8162 - acc: 0.6627 - val_loss: 0.8148 - val_acc: 0.6570\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "1s - loss: 0.8149 - acc: 0.6626 - val_loss: 0.8155 - val_acc: 0.6570\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81478 to 0.81341, saving model to best.model\n",
      "1s - loss: 0.8159 - acc: 0.6629 - val_loss: 0.8134 - val_acc: 0.6573\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "1s - loss: 0.8138 - acc: 0.6631 - val_loss: 0.8140 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81341 to 0.81216, saving model to best.model\n",
      "1s - loss: 0.8136 - acc: 0.6639 - val_loss: 0.8122 - val_acc: 0.6581\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81216 to 0.81201, saving model to best.model\n",
      "1s - loss: 0.8123 - acc: 0.6645 - val_loss: 0.8120 - val_acc: 0.6574\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81201 to 0.81025, saving model to best.model\n",
      "1s - loss: 0.8111 - acc: 0.6647 - val_loss: 0.8103 - val_acc: 0.6623\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81025 to 0.80964, saving model to best.model\n",
      "1s - loss: 0.8107 - acc: 0.6643 - val_loss: 0.8096 - val_acc: 0.6635\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80964 to 0.80867, saving model to best.model\n",
      "1s - loss: 0.8105 - acc: 0.6656 - val_loss: 0.8087 - val_acc: 0.6639\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80867 to 0.80856, saving model to best.model\n",
      "1s - loss: 0.8094 - acc: 0.6664 - val_loss: 0.8086 - val_acc: 0.6626\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80856 to 0.80833, saving model to best.model\n",
      "1s - loss: 0.8078 - acc: 0.6679 - val_loss: 0.8083 - val_acc: 0.6625\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80833 to 0.80640, saving model to best.model\n",
      "1s - loss: 0.8080 - acc: 0.6674 - val_loss: 0.8064 - val_acc: 0.6651\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80640 to 0.80486, saving model to best.model\n",
      "1s - loss: 0.8065 - acc: 0.6671 - val_loss: 0.8049 - val_acc: 0.6653\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "1s - loss: 0.8057 - acc: 0.6673 - val_loss: 0.8055 - val_acc: 0.6610\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80486 to 0.80208, saving model to best.model\n",
      "1s - loss: 0.8038 - acc: 0.6693 - val_loss: 0.8021 - val_acc: 0.6662\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80208 to 0.80129, saving model to best.model\n",
      "1s - loss: 0.8044 - acc: 0.6694 - val_loss: 0.8013 - val_acc: 0.6658\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80129 to 0.80103, saving model to best.model\n",
      "1s - loss: 0.8032 - acc: 0.6690 - val_loss: 0.8010 - val_acc: 0.6633\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80103 to 0.79774, saving model to best.model\n",
      "1s - loss: 0.8018 - acc: 0.6690 - val_loss: 0.7977 - val_acc: 0.6665\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79774 to 0.79447, saving model to best.model\n",
      "1s - loss: 0.7992 - acc: 0.6699 - val_loss: 0.7945 - val_acc: 0.6677\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "1s - loss: 0.7983 - acc: 0.6716 - val_loss: 0.7945 - val_acc: 0.6656\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79447 to 0.79061, saving model to best.model\n",
      "1s - loss: 0.7960 - acc: 0.6736 - val_loss: 0.7906 - val_acc: 0.6685\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79061 to 0.78721, saving model to best.model\n",
      "1s - loss: 0.7957 - acc: 0.6707 - val_loss: 0.7872 - val_acc: 0.6706\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78721 to 0.78511, saving model to best.model\n",
      "1s - loss: 0.7939 - acc: 0.6721 - val_loss: 0.7851 - val_acc: 0.6711\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78511 to 0.78417, saving model to best.model\n",
      "1s - loss: 0.7918 - acc: 0.6719 - val_loss: 0.7842 - val_acc: 0.6692\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78417 to 0.78015, saving model to best.model\n",
      "1s - loss: 0.7897 - acc: 0.6748 - val_loss: 0.7802 - val_acc: 0.6728\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78015 to 0.77832, saving model to best.model\n",
      "1s - loss: 0.7893 - acc: 0.6742 - val_loss: 0.7783 - val_acc: 0.6724\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77832 to 0.77328, saving model to best.model\n",
      "1s - loss: 0.7860 - acc: 0.6746 - val_loss: 0.7733 - val_acc: 0.6766\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77328 to 0.77258, saving model to best.model\n",
      "1s - loss: 0.7876 - acc: 0.6730 - val_loss: 0.7726 - val_acc: 0.6788\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77258 to 0.77055, saving model to best.model\n",
      "1s - loss: 0.7864 - acc: 0.6750 - val_loss: 0.7705 - val_acc: 0.6790\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "1s - loss: 0.7837 - acc: 0.6758 - val_loss: 0.7716 - val_acc: 0.6761\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77055 to 0.76807, saving model to best.model\n",
      "1s - loss: 0.7809 - acc: 0.6770 - val_loss: 0.7681 - val_acc: 0.6776\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76807 to 0.76458, saving model to best.model\n",
      "1s - loss: 0.7779 - acc: 0.6779 - val_loss: 0.7646 - val_acc: 0.6806\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76458 to 0.76398, saving model to best.model\n",
      "1s - loss: 0.7774 - acc: 0.6788 - val_loss: 0.7640 - val_acc: 0.6774\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76398 to 0.76146, saving model to best.model\n",
      "1s - loss: 0.7775 - acc: 0.6788 - val_loss: 0.7615 - val_acc: 0.6817\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76146 to 0.75899, saving model to best.model\n",
      "1s - loss: 0.7768 - acc: 0.6773 - val_loss: 0.7590 - val_acc: 0.6864\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75899 to 0.75445, saving model to best.model\n",
      "1s - loss: 0.7745 - acc: 0.6810 - val_loss: 0.7545 - val_acc: 0.6845\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75445 to 0.75224, saving model to best.model\n",
      "1s - loss: 0.7704 - acc: 0.6831 - val_loss: 0.7522 - val_acc: 0.6858\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "1s - loss: 0.7716 - acc: 0.6823 - val_loss: 0.7526 - val_acc: 0.6843\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75224 to 0.74796, saving model to best.model\n",
      "1s - loss: 0.7677 - acc: 0.6829 - val_loss: 0.7480 - val_acc: 0.6867\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74796 to 0.74552, saving model to best.model\n",
      "1s - loss: 0.7666 - acc: 0.6812 - val_loss: 0.7455 - val_acc: 0.6869\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74552 to 0.74289, saving model to best.model\n",
      "1s - loss: 0.7638 - acc: 0.6844 - val_loss: 0.7429 - val_acc: 0.6903\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74289 to 0.73873, saving model to best.model\n",
      "1s - loss: 0.7634 - acc: 0.6823 - val_loss: 0.7387 - val_acc: 0.6924\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73873 to 0.73772, saving model to best.model\n",
      "1s - loss: 0.7618 - acc: 0.6830 - val_loss: 0.7377 - val_acc: 0.6897\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73772 to 0.73517, saving model to best.model\n",
      "1s - loss: 0.7578 - acc: 0.6861 - val_loss: 0.7352 - val_acc: 0.6936\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73517 to 0.73484, saving model to best.model\n",
      "1s - loss: 0.7577 - acc: 0.6870 - val_loss: 0.7348 - val_acc: 0.6922\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73484 to 0.72904, saving model to best.model\n",
      "1s - loss: 0.7547 - acc: 0.6893 - val_loss: 0.7290 - val_acc: 0.6933\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72904 to 0.72781, saving model to best.model\n",
      "1s - loss: 0.7549 - acc: 0.6874 - val_loss: 0.7278 - val_acc: 0.6956\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72781 to 0.72058, saving model to best.model\n",
      "1s - loss: 0.7512 - acc: 0.6880 - val_loss: 0.7206 - val_acc: 0.6991\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.7518 - acc: 0.6886 - val_loss: 0.7238 - val_acc: 0.6944\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72058 to 0.71817, saving model to best.model\n",
      "1s - loss: 0.7461 - acc: 0.6937 - val_loss: 0.7182 - val_acc: 0.7008\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "1s - loss: 0.7472 - acc: 0.6921 - val_loss: 0.7209 - val_acc: 0.7000\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71817 to 0.71610, saving model to best.model\n",
      "1s - loss: 0.7464 - acc: 0.6904 - val_loss: 0.7161 - val_acc: 0.6998\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71610 to 0.71513, saving model to best.model\n",
      "1s - loss: 0.7455 - acc: 0.6912 - val_loss: 0.7151 - val_acc: 0.7014\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71513 to 0.71005, saving model to best.model\n",
      "1s - loss: 0.7431 - acc: 0.6912 - val_loss: 0.7100 - val_acc: 0.7029\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71005 to 0.70552, saving model to best.model\n",
      "1s - loss: 0.7411 - acc: 0.6937 - val_loss: 0.7055 - val_acc: 0.7066\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70552 to 0.70434, saving model to best.model\n",
      "1s - loss: 0.7378 - acc: 0.6952 - val_loss: 0.7043 - val_acc: 0.7048\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70434 to 0.70386, saving model to best.model\n",
      "1s - loss: 0.7376 - acc: 0.6988 - val_loss: 0.7039 - val_acc: 0.7047\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70386 to 0.69985, saving model to best.model\n",
      "1s - loss: 0.7369 - acc: 0.6969 - val_loss: 0.6998 - val_acc: 0.7121\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69985 to 0.69910, saving model to best.model\n",
      "1s - loss: 0.7356 - acc: 0.6959 - val_loss: 0.6991 - val_acc: 0.7097\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69910 to 0.69510, saving model to best.model\n",
      "1s - loss: 0.7300 - acc: 0.6992 - val_loss: 0.6951 - val_acc: 0.7107\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69510 to 0.69331, saving model to best.model\n",
      "1s - loss: 0.7286 - acc: 0.7001 - val_loss: 0.6933 - val_acc: 0.7126\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7298 - acc: 0.6997 - val_loss: 0.6952 - val_acc: 0.7129\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69331 to 0.69210, saving model to best.model\n",
      "1s - loss: 0.7272 - acc: 0.7015 - val_loss: 0.6921 - val_acc: 0.7180\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69210 to 0.69023, saving model to best.model\n",
      "1s - loss: 0.7281 - acc: 0.7011 - val_loss: 0.6902 - val_acc: 0.7122\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69023 to 0.68872, saving model to best.model\n",
      "1s - loss: 0.7247 - acc: 0.7043 - val_loss: 0.6887 - val_acc: 0.7136\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68872 to 0.68440, saving model to best.model\n",
      "1s - loss: 0.7252 - acc: 0.7022 - val_loss: 0.6844 - val_acc: 0.7171\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7244 - acc: 0.7027 - val_loss: 0.6853 - val_acc: 0.7185\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7210 - acc: 0.7038 - val_loss: 0.6855 - val_acc: 0.7119\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68440 to 0.68332, saving model to best.model\n",
      "1s - loss: 0.7210 - acc: 0.7032 - val_loss: 0.6833 - val_acc: 0.7213\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68332 to 0.68060, saving model to best.model\n",
      "1s - loss: 0.7180 - acc: 0.7046 - val_loss: 0.6806 - val_acc: 0.7169\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68060 to 0.67849, saving model to best.model\n",
      "1s - loss: 0.7180 - acc: 0.7059 - val_loss: 0.6785 - val_acc: 0.7184\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67849 to 0.67805, saving model to best.model\n",
      "2s - loss: 0.7177 - acc: 0.7031 - val_loss: 0.6781 - val_acc: 0.7248\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7171 - acc: 0.7044 - val_loss: 0.6792 - val_acc: 0.7170\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67805 to 0.67159, saving model to best.model\n",
      "1s - loss: 0.7122 - acc: 0.7074 - val_loss: 0.6716 - val_acc: 0.7263\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7126 - acc: 0.7080 - val_loss: 0.6717 - val_acc: 0.7233\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7137 - acc: 0.7068 - val_loss: 0.6718 - val_acc: 0.7227\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67159 to 0.66590, saving model to best.model\n",
      "1s - loss: 0.7094 - acc: 0.7105 - val_loss: 0.6659 - val_acc: 0.7287\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7107 - acc: 0.7086 - val_loss: 0.6697 - val_acc: 0.7247\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66590 to 0.66438, saving model to best.model\n",
      "1s - loss: 0.7096 - acc: 0.7096 - val_loss: 0.6644 - val_acc: 0.7294\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7076 - acc: 0.7120 - val_loss: 0.6664 - val_acc: 0.7230\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7072 - acc: 0.7105 - val_loss: 0.6659 - val_acc: 0.7275\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7066 - acc: 0.7104 - val_loss: 0.6651 - val_acc: 0.7297\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66438 to 0.66162, saving model to best.model\n",
      "1s - loss: 0.7052 - acc: 0.7133 - val_loss: 0.6616 - val_acc: 0.7270\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7039 - acc: 0.7143 - val_loss: 0.6620 - val_acc: 0.7302\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66162 to 0.65525, saving model to best.model\n",
      "1s - loss: 0.7019 - acc: 0.7128 - val_loss: 0.6553 - val_acc: 0.7330\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.7035 - acc: 0.7129 - val_loss: 0.6576 - val_acc: 0.7322\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65525 to 0.65032, saving model to best.model\n",
      "1s - loss: 0.6987 - acc: 0.7167 - val_loss: 0.6503 - val_acc: 0.7375\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7036 - acc: 0.7121 - val_loss: 0.6604 - val_acc: 0.7278\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.6990 - acc: 0.7146 - val_loss: 0.6537 - val_acc: 0.7370\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6979 - acc: 0.7156 - val_loss: 0.6579 - val_acc: 0.7276\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6989 - acc: 0.7143 - val_loss: 0.6545 - val_acc: 0.7327\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6967 - acc: 0.7151 - val_loss: 0.6522 - val_acc: 0.7323\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.65032 to 0.65027, saving model to best.model\n",
      "1s - loss: 0.6968 - acc: 0.7141 - val_loss: 0.6503 - val_acc: 0.7341\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65027 to 0.64825, saving model to best.model\n",
      "1s - loss: 0.6955 - acc: 0.7154 - val_loss: 0.6483 - val_acc: 0.7350\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6965 - acc: 0.7155 - val_loss: 0.6516 - val_acc: 0.7330\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.64825 to 0.64645, saving model to best.model\n",
      "1s - loss: 0.6933 - acc: 0.7177 - val_loss: 0.6464 - val_acc: 0.7398\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6916 - acc: 0.7158 - val_loss: 0.6488 - val_acc: 0.7335\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6914 - acc: 0.7197 - val_loss: 0.6501 - val_acc: 0.7344\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6942 - acc: 0.7176 - val_loss: 0.6478 - val_acc: 0.7347\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6884 - acc: 0.7167 - val_loss: 0.6473 - val_acc: 0.7355\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64645 to 0.64337, saving model to best.model\n",
      "1s - loss: 0.6907 - acc: 0.7193 - val_loss: 0.6434 - val_acc: 0.7371\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64337 to 0.64113, saving model to best.model\n",
      "1s - loss: 0.6881 - acc: 0.7193 - val_loss: 0.6411 - val_acc: 0.7412\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6872 - acc: 0.7184 - val_loss: 0.6412 - val_acc: 0.7390\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6894 - acc: 0.7172 - val_loss: 0.6449 - val_acc: 0.7351\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64113 to 0.63796, saving model to best.model\n",
      "1s - loss: 0.6854 - acc: 0.7200 - val_loss: 0.6380 - val_acc: 0.7425\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "1s - loss: 0.6892 - acc: 0.7188 - val_loss: 0.6393 - val_acc: 0.7397\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6859 - acc: 0.7207 - val_loss: 0.6406 - val_acc: 0.7392\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63796 to 0.63788, saving model to best.model\n",
      "1s - loss: 0.6855 - acc: 0.7216 - val_loss: 0.6379 - val_acc: 0.7395\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.63788 to 0.63766, saving model to best.model\n",
      "1s - loss: 0.6837 - acc: 0.7220 - val_loss: 0.6377 - val_acc: 0.7451\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6856 - acc: 0.7206 - val_loss: 0.6404 - val_acc: 0.7415\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6835 - acc: 0.7219 - val_loss: 0.6383 - val_acc: 0.7391\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63766 to 0.63526, saving model to best.model\n",
      "0s - loss: 0.6809 - acc: 0.7223 - val_loss: 0.6353 - val_acc: 0.7404\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6815 - acc: 0.7220 - val_loss: 0.6364 - val_acc: 0.7426\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6819 - acc: 0.7215 - val_loss: 0.6366 - val_acc: 0.7385\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63526 to 0.63317, saving model to best.model\n",
      "0s - loss: 0.6797 - acc: 0.7237 - val_loss: 0.6332 - val_acc: 0.7407\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63317 to 0.63098, saving model to best.model\n",
      "1s - loss: 0.6781 - acc: 0.7222 - val_loss: 0.6310 - val_acc: 0.7432\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6802 - acc: 0.7230 - val_loss: 0.6348 - val_acc: 0.7383\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6783 - acc: 0.7252 - val_loss: 0.6337 - val_acc: 0.7404\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63098 to 0.62866, saving model to best.model\n",
      "1s - loss: 0.6762 - acc: 0.7231 - val_loss: 0.6287 - val_acc: 0.7439\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6803 - acc: 0.7228 - val_loss: 0.6320 - val_acc: 0.7461\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62866 to 0.62727, saving model to best.model\n",
      "1s - loss: 0.6743 - acc: 0.7246 - val_loss: 0.6273 - val_acc: 0.7470\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6768 - acc: 0.7251 - val_loss: 0.6316 - val_acc: 0.7460\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6739 - acc: 0.7262 - val_loss: 0.6279 - val_acc: 0.7439\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6719 - acc: 0.7261 - val_loss: 0.6295 - val_acc: 0.7412\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6746 - acc: 0.7222 - val_loss: 0.6289 - val_acc: 0.7431\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62727 to 0.62455, saving model to best.model\n",
      "1s - loss: 0.6741 - acc: 0.7248 - val_loss: 0.6245 - val_acc: 0.7458\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.62455 to 0.62412, saving model to best.model\n",
      "1s - loss: 0.6681 - acc: 0.7290 - val_loss: 0.6241 - val_acc: 0.7454\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6720 - acc: 0.7259 - val_loss: 0.6251 - val_acc: 0.7456\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6714 - acc: 0.7275 - val_loss: 0.6261 - val_acc: 0.7456\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6728 - acc: 0.7251 - val_loss: 0.6249 - val_acc: 0.7460\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.62412 to 0.62231, saving model to best.model\n",
      "1s - loss: 0.6709 - acc: 0.7256 - val_loss: 0.6223 - val_acc: 0.7468\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62231 to 0.62207, saving model to best.model\n",
      "1s - loss: 0.6690 - acc: 0.7300 - val_loss: 0.6221 - val_acc: 0.7463\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62207 to 0.62119, saving model to best.model\n",
      "0s - loss: 0.6689 - acc: 0.7265 - val_loss: 0.6212 - val_acc: 0.7457\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62119 to 0.62047, saving model to best.model\n",
      "1s - loss: 0.6705 - acc: 0.7280 - val_loss: 0.6205 - val_acc: 0.7463\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6689 - acc: 0.7289 - val_loss: 0.6228 - val_acc: 0.7474\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62047 to 0.61974, saving model to best.model\n",
      "0s - loss: 0.6656 - acc: 0.7291 - val_loss: 0.6197 - val_acc: 0.7457\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.61974 to 0.61779, saving model to best.model\n",
      "0s - loss: 0.6680 - acc: 0.7283 - val_loss: 0.6178 - val_acc: 0.7482\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6652 - acc: 0.7284 - val_loss: 0.6197 - val_acc: 0.7496\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61779 to 0.61487, saving model to best.model\n",
      "1s - loss: 0.6640 - acc: 0.7294 - val_loss: 0.6149 - val_acc: 0.7511\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6650 - acc: 0.7278 - val_loss: 0.6216 - val_acc: 0.7458\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.6664 - acc: 0.7287 - val_loss: 0.6227 - val_acc: 0.7453\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6649 - acc: 0.7285 - val_loss: 0.6203 - val_acc: 0.7475\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6634 - acc: 0.7305 - val_loss: 0.6163 - val_acc: 0.7494\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6623 - acc: 0.7296 - val_loss: 0.6183 - val_acc: 0.7453\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.61487 to 0.61369, saving model to best.model\n",
      "1s - loss: 0.6624 - acc: 0.7293 - val_loss: 0.6137 - val_acc: 0.7488\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6661 - acc: 0.7270 - val_loss: 0.6182 - val_acc: 0.7480\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.61369 to 0.61339, saving model to best.model\n",
      "0s - loss: 0.6611 - acc: 0.7314 - val_loss: 0.6134 - val_acc: 0.7502\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6608 - acc: 0.7329 - val_loss: 0.6178 - val_acc: 0.7454\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61339 to 0.61173, saving model to best.model\n",
      "0s - loss: 0.6640 - acc: 0.7297 - val_loss: 0.6117 - val_acc: 0.7519\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6596 - acc: 0.7321 - val_loss: 0.6144 - val_acc: 0.7493\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6593 - acc: 0.7331 - val_loss: 0.6118 - val_acc: 0.7509\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6619 - acc: 0.7304 - val_loss: 0.6135 - val_acc: 0.7515\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6612 - acc: 0.7320 - val_loss: 0.6141 - val_acc: 0.7523\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61173 to 0.61118, saving model to best.model\n",
      "1s - loss: 0.6587 - acc: 0.7329 - val_loss: 0.6112 - val_acc: 0.7513\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61118 to 0.61102, saving model to best.model\n",
      "1s - loss: 0.6554 - acc: 0.7345 - val_loss: 0.6110 - val_acc: 0.7512\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6597 - acc: 0.7319 - val_loss: 0.6137 - val_acc: 0.7498\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6598 - acc: 0.7306 - val_loss: 0.6117 - val_acc: 0.7498\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.61102 to 0.61093, saving model to best.model\n",
      "0s - loss: 0.6581 - acc: 0.7304 - val_loss: 0.6109 - val_acc: 0.7516\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61093 to 0.61000, saving model to best.model\n",
      "0s - loss: 0.6552 - acc: 0.7341 - val_loss: 0.6100 - val_acc: 0.7543\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61000 to 0.60914, saving model to best.model\n",
      "0s - loss: 0.6581 - acc: 0.7332 - val_loss: 0.6091 - val_acc: 0.7526\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.60914 to 0.60776, saving model to best.model\n",
      "0s - loss: 0.6564 - acc: 0.7321 - val_loss: 0.6078 - val_acc: 0.7533\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6557 - acc: 0.7344 - val_loss: 0.6078 - val_acc: 0.7533\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6537 - acc: 0.7354 - val_loss: 0.6079 - val_acc: 0.7535\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60776 to 0.60650, saving model to best.model\n",
      "1s - loss: 0.6555 - acc: 0.7322 - val_loss: 0.6065 - val_acc: 0.7530\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6532 - acc: 0.7355 - val_loss: 0.6072 - val_acc: 0.7527\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6540 - acc: 0.7341 - val_loss: 0.6080 - val_acc: 0.7541\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60650 to 0.60634, saving model to best.model\n",
      "1s - loss: 0.6500 - acc: 0.7343 - val_loss: 0.6063 - val_acc: 0.7535\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.60634 to 0.60506, saving model to best.model\n",
      "0s - loss: 0.6508 - acc: 0.7354 - val_loss: 0.6051 - val_acc: 0.7553\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60506 to 0.60475, saving model to best.model\n",
      "0s - loss: 0.6520 - acc: 0.7340 - val_loss: 0.6048 - val_acc: 0.7540\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60475 to 0.60230, saving model to best.model\n",
      "0s - loss: 0.6498 - acc: 0.7358 - val_loss: 0.6023 - val_acc: 0.7562\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6508 - acc: 0.7361 - val_loss: 0.6040 - val_acc: 0.7553\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60230 to 0.60216, saving model to best.model\n",
      "0s - loss: 0.6511 - acc: 0.7338 - val_loss: 0.6022 - val_acc: 0.7548\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6520 - acc: 0.7347 - val_loss: 0.6026 - val_acc: 0.7537\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6477 - acc: 0.7376 - val_loss: 0.6024 - val_acc: 0.7539\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60216 to 0.60089, saving model to best.model\n",
      "0s - loss: 0.6506 - acc: 0.7335 - val_loss: 0.6009 - val_acc: 0.7556\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6471 - acc: 0.7353 - val_loss: 0.6036 - val_acc: 0.7540\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6470 - acc: 0.7377 - val_loss: 0.6022 - val_acc: 0.7560\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6474 - acc: 0.7371 - val_loss: 0.6014 - val_acc: 0.7562\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60089 to 0.60066, saving model to best.model\n",
      "1s - loss: 0.6485 - acc: 0.7337 - val_loss: 0.6007 - val_acc: 0.7560\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60066 to 0.59879, saving model to best.model\n",
      "0s - loss: 0.6462 - acc: 0.7366 - val_loss: 0.5988 - val_acc: 0.7555\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6457 - acc: 0.7366 - val_loss: 0.5989 - val_acc: 0.7559\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6495 - acc: 0.7345 - val_loss: 0.5994 - val_acc: 0.7568\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83322, saving model to best.model\n",
      "0s - loss: 0.9323 - acc: 0.6204 - val_loss: 0.8332 - val_acc: 0.6648\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.83322 to 0.83279, saving model to best.model\n",
      "1s - loss: 0.8634 - acc: 0.6568 - val_loss: 0.8328 - val_acc: 0.6648\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.8563 - acc: 0.6574 - val_loss: 0.8328 - val_acc: 0.6648\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.8535 - acc: 0.6574 - val_loss: 0.8332 - val_acc: 0.6648\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83279 to 0.82280, saving model to best.model\n",
      "1s - loss: 0.8468 - acc: 0.6574 - val_loss: 0.8228 - val_acc: 0.6648\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.8428 - acc: 0.6574 - val_loss: 0.8232 - val_acc: 0.6648\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82280 to 0.81780, saving model to best.model\n",
      "0s - loss: 0.8364 - acc: 0.6574 - val_loss: 0.8178 - val_acc: 0.6648\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81780 to 0.81495, saving model to best.model\n",
      "1s - loss: 0.8336 - acc: 0.6574 - val_loss: 0.8149 - val_acc: 0.6648\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81495 to 0.81411, saving model to best.model\n",
      "0s - loss: 0.8335 - acc: 0.6574 - val_loss: 0.8141 - val_acc: 0.6648\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81411 to 0.81367, saving model to best.model\n",
      "0s - loss: 0.8306 - acc: 0.6573 - val_loss: 0.8137 - val_acc: 0.6648\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81367 to 0.81320, saving model to best.model\n",
      "0s - loss: 0.8313 - acc: 0.6573 - val_loss: 0.8132 - val_acc: 0.6648\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.8292 - acc: 0.6574 - val_loss: 0.8137 - val_acc: 0.6648\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8293 - acc: 0.6575 - val_loss: 0.8137 - val_acc: 0.6648\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81320 to 0.81144, saving model to best.model\n",
      "0s - loss: 0.8276 - acc: 0.6574 - val_loss: 0.8114 - val_acc: 0.6648\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8263 - acc: 0.6573 - val_loss: 0.8123 - val_acc: 0.6648\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.8262 - acc: 0.6574 - val_loss: 0.8129 - val_acc: 0.6648\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81144 to 0.81118, saving model to best.model\n",
      "0s - loss: 0.8269 - acc: 0.6574 - val_loss: 0.8112 - val_acc: 0.6648\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81118 to 0.81016, saving model to best.model\n",
      "0s - loss: 0.8255 - acc: 0.6578 - val_loss: 0.8102 - val_acc: 0.6648\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81016 to 0.80918, saving model to best.model\n",
      "0s - loss: 0.8248 - acc: 0.6572 - val_loss: 0.8092 - val_acc: 0.6648\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.80918 to 0.80875, saving model to best.model\n",
      "1s - loss: 0.8248 - acc: 0.6570 - val_loss: 0.8088 - val_acc: 0.6648\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.80875 to 0.80792, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6574 - val_loss: 0.8079 - val_acc: 0.6655\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.80792 to 0.80703, saving model to best.model\n",
      "1s - loss: 0.8226 - acc: 0.6584 - val_loss: 0.8070 - val_acc: 0.6650\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.80703 to 0.80639, saving model to best.model\n",
      "1s - loss: 0.8215 - acc: 0.6587 - val_loss: 0.8064 - val_acc: 0.6652\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80639 to 0.80533, saving model to best.model\n",
      "1s - loss: 0.8204 - acc: 0.6600 - val_loss: 0.8053 - val_acc: 0.6683\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80533 to 0.80392, saving model to best.model\n",
      "0s - loss: 0.8183 - acc: 0.6603 - val_loss: 0.8039 - val_acc: 0.6673\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80392 to 0.80223, saving model to best.model\n",
      "1s - loss: 0.8179 - acc: 0.6599 - val_loss: 0.8022 - val_acc: 0.6692\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80223 to 0.80164, saving model to best.model\n",
      "1s - loss: 0.8175 - acc: 0.6612 - val_loss: 0.8016 - val_acc: 0.6711\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80164 to 0.79968, saving model to best.model\n",
      "1s - loss: 0.8147 - acc: 0.6630 - val_loss: 0.7997 - val_acc: 0.6707\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.79968 to 0.79847, saving model to best.model\n",
      "1s - loss: 0.8137 - acc: 0.6625 - val_loss: 0.7985 - val_acc: 0.6710\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.79847 to 0.79830, saving model to best.model\n",
      "0s - loss: 0.8147 - acc: 0.6626 - val_loss: 0.7983 - val_acc: 0.6710\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79830 to 0.79580, saving model to best.model\n",
      "1s - loss: 0.8115 - acc: 0.6642 - val_loss: 0.7958 - val_acc: 0.6721\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79580 to 0.79462, saving model to best.model\n",
      "1s - loss: 0.8093 - acc: 0.6657 - val_loss: 0.7946 - val_acc: 0.6742\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79462 to 0.79320, saving model to best.model\n",
      "1s - loss: 0.8096 - acc: 0.6628 - val_loss: 0.7932 - val_acc: 0.6717\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79320 to 0.79205, saving model to best.model\n",
      "1s - loss: 0.8080 - acc: 0.6642 - val_loss: 0.7921 - val_acc: 0.6741\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79205 to 0.78755, saving model to best.model\n",
      "1s - loss: 0.8055 - acc: 0.6663 - val_loss: 0.7876 - val_acc: 0.6742\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.78755 to 0.78595, saving model to best.model\n",
      "1s - loss: 0.8031 - acc: 0.6654 - val_loss: 0.7859 - val_acc: 0.6741\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78595 to 0.78414, saving model to best.model\n",
      "1s - loss: 0.8027 - acc: 0.6677 - val_loss: 0.7841 - val_acc: 0.6760\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78414 to 0.78209, saving model to best.model\n",
      "0s - loss: 0.8020 - acc: 0.6666 - val_loss: 0.7821 - val_acc: 0.6774\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78209 to 0.78005, saving model to best.model\n",
      "0s - loss: 0.7992 - acc: 0.6678 - val_loss: 0.7800 - val_acc: 0.6765\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78005 to 0.77807, saving model to best.model\n",
      "1s - loss: 0.7971 - acc: 0.6689 - val_loss: 0.7781 - val_acc: 0.6778\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77807 to 0.77621, saving model to best.model\n",
      "1s - loss: 0.7978 - acc: 0.6682 - val_loss: 0.7762 - val_acc: 0.6770\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77621 to 0.77537, saving model to best.model\n",
      "1s - loss: 0.7958 - acc: 0.6686 - val_loss: 0.7754 - val_acc: 0.6779\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77537 to 0.77146, saving model to best.model\n",
      "1s - loss: 0.7947 - acc: 0.6701 - val_loss: 0.7715 - val_acc: 0.6808\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77146 to 0.76976, saving model to best.model\n",
      "1s - loss: 0.7925 - acc: 0.6712 - val_loss: 0.7698 - val_acc: 0.6848\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76976 to 0.76685, saving model to best.model\n",
      "1s - loss: 0.7919 - acc: 0.6698 - val_loss: 0.7668 - val_acc: 0.6847\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "1s - loss: 0.7922 - acc: 0.6710 - val_loss: 0.7680 - val_acc: 0.6844\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76685 to 0.76442, saving model to best.model\n",
      "1s - loss: 0.7892 - acc: 0.6716 - val_loss: 0.7644 - val_acc: 0.6837\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76442 to 0.76298, saving model to best.model\n",
      "1s - loss: 0.7875 - acc: 0.6739 - val_loss: 0.7630 - val_acc: 0.6852\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76298 to 0.75978, saving model to best.model\n",
      "1s - loss: 0.7839 - acc: 0.6738 - val_loss: 0.7598 - val_acc: 0.6907\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.75978 to 0.75727, saving model to best.model\n",
      "1s - loss: 0.7835 - acc: 0.6744 - val_loss: 0.7573 - val_acc: 0.6878\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "1s - loss: 0.7803 - acc: 0.6754 - val_loss: 0.7574 - val_acc: 0.6903\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75727 to 0.75241, saving model to best.model\n",
      "0s - loss: 0.7799 - acc: 0.6767 - val_loss: 0.7524 - val_acc: 0.6922\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75241 to 0.74936, saving model to best.model\n",
      "0s - loss: 0.7773 - acc: 0.6760 - val_loss: 0.7494 - val_acc: 0.6934\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74936 to 0.74780, saving model to best.model\n",
      "1s - loss: 0.7757 - acc: 0.6778 - val_loss: 0.7478 - val_acc: 0.6943\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74780 to 0.74409, saving model to best.model\n",
      "0s - loss: 0.7741 - acc: 0.6777 - val_loss: 0.7441 - val_acc: 0.6959\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74409 to 0.74300, saving model to best.model\n",
      "1s - loss: 0.7731 - acc: 0.6792 - val_loss: 0.7430 - val_acc: 0.6958\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74300 to 0.73751, saving model to best.model\n",
      "0s - loss: 0.7697 - acc: 0.6810 - val_loss: 0.7375 - val_acc: 0.7013\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.73751 to 0.73709, saving model to best.model\n",
      "0s - loss: 0.7698 - acc: 0.6833 - val_loss: 0.7371 - val_acc: 0.6967\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73709 to 0.73651, saving model to best.model\n",
      "0s - loss: 0.7688 - acc: 0.6809 - val_loss: 0.7365 - val_acc: 0.7032\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73651 to 0.73220, saving model to best.model\n",
      "0s - loss: 0.7671 - acc: 0.6856 - val_loss: 0.7322 - val_acc: 0.7006\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73220 to 0.72918, saving model to best.model\n",
      "0s - loss: 0.7631 - acc: 0.6858 - val_loss: 0.7292 - val_acc: 0.7056\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72918 to 0.72873, saving model to best.model\n",
      "1s - loss: 0.7619 - acc: 0.6860 - val_loss: 0.7287 - val_acc: 0.7020\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.72873 to 0.72673, saving model to best.model\n",
      "0s - loss: 0.7596 - acc: 0.6861 - val_loss: 0.7267 - val_acc: 0.7034\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72673 to 0.72642, saving model to best.model\n",
      "0s - loss: 0.7591 - acc: 0.6878 - val_loss: 0.7264 - val_acc: 0.7008\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72642 to 0.72158, saving model to best.model\n",
      "0s - loss: 0.7574 - acc: 0.6861 - val_loss: 0.7216 - val_acc: 0.7082\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72158 to 0.71523, saving model to best.model\n",
      "0s - loss: 0.7548 - acc: 0.6883 - val_loss: 0.7152 - val_acc: 0.7107\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.71523 to 0.71418, saving model to best.model\n",
      "0s - loss: 0.7547 - acc: 0.6868 - val_loss: 0.7142 - val_acc: 0.7121\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71418 to 0.71304, saving model to best.model\n",
      "0s - loss: 0.7514 - acc: 0.6906 - val_loss: 0.7130 - val_acc: 0.7111\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.7525 - acc: 0.6895 - val_loss: 0.7155 - val_acc: 0.7121\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71304 to 0.70802, saving model to best.model\n",
      "0s - loss: 0.7473 - acc: 0.6925 - val_loss: 0.7080 - val_acc: 0.7121\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70802 to 0.70388, saving model to best.model\n",
      "0s - loss: 0.7461 - acc: 0.6923 - val_loss: 0.7039 - val_acc: 0.7165\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70388 to 0.70045, saving model to best.model\n",
      "1s - loss: 0.7448 - acc: 0.6930 - val_loss: 0.7004 - val_acc: 0.7181\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70045 to 0.69899, saving model to best.model\n",
      "0s - loss: 0.7438 - acc: 0.6932 - val_loss: 0.6990 - val_acc: 0.7144\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.7371 - acc: 0.6974 - val_loss: 0.6992 - val_acc: 0.7179\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69899 to 0.69764, saving model to best.model\n",
      "0s - loss: 0.7411 - acc: 0.6954 - val_loss: 0.6976 - val_acc: 0.7192\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69764 to 0.69303, saving model to best.model\n",
      "0s - loss: 0.7402 - acc: 0.6950 - val_loss: 0.6930 - val_acc: 0.7212\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69303 to 0.69233, saving model to best.model\n",
      "1s - loss: 0.7346 - acc: 0.6982 - val_loss: 0.6923 - val_acc: 0.7232\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69233 to 0.69183, saving model to best.model\n",
      "1s - loss: 0.7357 - acc: 0.6974 - val_loss: 0.6918 - val_acc: 0.7190\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69183 to 0.68710, saving model to best.model\n",
      "0s - loss: 0.7329 - acc: 0.6979 - val_loss: 0.6871 - val_acc: 0.7215\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.7314 - acc: 0.6990 - val_loss: 0.6872 - val_acc: 0.7232\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68710 to 0.68526, saving model to best.model\n",
      "0s - loss: 0.7326 - acc: 0.7004 - val_loss: 0.6853 - val_acc: 0.7268\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.68526 to 0.67724, saving model to best.model\n",
      "0s - loss: 0.7289 - acc: 0.6991 - val_loss: 0.6772 - val_acc: 0.7262\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.7283 - acc: 0.6991 - val_loss: 0.6786 - val_acc: 0.7281\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7267 - acc: 0.6987 - val_loss: 0.6807 - val_acc: 0.7197\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.67724 to 0.67673, saving model to best.model\n",
      "0s - loss: 0.7268 - acc: 0.7019 - val_loss: 0.6767 - val_acc: 0.7288\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67673 to 0.67201, saving model to best.model\n",
      "1s - loss: 0.7249 - acc: 0.7037 - val_loss: 0.6720 - val_acc: 0.7330\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7236 - acc: 0.7019 - val_loss: 0.6732 - val_acc: 0.7302\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67201 to 0.66705, saving model to best.model\n",
      "1s - loss: 0.7191 - acc: 0.7047 - val_loss: 0.6671 - val_acc: 0.7307\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.66705 to 0.66668, saving model to best.model\n",
      "0s - loss: 0.7197 - acc: 0.7043 - val_loss: 0.6667 - val_acc: 0.7296\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.7174 - acc: 0.7053 - val_loss: 0.6672 - val_acc: 0.7342\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.66668 to 0.66611, saving model to best.model\n",
      "1s - loss: 0.7168 - acc: 0.7085 - val_loss: 0.6661 - val_acc: 0.7302\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66611 to 0.66312, saving model to best.model\n",
      "1s - loss: 0.7161 - acc: 0.7077 - val_loss: 0.6631 - val_acc: 0.7316\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66312 to 0.66082, saving model to best.model\n",
      "1s - loss: 0.7165 - acc: 0.7034 - val_loss: 0.6608 - val_acc: 0.7327\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66082 to 0.65978, saving model to best.model\n",
      "1s - loss: 0.7127 - acc: 0.7091 - val_loss: 0.6598 - val_acc: 0.7344\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7117 - acc: 0.7077 - val_loss: 0.6602 - val_acc: 0.7324\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.65978 to 0.65432, saving model to best.model\n",
      "1s - loss: 0.7094 - acc: 0.7079 - val_loss: 0.6543 - val_acc: 0.7381\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7080 - acc: 0.7085 - val_loss: 0.6601 - val_acc: 0.7299\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7099 - acc: 0.7105 - val_loss: 0.6562 - val_acc: 0.7324\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.65432 to 0.65082, saving model to best.model\n",
      "0s - loss: 0.7077 - acc: 0.7116 - val_loss: 0.6508 - val_acc: 0.7358\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7093 - acc: 0.7078 - val_loss: 0.6535 - val_acc: 0.7356\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7068 - acc: 0.7098 - val_loss: 0.6549 - val_acc: 0.7356\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.65082 to 0.64626, saving model to best.model\n",
      "1s - loss: 0.7037 - acc: 0.7120 - val_loss: 0.6463 - val_acc: 0.7391\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.64626 to 0.64571, saving model to best.model\n",
      "1s - loss: 0.7022 - acc: 0.7140 - val_loss: 0.6457 - val_acc: 0.7402\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.7018 - acc: 0.7121 - val_loss: 0.6469 - val_acc: 0.7388\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.64571 to 0.64540, saving model to best.model\n",
      "1s - loss: 0.7041 - acc: 0.7090 - val_loss: 0.6454 - val_acc: 0.7391\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7027 - acc: 0.7119 - val_loss: 0.6470 - val_acc: 0.7378\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.6993 - acc: 0.7144 - val_loss: 0.6463 - val_acc: 0.7355\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7025 - acc: 0.7113 - val_loss: 0.6467 - val_acc: 0.7403\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64540 to 0.63844, saving model to best.model\n",
      "1s - loss: 0.6978 - acc: 0.7128 - val_loss: 0.6384 - val_acc: 0.7417\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6955 - acc: 0.7153 - val_loss: 0.6405 - val_acc: 0.7392\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6968 - acc: 0.7146 - val_loss: 0.6415 - val_acc: 0.7382\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6972 - acc: 0.7145 - val_loss: 0.6435 - val_acc: 0.7411\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6966 - acc: 0.7133 - val_loss: 0.6411 - val_acc: 0.7426\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.63844 to 0.63708, saving model to best.model\n",
      "0s - loss: 0.6927 - acc: 0.7179 - val_loss: 0.6371 - val_acc: 0.7424\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6935 - acc: 0.7174 - val_loss: 0.6377 - val_acc: 0.7438\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.6927 - acc: 0.7191 - val_loss: 0.6373 - val_acc: 0.7447\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.63708 to 0.63436, saving model to best.model\n",
      "0s - loss: 0.6909 - acc: 0.7157 - val_loss: 0.6344 - val_acc: 0.7423\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.63436 to 0.63324, saving model to best.model\n",
      "1s - loss: 0.6909 - acc: 0.7189 - val_loss: 0.6332 - val_acc: 0.7464\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6873 - acc: 0.7183 - val_loss: 0.6359 - val_acc: 0.7393\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.6891 - acc: 0.7192 - val_loss: 0.6356 - val_acc: 0.7415\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.63324 to 0.63310, saving model to best.model\n",
      "1s - loss: 0.6888 - acc: 0.7189 - val_loss: 0.6331 - val_acc: 0.7419\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.63310 to 0.63288, saving model to best.model\n",
      "0s - loss: 0.6889 - acc: 0.7186 - val_loss: 0.6329 - val_acc: 0.7477\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.63288 to 0.62959, saving model to best.model\n",
      "0s - loss: 0.6891 - acc: 0.7176 - val_loss: 0.6296 - val_acc: 0.7482\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.62959 to 0.62912, saving model to best.model\n",
      "1s - loss: 0.6835 - acc: 0.7200 - val_loss: 0.6291 - val_acc: 0.7495\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.62912 to 0.62839, saving model to best.model\n",
      "1s - loss: 0.6839 - acc: 0.7204 - val_loss: 0.6284 - val_acc: 0.7499\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6839 - acc: 0.7198 - val_loss: 0.6287 - val_acc: 0.7501\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62839 to 0.62623, saving model to best.model\n",
      "1s - loss: 0.6825 - acc: 0.7219 - val_loss: 0.6262 - val_acc: 0.7475\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.62623 to 0.62517, saving model to best.model\n",
      "1s - loss: 0.6835 - acc: 0.7208 - val_loss: 0.6252 - val_acc: 0.7492\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6864 - acc: 0.7181 - val_loss: 0.6298 - val_acc: 0.7467\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6816 - acc: 0.7208 - val_loss: 0.6264 - val_acc: 0.7470\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6805 - acc: 0.7238 - val_loss: 0.6275 - val_acc: 0.7505\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6801 - acc: 0.7208 - val_loss: 0.6259 - val_acc: 0.7482\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.62517 to 0.62241, saving model to best.model\n",
      "1s - loss: 0.6770 - acc: 0.7212 - val_loss: 0.6224 - val_acc: 0.7504\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6776 - acc: 0.7229 - val_loss: 0.6225 - val_acc: 0.7522\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6767 - acc: 0.7249 - val_loss: 0.6229 - val_acc: 0.7479\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.62241 to 0.62138, saving model to best.model\n",
      "1s - loss: 0.6779 - acc: 0.7231 - val_loss: 0.6214 - val_acc: 0.7511\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.62138 to 0.61987, saving model to best.model\n",
      "1s - loss: 0.6770 - acc: 0.7230 - val_loss: 0.6199 - val_acc: 0.7525\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6797 - acc: 0.7203 - val_loss: 0.6251 - val_acc: 0.7500\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.61987 to 0.61728, saving model to best.model\n",
      "0s - loss: 0.6728 - acc: 0.7268 - val_loss: 0.6173 - val_acc: 0.7515\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6723 - acc: 0.7267 - val_loss: 0.6174 - val_acc: 0.7513\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.61728 to 0.61616, saving model to best.model\n",
      "1s - loss: 0.6726 - acc: 0.7250 - val_loss: 0.6162 - val_acc: 0.7516\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.61616 to 0.61444, saving model to best.model\n",
      "1s - loss: 0.6727 - acc: 0.7249 - val_loss: 0.6144 - val_acc: 0.7536\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6726 - acc: 0.7244 - val_loss: 0.6177 - val_acc: 0.7492\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6737 - acc: 0.7248 - val_loss: 0.6149 - val_acc: 0.7514\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.61444 to 0.61398, saving model to best.model\n",
      "1s - loss: 0.6700 - acc: 0.7260 - val_loss: 0.6140 - val_acc: 0.7512\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.61398 to 0.61229, saving model to best.model\n",
      "0s - loss: 0.6719 - acc: 0.7233 - val_loss: 0.6123 - val_acc: 0.7534\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6679 - acc: 0.7273 - val_loss: 0.6138 - val_acc: 0.7518\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6688 - acc: 0.7252 - val_loss: 0.6132 - val_acc: 0.7530\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.61229 to 0.61073, saving model to best.model\n",
      "0s - loss: 0.6651 - acc: 0.7281 - val_loss: 0.6107 - val_acc: 0.7528\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6696 - acc: 0.7261 - val_loss: 0.6120 - val_acc: 0.7543\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6688 - acc: 0.7261 - val_loss: 0.6129 - val_acc: 0.7521\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.61073 to 0.61034, saving model to best.model\n",
      "1s - loss: 0.6635 - acc: 0.7277 - val_loss: 0.6103 - val_acc: 0.7519\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6646 - acc: 0.7296 - val_loss: 0.6105 - val_acc: 0.7528\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.6644 - acc: 0.7265 - val_loss: 0.6105 - val_acc: 0.7536\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.61034 to 0.60867, saving model to best.model\n",
      "0s - loss: 0.6641 - acc: 0.7269 - val_loss: 0.6087 - val_acc: 0.7533\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.60867 to 0.60804, saving model to best.model\n",
      "1s - loss: 0.6678 - acc: 0.7281 - val_loss: 0.6080 - val_acc: 0.7553\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6618 - acc: 0.7279 - val_loss: 0.6098 - val_acc: 0.7546\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.60804 to 0.60742, saving model to best.model\n",
      "0s - loss: 0.6646 - acc: 0.7279 - val_loss: 0.6074 - val_acc: 0.7547\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.60742 to 0.60730, saving model to best.model\n",
      "1s - loss: 0.6638 - acc: 0.7292 - val_loss: 0.6073 - val_acc: 0.7529\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.60730 to 0.60377, saving model to best.model\n",
      "1s - loss: 0.6613 - acc: 0.7293 - val_loss: 0.6038 - val_acc: 0.7562\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6594 - acc: 0.7318 - val_loss: 0.6056 - val_acc: 0.7563\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.60377 to 0.60236, saving model to best.model\n",
      "1s - loss: 0.6624 - acc: 0.7285 - val_loss: 0.6024 - val_acc: 0.7575\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6611 - acc: 0.7319 - val_loss: 0.6054 - val_acc: 0.7564\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6616 - acc: 0.7309 - val_loss: 0.6040 - val_acc: 0.7567\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6573 - acc: 0.7306 - val_loss: 0.6050 - val_acc: 0.7546\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6563 - acc: 0.7317 - val_loss: 0.6033 - val_acc: 0.7555\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.60236 to 0.60080, saving model to best.model\n",
      "1s - loss: 0.6602 - acc: 0.7310 - val_loss: 0.6008 - val_acc: 0.7567\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6579 - acc: 0.7317 - val_loss: 0.6065 - val_acc: 0.7560\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.60080 to 0.59982, saving model to best.model\n",
      "1s - loss: 0.6566 - acc: 0.7323 - val_loss: 0.5998 - val_acc: 0.7576\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.59982 to 0.59913, saving model to best.model\n",
      "1s - loss: 0.6577 - acc: 0.7306 - val_loss: 0.5991 - val_acc: 0.7571\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6539 - acc: 0.7319 - val_loss: 0.5994 - val_acc: 0.7567\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6592 - acc: 0.7307 - val_loss: 0.6008 - val_acc: 0.7571\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.59913 to 0.59658, saving model to best.model\n",
      "1s - loss: 0.6529 - acc: 0.7328 - val_loss: 0.5966 - val_acc: 0.7570\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6551 - acc: 0.7311 - val_loss: 0.5977 - val_acc: 0.7567\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6524 - acc: 0.7330 - val_loss: 0.5979 - val_acc: 0.7577\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6560 - acc: 0.7307 - val_loss: 0.5975 - val_acc: 0.7578\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.59658 to 0.59419, saving model to best.model\n",
      "0s - loss: 0.6555 - acc: 0.7312 - val_loss: 0.5942 - val_acc: 0.7582\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.6523 - acc: 0.7319 - val_loss: 0.5962 - val_acc: 0.7585\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6559 - acc: 0.7334 - val_loss: 0.6000 - val_acc: 0.7584\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.6540 - acc: 0.7333 - val_loss: 0.5980 - val_acc: 0.7581\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6528 - acc: 0.7317 - val_loss: 0.5970 - val_acc: 0.7583\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6557 - acc: 0.7332 - val_loss: 0.5976 - val_acc: 0.7585\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.59419 to 0.59335, saving model to best.model\n",
      "0s - loss: 0.6492 - acc: 0.7336 - val_loss: 0.5934 - val_acc: 0.7596\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6533 - acc: 0.7322 - val_loss: 0.5934 - val_acc: 0.7605\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6528 - acc: 0.7336 - val_loss: 0.5936 - val_acc: 0.7584\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.59335 to 0.59101, saving model to best.model\n",
      "0s - loss: 0.6496 - acc: 0.7330 - val_loss: 0.5910 - val_acc: 0.7598\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59101 to 0.59098, saving model to best.model\n",
      "0s - loss: 0.6461 - acc: 0.7330 - val_loss: 0.5910 - val_acc: 0.7593\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6480 - acc: 0.7350 - val_loss: 0.5924 - val_acc: 0.7601\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6477 - acc: 0.7360 - val_loss: 0.5921 - val_acc: 0.7588\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6448 - acc: 0.7355 - val_loss: 0.5927 - val_acc: 0.7601\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.59098 to 0.58908, saving model to best.model\n",
      "0s - loss: 0.6471 - acc: 0.7340 - val_loss: 0.5891 - val_acc: 0.7603\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6499 - acc: 0.7349 - val_loss: 0.5909 - val_acc: 0.7603\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6437 - acc: 0.7353 - val_loss: 0.5905 - val_acc: 0.7598\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6474 - acc: 0.7339 - val_loss: 0.5905 - val_acc: 0.7605\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6461 - acc: 0.7354 - val_loss: 0.5910 - val_acc: 0.7614\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6461 - acc: 0.7367 - val_loss: 0.5914 - val_acc: 0.7621\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6464 - acc: 0.7357 - val_loss: 0.5920 - val_acc: 0.7602\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.58908 to 0.58757, saving model to best.model\n",
      "1s - loss: 0.6452 - acc: 0.7386 - val_loss: 0.5876 - val_acc: 0.7612\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.58757 to 0.58741, saving model to best.model\n",
      "1s - loss: 0.6464 - acc: 0.7367 - val_loss: 0.5874 - val_acc: 0.7616\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6449 - acc: 0.7377 - val_loss: 0.5902 - val_acc: 0.7598\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.85308, saving model to best.model\n",
      "1s - loss: 0.9012 - acc: 0.6356 - val_loss: 0.8531 - val_acc: 0.6461\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.85308 to 0.85202, saving model to best.model\n",
      "1s - loss: 0.8572 - acc: 0.6564 - val_loss: 0.8520 - val_acc: 0.6461\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.85202 to 0.85010, saving model to best.model\n",
      "0s - loss: 0.8527 - acc: 0.6565 - val_loss: 0.8501 - val_acc: 0.6461\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.85010 to 0.84460, saving model to best.model\n",
      "1s - loss: 0.8468 - acc: 0.6565 - val_loss: 0.8446 - val_acc: 0.6461\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84460 to 0.83921, saving model to best.model\n",
      "1s - loss: 0.8421 - acc: 0.6565 - val_loss: 0.8392 - val_acc: 0.6461\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83921 to 0.83398, saving model to best.model\n",
      "1s - loss: 0.8382 - acc: 0.6565 - val_loss: 0.8340 - val_acc: 0.6461\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83398 to 0.83191, saving model to best.model\n",
      "1s - loss: 0.8350 - acc: 0.6565 - val_loss: 0.8319 - val_acc: 0.6461\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.83191 to 0.83039, saving model to best.model\n",
      "1s - loss: 0.8325 - acc: 0.6565 - val_loss: 0.8304 - val_acc: 0.6461\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.83039 to 0.82968, saving model to best.model\n",
      "1s - loss: 0.8301 - acc: 0.6565 - val_loss: 0.8297 - val_acc: 0.6461\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8314 - acc: 0.6565 - val_loss: 0.8303 - val_acc: 0.6461\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82968 to 0.82839, saving model to best.model\n",
      "1s - loss: 0.8302 - acc: 0.6565 - val_loss: 0.8284 - val_acc: 0.6461\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8291 - acc: 0.6565 - val_loss: 0.8289 - val_acc: 0.6461\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8282 - acc: 0.6565 - val_loss: 0.8285 - val_acc: 0.6461\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82839 to 0.82698, saving model to best.model\n",
      "0s - loss: 0.8273 - acc: 0.6565 - val_loss: 0.8270 - val_acc: 0.6461\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8267 - acc: 0.6565 - val_loss: 0.8272 - val_acc: 0.6461\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82698 to 0.82637, saving model to best.model\n",
      "0s - loss: 0.8255 - acc: 0.6565 - val_loss: 0.8264 - val_acc: 0.6461\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82637 to 0.82624, saving model to best.model\n",
      "0s - loss: 0.8257 - acc: 0.6565 - val_loss: 0.8262 - val_acc: 0.6461\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82624 to 0.82494, saving model to best.model\n",
      "0s - loss: 0.8251 - acc: 0.6565 - val_loss: 0.8249 - val_acc: 0.6461\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.8231 - acc: 0.6565 - val_loss: 0.8252 - val_acc: 0.6461\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82494 to 0.82378, saving model to best.model\n",
      "0s - loss: 0.8233 - acc: 0.6565 - val_loss: 0.8238 - val_acc: 0.6461\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82378 to 0.82323, saving model to best.model\n",
      "0s - loss: 0.8236 - acc: 0.6565 - val_loss: 0.8232 - val_acc: 0.6461\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82323 to 0.82275, saving model to best.model\n",
      "0s - loss: 0.8219 - acc: 0.6567 - val_loss: 0.8228 - val_acc: 0.6461\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.82275 to 0.82169, saving model to best.model\n",
      "0s - loss: 0.8201 - acc: 0.6564 - val_loss: 0.8217 - val_acc: 0.6461\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.82169 to 0.82141, saving model to best.model\n",
      "0s - loss: 0.8207 - acc: 0.6562 - val_loss: 0.8214 - val_acc: 0.6461\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.82141 to 0.81963, saving model to best.model\n",
      "1s - loss: 0.8200 - acc: 0.6565 - val_loss: 0.8196 - val_acc: 0.6461\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81963 to 0.81833, saving model to best.model\n",
      "1s - loss: 0.8181 - acc: 0.6567 - val_loss: 0.8183 - val_acc: 0.6461\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81833 to 0.81740, saving model to best.model\n",
      "0s - loss: 0.8176 - acc: 0.6573 - val_loss: 0.8174 - val_acc: 0.6463\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81740 to 0.81463, saving model to best.model\n",
      "0s - loss: 0.8163 - acc: 0.6574 - val_loss: 0.8146 - val_acc: 0.6471\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81463 to 0.81396, saving model to best.model\n",
      "0s - loss: 0.8154 - acc: 0.6574 - val_loss: 0.8140 - val_acc: 0.6471\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81396 to 0.81181, saving model to best.model\n",
      "0s - loss: 0.8131 - acc: 0.6579 - val_loss: 0.8118 - val_acc: 0.6475\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.81181 to 0.81102, saving model to best.model\n",
      "0s - loss: 0.8126 - acc: 0.6593 - val_loss: 0.8110 - val_acc: 0.6487\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.81102 to 0.80934, saving model to best.model\n",
      "0s - loss: 0.8121 - acc: 0.6587 - val_loss: 0.8093 - val_acc: 0.6481\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80934 to 0.80644, saving model to best.model\n",
      "0s - loss: 0.8097 - acc: 0.6590 - val_loss: 0.8064 - val_acc: 0.6521\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80644 to 0.80380, saving model to best.model\n",
      "1s - loss: 0.8069 - acc: 0.6618 - val_loss: 0.8038 - val_acc: 0.6521\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80380 to 0.80194, saving model to best.model\n",
      "1s - loss: 0.8080 - acc: 0.6606 - val_loss: 0.8019 - val_acc: 0.6522\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80194 to 0.79941, saving model to best.model\n",
      "1s - loss: 0.8040 - acc: 0.6626 - val_loss: 0.7994 - val_acc: 0.6542\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79941 to 0.79677, saving model to best.model\n",
      "0s - loss: 0.8023 - acc: 0.6626 - val_loss: 0.7968 - val_acc: 0.6560\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.8029 - acc: 0.6627 - val_loss: 0.7980 - val_acc: 0.6533\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79677 to 0.79311, saving model to best.model\n",
      "1s - loss: 0.7982 - acc: 0.6654 - val_loss: 0.7931 - val_acc: 0.6587\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79311 to 0.79202, saving model to best.model\n",
      "0s - loss: 0.7995 - acc: 0.6649 - val_loss: 0.7920 - val_acc: 0.6583\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79202 to 0.78748, saving model to best.model\n",
      "0s - loss: 0.7961 - acc: 0.6647 - val_loss: 0.7875 - val_acc: 0.6644\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "1s - loss: 0.7955 - acc: 0.6645 - val_loss: 0.7877 - val_acc: 0.6612\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78748 to 0.78659, saving model to best.model\n",
      "1s - loss: 0.7938 - acc: 0.6669 - val_loss: 0.7866 - val_acc: 0.6598\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78659 to 0.78173, saving model to best.model\n",
      "1s - loss: 0.7924 - acc: 0.6672 - val_loss: 0.7817 - val_acc: 0.6667\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78173 to 0.77870, saving model to best.model\n",
      "1s - loss: 0.7892 - acc: 0.6689 - val_loss: 0.7787 - val_acc: 0.6692\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77870 to 0.77710, saving model to best.model\n",
      "0s - loss: 0.7897 - acc: 0.6685 - val_loss: 0.7771 - val_acc: 0.6700\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77710 to 0.77685, saving model to best.model\n",
      "0s - loss: 0.7852 - acc: 0.6709 - val_loss: 0.7769 - val_acc: 0.6672\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77685 to 0.77441, saving model to best.model\n",
      "1s - loss: 0.7851 - acc: 0.6703 - val_loss: 0.7744 - val_acc: 0.6685\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77441 to 0.77373, saving model to best.model\n",
      "1s - loss: 0.7832 - acc: 0.6711 - val_loss: 0.7737 - val_acc: 0.6658\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77373 to 0.76914, saving model to best.model\n",
      "0s - loss: 0.7818 - acc: 0.6723 - val_loss: 0.7691 - val_acc: 0.6714\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76914 to 0.76703, saving model to best.model\n",
      "0s - loss: 0.7803 - acc: 0.6740 - val_loss: 0.7670 - val_acc: 0.6746\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.7764 - acc: 0.6751 - val_loss: 0.7671 - val_acc: 0.6751\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76703 to 0.76037, saving model to best.model\n",
      "0s - loss: 0.7760 - acc: 0.6753 - val_loss: 0.7604 - val_acc: 0.6770\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76037 to 0.75691, saving model to best.model\n",
      "0s - loss: 0.7750 - acc: 0.6768 - val_loss: 0.7569 - val_acc: 0.6829\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75691 to 0.75496, saving model to best.model\n",
      "0s - loss: 0.7711 - acc: 0.6775 - val_loss: 0.7550 - val_acc: 0.6826\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75496 to 0.75318, saving model to best.model\n",
      "1s - loss: 0.7718 - acc: 0.6783 - val_loss: 0.7532 - val_acc: 0.6849\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75318 to 0.75207, saving model to best.model\n",
      "1s - loss: 0.7689 - acc: 0.6801 - val_loss: 0.7521 - val_acc: 0.6857\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75207 to 0.74889, saving model to best.model\n",
      "1s - loss: 0.7667 - acc: 0.6799 - val_loss: 0.7489 - val_acc: 0.6849\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74889 to 0.74733, saving model to best.model\n",
      "1s - loss: 0.7646 - acc: 0.6813 - val_loss: 0.7473 - val_acc: 0.6835\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74733 to 0.74526, saving model to best.model\n",
      "1s - loss: 0.7647 - acc: 0.6794 - val_loss: 0.7453 - val_acc: 0.6877\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74526 to 0.73968, saving model to best.model\n",
      "1s - loss: 0.7612 - acc: 0.6820 - val_loss: 0.7397 - val_acc: 0.6922\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.7584 - acc: 0.6840 - val_loss: 0.7426 - val_acc: 0.6851\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73968 to 0.73814, saving model to best.model\n",
      "0s - loss: 0.7584 - acc: 0.6857 - val_loss: 0.7381 - val_acc: 0.6889\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73814 to 0.73655, saving model to best.model\n",
      "1s - loss: 0.7551 - acc: 0.6864 - val_loss: 0.7366 - val_acc: 0.6893\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73655 to 0.72823, saving model to best.model\n",
      "1s - loss: 0.7527 - acc: 0.6880 - val_loss: 0.7282 - val_acc: 0.6987\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.7524 - acc: 0.6867 - val_loss: 0.7294 - val_acc: 0.6972\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72823 to 0.72642, saving model to best.model\n",
      "1s - loss: 0.7492 - acc: 0.6886 - val_loss: 0.7264 - val_acc: 0.6979\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72642 to 0.72309, saving model to best.model\n",
      "1s - loss: 0.7476 - acc: 0.6901 - val_loss: 0.7231 - val_acc: 0.7019\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "1s - loss: 0.7452 - acc: 0.6925 - val_loss: 0.7232 - val_acc: 0.7011\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72309 to 0.71743, saving model to best.model\n",
      "1s - loss: 0.7454 - acc: 0.6903 - val_loss: 0.7174 - val_acc: 0.7056\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "1s - loss: 0.7435 - acc: 0.6913 - val_loss: 0.7195 - val_acc: 0.6999\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.7405 - acc: 0.6957 - val_loss: 0.7185 - val_acc: 0.6995\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71743 to 0.71476, saving model to best.model\n",
      "1s - loss: 0.7406 - acc: 0.6940 - val_loss: 0.7148 - val_acc: 0.7043\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71476 to 0.71152, saving model to best.model\n",
      "1s - loss: 0.7372 - acc: 0.6973 - val_loss: 0.7115 - val_acc: 0.7052\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71152 to 0.70568, saving model to best.model\n",
      "1s - loss: 0.7351 - acc: 0.6986 - val_loss: 0.7057 - val_acc: 0.7091\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70568 to 0.70557, saving model to best.model\n",
      "1s - loss: 0.7353 - acc: 0.6963 - val_loss: 0.7056 - val_acc: 0.7101\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7327 - acc: 0.7004 - val_loss: 0.7081 - val_acc: 0.7046\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70557 to 0.70098, saving model to best.model\n",
      "1s - loss: 0.7278 - acc: 0.6999 - val_loss: 0.7010 - val_acc: 0.7153\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70098 to 0.69743, saving model to best.model\n",
      "1s - loss: 0.7254 - acc: 0.7007 - val_loss: 0.6974 - val_acc: 0.7122\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7259 - acc: 0.7026 - val_loss: 0.6997 - val_acc: 0.7104\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69743 to 0.69558, saving model to best.model\n",
      "1s - loss: 0.7246 - acc: 0.7010 - val_loss: 0.6956 - val_acc: 0.7139\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69558 to 0.69534, saving model to best.model\n",
      "1s - loss: 0.7260 - acc: 0.6998 - val_loss: 0.6953 - val_acc: 0.7121\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69534 to 0.69191, saving model to best.model\n",
      "1s - loss: 0.7226 - acc: 0.7026 - val_loss: 0.6919 - val_acc: 0.7143\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7227 - acc: 0.7037 - val_loss: 0.6940 - val_acc: 0.7126\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69191 to 0.69127, saving model to best.model\n",
      "1s - loss: 0.7199 - acc: 0.7033 - val_loss: 0.6913 - val_acc: 0.7153\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69127 to 0.69122, saving model to best.model\n",
      "0s - loss: 0.7212 - acc: 0.7047 - val_loss: 0.6912 - val_acc: 0.7138\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.69122 to 0.68636, saving model to best.model\n",
      "0s - loss: 0.7190 - acc: 0.7044 - val_loss: 0.6864 - val_acc: 0.7163\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.7160 - acc: 0.7043 - val_loss: 0.6864 - val_acc: 0.7157\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68636 to 0.68600, saving model to best.model\n",
      "0s - loss: 0.7174 - acc: 0.7045 - val_loss: 0.6860 - val_acc: 0.7162\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68600 to 0.68478, saving model to best.model\n",
      "0s - loss: 0.7158 - acc: 0.7033 - val_loss: 0.6848 - val_acc: 0.7173\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68478 to 0.68163, saving model to best.model\n",
      "0s - loss: 0.7131 - acc: 0.7073 - val_loss: 0.6816 - val_acc: 0.7218\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68163 to 0.67869, saving model to best.model\n",
      "1s - loss: 0.7128 - acc: 0.7064 - val_loss: 0.6787 - val_acc: 0.7231\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.7117 - acc: 0.7077 - val_loss: 0.6789 - val_acc: 0.7205\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.7075 - acc: 0.7086 - val_loss: 0.6800 - val_acc: 0.7220\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67869 to 0.67672, saving model to best.model\n",
      "0s - loss: 0.7109 - acc: 0.7089 - val_loss: 0.6767 - val_acc: 0.7214\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7094 - acc: 0.7064 - val_loss: 0.6786 - val_acc: 0.7208\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.67672 to 0.67312, saving model to best.model\n",
      "0s - loss: 0.7055 - acc: 0.7109 - val_loss: 0.6731 - val_acc: 0.7240\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.7044 - acc: 0.7099 - val_loss: 0.6750 - val_acc: 0.7239\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67312 to 0.66976, saving model to best.model\n",
      "0s - loss: 0.7036 - acc: 0.7138 - val_loss: 0.6698 - val_acc: 0.7253\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7037 - acc: 0.7117 - val_loss: 0.6707 - val_acc: 0.7251\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66976 to 0.66675, saving model to best.model\n",
      "0s - loss: 0.7017 - acc: 0.7138 - val_loss: 0.6667 - val_acc: 0.7276\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.6999 - acc: 0.7111 - val_loss: 0.6677 - val_acc: 0.7247\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7046 - acc: 0.7106 - val_loss: 0.6702 - val_acc: 0.7232\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66675 to 0.66640, saving model to best.model\n",
      "0s - loss: 0.7000 - acc: 0.7135 - val_loss: 0.6664 - val_acc: 0.7268\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.7014 - acc: 0.7127 - val_loss: 0.6693 - val_acc: 0.7240\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7001 - acc: 0.7126 - val_loss: 0.6668 - val_acc: 0.7270\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66640 to 0.66454, saving model to best.model\n",
      "0s - loss: 0.6986 - acc: 0.7127 - val_loss: 0.6645 - val_acc: 0.7262\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.66454 to 0.66333, saving model to best.model\n",
      "1s - loss: 0.6939 - acc: 0.7181 - val_loss: 0.6633 - val_acc: 0.7258\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6962 - acc: 0.7157 - val_loss: 0.6637 - val_acc: 0.7281\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66333 to 0.66304, saving model to best.model\n",
      "1s - loss: 0.6933 - acc: 0.7148 - val_loss: 0.6630 - val_acc: 0.7261\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.66304 to 0.65861, saving model to best.model\n",
      "1s - loss: 0.6938 - acc: 0.7149 - val_loss: 0.6586 - val_acc: 0.7326\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.6947 - acc: 0.7161 - val_loss: 0.6617 - val_acc: 0.7270\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65861 to 0.65826, saving model to best.model\n",
      "1s - loss: 0.6911 - acc: 0.7155 - val_loss: 0.6583 - val_acc: 0.7288\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65826 to 0.65555, saving model to best.model\n",
      "1s - loss: 0.6919 - acc: 0.7150 - val_loss: 0.6556 - val_acc: 0.7306\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6928 - acc: 0.7175 - val_loss: 0.6582 - val_acc: 0.7278\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6910 - acc: 0.7164 - val_loss: 0.6569 - val_acc: 0.7310\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.65555 to 0.65313, saving model to best.model\n",
      "1s - loss: 0.6906 - acc: 0.7195 - val_loss: 0.6531 - val_acc: 0.7308\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.6874 - acc: 0.7184 - val_loss: 0.6563 - val_acc: 0.7294\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6892 - acc: 0.7170 - val_loss: 0.6576 - val_acc: 0.7276\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65313 to 0.65242, saving model to best.model\n",
      "1s - loss: 0.6846 - acc: 0.7200 - val_loss: 0.6524 - val_acc: 0.7316\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6883 - acc: 0.7188 - val_loss: 0.6558 - val_acc: 0.7288\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.65242 to 0.65106, saving model to best.model\n",
      "0s - loss: 0.6864 - acc: 0.7200 - val_loss: 0.6511 - val_acc: 0.7334\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.65106 to 0.64690, saving model to best.model\n",
      "1s - loss: 0.6830 - acc: 0.7195 - val_loss: 0.6469 - val_acc: 0.7340\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6849 - acc: 0.7211 - val_loss: 0.6509 - val_acc: 0.7310\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64690 to 0.64425, saving model to best.model\n",
      "0s - loss: 0.6800 - acc: 0.7231 - val_loss: 0.6443 - val_acc: 0.7350\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.6847 - acc: 0.7211 - val_loss: 0.6511 - val_acc: 0.7315\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.6795 - acc: 0.7212 - val_loss: 0.6444 - val_acc: 0.7347\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6816 - acc: 0.7204 - val_loss: 0.6463 - val_acc: 0.7328\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.64425 to 0.64421, saving model to best.model\n",
      "1s - loss: 0.6824 - acc: 0.7213 - val_loss: 0.6442 - val_acc: 0.7343\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6787 - acc: 0.7236 - val_loss: 0.6484 - val_acc: 0.7307\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6806 - acc: 0.7218 - val_loss: 0.6462 - val_acc: 0.7324\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.64421 to 0.64334, saving model to best.model\n",
      "0s - loss: 0.6781 - acc: 0.7231 - val_loss: 0.6433 - val_acc: 0.7343\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.64334 to 0.64150, saving model to best.model\n",
      "0s - loss: 0.6798 - acc: 0.7237 - val_loss: 0.6415 - val_acc: 0.7355\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6740 - acc: 0.7249 - val_loss: 0.6416 - val_acc: 0.7352\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.64150 to 0.64050, saving model to best.model\n",
      "0s - loss: 0.6755 - acc: 0.7230 - val_loss: 0.6405 - val_acc: 0.7363\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64050 to 0.64026, saving model to best.model\n",
      "0s - loss: 0.6738 - acc: 0.7260 - val_loss: 0.6403 - val_acc: 0.7359\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6767 - acc: 0.7239 - val_loss: 0.6414 - val_acc: 0.7354\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6738 - acc: 0.7212 - val_loss: 0.6407 - val_acc: 0.7338\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.64026 to 0.63884, saving model to best.model\n",
      "1s - loss: 0.6769 - acc: 0.7224 - val_loss: 0.6388 - val_acc: 0.7368\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6716 - acc: 0.7261 - val_loss: 0.6398 - val_acc: 0.7356\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.6738 - acc: 0.7268 - val_loss: 0.6408 - val_acc: 0.7328\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.63884 to 0.63772, saving model to best.model\n",
      "1s - loss: 0.6736 - acc: 0.7235 - val_loss: 0.6377 - val_acc: 0.7379\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63772 to 0.63269, saving model to best.model\n",
      "0s - loss: 0.6677 - acc: 0.7273 - val_loss: 0.6327 - val_acc: 0.7385\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6713 - acc: 0.7242 - val_loss: 0.6352 - val_acc: 0.7363\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63269 to 0.63242, saving model to best.model\n",
      "0s - loss: 0.6689 - acc: 0.7285 - val_loss: 0.6324 - val_acc: 0.7400\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6687 - acc: 0.7263 - val_loss: 0.6333 - val_acc: 0.7399\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6666 - acc: 0.7285 - val_loss: 0.6348 - val_acc: 0.7375\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.63242 to 0.63154, saving model to best.model\n",
      "1s - loss: 0.6704 - acc: 0.7245 - val_loss: 0.6315 - val_acc: 0.7404\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63154 to 0.63145, saving model to best.model\n",
      "1s - loss: 0.6664 - acc: 0.7289 - val_loss: 0.6314 - val_acc: 0.7390\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6672 - acc: 0.7280 - val_loss: 0.6332 - val_acc: 0.7367\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.63145 to 0.63102, saving model to best.model\n",
      "1s - loss: 0.6649 - acc: 0.7325 - val_loss: 0.6310 - val_acc: 0.7413\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6670 - acc: 0.7282 - val_loss: 0.6311 - val_acc: 0.7386\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.63102 to 0.62990, saving model to best.model\n",
      "1s - loss: 0.6655 - acc: 0.7288 - val_loss: 0.6299 - val_acc: 0.7397\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62990 to 0.62718, saving model to best.model\n",
      "1s - loss: 0.6656 - acc: 0.7296 - val_loss: 0.6272 - val_acc: 0.7425\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62718 to 0.62688, saving model to best.model\n",
      "1s - loss: 0.6638 - acc: 0.7291 - val_loss: 0.6269 - val_acc: 0.7398\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6607 - acc: 0.7294 - val_loss: 0.6292 - val_acc: 0.7377\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6610 - acc: 0.7292 - val_loss: 0.6276 - val_acc: 0.7410\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7319 - val_loss: 0.6282 - val_acc: 0.7382\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62688 to 0.62524, saving model to best.model\n",
      "1s - loss: 0.6627 - acc: 0.7294 - val_loss: 0.6252 - val_acc: 0.7430\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6586 - acc: 0.7298 - val_loss: 0.6262 - val_acc: 0.7400\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62524 to 0.62360, saving model to best.model\n",
      "1s - loss: 0.6597 - acc: 0.7333 - val_loss: 0.6236 - val_acc: 0.7410\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6615 - acc: 0.7305 - val_loss: 0.6258 - val_acc: 0.7400\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6606 - acc: 0.7313 - val_loss: 0.6262 - val_acc: 0.7407\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.62360 to 0.62268, saving model to best.model\n",
      "1s - loss: 0.6593 - acc: 0.7321 - val_loss: 0.6227 - val_acc: 0.7433\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.62268 to 0.62109, saving model to best.model\n",
      "1s - loss: 0.6585 - acc: 0.7291 - val_loss: 0.6211 - val_acc: 0.7438\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.62109 to 0.62085, saving model to best.model\n",
      "1s - loss: 0.6558 - acc: 0.7339 - val_loss: 0.6209 - val_acc: 0.7438\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6576 - acc: 0.7312 - val_loss: 0.6223 - val_acc: 0.7431\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6603 - acc: 0.7311 - val_loss: 0.6213 - val_acc: 0.7437\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.62085 to 0.61811, saving model to best.model\n",
      "1s - loss: 0.6567 - acc: 0.7328 - val_loss: 0.6181 - val_acc: 0.7444\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6532 - acc: 0.7340 - val_loss: 0.6185 - val_acc: 0.7450\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6561 - acc: 0.7331 - val_loss: 0.6195 - val_acc: 0.7437\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6556 - acc: 0.7339 - val_loss: 0.6222 - val_acc: 0.7415\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6542 - acc: 0.7343 - val_loss: 0.6196 - val_acc: 0.7433\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.61811 to 0.61742, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7330 - val_loss: 0.6174 - val_acc: 0.7466\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61742 to 0.61627, saving model to best.model\n",
      "1s - loss: 0.6549 - acc: 0.7348 - val_loss: 0.6163 - val_acc: 0.7452\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6529 - acc: 0.7366 - val_loss: 0.6193 - val_acc: 0.7429\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6544 - acc: 0.7355 - val_loss: 0.6172 - val_acc: 0.7448\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61627 to 0.61541, saving model to best.model\n",
      "0s - loss: 0.6547 - acc: 0.7340 - val_loss: 0.6154 - val_acc: 0.7445\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6541 - acc: 0.7338 - val_loss: 0.6183 - val_acc: 0.7441\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61541 to 0.61485, saving model to best.model\n",
      "0s - loss: 0.6546 - acc: 0.7329 - val_loss: 0.6148 - val_acc: 0.7467\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6529 - acc: 0.7311 - val_loss: 0.6154 - val_acc: 0.7461\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6487 - acc: 0.7341 - val_loss: 0.6151 - val_acc: 0.7447\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6514 - acc: 0.7348 - val_loss: 0.6155 - val_acc: 0.7450\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.61485 to 0.61449, saving model to best.model\n",
      "1s - loss: 0.6523 - acc: 0.7352 - val_loss: 0.6145 - val_acc: 0.7466\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61449 to 0.61069, saving model to best.model\n",
      "1s - loss: 0.6502 - acc: 0.7354 - val_loss: 0.6107 - val_acc: 0.7474\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6471 - acc: 0.7374 - val_loss: 0.6171 - val_acc: 0.7424\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.6498 - acc: 0.7348 - val_loss: 0.6110 - val_acc: 0.7472\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6480 - acc: 0.7357 - val_loss: 0.6111 - val_acc: 0.7479\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6508 - acc: 0.7359 - val_loss: 0.6132 - val_acc: 0.7467\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.61069 to 0.60957, saving model to best.model\n",
      "1s - loss: 0.6454 - acc: 0.7357 - val_loss: 0.6096 - val_acc: 0.7474\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60957 to 0.60781, saving model to best.model\n",
      "1s - loss: 0.6515 - acc: 0.7354 - val_loss: 0.6078 - val_acc: 0.7511\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6459 - acc: 0.7366 - val_loss: 0.6094 - val_acc: 0.7489\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6478 - acc: 0.7362 - val_loss: 0.6098 - val_acc: 0.7485\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6439 - acc: 0.7387 - val_loss: 0.6097 - val_acc: 0.7484\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60781 to 0.60657, saving model to best.model\n",
      "1s - loss: 0.6451 - acc: 0.7385 - val_loss: 0.6066 - val_acc: 0.7501\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6449 - acc: 0.7353 - val_loss: 0.6085 - val_acc: 0.7486\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60657 to 0.60335, saving model to best.model\n",
      "0s - loss: 0.6423 - acc: 0.7387 - val_loss: 0.6034 - val_acc: 0.7532\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.6440 - acc: 0.7362 - val_loss: 0.6064 - val_acc: 0.7492\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6432 - acc: 0.7398 - val_loss: 0.6067 - val_acc: 0.7501\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6441 - acc: 0.7370 - val_loss: 0.6052 - val_acc: 0.7505\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84379, saving model to best.model\n",
      "1s - loss: 0.9280 - acc: 0.6245 - val_loss: 0.8438 - val_acc: 0.6593\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.8606 - acc: 0.6611 - val_loss: 0.8441 - val_acc: 0.6593\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84379 to 0.84264, saving model to best.model\n",
      "0s - loss: 0.8512 - acc: 0.6615 - val_loss: 0.8426 - val_acc: 0.6593\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84264 to 0.84209, saving model to best.model\n",
      "0s - loss: 0.8494 - acc: 0.6615 - val_loss: 0.8421 - val_acc: 0.6593\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84209 to 0.83474, saving model to best.model\n",
      "0s - loss: 0.8448 - acc: 0.6615 - val_loss: 0.8347 - val_acc: 0.6593\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83474 to 0.82983, saving model to best.model\n",
      "0s - loss: 0.8403 - acc: 0.6615 - val_loss: 0.8298 - val_acc: 0.6593\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82983 to 0.82885, saving model to best.model\n",
      "0s - loss: 0.8344 - acc: 0.6615 - val_loss: 0.8289 - val_acc: 0.6593\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82885 to 0.82357, saving model to best.model\n",
      "0s - loss: 0.8301 - acc: 0.6616 - val_loss: 0.8236 - val_acc: 0.6593\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82357 to 0.82269, saving model to best.model\n",
      "0s - loss: 0.8281 - acc: 0.6616 - val_loss: 0.8227 - val_acc: 0.6593\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82269 to 0.82204, saving model to best.model\n",
      "0s - loss: 0.8271 - acc: 0.6617 - val_loss: 0.8220 - val_acc: 0.6593\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82204 to 0.82160, saving model to best.model\n",
      "0s - loss: 0.8257 - acc: 0.6616 - val_loss: 0.8216 - val_acc: 0.6593\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.8249 - acc: 0.6613 - val_loss: 0.8229 - val_acc: 0.6593\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82160 to 0.82107, saving model to best.model\n",
      "1s - loss: 0.8252 - acc: 0.6616 - val_loss: 0.8211 - val_acc: 0.6593\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82107 to 0.82059, saving model to best.model\n",
      "0s - loss: 0.8246 - acc: 0.6614 - val_loss: 0.8206 - val_acc: 0.6593\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8227 - acc: 0.6615 - val_loss: 0.8207 - val_acc: 0.6593\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.8226 - acc: 0.6616 - val_loss: 0.8207 - val_acc: 0.6593\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82059 to 0.81953, saving model to best.model\n",
      "0s - loss: 0.8209 - acc: 0.6611 - val_loss: 0.8195 - val_acc: 0.6593\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81953 to 0.81912, saving model to best.model\n",
      "0s - loss: 0.8217 - acc: 0.6617 - val_loss: 0.8191 - val_acc: 0.6593\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81912 to 0.81848, saving model to best.model\n",
      "0s - loss: 0.8202 - acc: 0.6620 - val_loss: 0.8185 - val_acc: 0.6593\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81848 to 0.81736, saving model to best.model\n",
      "1s - loss: 0.8179 - acc: 0.6611 - val_loss: 0.8174 - val_acc: 0.6593\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.8195 - acc: 0.6611 - val_loss: 0.8185 - val_acc: 0.6593\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81736 to 0.81696, saving model to best.model\n",
      "0s - loss: 0.8163 - acc: 0.6625 - val_loss: 0.8170 - val_acc: 0.6593\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81696 to 0.81425, saving model to best.model\n",
      "0s - loss: 0.8159 - acc: 0.6620 - val_loss: 0.8143 - val_acc: 0.6609\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81425 to 0.81371, saving model to best.model\n",
      "0s - loss: 0.8161 - acc: 0.6638 - val_loss: 0.8137 - val_acc: 0.6607\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81371 to 0.81190, saving model to best.model\n",
      "1s - loss: 0.8150 - acc: 0.6638 - val_loss: 0.8119 - val_acc: 0.6644\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.8130 - acc: 0.6654 - val_loss: 0.8120 - val_acc: 0.6626\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81190 to 0.81012, saving model to best.model\n",
      "0s - loss: 0.8133 - acc: 0.6646 - val_loss: 0.8101 - val_acc: 0.6636\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81012 to 0.80960, saving model to best.model\n",
      "0s - loss: 0.8129 - acc: 0.6647 - val_loss: 0.8096 - val_acc: 0.6629\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80960 to 0.80838, saving model to best.model\n",
      "1s - loss: 0.8104 - acc: 0.6669 - val_loss: 0.8084 - val_acc: 0.6637\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80838 to 0.80684, saving model to best.model\n",
      "1s - loss: 0.8080 - acc: 0.6671 - val_loss: 0.8068 - val_acc: 0.6649\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80684 to 0.80602, saving model to best.model\n",
      "1s - loss: 0.8079 - acc: 0.6664 - val_loss: 0.8060 - val_acc: 0.6648\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80602 to 0.80432, saving model to best.model\n",
      "1s - loss: 0.8070 - acc: 0.6689 - val_loss: 0.8043 - val_acc: 0.6660\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80432 to 0.80244, saving model to best.model\n",
      "1s - loss: 0.8052 - acc: 0.6684 - val_loss: 0.8024 - val_acc: 0.6650\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80244 to 0.80064, saving model to best.model\n",
      "1s - loss: 0.8045 - acc: 0.6686 - val_loss: 0.8006 - val_acc: 0.6655\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80064 to 0.79881, saving model to best.model\n",
      "1s - loss: 0.8036 - acc: 0.6697 - val_loss: 0.7988 - val_acc: 0.6660\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79881 to 0.79664, saving model to best.model\n",
      "1s - loss: 0.8017 - acc: 0.6702 - val_loss: 0.7966 - val_acc: 0.6693\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79664 to 0.79621, saving model to best.model\n",
      "0s - loss: 0.8026 - acc: 0.6690 - val_loss: 0.7962 - val_acc: 0.6660\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79621 to 0.79159, saving model to best.model\n",
      "0s - loss: 0.7993 - acc: 0.6694 - val_loss: 0.7916 - val_acc: 0.6701\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79159 to 0.79019, saving model to best.model\n",
      "1s - loss: 0.7970 - acc: 0.6712 - val_loss: 0.7902 - val_acc: 0.6698\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79019 to 0.78939, saving model to best.model\n",
      "1s - loss: 0.7973 - acc: 0.6717 - val_loss: 0.7894 - val_acc: 0.6711\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78939 to 0.78704, saving model to best.model\n",
      "0s - loss: 0.7956 - acc: 0.6710 - val_loss: 0.7870 - val_acc: 0.6734\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78704 to 0.78619, saving model to best.model\n",
      "1s - loss: 0.7948 - acc: 0.6730 - val_loss: 0.7862 - val_acc: 0.6711\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78619 to 0.78324, saving model to best.model\n",
      "1s - loss: 0.7945 - acc: 0.6718 - val_loss: 0.7832 - val_acc: 0.6758\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78324 to 0.78299, saving model to best.model\n",
      "0s - loss: 0.7914 - acc: 0.6729 - val_loss: 0.7830 - val_acc: 0.6706\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78299 to 0.78146, saving model to best.model\n",
      "0s - loss: 0.7907 - acc: 0.6739 - val_loss: 0.7815 - val_acc: 0.6741\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78146 to 0.78070, saving model to best.model\n",
      "1s - loss: 0.7891 - acc: 0.6744 - val_loss: 0.7807 - val_acc: 0.6705\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.78070 to 0.77728, saving model to best.model\n",
      "1s - loss: 0.7885 - acc: 0.6742 - val_loss: 0.7773 - val_acc: 0.6735\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77728 to 0.77484, saving model to best.model\n",
      "1s - loss: 0.7881 - acc: 0.6738 - val_loss: 0.7748 - val_acc: 0.6783\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77484 to 0.77339, saving model to best.model\n",
      "1s - loss: 0.7857 - acc: 0.6752 - val_loss: 0.7734 - val_acc: 0.6779\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77339 to 0.77183, saving model to best.model\n",
      "1s - loss: 0.7840 - acc: 0.6767 - val_loss: 0.7718 - val_acc: 0.6807\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77183 to 0.76785, saving model to best.model\n",
      "1s - loss: 0.7815 - acc: 0.6778 - val_loss: 0.7678 - val_acc: 0.6796\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76785 to 0.76579, saving model to best.model\n",
      "1s - loss: 0.7820 - acc: 0.6761 - val_loss: 0.7658 - val_acc: 0.6804\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76579 to 0.76438, saving model to best.model\n",
      "1s - loss: 0.7803 - acc: 0.6794 - val_loss: 0.7644 - val_acc: 0.6799\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76438 to 0.76118, saving model to best.model\n",
      "1s - loss: 0.7774 - acc: 0.6781 - val_loss: 0.7612 - val_acc: 0.6845\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76118 to 0.76029, saving model to best.model\n",
      "1s - loss: 0.7742 - acc: 0.6793 - val_loss: 0.7603 - val_acc: 0.6837\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76029 to 0.75697, saving model to best.model\n",
      "1s - loss: 0.7742 - acc: 0.6803 - val_loss: 0.7570 - val_acc: 0.6843\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75697 to 0.75381, saving model to best.model\n",
      "0s - loss: 0.7727 - acc: 0.6797 - val_loss: 0.7538 - val_acc: 0.6889\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75381 to 0.75077, saving model to best.model\n",
      "1s - loss: 0.7692 - acc: 0.6835 - val_loss: 0.7508 - val_acc: 0.6868\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75077 to 0.74924, saving model to best.model\n",
      "1s - loss: 0.7708 - acc: 0.6806 - val_loss: 0.7492 - val_acc: 0.6882\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74924 to 0.74689, saving model to best.model\n",
      "0s - loss: 0.7655 - acc: 0.6830 - val_loss: 0.7469 - val_acc: 0.6906\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74689 to 0.74624, saving model to best.model\n",
      "0s - loss: 0.7649 - acc: 0.6848 - val_loss: 0.7462 - val_acc: 0.6879\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74624 to 0.73955, saving model to best.model\n",
      "0s - loss: 0.7642 - acc: 0.6837 - val_loss: 0.7396 - val_acc: 0.6918\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.7637 - acc: 0.6841 - val_loss: 0.7401 - val_acc: 0.6902\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73955 to 0.73553, saving model to best.model\n",
      "0s - loss: 0.7610 - acc: 0.6852 - val_loss: 0.7355 - val_acc: 0.6957\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73553 to 0.73546, saving model to best.model\n",
      "0s - loss: 0.7580 - acc: 0.6846 - val_loss: 0.7355 - val_acc: 0.6951\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73546 to 0.72957, saving model to best.model\n",
      "0s - loss: 0.7561 - acc: 0.6874 - val_loss: 0.7296 - val_acc: 0.6973\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72957 to 0.72878, saving model to best.model\n",
      "0s - loss: 0.7570 - acc: 0.6879 - val_loss: 0.7288 - val_acc: 0.6970\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72878 to 0.72721, saving model to best.model\n",
      "0s - loss: 0.7506 - acc: 0.6922 - val_loss: 0.7272 - val_acc: 0.6965\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72721 to 0.72427, saving model to best.model\n",
      "0s - loss: 0.7516 - acc: 0.6886 - val_loss: 0.7243 - val_acc: 0.6979\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72427 to 0.72230, saving model to best.model\n",
      "0s - loss: 0.7530 - acc: 0.6890 - val_loss: 0.7223 - val_acc: 0.7059\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72230 to 0.71998, saving model to best.model\n",
      "0s - loss: 0.7500 - acc: 0.6914 - val_loss: 0.7200 - val_acc: 0.7004\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71998 to 0.71587, saving model to best.model\n",
      "0s - loss: 0.7451 - acc: 0.6914 - val_loss: 0.7159 - val_acc: 0.7026\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71587 to 0.71338, saving model to best.model\n",
      "0s - loss: 0.7416 - acc: 0.6933 - val_loss: 0.7134 - val_acc: 0.7049\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71338 to 0.71327, saving model to best.model\n",
      "0s - loss: 0.7432 - acc: 0.6907 - val_loss: 0.7133 - val_acc: 0.7021\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71327 to 0.70968, saving model to best.model\n",
      "0s - loss: 0.7411 - acc: 0.6940 - val_loss: 0.7097 - val_acc: 0.7063\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.7370 - acc: 0.6964 - val_loss: 0.7120 - val_acc: 0.7056\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70968 to 0.70670, saving model to best.model\n",
      "0s - loss: 0.7407 - acc: 0.6969 - val_loss: 0.7067 - val_acc: 0.7077\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70670 to 0.70299, saving model to best.model\n",
      "0s - loss: 0.7409 - acc: 0.6952 - val_loss: 0.7030 - val_acc: 0.7098\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.7382 - acc: 0.6955 - val_loss: 0.7035 - val_acc: 0.7075\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70299 to 0.69916, saving model to best.model\n",
      "0s - loss: 0.7353 - acc: 0.6939 - val_loss: 0.6992 - val_acc: 0.7158\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69916 to 0.69813, saving model to best.model\n",
      "0s - loss: 0.7323 - acc: 0.6977 - val_loss: 0.6981 - val_acc: 0.7105\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69813 to 0.69598, saving model to best.model\n",
      "0s - loss: 0.7291 - acc: 0.7002 - val_loss: 0.6960 - val_acc: 0.7124\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69598 to 0.69109, saving model to best.model\n",
      "0s - loss: 0.7308 - acc: 0.6972 - val_loss: 0.6911 - val_acc: 0.7157\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.7282 - acc: 0.7008 - val_loss: 0.6939 - val_acc: 0.7138\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7311 - acc: 0.6976 - val_loss: 0.6926 - val_acc: 0.7178\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69109 to 0.68975, saving model to best.model\n",
      "0s - loss: 0.7254 - acc: 0.7022 - val_loss: 0.6897 - val_acc: 0.7144\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7254 - acc: 0.7010 - val_loss: 0.6905 - val_acc: 0.7157\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68975 to 0.68624, saving model to best.model\n",
      "0s - loss: 0.7239 - acc: 0.7007 - val_loss: 0.6862 - val_acc: 0.7173\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68624 to 0.68606, saving model to best.model\n",
      "0s - loss: 0.7228 - acc: 0.7028 - val_loss: 0.6861 - val_acc: 0.7192\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68606 to 0.68446, saving model to best.model\n",
      "0s - loss: 0.7230 - acc: 0.7035 - val_loss: 0.6845 - val_acc: 0.7197\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68446 to 0.68132, saving model to best.model\n",
      "1s - loss: 0.7223 - acc: 0.7030 - val_loss: 0.6813 - val_acc: 0.7189\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68132 to 0.67924, saving model to best.model\n",
      "1s - loss: 0.7165 - acc: 0.7040 - val_loss: 0.6792 - val_acc: 0.7201\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.7170 - acc: 0.7052 - val_loss: 0.6805 - val_acc: 0.7199\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.7202 - acc: 0.7068 - val_loss: 0.6808 - val_acc: 0.7180\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67924 to 0.67619, saving model to best.model\n",
      "0s - loss: 0.7123 - acc: 0.7071 - val_loss: 0.6762 - val_acc: 0.7208\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7134 - acc: 0.7083 - val_loss: 0.6771 - val_acc: 0.7171\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.67619 to 0.67120, saving model to best.model\n",
      "0s - loss: 0.7143 - acc: 0.7058 - val_loss: 0.6712 - val_acc: 0.7238\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.7122 - acc: 0.7066 - val_loss: 0.6724 - val_acc: 0.7220\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67120 to 0.67103, saving model to best.model\n",
      "0s - loss: 0.7106 - acc: 0.7079 - val_loss: 0.6710 - val_acc: 0.7233\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7120 - acc: 0.7056 - val_loss: 0.6740 - val_acc: 0.7251\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7115 - acc: 0.7073 - val_loss: 0.6726 - val_acc: 0.7207\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67103 to 0.66597, saving model to best.model\n",
      "0s - loss: 0.7079 - acc: 0.7102 - val_loss: 0.6660 - val_acc: 0.7276\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7082 - acc: 0.7101 - val_loss: 0.6698 - val_acc: 0.7217\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.7083 - acc: 0.7112 - val_loss: 0.6683 - val_acc: 0.7258\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66597 to 0.66422, saving model to best.model\n",
      "0s - loss: 0.7089 - acc: 0.7091 - val_loss: 0.6642 - val_acc: 0.7232\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7054 - acc: 0.7121 - val_loss: 0.6651 - val_acc: 0.7242\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66422 to 0.66402, saving model to best.model\n",
      "0s - loss: 0.7037 - acc: 0.7118 - val_loss: 0.6640 - val_acc: 0.7241\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.7032 - acc: 0.7123 - val_loss: 0.6645 - val_acc: 0.7213\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66402 to 0.66254, saving model to best.model\n",
      "0s - loss: 0.7061 - acc: 0.7113 - val_loss: 0.6625 - val_acc: 0.7247\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66254 to 0.65664, saving model to best.model\n",
      "0s - loss: 0.6980 - acc: 0.7144 - val_loss: 0.6566 - val_acc: 0.7290\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.7014 - acc: 0.7127 - val_loss: 0.6617 - val_acc: 0.7238\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.6999 - acc: 0.7135 - val_loss: 0.6584 - val_acc: 0.7274\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65664 to 0.65607, saving model to best.model\n",
      "0s - loss: 0.6982 - acc: 0.7143 - val_loss: 0.6561 - val_acc: 0.7301\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.7003 - acc: 0.7139 - val_loss: 0.6566 - val_acc: 0.7296\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6963 - acc: 0.7149 - val_loss: 0.6628 - val_acc: 0.7211\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65607 to 0.65445, saving model to best.model\n",
      "0s - loss: 0.6925 - acc: 0.7170 - val_loss: 0.6545 - val_acc: 0.7296\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.65445 to 0.65227, saving model to best.model\n",
      "0s - loss: 0.6939 - acc: 0.7161 - val_loss: 0.6523 - val_acc: 0.7307\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.6942 - acc: 0.7169 - val_loss: 0.6528 - val_acc: 0.7295\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6963 - acc: 0.7152 - val_loss: 0.6559 - val_acc: 0.7276\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65227 to 0.65003, saving model to best.model\n",
      "0s - loss: 0.6940 - acc: 0.7145 - val_loss: 0.6500 - val_acc: 0.7321\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6920 - acc: 0.7180 - val_loss: 0.6531 - val_acc: 0.7287\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6920 - acc: 0.7157 - val_loss: 0.6516 - val_acc: 0.7303\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6889 - acc: 0.7179 - val_loss: 0.6514 - val_acc: 0.7300\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.65003 to 0.64865, saving model to best.model\n",
      "0s - loss: 0.6886 - acc: 0.7176 - val_loss: 0.6486 - val_acc: 0.7309\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64865 to 0.64712, saving model to best.model\n",
      "0s - loss: 0.6889 - acc: 0.7202 - val_loss: 0.6471 - val_acc: 0.7315\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.64712 to 0.64537, saving model to best.model\n",
      "0s - loss: 0.6895 - acc: 0.7185 - val_loss: 0.6454 - val_acc: 0.7354\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.64537 to 0.64530, saving model to best.model\n",
      "0s - loss: 0.6873 - acc: 0.7187 - val_loss: 0.6453 - val_acc: 0.7356\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64530 to 0.64345, saving model to best.model\n",
      "0s - loss: 0.6867 - acc: 0.7196 - val_loss: 0.6435 - val_acc: 0.7349\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6873 - acc: 0.7185 - val_loss: 0.6476 - val_acc: 0.7309\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64345 to 0.64337, saving model to best.model\n",
      "0s - loss: 0.6857 - acc: 0.7196 - val_loss: 0.6434 - val_acc: 0.7355\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6879 - acc: 0.7197 - val_loss: 0.6476 - val_acc: 0.7307\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6823 - acc: 0.7208 - val_loss: 0.6461 - val_acc: 0.7302\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6837 - acc: 0.7210 - val_loss: 0.6436 - val_acc: 0.7311\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.64337 to 0.64301, saving model to best.model\n",
      "0s - loss: 0.6847 - acc: 0.7229 - val_loss: 0.6430 - val_acc: 0.7359\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.64301 to 0.64174, saving model to best.model\n",
      "0s - loss: 0.6836 - acc: 0.7180 - val_loss: 0.6417 - val_acc: 0.7365\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64174 to 0.64045, saving model to best.model\n",
      "1s - loss: 0.6778 - acc: 0.7220 - val_loss: 0.6404 - val_acc: 0.7378\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.64045 to 0.63916, saving model to best.model\n",
      "1s - loss: 0.6805 - acc: 0.7203 - val_loss: 0.6392 - val_acc: 0.7350\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6832 - acc: 0.7186 - val_loss: 0.6417 - val_acc: 0.7344\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6814 - acc: 0.7224 - val_loss: 0.6399 - val_acc: 0.7351\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63916 to 0.63505, saving model to best.model\n",
      "1s - loss: 0.6768 - acc: 0.7235 - val_loss: 0.6351 - val_acc: 0.7430\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6775 - acc: 0.7240 - val_loss: 0.6352 - val_acc: 0.7383\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6766 - acc: 0.7252 - val_loss: 0.6378 - val_acc: 0.7376\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63505 to 0.63399, saving model to best.model\n",
      "1s - loss: 0.6762 - acc: 0.7234 - val_loss: 0.6340 - val_acc: 0.7412\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6762 - acc: 0.7220 - val_loss: 0.6373 - val_acc: 0.7367\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63399 to 0.63339, saving model to best.model\n",
      "1s - loss: 0.6773 - acc: 0.7224 - val_loss: 0.6334 - val_acc: 0.7429\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6765 - acc: 0.7257 - val_loss: 0.6375 - val_acc: 0.7347\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6770 - acc: 0.7229 - val_loss: 0.6348 - val_acc: 0.7420\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.63339 to 0.63292, saving model to best.model\n",
      "1s - loss: 0.6754 - acc: 0.7241 - val_loss: 0.6329 - val_acc: 0.7405\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63292 to 0.63212, saving model to best.model\n",
      "1s - loss: 0.6736 - acc: 0.7262 - val_loss: 0.6321 - val_acc: 0.7425\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.63212 to 0.63022, saving model to best.model\n",
      "1s - loss: 0.6715 - acc: 0.7267 - val_loss: 0.6302 - val_acc: 0.7417\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6716 - acc: 0.7268 - val_loss: 0.6338 - val_acc: 0.7354\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6719 - acc: 0.7239 - val_loss: 0.6305 - val_acc: 0.7412\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.63022 to 0.62979, saving model to best.model\n",
      "1s - loss: 0.6686 - acc: 0.7266 - val_loss: 0.6298 - val_acc: 0.7413\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.6755 - acc: 0.7252 - val_loss: 0.6335 - val_acc: 0.7399\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6725 - acc: 0.7251 - val_loss: 0.6304 - val_acc: 0.7385\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62979 to 0.62959, saving model to best.model\n",
      "1s - loss: 0.6680 - acc: 0.7262 - val_loss: 0.6296 - val_acc: 0.7381\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6690 - acc: 0.7256 - val_loss: 0.6308 - val_acc: 0.7379\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.62959 to 0.62754, saving model to best.model\n",
      "1s - loss: 0.6702 - acc: 0.7257 - val_loss: 0.6275 - val_acc: 0.7454\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62754 to 0.62530, saving model to best.model\n",
      "1s - loss: 0.6671 - acc: 0.7270 - val_loss: 0.6253 - val_acc: 0.7464\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6683 - acc: 0.7251 - val_loss: 0.6295 - val_acc: 0.7357\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6684 - acc: 0.7260 - val_loss: 0.6294 - val_acc: 0.7410\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6657 - acc: 0.7286 - val_loss: 0.6255 - val_acc: 0.7427\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62530 to 0.62402, saving model to best.model\n",
      "1s - loss: 0.6675 - acc: 0.7274 - val_loss: 0.6240 - val_acc: 0.7424\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6681 - acc: 0.7281 - val_loss: 0.6249 - val_acc: 0.7458\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6647 - acc: 0.7282 - val_loss: 0.6264 - val_acc: 0.7409\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6673 - acc: 0.7282 - val_loss: 0.6268 - val_acc: 0.7386\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6661 - acc: 0.7292 - val_loss: 0.6256 - val_acc: 0.7453\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.62402 to 0.61915, saving model to best.model\n",
      "1s - loss: 0.6636 - acc: 0.7295 - val_loss: 0.6192 - val_acc: 0.7459\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6659 - acc: 0.7281 - val_loss: 0.6216 - val_acc: 0.7463\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6652 - acc: 0.7301 - val_loss: 0.6208 - val_acc: 0.7465\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61915 to 0.61833, saving model to best.model\n",
      "1s - loss: 0.6630 - acc: 0.7305 - val_loss: 0.6183 - val_acc: 0.7465\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6611 - acc: 0.7312 - val_loss: 0.6242 - val_acc: 0.7375\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6638 - acc: 0.7287 - val_loss: 0.6221 - val_acc: 0.7444\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6643 - acc: 0.7261 - val_loss: 0.6229 - val_acc: 0.7417\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6613 - acc: 0.7295 - val_loss: 0.6195 - val_acc: 0.7454\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6613 - acc: 0.7299 - val_loss: 0.6193 - val_acc: 0.7464\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6595 - acc: 0.7304 - val_loss: 0.6184 - val_acc: 0.7453\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61833 to 0.61700, saving model to best.model\n",
      "0s - loss: 0.6603 - acc: 0.7296 - val_loss: 0.6170 - val_acc: 0.7457\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61700 to 0.61659, saving model to best.model\n",
      "0s - loss: 0.6583 - acc: 0.7335 - val_loss: 0.6166 - val_acc: 0.7466\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61659 to 0.61646, saving model to best.model\n",
      "1s - loss: 0.6590 - acc: 0.7302 - val_loss: 0.6165 - val_acc: 0.7454\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.61646 to 0.61604, saving model to best.model\n",
      "0s - loss: 0.6590 - acc: 0.7306 - val_loss: 0.6160 - val_acc: 0.7456\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.61604 to 0.61405, saving model to best.model\n",
      "0s - loss: 0.6567 - acc: 0.7275 - val_loss: 0.6140 - val_acc: 0.7495\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6574 - acc: 0.7300 - val_loss: 0.6161 - val_acc: 0.7486\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6557 - acc: 0.7327 - val_loss: 0.6141 - val_acc: 0.7489\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6591 - acc: 0.7304 - val_loss: 0.6148 - val_acc: 0.7484\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.61405 to 0.61389, saving model to best.model\n",
      "0s - loss: 0.6540 - acc: 0.7334 - val_loss: 0.6139 - val_acc: 0.7480\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.61389 to 0.61232, saving model to best.model\n",
      "0s - loss: 0.6572 - acc: 0.7293 - val_loss: 0.6123 - val_acc: 0.7486\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.61232 to 0.61225, saving model to best.model\n",
      "0s - loss: 0.6566 - acc: 0.7338 - val_loss: 0.6123 - val_acc: 0.7504\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.61225 to 0.61164, saving model to best.model\n",
      "0s - loss: 0.6570 - acc: 0.7327 - val_loss: 0.6116 - val_acc: 0.7505\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6556 - acc: 0.7330 - val_loss: 0.6149 - val_acc: 0.7500\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6547 - acc: 0.7331 - val_loss: 0.6147 - val_acc: 0.7467\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6568 - acc: 0.7298 - val_loss: 0.6132 - val_acc: 0.7468\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.61164 to 0.60977, saving model to best.model\n",
      "0s - loss: 0.6549 - acc: 0.7325 - val_loss: 0.6098 - val_acc: 0.7500\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6517 - acc: 0.7342 - val_loss: 0.6109 - val_acc: 0.7475\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60977 to 0.60920, saving model to best.model\n",
      "0s - loss: 0.6528 - acc: 0.7358 - val_loss: 0.6092 - val_acc: 0.7506\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6521 - acc: 0.7340 - val_loss: 0.6096 - val_acc: 0.7502\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6540 - acc: 0.7327 - val_loss: 0.6103 - val_acc: 0.7487\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60920 to 0.60743, saving model to best.model\n",
      "0s - loss: 0.6481 - acc: 0.7356 - val_loss: 0.6074 - val_acc: 0.7519\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.6506 - acc: 0.7361 - val_loss: 0.6086 - val_acc: 0.7495\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6496 - acc: 0.7344 - val_loss: 0.6109 - val_acc: 0.7477\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84577, saving model to best.model\n",
      "0s - loss: 0.9198 - acc: 0.6284 - val_loss: 0.8458 - val_acc: 0.6556\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.8638 - acc: 0.6567 - val_loss: 0.8466 - val_acc: 0.6556\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84577 to 0.84524, saving model to best.model\n",
      "0s - loss: 0.8554 - acc: 0.6571 - val_loss: 0.8452 - val_acc: 0.6556\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84524 to 0.84377, saving model to best.model\n",
      "0s - loss: 0.8531 - acc: 0.6571 - val_loss: 0.8438 - val_acc: 0.6556\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84377 to 0.83698, saving model to best.model\n",
      "0s - loss: 0.8491 - acc: 0.6571 - val_loss: 0.8370 - val_acc: 0.6556\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83698 to 0.83475, saving model to best.model\n",
      "0s - loss: 0.8433 - acc: 0.6571 - val_loss: 0.8347 - val_acc: 0.6556\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83475 to 0.82722, saving model to best.model\n",
      "0s - loss: 0.8389 - acc: 0.6571 - val_loss: 0.8272 - val_acc: 0.6556\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82722 to 0.82492, saving model to best.model\n",
      "0s - loss: 0.8343 - acc: 0.6571 - val_loss: 0.8249 - val_acc: 0.6556\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82492 to 0.82446, saving model to best.model\n",
      "0s - loss: 0.8351 - acc: 0.6571 - val_loss: 0.8245 - val_acc: 0.6556\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82446 to 0.82359, saving model to best.model\n",
      "0s - loss: 0.8329 - acc: 0.6571 - val_loss: 0.8236 - val_acc: 0.6556\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82359 to 0.82207, saving model to best.model\n",
      "0s - loss: 0.8310 - acc: 0.6571 - val_loss: 0.8221 - val_acc: 0.6556\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.8304 - acc: 0.6572 - val_loss: 0.8222 - val_acc: 0.6556\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82207 to 0.82185, saving model to best.model\n",
      "0s - loss: 0.8290 - acc: 0.6571 - val_loss: 0.8219 - val_acc: 0.6556\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82185 to 0.82156, saving model to best.model\n",
      "0s - loss: 0.8290 - acc: 0.6572 - val_loss: 0.8216 - val_acc: 0.6556\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82156 to 0.82043, saving model to best.model\n",
      "0s - loss: 0.8283 - acc: 0.6570 - val_loss: 0.8204 - val_acc: 0.6556\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82043 to 0.81957, saving model to best.model\n",
      "0s - loss: 0.8266 - acc: 0.6575 - val_loss: 0.8196 - val_acc: 0.6556\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81957 to 0.81911, saving model to best.model\n",
      "0s - loss: 0.8269 - acc: 0.6570 - val_loss: 0.8191 - val_acc: 0.6556\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.8258 - acc: 0.6576 - val_loss: 0.8192 - val_acc: 0.6556\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81911 to 0.81839, saving model to best.model\n",
      "0s - loss: 0.8241 - acc: 0.6574 - val_loss: 0.8184 - val_acc: 0.6556\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81839 to 0.81643, saving model to best.model\n",
      "0s - loss: 0.8231 - acc: 0.6570 - val_loss: 0.8164 - val_acc: 0.6556\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81643 to 0.81642, saving model to best.model\n",
      "0s - loss: 0.8235 - acc: 0.6572 - val_loss: 0.8164 - val_acc: 0.6557\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81642 to 0.81576, saving model to best.model\n",
      "0s - loss: 0.8218 - acc: 0.6588 - val_loss: 0.8158 - val_acc: 0.6560\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81576 to 0.81544, saving model to best.model\n",
      "0s - loss: 0.8210 - acc: 0.6588 - val_loss: 0.8154 - val_acc: 0.6562\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81544 to 0.81401, saving model to best.model\n",
      "0s - loss: 0.8208 - acc: 0.6589 - val_loss: 0.8140 - val_acc: 0.6571\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.8197 - acc: 0.6599 - val_loss: 0.8152 - val_acc: 0.6584\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81401 to 0.81218, saving model to best.model\n",
      "0s - loss: 0.8194 - acc: 0.6599 - val_loss: 0.8122 - val_acc: 0.6593\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81218 to 0.81166, saving model to best.model\n",
      "0s - loss: 0.8180 - acc: 0.6614 - val_loss: 0.8117 - val_acc: 0.6587\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81166 to 0.81073, saving model to best.model\n",
      "0s - loss: 0.8180 - acc: 0.6612 - val_loss: 0.8107 - val_acc: 0.6600\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.8158 - acc: 0.6631 - val_loss: 0.8116 - val_acc: 0.6576\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81073 to 0.80908, saving model to best.model\n",
      "0s - loss: 0.8157 - acc: 0.6630 - val_loss: 0.8091 - val_acc: 0.6600\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80908 to 0.80771, saving model to best.model\n",
      "0s - loss: 0.8147 - acc: 0.6632 - val_loss: 0.8077 - val_acc: 0.6611\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80771 to 0.80585, saving model to best.model\n",
      "0s - loss: 0.8134 - acc: 0.6627 - val_loss: 0.8059 - val_acc: 0.6614\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80585 to 0.80485, saving model to best.model\n",
      "0s - loss: 0.8124 - acc: 0.6643 - val_loss: 0.8049 - val_acc: 0.6608\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80485 to 0.80343, saving model to best.model\n",
      "1s - loss: 0.8097 - acc: 0.6638 - val_loss: 0.8034 - val_acc: 0.6610\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80343 to 0.80245, saving model to best.model\n",
      "0s - loss: 0.8093 - acc: 0.6653 - val_loss: 0.8025 - val_acc: 0.6616\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80245 to 0.79858, saving model to best.model\n",
      "0s - loss: 0.8069 - acc: 0.6654 - val_loss: 0.7986 - val_acc: 0.6650\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79858 to 0.79695, saving model to best.model\n",
      "0s - loss: 0.8061 - acc: 0.6658 - val_loss: 0.7969 - val_acc: 0.6657\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79695 to 0.79443, saving model to best.model\n",
      "0s - loss: 0.8032 - acc: 0.6660 - val_loss: 0.7944 - val_acc: 0.6641\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79443 to 0.79289, saving model to best.model\n",
      "0s - loss: 0.8038 - acc: 0.6669 - val_loss: 0.7929 - val_acc: 0.6653\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79289 to 0.79003, saving model to best.model\n",
      "0s - loss: 0.8017 - acc: 0.6677 - val_loss: 0.7900 - val_acc: 0.6642\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79003 to 0.78915, saving model to best.model\n",
      "0s - loss: 0.8002 - acc: 0.6677 - val_loss: 0.7892 - val_acc: 0.6639\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78915 to 0.78663, saving model to best.model\n",
      "0s - loss: 0.7984 - acc: 0.6675 - val_loss: 0.7866 - val_acc: 0.6665\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78663 to 0.78297, saving model to best.model\n",
      "0s - loss: 0.7962 - acc: 0.6697 - val_loss: 0.7830 - val_acc: 0.6669\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78297 to 0.78053, saving model to best.model\n",
      "1s - loss: 0.7956 - acc: 0.6707 - val_loss: 0.7805 - val_acc: 0.6703\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78053 to 0.78007, saving model to best.model\n",
      "1s - loss: 0.7928 - acc: 0.6712 - val_loss: 0.7801 - val_acc: 0.6676\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78007 to 0.77841, saving model to best.model\n",
      "0s - loss: 0.7915 - acc: 0.6716 - val_loss: 0.7784 - val_acc: 0.6674\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77841 to 0.77363, saving model to best.model\n",
      "1s - loss: 0.7900 - acc: 0.6717 - val_loss: 0.7736 - val_acc: 0.6734\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77363 to 0.77137, saving model to best.model\n",
      "0s - loss: 0.7864 - acc: 0.6740 - val_loss: 0.7714 - val_acc: 0.6786\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.7865 - acc: 0.6741 - val_loss: 0.7715 - val_acc: 0.6714\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77137 to 0.76555, saving model to best.model\n",
      "0s - loss: 0.7854 - acc: 0.6751 - val_loss: 0.7655 - val_acc: 0.6802\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76555 to 0.76360, saving model to best.model\n",
      "1s - loss: 0.7834 - acc: 0.6749 - val_loss: 0.7636 - val_acc: 0.6774\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76360 to 0.75935, saving model to best.model\n",
      "1s - loss: 0.7818 - acc: 0.6753 - val_loss: 0.7593 - val_acc: 0.6803\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75935 to 0.75909, saving model to best.model\n",
      "1s - loss: 0.7780 - acc: 0.6773 - val_loss: 0.7591 - val_acc: 0.6796\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75909 to 0.75536, saving model to best.model\n",
      "0s - loss: 0.7773 - acc: 0.6772 - val_loss: 0.7554 - val_acc: 0.6810\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75536 to 0.75273, saving model to best.model\n",
      "0s - loss: 0.7753 - acc: 0.6781 - val_loss: 0.7527 - val_acc: 0.6848\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.7733 - acc: 0.6790 - val_loss: 0.7531 - val_acc: 0.6813\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75273 to 0.74461, saving model to best.model\n",
      "0s - loss: 0.7697 - acc: 0.6813 - val_loss: 0.7446 - val_acc: 0.6882\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74461 to 0.74455, saving model to best.model\n",
      "0s - loss: 0.7708 - acc: 0.6810 - val_loss: 0.7446 - val_acc: 0.6881\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74455 to 0.74232, saving model to best.model\n",
      "0s - loss: 0.7685 - acc: 0.6809 - val_loss: 0.7423 - val_acc: 0.6862\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74232 to 0.73801, saving model to best.model\n",
      "0s - loss: 0.7644 - acc: 0.6825 - val_loss: 0.7380 - val_acc: 0.6882\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73801 to 0.73375, saving model to best.model\n",
      "0s - loss: 0.7611 - acc: 0.6841 - val_loss: 0.7338 - val_acc: 0.6890\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73375 to 0.72628, saving model to best.model\n",
      "1s - loss: 0.7579 - acc: 0.6866 - val_loss: 0.7263 - val_acc: 0.6959\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.7584 - acc: 0.6860 - val_loss: 0.7272 - val_acc: 0.6953\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72628 to 0.72213, saving model to best.model\n",
      "1s - loss: 0.7559 - acc: 0.6885 - val_loss: 0.7221 - val_acc: 0.6999\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72213 to 0.72186, saving model to best.model\n",
      "1s - loss: 0.7536 - acc: 0.6873 - val_loss: 0.7219 - val_acc: 0.7011\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72186 to 0.71536, saving model to best.model\n",
      "1s - loss: 0.7500 - acc: 0.6919 - val_loss: 0.7154 - val_acc: 0.6994\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.71536 to 0.71442, saving model to best.model\n",
      "0s - loss: 0.7462 - acc: 0.6919 - val_loss: 0.7144 - val_acc: 0.7069\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71442 to 0.70905, saving model to best.model\n",
      "0s - loss: 0.7466 - acc: 0.6922 - val_loss: 0.7090 - val_acc: 0.7049\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.70905 to 0.70787, saving model to best.model\n",
      "1s - loss: 0.7413 - acc: 0.6930 - val_loss: 0.7079 - val_acc: 0.7027\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70787 to 0.70476, saving model to best.model\n",
      "1s - loss: 0.7419 - acc: 0.6933 - val_loss: 0.7048 - val_acc: 0.7114\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70476 to 0.70277, saving model to best.model\n",
      "1s - loss: 0.7420 - acc: 0.6930 - val_loss: 0.7028 - val_acc: 0.7115\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70277 to 0.70043, saving model to best.model\n",
      "1s - loss: 0.7380 - acc: 0.6932 - val_loss: 0.7004 - val_acc: 0.7111\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70043 to 0.69937, saving model to best.model\n",
      "1s - loss: 0.7394 - acc: 0.6964 - val_loss: 0.6994 - val_acc: 0.7117\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.7333 - acc: 0.6984 - val_loss: 0.7008 - val_acc: 0.7047\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69937 to 0.69599, saving model to best.model\n",
      "1s - loss: 0.7351 - acc: 0.6980 - val_loss: 0.6960 - val_acc: 0.7110\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69599 to 0.69454, saving model to best.model\n",
      "1s - loss: 0.7336 - acc: 0.6989 - val_loss: 0.6945 - val_acc: 0.7142\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7308 - acc: 0.6990 - val_loss: 0.6967 - val_acc: 0.7077\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69454 to 0.68815, saving model to best.model\n",
      "1s - loss: 0.7306 - acc: 0.6994 - val_loss: 0.6881 - val_acc: 0.7193\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7286 - acc: 0.7014 - val_loss: 0.6883 - val_acc: 0.7196\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.7272 - acc: 0.7037 - val_loss: 0.6927 - val_acc: 0.7101\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68815 to 0.68300, saving model to best.model\n",
      "0s - loss: 0.7244 - acc: 0.7035 - val_loss: 0.6830 - val_acc: 0.7189\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.7267 - acc: 0.7017 - val_loss: 0.6851 - val_acc: 0.7173\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.7234 - acc: 0.7042 - val_loss: 0.6849 - val_acc: 0.7176\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68300 to 0.67804, saving model to best.model\n",
      "0s - loss: 0.7194 - acc: 0.7043 - val_loss: 0.6780 - val_acc: 0.7232\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7211 - acc: 0.7061 - val_loss: 0.6800 - val_acc: 0.7208\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.7186 - acc: 0.7079 - val_loss: 0.6795 - val_acc: 0.7224\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67804 to 0.67629, saving model to best.model\n",
      "0s - loss: 0.7165 - acc: 0.7064 - val_loss: 0.6763 - val_acc: 0.7213\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67629 to 0.67533, saving model to best.model\n",
      "0s - loss: 0.7195 - acc: 0.7071 - val_loss: 0.6753 - val_acc: 0.7245\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.7146 - acc: 0.7089 - val_loss: 0.6762 - val_acc: 0.7231\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67533 to 0.67407, saving model to best.model\n",
      "0s - loss: 0.7143 - acc: 0.7087 - val_loss: 0.6741 - val_acc: 0.7228\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.7141 - acc: 0.7090 - val_loss: 0.6769 - val_acc: 0.7194\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.7124 - acc: 0.7100 - val_loss: 0.6742 - val_acc: 0.7232\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67407 to 0.66694, saving model to best.model\n",
      "0s - loss: 0.7100 - acc: 0.7096 - val_loss: 0.6669 - val_acc: 0.7283\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.7097 - acc: 0.7092 - val_loss: 0.6711 - val_acc: 0.7235\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.7112 - acc: 0.7102 - val_loss: 0.6695 - val_acc: 0.7278\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.66694 to 0.66657, saving model to best.model\n",
      "0s - loss: 0.7099 - acc: 0.7082 - val_loss: 0.6666 - val_acc: 0.7246\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66657 to 0.66446, saving model to best.model\n",
      "0s - loss: 0.7064 - acc: 0.7122 - val_loss: 0.6645 - val_acc: 0.7283\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.66446 to 0.66412, saving model to best.model\n",
      "0s - loss: 0.7073 - acc: 0.7100 - val_loss: 0.6641 - val_acc: 0.7314\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66412 to 0.66288, saving model to best.model\n",
      "0s - loss: 0.7040 - acc: 0.7118 - val_loss: 0.6629 - val_acc: 0.7272\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66288 to 0.66102, saving model to best.model\n",
      "0s - loss: 0.7037 - acc: 0.7127 - val_loss: 0.6610 - val_acc: 0.7287\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7042 - acc: 0.7130 - val_loss: 0.6645 - val_acc: 0.7270\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66102 to 0.65876, saving model to best.model\n",
      "0s - loss: 0.7034 - acc: 0.7126 - val_loss: 0.6588 - val_acc: 0.7310\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.6998 - acc: 0.7153 - val_loss: 0.6599 - val_acc: 0.7287\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65876 to 0.65752, saving model to best.model\n",
      "0s - loss: 0.6980 - acc: 0.7145 - val_loss: 0.6575 - val_acc: 0.7315\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65752 to 0.65672, saving model to best.model\n",
      "1s - loss: 0.7009 - acc: 0.7132 - val_loss: 0.6567 - val_acc: 0.7322\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65672 to 0.65548, saving model to best.model\n",
      "0s - loss: 0.6948 - acc: 0.7172 - val_loss: 0.6555 - val_acc: 0.7333\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.6983 - acc: 0.7161 - val_loss: 0.6559 - val_acc: 0.7313\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65548 to 0.65216, saving model to best.model\n",
      "0s - loss: 0.6965 - acc: 0.7155 - val_loss: 0.6522 - val_acc: 0.7345\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6953 - acc: 0.7164 - val_loss: 0.6526 - val_acc: 0.7340\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6950 - acc: 0.7158 - val_loss: 0.6534 - val_acc: 0.7301\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.6945 - acc: 0.7162 - val_loss: 0.6526 - val_acc: 0.7341\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65216 to 0.65185, saving model to best.model\n",
      "0s - loss: 0.6940 - acc: 0.7166 - val_loss: 0.6519 - val_acc: 0.7345\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65185 to 0.65067, saving model to best.model\n",
      "0s - loss: 0.6938 - acc: 0.7180 - val_loss: 0.6507 - val_acc: 0.7355\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.6920 - acc: 0.7173 - val_loss: 0.6567 - val_acc: 0.7249\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.65067 to 0.64904, saving model to best.model\n",
      "0s - loss: 0.6914 - acc: 0.7183 - val_loss: 0.6490 - val_acc: 0.7361\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.6934 - acc: 0.7164 - val_loss: 0.6516 - val_acc: 0.7344\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64904 to 0.64772, saving model to best.model\n",
      "0s - loss: 0.6932 - acc: 0.7189 - val_loss: 0.6477 - val_acc: 0.7355\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.64772 to 0.64524, saving model to best.model\n",
      "0s - loss: 0.6917 - acc: 0.7198 - val_loss: 0.6452 - val_acc: 0.7367\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6908 - acc: 0.7192 - val_loss: 0.6483 - val_acc: 0.7352\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64524 to 0.64481, saving model to best.model\n",
      "0s - loss: 0.6860 - acc: 0.7207 - val_loss: 0.6448 - val_acc: 0.7368\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64481 to 0.64471, saving model to best.model\n",
      "0s - loss: 0.6857 - acc: 0.7213 - val_loss: 0.6447 - val_acc: 0.7371\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.64471 to 0.64380, saving model to best.model\n",
      "0s - loss: 0.6872 - acc: 0.7179 - val_loss: 0.6438 - val_acc: 0.7376\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6849 - acc: 0.7212 - val_loss: 0.6445 - val_acc: 0.7361\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6853 - acc: 0.7217 - val_loss: 0.6454 - val_acc: 0.7335\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64380 to 0.64043, saving model to best.model\n",
      "1s - loss: 0.6849 - acc: 0.7225 - val_loss: 0.6404 - val_acc: 0.7382\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6829 - acc: 0.7221 - val_loss: 0.6422 - val_acc: 0.7349\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6836 - acc: 0.7208 - val_loss: 0.6419 - val_acc: 0.7381\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6810 - acc: 0.7213 - val_loss: 0.6407 - val_acc: 0.7371\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6821 - acc: 0.7227 - val_loss: 0.6410 - val_acc: 0.7357\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64043 to 0.64024, saving model to best.model\n",
      "1s - loss: 0.6810 - acc: 0.7224 - val_loss: 0.6402 - val_acc: 0.7377\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64024 to 0.63955, saving model to best.model\n",
      "1s - loss: 0.6830 - acc: 0.7205 - val_loss: 0.6395 - val_acc: 0.7368\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.63955 to 0.63809, saving model to best.model\n",
      "1s - loss: 0.6817 - acc: 0.7237 - val_loss: 0.6381 - val_acc: 0.7381\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6801 - acc: 0.7214 - val_loss: 0.6382 - val_acc: 0.7383\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63809 to 0.63774, saving model to best.model\n",
      "1s - loss: 0.6758 - acc: 0.7250 - val_loss: 0.6377 - val_acc: 0.7392\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.63774 to 0.63376, saving model to best.model\n",
      "0s - loss: 0.6761 - acc: 0.7221 - val_loss: 0.6338 - val_acc: 0.7399\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.6762 - acc: 0.7252 - val_loss: 0.6352 - val_acc: 0.7392\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6779 - acc: 0.7244 - val_loss: 0.6374 - val_acc: 0.7355\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63376 to 0.63166, saving model to best.model\n",
      "1s - loss: 0.6765 - acc: 0.7256 - val_loss: 0.6317 - val_acc: 0.7416\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.63166 to 0.63004, saving model to best.model\n",
      "1s - loss: 0.6750 - acc: 0.7249 - val_loss: 0.6300 - val_acc: 0.7419\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6757 - acc: 0.7245 - val_loss: 0.6343 - val_acc: 0.7391\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6735 - acc: 0.7270 - val_loss: 0.6307 - val_acc: 0.7433\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6723 - acc: 0.7260 - val_loss: 0.6304 - val_acc: 0.7432\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63004 to 0.62844, saving model to best.model\n",
      "0s - loss: 0.6697 - acc: 0.7272 - val_loss: 0.6284 - val_acc: 0.7422\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6744 - acc: 0.7251 - val_loss: 0.6307 - val_acc: 0.7413\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6733 - acc: 0.7266 - val_loss: 0.6312 - val_acc: 0.7413\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62844 to 0.62761, saving model to best.model\n",
      "1s - loss: 0.6694 - acc: 0.7268 - val_loss: 0.6276 - val_acc: 0.7425\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.62761 to 0.62748, saving model to best.model\n",
      "1s - loss: 0.6710 - acc: 0.7290 - val_loss: 0.6275 - val_acc: 0.7444\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7281 - val_loss: 0.6300 - val_acc: 0.7415\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6689 - acc: 0.7294 - val_loss: 0.6301 - val_acc: 0.7412\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62748 to 0.62527, saving model to best.model\n",
      "0s - loss: 0.6654 - acc: 0.7282 - val_loss: 0.6253 - val_acc: 0.7434\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6667 - acc: 0.7282 - val_loss: 0.6254 - val_acc: 0.7447\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62527 to 0.62344, saving model to best.model\n",
      "1s - loss: 0.6649 - acc: 0.7285 - val_loss: 0.6234 - val_acc: 0.7451\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6694 - acc: 0.7275 - val_loss: 0.6263 - val_acc: 0.7417\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62344 to 0.62175, saving model to best.model\n",
      "0s - loss: 0.6679 - acc: 0.7297 - val_loss: 0.6217 - val_acc: 0.7463\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6664 - acc: 0.7309 - val_loss: 0.6238 - val_acc: 0.7459\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6654 - acc: 0.7309 - val_loss: 0.6243 - val_acc: 0.7445\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6646 - acc: 0.7285 - val_loss: 0.6225 - val_acc: 0.7451\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.62175 to 0.62101, saving model to best.model\n",
      "1s - loss: 0.6630 - acc: 0.7311 - val_loss: 0.6210 - val_acc: 0.7466\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.6636 - acc: 0.7301 - val_loss: 0.6225 - val_acc: 0.7448\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6619 - acc: 0.7316 - val_loss: 0.6212 - val_acc: 0.7457\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62101 to 0.62009, saving model to best.model\n",
      "0s - loss: 0.6620 - acc: 0.7302 - val_loss: 0.6201 - val_acc: 0.7450\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6637 - acc: 0.7296 - val_loss: 0.6201 - val_acc: 0.7465\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62009 to 0.61800, saving model to best.model\n",
      "0s - loss: 0.6613 - acc: 0.7310 - val_loss: 0.6180 - val_acc: 0.7482\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6619 - acc: 0.7300 - val_loss: 0.6202 - val_acc: 0.7450\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6623 - acc: 0.7302 - val_loss: 0.6188 - val_acc: 0.7463\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6615 - acc: 0.7302 - val_loss: 0.6195 - val_acc: 0.7472\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6609 - acc: 0.7318 - val_loss: 0.6196 - val_acc: 0.7451\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61800 to 0.61723, saving model to best.model\n",
      "1s - loss: 0.6639 - acc: 0.7314 - val_loss: 0.6172 - val_acc: 0.7489\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.61723 to 0.61694, saving model to best.model\n",
      "1s - loss: 0.6581 - acc: 0.7302 - val_loss: 0.6169 - val_acc: 0.7475\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61694 to 0.61597, saving model to best.model\n",
      "1s - loss: 0.6588 - acc: 0.7326 - val_loss: 0.6160 - val_acc: 0.7492\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6604 - acc: 0.7307 - val_loss: 0.6168 - val_acc: 0.7477\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6599 - acc: 0.7310 - val_loss: 0.6169 - val_acc: 0.7487\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61597 to 0.61433, saving model to best.model\n",
      "1s - loss: 0.6588 - acc: 0.7312 - val_loss: 0.6143 - val_acc: 0.7495\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6566 - acc: 0.7343 - val_loss: 0.6152 - val_acc: 0.7482\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6588 - acc: 0.7327 - val_loss: 0.6149 - val_acc: 0.7488\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6568 - acc: 0.7332 - val_loss: 0.6151 - val_acc: 0.7465\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61433 to 0.61339, saving model to best.model\n",
      "1s - loss: 0.6545 - acc: 0.7325 - val_loss: 0.6134 - val_acc: 0.7484\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61339 to 0.61255, saving model to best.model\n",
      "1s - loss: 0.6560 - acc: 0.7343 - val_loss: 0.6126 - val_acc: 0.7505\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6557 - acc: 0.7340 - val_loss: 0.6127 - val_acc: 0.7494\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61255 to 0.61232, saving model to best.model\n",
      "1s - loss: 0.6543 - acc: 0.7337 - val_loss: 0.6123 - val_acc: 0.7499\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.61232 to 0.61196, saving model to best.model\n",
      "1s - loss: 0.6522 - acc: 0.7353 - val_loss: 0.6120 - val_acc: 0.7501\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6535 - acc: 0.7333 - val_loss: 0.6135 - val_acc: 0.7470\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.61196 to 0.61153, saving model to best.model\n",
      "1s - loss: 0.6555 - acc: 0.7336 - val_loss: 0.6115 - val_acc: 0.7500\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.61153 to 0.60990, saving model to best.model\n",
      "1s - loss: 0.6535 - acc: 0.7340 - val_loss: 0.6099 - val_acc: 0.7508\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6561 - acc: 0.7312 - val_loss: 0.6099 - val_acc: 0.7502\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.60990 to 0.60786, saving model to best.model\n",
      "1s - loss: 0.6497 - acc: 0.7352 - val_loss: 0.6079 - val_acc: 0.7500\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7325 - val_loss: 0.6085 - val_acc: 0.7511\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6505 - acc: 0.7353 - val_loss: 0.6098 - val_acc: 0.7505\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6557 - acc: 0.7343 - val_loss: 0.6100 - val_acc: 0.7493\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6529 - acc: 0.7349 - val_loss: 0.6106 - val_acc: 0.7471\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60786 to 0.60645, saving model to best.model\n",
      "1s - loss: 0.6508 - acc: 0.7346 - val_loss: 0.6064 - val_acc: 0.7509\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6513 - acc: 0.7339 - val_loss: 0.6083 - val_acc: 0.7511\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6492 - acc: 0.7356 - val_loss: 0.6097 - val_acc: 0.7484\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60645 to 0.60520, saving model to best.model\n",
      "0s - loss: 0.6497 - acc: 0.7351 - val_loss: 0.6052 - val_acc: 0.7521\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60520 to 0.60467, saving model to best.model\n",
      "0s - loss: 0.6505 - acc: 0.7352 - val_loss: 0.6047 - val_acc: 0.7530\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6502 - acc: 0.7334 - val_loss: 0.6078 - val_acc: 0.7499\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60467 to 0.60307, saving model to best.model\n",
      "1s - loss: 0.6467 - acc: 0.7389 - val_loss: 0.6031 - val_acc: 0.7540\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.6508 - acc: 0.7364 - val_loss: 0.6061 - val_acc: 0.7507\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.6489 - acc: 0.7369 - val_loss: 0.6043 - val_acc: 0.7516\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6483 - acc: 0.7343 - val_loss: 0.6062 - val_acc: 0.7504\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83537, saving model to best.model\n",
      "1s - loss: 0.9701 - acc: 0.6016 - val_loss: 0.8354 - val_acc: 0.6686\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.83537 to 0.83350, saving model to best.model\n",
      "0s - loss: 0.8675 - acc: 0.6574 - val_loss: 0.8335 - val_acc: 0.6686\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.8553 - acc: 0.6597 - val_loss: 0.8336 - val_acc: 0.6686\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.8517 - acc: 0.6598 - val_loss: 0.8341 - val_acc: 0.6686\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83350 to 0.83018, saving model to best.model\n",
      "0s - loss: 0.8485 - acc: 0.6598 - val_loss: 0.8302 - val_acc: 0.6686\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83018 to 0.82258, saving model to best.model\n",
      "0s - loss: 0.8442 - acc: 0.6598 - val_loss: 0.8226 - val_acc: 0.6686\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82258 to 0.81615, saving model to best.model\n",
      "0s - loss: 0.8380 - acc: 0.6597 - val_loss: 0.8162 - val_acc: 0.6686\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81615 to 0.81295, saving model to best.model\n",
      "0s - loss: 0.8353 - acc: 0.6598 - val_loss: 0.8129 - val_acc: 0.6686\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81295 to 0.81091, saving model to best.model\n",
      "0s - loss: 0.8312 - acc: 0.6597 - val_loss: 0.8109 - val_acc: 0.6686\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81091 to 0.81025, saving model to best.model\n",
      "0s - loss: 0.8283 - acc: 0.6599 - val_loss: 0.8103 - val_acc: 0.6686\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.8286 - acc: 0.6597 - val_loss: 0.8113 - val_acc: 0.6686\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81025 to 0.80952, saving model to best.model\n",
      "1s - loss: 0.8265 - acc: 0.6596 - val_loss: 0.8095 - val_acc: 0.6686\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.80952 to 0.80940, saving model to best.model\n",
      "1s - loss: 0.8266 - acc: 0.6594 - val_loss: 0.8094 - val_acc: 0.6686\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.80940 to 0.80831, saving model to best.model\n",
      "1s - loss: 0.8259 - acc: 0.6596 - val_loss: 0.8083 - val_acc: 0.6686\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.80831 to 0.80818, saving model to best.model\n",
      "0s - loss: 0.8256 - acc: 0.6598 - val_loss: 0.8082 - val_acc: 0.6686\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.8244 - acc: 0.6596 - val_loss: 0.8115 - val_acc: 0.6686\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8239 - acc: 0.6599 - val_loss: 0.8137 - val_acc: 0.6686\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "1s - loss: 0.8224 - acc: 0.6597 - val_loss: 0.8110 - val_acc: 0.6686\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.80818 to 0.80694, saving model to best.model\n",
      "1s - loss: 0.8228 - acc: 0.6598 - val_loss: 0.8069 - val_acc: 0.6686\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.8216 - acc: 0.6594 - val_loss: 0.8071 - val_acc: 0.6686\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.80694 to 0.80629, saving model to best.model\n",
      "1s - loss: 0.8210 - acc: 0.6595 - val_loss: 0.8063 - val_acc: 0.6686\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.80629 to 0.80541, saving model to best.model\n",
      "0s - loss: 0.8205 - acc: 0.6601 - val_loss: 0.8054 - val_acc: 0.6686\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.8192 - acc: 0.6594 - val_loss: 0.8056 - val_acc: 0.6686\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80541 to 0.80392, saving model to best.model\n",
      "1s - loss: 0.8205 - acc: 0.6598 - val_loss: 0.8039 - val_acc: 0.6686\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.8197 - acc: 0.6595 - val_loss: 0.8057 - val_acc: 0.6686\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80392 to 0.80309, saving model to best.model\n",
      "0s - loss: 0.8186 - acc: 0.6594 - val_loss: 0.8031 - val_acc: 0.6686\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.8167 - acc: 0.6604 - val_loss: 0.8032 - val_acc: 0.6686\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80309 to 0.80085, saving model to best.model\n",
      "0s - loss: 0.8164 - acc: 0.6607 - val_loss: 0.8009 - val_acc: 0.6690\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80085 to 0.80058, saving model to best.model\n",
      "1s - loss: 0.8164 - acc: 0.6613 - val_loss: 0.8006 - val_acc: 0.6696\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80058 to 0.79794, saving model to best.model\n",
      "0s - loss: 0.8138 - acc: 0.6620 - val_loss: 0.7979 - val_acc: 0.6715\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79794 to 0.79783, saving model to best.model\n",
      "0s - loss: 0.8118 - acc: 0.6629 - val_loss: 0.7978 - val_acc: 0.6713\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79783 to 0.79631, saving model to best.model\n",
      "0s - loss: 0.8122 - acc: 0.6621 - val_loss: 0.7963 - val_acc: 0.6732\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79631 to 0.79384, saving model to best.model\n",
      "0s - loss: 0.8115 - acc: 0.6631 - val_loss: 0.7938 - val_acc: 0.6731\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79384 to 0.79309, saving model to best.model\n",
      "1s - loss: 0.8082 - acc: 0.6649 - val_loss: 0.7931 - val_acc: 0.6734\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79309 to 0.79080, saving model to best.model\n",
      "1s - loss: 0.8079 - acc: 0.6646 - val_loss: 0.7908 - val_acc: 0.6733\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79080 to 0.78880, saving model to best.model\n",
      "1s - loss: 0.8074 - acc: 0.6663 - val_loss: 0.7888 - val_acc: 0.6758\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.8055 - acc: 0.6654 - val_loss: 0.7889 - val_acc: 0.6787\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78880 to 0.78333, saving model to best.model\n",
      "0s - loss: 0.8038 - acc: 0.6661 - val_loss: 0.7833 - val_acc: 0.6830\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78333 to 0.78154, saving model to best.model\n",
      "0s - loss: 0.8008 - acc: 0.6682 - val_loss: 0.7815 - val_acc: 0.6807\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78154 to 0.78048, saving model to best.model\n",
      "1s - loss: 0.8005 - acc: 0.6678 - val_loss: 0.7805 - val_acc: 0.6823\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78048 to 0.77687, saving model to best.model\n",
      "1s - loss: 0.7984 - acc: 0.6693 - val_loss: 0.7769 - val_acc: 0.6844\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77687 to 0.77490, saving model to best.model\n",
      "0s - loss: 0.7975 - acc: 0.6688 - val_loss: 0.7749 - val_acc: 0.6857\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77490 to 0.77291, saving model to best.model\n",
      "0s - loss: 0.7939 - acc: 0.6723 - val_loss: 0.7729 - val_acc: 0.6851\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77291 to 0.76953, saving model to best.model\n",
      "0s - loss: 0.7926 - acc: 0.6717 - val_loss: 0.7695 - val_acc: 0.6865\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76953 to 0.76691, saving model to best.model\n",
      "0s - loss: 0.7892 - acc: 0.6731 - val_loss: 0.7669 - val_acc: 0.6869\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76691 to 0.76337, saving model to best.model\n",
      "0s - loss: 0.7889 - acc: 0.6727 - val_loss: 0.7634 - val_acc: 0.6925\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76337 to 0.76280, saving model to best.model\n",
      "1s - loss: 0.7878 - acc: 0.6727 - val_loss: 0.7628 - val_acc: 0.6892\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76280 to 0.75966, saving model to best.model\n",
      "1s - loss: 0.7858 - acc: 0.6757 - val_loss: 0.7597 - val_acc: 0.6915\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.7840 - acc: 0.6756 - val_loss: 0.7604 - val_acc: 0.6925\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.75966 to 0.75311, saving model to best.model\n",
      "0s - loss: 0.7807 - acc: 0.6764 - val_loss: 0.7531 - val_acc: 0.6946\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75311 to 0.75287, saving model to best.model\n",
      "0s - loss: 0.7807 - acc: 0.6776 - val_loss: 0.7529 - val_acc: 0.6959\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75287 to 0.74794, saving model to best.model\n",
      "1s - loss: 0.7786 - acc: 0.6765 - val_loss: 0.7479 - val_acc: 0.6985\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.74794 to 0.74454, saving model to best.model\n",
      "0s - loss: 0.7767 - acc: 0.6777 - val_loss: 0.7445 - val_acc: 0.7012\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74454 to 0.74321, saving model to best.model\n",
      "1s - loss: 0.7742 - acc: 0.6804 - val_loss: 0.7432 - val_acc: 0.7044\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74321 to 0.73996, saving model to best.model\n",
      "0s - loss: 0.7722 - acc: 0.6798 - val_loss: 0.7400 - val_acc: 0.7048\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.73996 to 0.73822, saving model to best.model\n",
      "1s - loss: 0.7693 - acc: 0.6825 - val_loss: 0.7382 - val_acc: 0.7028\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.73822 to 0.73747, saving model to best.model\n",
      "0s - loss: 0.7693 - acc: 0.6808 - val_loss: 0.7375 - val_acc: 0.7005\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.73747 to 0.73060, saving model to best.model\n",
      "0s - loss: 0.7635 - acc: 0.6867 - val_loss: 0.7306 - val_acc: 0.7071\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73060 to 0.72965, saving model to best.model\n",
      "0s - loss: 0.7650 - acc: 0.6826 - val_loss: 0.7297 - val_acc: 0.7041\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.72965 to 0.72740, saving model to best.model\n",
      "0s - loss: 0.7630 - acc: 0.6846 - val_loss: 0.7274 - val_acc: 0.7078\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.72740 to 0.72198, saving model to best.model\n",
      "0s - loss: 0.7628 - acc: 0.6861 - val_loss: 0.7220 - val_acc: 0.7104\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72198 to 0.72095, saving model to best.model\n",
      "1s - loss: 0.7585 - acc: 0.6885 - val_loss: 0.7210 - val_acc: 0.7114\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.72095 to 0.71824, saving model to best.model\n",
      "1s - loss: 0.7584 - acc: 0.6879 - val_loss: 0.7182 - val_acc: 0.7121\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.7540 - acc: 0.6905 - val_loss: 0.7184 - val_acc: 0.7153\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.71824 to 0.71029, saving model to best.model\n",
      "1s - loss: 0.7515 - acc: 0.6911 - val_loss: 0.7103 - val_acc: 0.7200\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "1s - loss: 0.7499 - acc: 0.6919 - val_loss: 0.7138 - val_acc: 0.7199\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.71029 to 0.70737, saving model to best.model\n",
      "1s - loss: 0.7488 - acc: 0.6926 - val_loss: 0.7074 - val_acc: 0.7210\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.70737 to 0.70720, saving model to best.model\n",
      "1s - loss: 0.7480 - acc: 0.6917 - val_loss: 0.7072 - val_acc: 0.7201\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.70720 to 0.70486, saving model to best.model\n",
      "1s - loss: 0.7454 - acc: 0.6943 - val_loss: 0.7049 - val_acc: 0.7222\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "1s - loss: 0.7456 - acc: 0.6946 - val_loss: 0.7057 - val_acc: 0.7196\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70486 to 0.70372, saving model to best.model\n",
      "0s - loss: 0.7456 - acc: 0.6933 - val_loss: 0.7037 - val_acc: 0.7194\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70372 to 0.70197, saving model to best.model\n",
      "1s - loss: 0.7415 - acc: 0.6972 - val_loss: 0.7020 - val_acc: 0.7220\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70197 to 0.69973, saving model to best.model\n",
      "1s - loss: 0.7397 - acc: 0.6972 - val_loss: 0.6997 - val_acc: 0.7206\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.69973 to 0.69691, saving model to best.model\n",
      "1s - loss: 0.7386 - acc: 0.6973 - val_loss: 0.6969 - val_acc: 0.7179\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69691 to 0.69263, saving model to best.model\n",
      "1s - loss: 0.7398 - acc: 0.6960 - val_loss: 0.6926 - val_acc: 0.7233\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "1s - loss: 0.7371 - acc: 0.6981 - val_loss: 0.6951 - val_acc: 0.7311\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69263 to 0.69089, saving model to best.model\n",
      "1s - loss: 0.7363 - acc: 0.6992 - val_loss: 0.6909 - val_acc: 0.7278\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69089 to 0.68730, saving model to best.model\n",
      "1s - loss: 0.7342 - acc: 0.7002 - val_loss: 0.6873 - val_acc: 0.7273\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.68730 to 0.68690, saving model to best.model\n",
      "1s - loss: 0.7306 - acc: 0.7015 - val_loss: 0.6869 - val_acc: 0.7275\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.68690 to 0.68315, saving model to best.model\n",
      "1s - loss: 0.7297 - acc: 0.7017 - val_loss: 0.6831 - val_acc: 0.7276\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68315 to 0.68172, saving model to best.model\n",
      "1s - loss: 0.7289 - acc: 0.7047 - val_loss: 0.6817 - val_acc: 0.7290\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7313 - acc: 0.7013 - val_loss: 0.6855 - val_acc: 0.7272\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68172 to 0.67981, saving model to best.model\n",
      "1s - loss: 0.7257 - acc: 0.7044 - val_loss: 0.6798 - val_acc: 0.7302\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67981 to 0.67861, saving model to best.model\n",
      "1s - loss: 0.7244 - acc: 0.7065 - val_loss: 0.6786 - val_acc: 0.7297\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.67861 to 0.67568, saving model to best.model\n",
      "1s - loss: 0.7243 - acc: 0.7044 - val_loss: 0.6757 - val_acc: 0.7306\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7230 - acc: 0.7041 - val_loss: 0.6796 - val_acc: 0.7295\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67568 to 0.67255, saving model to best.model\n",
      "1s - loss: 0.7235 - acc: 0.7051 - val_loss: 0.6726 - val_acc: 0.7363\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7196 - acc: 0.7090 - val_loss: 0.6746 - val_acc: 0.7324\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.7189 - acc: 0.7096 - val_loss: 0.6727 - val_acc: 0.7327\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67255 to 0.67096, saving model to best.model\n",
      "1s - loss: 0.7181 - acc: 0.7084 - val_loss: 0.6710 - val_acc: 0.7341\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67096 to 0.66833, saving model to best.model\n",
      "1s - loss: 0.7185 - acc: 0.7079 - val_loss: 0.6683 - val_acc: 0.7331\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66833 to 0.66624, saving model to best.model\n",
      "1s - loss: 0.7171 - acc: 0.7080 - val_loss: 0.6662 - val_acc: 0.7349\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7152 - acc: 0.7085 - val_loss: 0.6664 - val_acc: 0.7369\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66624 to 0.66474, saving model to best.model\n",
      "1s - loss: 0.7142 - acc: 0.7097 - val_loss: 0.6647 - val_acc: 0.7352\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7120 - acc: 0.7090 - val_loss: 0.6657 - val_acc: 0.7368\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.66474 to 0.66361, saving model to best.model\n",
      "1s - loss: 0.7130 - acc: 0.7113 - val_loss: 0.6636 - val_acc: 0.7365\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66361 to 0.66162, saving model to best.model\n",
      "1s - loss: 0.7130 - acc: 0.7117 - val_loss: 0.6616 - val_acc: 0.7348\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.66162 to 0.66122, saving model to best.model\n",
      "1s - loss: 0.7118 - acc: 0.7116 - val_loss: 0.6612 - val_acc: 0.7363\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66122 to 0.65883, saving model to best.model\n",
      "1s - loss: 0.7105 - acc: 0.7132 - val_loss: 0.6588 - val_acc: 0.7363\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7107 - acc: 0.7128 - val_loss: 0.6616 - val_acc: 0.7365\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.65883 to 0.65658, saving model to best.model\n",
      "1s - loss: 0.7088 - acc: 0.7136 - val_loss: 0.6566 - val_acc: 0.7370\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7061 - acc: 0.7150 - val_loss: 0.6584 - val_acc: 0.7361\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7094 - acc: 0.7120 - val_loss: 0.6579 - val_acc: 0.7363\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65658 to 0.65561, saving model to best.model\n",
      "1s - loss: 0.7096 - acc: 0.7103 - val_loss: 0.6556 - val_acc: 0.7384\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65561 to 0.65506, saving model to best.model\n",
      "1s - loss: 0.7060 - acc: 0.7139 - val_loss: 0.6551 - val_acc: 0.7371\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65506 to 0.65424, saving model to best.model\n",
      "1s - loss: 0.7041 - acc: 0.7131 - val_loss: 0.6542 - val_acc: 0.7367\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.7010 - acc: 0.7168 - val_loss: 0.6570 - val_acc: 0.7382\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65424 to 0.64869, saving model to best.model\n",
      "1s - loss: 0.7031 - acc: 0.7140 - val_loss: 0.6487 - val_acc: 0.7381\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6982 - acc: 0.7167 - val_loss: 0.6489 - val_acc: 0.7392\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.7009 - acc: 0.7153 - val_loss: 0.6507 - val_acc: 0.7399\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.7022 - acc: 0.7152 - val_loss: 0.6494 - val_acc: 0.7410\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.64869 to 0.64728, saving model to best.model\n",
      "1s - loss: 0.6985 - acc: 0.7172 - val_loss: 0.6473 - val_acc: 0.7420\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.7004 - acc: 0.7160 - val_loss: 0.6513 - val_acc: 0.7354\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.64728 to 0.64516, saving model to best.model\n",
      "1s - loss: 0.6976 - acc: 0.7166 - val_loss: 0.6452 - val_acc: 0.7420\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.64516 to 0.64466, saving model to best.model\n",
      "1s - loss: 0.6957 - acc: 0.7180 - val_loss: 0.6447 - val_acc: 0.7413\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6963 - acc: 0.7172 - val_loss: 0.6476 - val_acc: 0.7405\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6969 - acc: 0.7185 - val_loss: 0.6457 - val_acc: 0.7398\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.64466 to 0.64078, saving model to best.model\n",
      "1s - loss: 0.6936 - acc: 0.7214 - val_loss: 0.6408 - val_acc: 0.7407\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64078 to 0.63910, saving model to best.model\n",
      "1s - loss: 0.6917 - acc: 0.7212 - val_loss: 0.6391 - val_acc: 0.7433\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6930 - acc: 0.7185 - val_loss: 0.6406 - val_acc: 0.7422\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6934 - acc: 0.7203 - val_loss: 0.6421 - val_acc: 0.7413\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.63910 to 0.63894, saving model to best.model\n",
      "1s - loss: 0.6891 - acc: 0.7227 - val_loss: 0.6389 - val_acc: 0.7413\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.63894 to 0.63859, saving model to best.model\n",
      "1s - loss: 0.6913 - acc: 0.7196 - val_loss: 0.6386 - val_acc: 0.7405\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.63859 to 0.63700, saving model to best.model\n",
      "1s - loss: 0.6908 - acc: 0.7208 - val_loss: 0.6370 - val_acc: 0.7412\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6895 - acc: 0.7208 - val_loss: 0.6377 - val_acc: 0.7403\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63700 to 0.63332, saving model to best.model\n",
      "1s - loss: 0.6876 - acc: 0.7217 - val_loss: 0.6333 - val_acc: 0.7411\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6884 - acc: 0.7220 - val_loss: 0.6369 - val_acc: 0.7425\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6883 - acc: 0.7214 - val_loss: 0.6359 - val_acc: 0.7465\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.63332 to 0.63039, saving model to best.model\n",
      "1s - loss: 0.6847 - acc: 0.7223 - val_loss: 0.6304 - val_acc: 0.7444\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63039 to 0.62996, saving model to best.model\n",
      "1s - loss: 0.6867 - acc: 0.7197 - val_loss: 0.6300 - val_acc: 0.7434\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6845 - acc: 0.7235 - val_loss: 0.6327 - val_acc: 0.7420\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6840 - acc: 0.7233 - val_loss: 0.6329 - val_acc: 0.7402\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.62996 to 0.62741, saving model to best.model\n",
      "1s - loss: 0.6816 - acc: 0.7239 - val_loss: 0.6274 - val_acc: 0.7438\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.62741 to 0.62636, saving model to best.model\n",
      "1s - loss: 0.6822 - acc: 0.7227 - val_loss: 0.6264 - val_acc: 0.7436\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6795 - acc: 0.7244 - val_loss: 0.6271 - val_acc: 0.7407\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6844 - acc: 0.7229 - val_loss: 0.6281 - val_acc: 0.7424\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6782 - acc: 0.7257 - val_loss: 0.6264 - val_acc: 0.7438\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62636 to 0.62537, saving model to best.model\n",
      "1s - loss: 0.6797 - acc: 0.7244 - val_loss: 0.6254 - val_acc: 0.7443\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62537 to 0.62523, saving model to best.model\n",
      "1s - loss: 0.6809 - acc: 0.7227 - val_loss: 0.6252 - val_acc: 0.7444\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.62523 to 0.62276, saving model to best.model\n",
      "1s - loss: 0.6780 - acc: 0.7249 - val_loss: 0.6228 - val_acc: 0.7436\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62276 to 0.62263, saving model to best.model\n",
      "1s - loss: 0.6764 - acc: 0.7237 - val_loss: 0.6226 - val_acc: 0.7453\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.62263 to 0.61984, saving model to best.model\n",
      "1s - loss: 0.6761 - acc: 0.7263 - val_loss: 0.6198 - val_acc: 0.7456\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6774 - acc: 0.7273 - val_loss: 0.6204 - val_acc: 0.7465\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.61984 to 0.61977, saving model to best.model\n",
      "1s - loss: 0.6779 - acc: 0.7275 - val_loss: 0.6198 - val_acc: 0.7474\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6768 - acc: 0.7256 - val_loss: 0.6216 - val_acc: 0.7429\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6785 - acc: 0.7251 - val_loss: 0.6219 - val_acc: 0.7446\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6720 - acc: 0.7280 - val_loss: 0.6216 - val_acc: 0.7459\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.61977 to 0.61888, saving model to best.model\n",
      "1s - loss: 0.6733 - acc: 0.7261 - val_loss: 0.6189 - val_acc: 0.7459\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6751 - acc: 0.7254 - val_loss: 0.6202 - val_acc: 0.7456\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6734 - acc: 0.7284 - val_loss: 0.6199 - val_acc: 0.7479\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.61888 to 0.61734, saving model to best.model\n",
      "1s - loss: 0.6733 - acc: 0.7268 - val_loss: 0.6173 - val_acc: 0.7440\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.61734 to 0.61344, saving model to best.model\n",
      "1s - loss: 0.6712 - acc: 0.7266 - val_loss: 0.6134 - val_acc: 0.7495\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6703 - acc: 0.7283 - val_loss: 0.6167 - val_acc: 0.7494\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6690 - acc: 0.7297 - val_loss: 0.6136 - val_acc: 0.7501\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6712 - acc: 0.7273 - val_loss: 0.6137 - val_acc: 0.7457\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6692 - acc: 0.7301 - val_loss: 0.6142 - val_acc: 0.7445\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61344 to 0.61228, saving model to best.model\n",
      "1s - loss: 0.6680 - acc: 0.7285 - val_loss: 0.6123 - val_acc: 0.7509\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6714 - acc: 0.7263 - val_loss: 0.6169 - val_acc: 0.7420\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.6655 - acc: 0.7293 - val_loss: 0.6126 - val_acc: 0.7472\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.61228 to 0.61207, saving model to best.model\n",
      "0s - loss: 0.6670 - acc: 0.7306 - val_loss: 0.6121 - val_acc: 0.7465\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.61207 to 0.60736, saving model to best.model\n",
      "0s - loss: 0.6651 - acc: 0.7298 - val_loss: 0.6074 - val_acc: 0.7492\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6629 - acc: 0.7314 - val_loss: 0.6097 - val_acc: 0.7463\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6664 - acc: 0.7311 - val_loss: 0.6114 - val_acc: 0.7464\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6637 - acc: 0.7303 - val_loss: 0.6085 - val_acc: 0.7479\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6633 - acc: 0.7307 - val_loss: 0.6109 - val_acc: 0.7481\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6620 - acc: 0.7309 - val_loss: 0.6085 - val_acc: 0.7464\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6643 - acc: 0.7298 - val_loss: 0.6108 - val_acc: 0.7453\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.60736 to 0.60697, saving model to best.model\n",
      "0s - loss: 0.6605 - acc: 0.7318 - val_loss: 0.6070 - val_acc: 0.7459\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.60697 to 0.60606, saving model to best.model\n",
      "0s - loss: 0.6622 - acc: 0.7307 - val_loss: 0.6061 - val_acc: 0.7475\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6631 - acc: 0.7294 - val_loss: 0.6068 - val_acc: 0.7471\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.60606 to 0.60564, saving model to best.model\n",
      "1s - loss: 0.6636 - acc: 0.7329 - val_loss: 0.6056 - val_acc: 0.7479\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6603 - acc: 0.7352 - val_loss: 0.6057 - val_acc: 0.7467\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.60564 to 0.60554, saving model to best.model\n",
      "0s - loss: 0.6621 - acc: 0.7312 - val_loss: 0.6055 - val_acc: 0.7501\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6631 - acc: 0.7313 - val_loss: 0.6078 - val_acc: 0.7477\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.60554 to 0.60226, saving model to best.model\n",
      "1s - loss: 0.6560 - acc: 0.7333 - val_loss: 0.6023 - val_acc: 0.7475\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.60226 to 0.60076, saving model to best.model\n",
      "0s - loss: 0.6600 - acc: 0.7326 - val_loss: 0.6008 - val_acc: 0.7529\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6638 - acc: 0.7318 - val_loss: 0.6046 - val_acc: 0.7516\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6579 - acc: 0.7335 - val_loss: 0.6032 - val_acc: 0.7489\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6585 - acc: 0.7342 - val_loss: 0.6012 - val_acc: 0.7501\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.6552 - acc: 0.7330 - val_loss: 0.6022 - val_acc: 0.7485\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60076 to 0.60068, saving model to best.model\n",
      "1s - loss: 0.6545 - acc: 0.7334 - val_loss: 0.6007 - val_acc: 0.7526\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60068 to 0.60049, saving model to best.model\n",
      "1s - loss: 0.6548 - acc: 0.7351 - val_loss: 0.6005 - val_acc: 0.7482\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60049 to 0.59734, saving model to best.model\n",
      "1s - loss: 0.6576 - acc: 0.7336 - val_loss: 0.5973 - val_acc: 0.7534\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6568 - acc: 0.7337 - val_loss: 0.5985 - val_acc: 0.7530\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7352 - val_loss: 0.6007 - val_acc: 0.7493\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6530 - acc: 0.7370 - val_loss: 0.5975 - val_acc: 0.7539\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59734 to 0.59690, saving model to best.model\n",
      "0s - loss: 0.6503 - acc: 0.7375 - val_loss: 0.5969 - val_acc: 0.7521\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6566 - acc: 0.7351 - val_loss: 0.5970 - val_acc: 0.7526\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.59690 to 0.59534, saving model to best.model\n",
      "1s - loss: 0.6516 - acc: 0.7354 - val_loss: 0.5953 - val_acc: 0.7544\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6511 - acc: 0.7368 - val_loss: 0.5964 - val_acc: 0.7505\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.59534 to 0.59474, saving model to best.model\n",
      "0s - loss: 0.6504 - acc: 0.7335 - val_loss: 0.5947 - val_acc: 0.7564\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.59474 to 0.59412, saving model to best.model\n",
      "1s - loss: 0.6518 - acc: 0.7352 - val_loss: 0.5941 - val_acc: 0.7564\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6506 - acc: 0.7358 - val_loss: 0.5961 - val_acc: 0.7556\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.59412 to 0.59322, saving model to best.model\n",
      "1s - loss: 0.6493 - acc: 0.7352 - val_loss: 0.5932 - val_acc: 0.7542\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6527 - acc: 0.7345 - val_loss: 0.5959 - val_acc: 0.7516\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6515 - acc: 0.7367 - val_loss: 0.5952 - val_acc: 0.7528\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.59322 to 0.59209, saving model to best.model\n",
      "1s - loss: 0.6496 - acc: 0.7373 - val_loss: 0.5921 - val_acc: 0.7533\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6503 - acc: 0.7368 - val_loss: 0.5925 - val_acc: 0.7516\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6499 - acc: 0.7363 - val_loss: 0.5940 - val_acc: 0.7521\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6493 - acc: 0.7375 - val_loss: 0.5926 - val_acc: 0.7546\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.82645, saving model to best.model\n",
      "0s - loss: 0.9385 - acc: 0.6179 - val_loss: 0.8265 - val_acc: 0.6731\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.8656 - acc: 0.6557 - val_loss: 0.8265 - val_acc: 0.6731\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.82645 to 0.82485, saving model to best.model\n",
      "0s - loss: 0.8564 - acc: 0.6563 - val_loss: 0.8248 - val_acc: 0.6731\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.82485 to 0.82476, saving model to best.model\n",
      "1s - loss: 0.8527 - acc: 0.6563 - val_loss: 0.8248 - val_acc: 0.6731\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.82476 to 0.81514, saving model to best.model\n",
      "1s - loss: 0.8481 - acc: 0.6563 - val_loss: 0.8151 - val_acc: 0.6731\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.81514 to 0.80976, saving model to best.model\n",
      "1s - loss: 0.8440 - acc: 0.6563 - val_loss: 0.8098 - val_acc: 0.6731\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.80976 to 0.80443, saving model to best.model\n",
      "1s - loss: 0.8380 - acc: 0.6563 - val_loss: 0.8044 - val_acc: 0.6731\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "1s - loss: 0.8362 - acc: 0.6563 - val_loss: 0.8045 - val_acc: 0.6731\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.80443 to 0.80204, saving model to best.model\n",
      "1s - loss: 0.8341 - acc: 0.6563 - val_loss: 0.8020 - val_acc: 0.6731\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.80204 to 0.79995, saving model to best.model\n",
      "1s - loss: 0.8329 - acc: 0.6563 - val_loss: 0.7999 - val_acc: 0.6731\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.79995 to 0.79942, saving model to best.model\n",
      "1s - loss: 0.8327 - acc: 0.6563 - val_loss: 0.7994 - val_acc: 0.6731\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.79942 to 0.79799, saving model to best.model\n",
      "1s - loss: 0.8313 - acc: 0.6563 - val_loss: 0.7980 - val_acc: 0.6731\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.8303 - acc: 0.6563 - val_loss: 0.8006 - val_acc: 0.6731\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8291 - acc: 0.6563 - val_loss: 0.7983 - val_acc: 0.6731\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.79799 to 0.79740, saving model to best.model\n",
      "1s - loss: 0.8298 - acc: 0.6563 - val_loss: 0.7974 - val_acc: 0.6731\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8293 - acc: 0.6563 - val_loss: 0.7976 - val_acc: 0.6731\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.79740 to 0.79631, saving model to best.model\n",
      "1s - loss: 0.8282 - acc: 0.6563 - val_loss: 0.7963 - val_acc: 0.6731\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.79631 to 0.79575, saving model to best.model\n",
      "1s - loss: 0.8265 - acc: 0.6563 - val_loss: 0.7957 - val_acc: 0.6731\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.79575 to 0.79398, saving model to best.model\n",
      "1s - loss: 0.8251 - acc: 0.6562 - val_loss: 0.7940 - val_acc: 0.6731\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.8262 - acc: 0.6560 - val_loss: 0.7963 - val_acc: 0.6731\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.8255 - acc: 0.6562 - val_loss: 0.7960 - val_acc: 0.6731\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "1s - loss: 0.8249 - acc: 0.6563 - val_loss: 0.7953 - val_acc: 0.6731\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "1s - loss: 0.8235 - acc: 0.6561 - val_loss: 0.7950 - val_acc: 0.6731\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.79398 to 0.79136, saving model to best.model\n",
      "1s - loss: 0.8209 - acc: 0.6562 - val_loss: 0.7914 - val_acc: 0.6731\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.79136 to 0.79133, saving model to best.model\n",
      "1s - loss: 0.8221 - acc: 0.6568 - val_loss: 0.7913 - val_acc: 0.6731\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.8207 - acc: 0.6569 - val_loss: 0.7935 - val_acc: 0.6732\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.79133 to 0.78885, saving model to best.model\n",
      "0s - loss: 0.8192 - acc: 0.6575 - val_loss: 0.7888 - val_acc: 0.6733\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.8193 - acc: 0.6583 - val_loss: 0.7889 - val_acc: 0.6738\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.8182 - acc: 0.6582 - val_loss: 0.7894 - val_acc: 0.6734\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.78885 to 0.78642, saving model to best.model\n",
      "0s - loss: 0.8181 - acc: 0.6589 - val_loss: 0.7864 - val_acc: 0.6737\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.8167 - acc: 0.6599 - val_loss: 0.7864 - val_acc: 0.6755\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.78642 to 0.78277, saving model to best.model\n",
      "0s - loss: 0.8146 - acc: 0.6603 - val_loss: 0.7828 - val_acc: 0.6760\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "1s - loss: 0.8141 - acc: 0.6604 - val_loss: 0.7860 - val_acc: 0.6803\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.78277 to 0.78137, saving model to best.model\n",
      "1s - loss: 0.8118 - acc: 0.6601 - val_loss: 0.7814 - val_acc: 0.6759\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.78137 to 0.77846, saving model to best.model\n",
      "1s - loss: 0.8115 - acc: 0.6618 - val_loss: 0.7785 - val_acc: 0.6804\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "2s - loss: 0.8097 - acc: 0.6613 - val_loss: 0.7793 - val_acc: 0.6838\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.77846 to 0.77703, saving model to best.model\n",
      "2s - loss: 0.8078 - acc: 0.6619 - val_loss: 0.7770 - val_acc: 0.6807\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.77703 to 0.77496, saving model to best.model\n",
      "1s - loss: 0.8069 - acc: 0.6630 - val_loss: 0.7750 - val_acc: 0.6779\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "1s - loss: 0.8043 - acc: 0.6625 - val_loss: 0.7751 - val_acc: 0.6815\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.77496 to 0.77130, saving model to best.model\n",
      "1s - loss: 0.8044 - acc: 0.6635 - val_loss: 0.7713 - val_acc: 0.6857\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77130 to 0.76923, saving model to best.model\n",
      "1s - loss: 0.8029 - acc: 0.6633 - val_loss: 0.7692 - val_acc: 0.6801\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.76923 to 0.76665, saving model to best.model\n",
      "1s - loss: 0.8009 - acc: 0.6663 - val_loss: 0.7666 - val_acc: 0.6858\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.76665 to 0.76525, saving model to best.model\n",
      "1s - loss: 0.8012 - acc: 0.6655 - val_loss: 0.7652 - val_acc: 0.6878\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.76525 to 0.76338, saving model to best.model\n",
      "1s - loss: 0.7979 - acc: 0.6669 - val_loss: 0.7634 - val_acc: 0.6859\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76338 to 0.76157, saving model to best.model\n",
      "1s - loss: 0.7947 - acc: 0.6695 - val_loss: 0.7616 - val_acc: 0.6902\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76157 to 0.75910, saving model to best.model\n",
      "1s - loss: 0.7960 - acc: 0.6664 - val_loss: 0.7591 - val_acc: 0.6878\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.75910 to 0.75551, saving model to best.model\n",
      "1s - loss: 0.7936 - acc: 0.6681 - val_loss: 0.7555 - val_acc: 0.6904\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "1s - loss: 0.7910 - acc: 0.6705 - val_loss: 0.7557 - val_acc: 0.6849\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.75551 to 0.75302, saving model to best.model\n",
      "1s - loss: 0.7909 - acc: 0.6687 - val_loss: 0.7530 - val_acc: 0.6938\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.75302 to 0.74776, saving model to best.model\n",
      "1s - loss: 0.7882 - acc: 0.6722 - val_loss: 0.7478 - val_acc: 0.6959\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.74776 to 0.74706, saving model to best.model\n",
      "1s - loss: 0.7870 - acc: 0.6710 - val_loss: 0.7471 - val_acc: 0.6963\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.74706 to 0.74350, saving model to best.model\n",
      "1s - loss: 0.7849 - acc: 0.6729 - val_loss: 0.7435 - val_acc: 0.6950\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.74350 to 0.74214, saving model to best.model\n",
      "1s - loss: 0.7840 - acc: 0.6712 - val_loss: 0.7421 - val_acc: 0.6957\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74214 to 0.73898, saving model to best.model\n",
      "1s - loss: 0.7825 - acc: 0.6724 - val_loss: 0.7390 - val_acc: 0.6932\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.73898 to 0.73782, saving model to best.model\n",
      "1s - loss: 0.7793 - acc: 0.6723 - val_loss: 0.7378 - val_acc: 0.6941\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.73782 to 0.73599, saving model to best.model\n",
      "1s - loss: 0.7775 - acc: 0.6750 - val_loss: 0.7360 - val_acc: 0.6972\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.73599 to 0.72807, saving model to best.model\n",
      "1s - loss: 0.7749 - acc: 0.6769 - val_loss: 0.7281 - val_acc: 0.7027\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "1s - loss: 0.7730 - acc: 0.6776 - val_loss: 0.7294 - val_acc: 0.7036\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.72807 to 0.72170, saving model to best.model\n",
      "1s - loss: 0.7698 - acc: 0.6801 - val_loss: 0.7217 - val_acc: 0.7050\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.72170 to 0.71981, saving model to best.model\n",
      "1s - loss: 0.7691 - acc: 0.6812 - val_loss: 0.7198 - val_acc: 0.7088\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.71981 to 0.71867, saving model to best.model\n",
      "1s - loss: 0.7665 - acc: 0.6808 - val_loss: 0.7187 - val_acc: 0.7101\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.71867 to 0.71471, saving model to best.model\n",
      "1s - loss: 0.7629 - acc: 0.6827 - val_loss: 0.7147 - val_acc: 0.7096\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "1s - loss: 0.7617 - acc: 0.6824 - val_loss: 0.7150 - val_acc: 0.7116\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.71471 to 0.71012, saving model to best.model\n",
      "1s - loss: 0.7612 - acc: 0.6820 - val_loss: 0.7101 - val_acc: 0.7158\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.71012 to 0.70958, saving model to best.model\n",
      "1s - loss: 0.7594 - acc: 0.6846 - val_loss: 0.7096 - val_acc: 0.7169\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.70958 to 0.70417, saving model to best.model\n",
      "1s - loss: 0.7552 - acc: 0.6857 - val_loss: 0.7042 - val_acc: 0.7155\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.70417 to 0.70188, saving model to best.model\n",
      "1s - loss: 0.7533 - acc: 0.6901 - val_loss: 0.7019 - val_acc: 0.7118\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.70188 to 0.69640, saving model to best.model\n",
      "1s - loss: 0.7527 - acc: 0.6875 - val_loss: 0.6964 - val_acc: 0.7170\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "1s - loss: 0.7499 - acc: 0.6881 - val_loss: 0.6971 - val_acc: 0.7220\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.69640 to 0.69460, saving model to best.model\n",
      "1s - loss: 0.7488 - acc: 0.6901 - val_loss: 0.6946 - val_acc: 0.7215\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.69460 to 0.69423, saving model to best.model\n",
      "1s - loss: 0.7454 - acc: 0.6925 - val_loss: 0.6942 - val_acc: 0.7199\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.69423 to 0.69132, saving model to best.model\n",
      "1s - loss: 0.7469 - acc: 0.6886 - val_loss: 0.6913 - val_acc: 0.7201\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.69132 to 0.68815, saving model to best.model\n",
      "1s - loss: 0.7424 - acc: 0.6932 - val_loss: 0.6882 - val_acc: 0.7179\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.7451 - acc: 0.6932 - val_loss: 0.6903 - val_acc: 0.7220\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.68815 to 0.68546, saving model to best.model\n",
      "1s - loss: 0.7399 - acc: 0.6957 - val_loss: 0.6855 - val_acc: 0.7235\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.68546 to 0.68465, saving model to best.model\n",
      "1s - loss: 0.7422 - acc: 0.6935 - val_loss: 0.6847 - val_acc: 0.7285\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.68465 to 0.67938, saving model to best.model\n",
      "1s - loss: 0.7360 - acc: 0.6932 - val_loss: 0.6794 - val_acc: 0.7231\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "1s - loss: 0.7359 - acc: 0.6965 - val_loss: 0.6796 - val_acc: 0.7300\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.67938 to 0.67821, saving model to best.model\n",
      "0s - loss: 0.7327 - acc: 0.6996 - val_loss: 0.6782 - val_acc: 0.7276\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.67821 to 0.67511, saving model to best.model\n",
      "1s - loss: 0.7347 - acc: 0.6974 - val_loss: 0.6751 - val_acc: 0.7300\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.67511 to 0.67193, saving model to best.model\n",
      "1s - loss: 0.7281 - acc: 0.7018 - val_loss: 0.6719 - val_acc: 0.7301\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.67193 to 0.67165, saving model to best.model\n",
      "1s - loss: 0.7295 - acc: 0.6988 - val_loss: 0.6717 - val_acc: 0.7304\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.67165 to 0.66953, saving model to best.model\n",
      "1s - loss: 0.7274 - acc: 0.7011 - val_loss: 0.6695 - val_acc: 0.7329\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.66953 to 0.66665, saving model to best.model\n",
      "1s - loss: 0.7245 - acc: 0.7022 - val_loss: 0.6667 - val_acc: 0.7327\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7253 - acc: 0.7004 - val_loss: 0.6670 - val_acc: 0.7361\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.7233 - acc: 0.7027 - val_loss: 0.6669 - val_acc: 0.7321\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.66665 to 0.66451, saving model to best.model\n",
      "0s - loss: 0.7255 - acc: 0.7014 - val_loss: 0.6645 - val_acc: 0.7371\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.66451 to 0.66332, saving model to best.model\n",
      "0s - loss: 0.7248 - acc: 0.7007 - val_loss: 0.6633 - val_acc: 0.7342\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.66332 to 0.66161, saving model to best.model\n",
      "0s - loss: 0.7211 - acc: 0.7054 - val_loss: 0.6616 - val_acc: 0.7330\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.7197 - acc: 0.7040 - val_loss: 0.6625 - val_acc: 0.7395\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.66161 to 0.65976, saving model to best.model\n",
      "0s - loss: 0.7179 - acc: 0.7051 - val_loss: 0.6598 - val_acc: 0.7388\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.65976 to 0.65733, saving model to best.model\n",
      "0s - loss: 0.7137 - acc: 0.7079 - val_loss: 0.6573 - val_acc: 0.7354\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.7183 - acc: 0.7048 - val_loss: 0.6578 - val_acc: 0.7377\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.65733 to 0.65709, saving model to best.model\n",
      "0s - loss: 0.7142 - acc: 0.7066 - val_loss: 0.6571 - val_acc: 0.7402\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.65709 to 0.65086, saving model to best.model\n",
      "0s - loss: 0.7115 - acc: 0.7082 - val_loss: 0.6509 - val_acc: 0.7388\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7140 - acc: 0.7083 - val_loss: 0.6543 - val_acc: 0.7374\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.65086 to 0.64902, saving model to best.model\n",
      "1s - loss: 0.7120 - acc: 0.7075 - val_loss: 0.6490 - val_acc: 0.7418\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.64902 to 0.64893, saving model to best.model\n",
      "0s - loss: 0.7095 - acc: 0.7087 - val_loss: 0.6489 - val_acc: 0.7426\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.7123 - acc: 0.7084 - val_loss: 0.6497 - val_acc: 0.7409\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7098 - acc: 0.7095 - val_loss: 0.6494 - val_acc: 0.7427\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.64893 to 0.64342, saving model to best.model\n",
      "0s - loss: 0.7070 - acc: 0.7105 - val_loss: 0.6434 - val_acc: 0.7444\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.7083 - acc: 0.7097 - val_loss: 0.6442 - val_acc: 0.7437\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.64342 to 0.64304, saving model to best.model\n",
      "0s - loss: 0.7038 - acc: 0.7116 - val_loss: 0.6430 - val_acc: 0.7433\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.64304 to 0.64193, saving model to best.model\n",
      "0s - loss: 0.7045 - acc: 0.7144 - val_loss: 0.6419 - val_acc: 0.7437\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.7048 - acc: 0.7129 - val_loss: 0.6451 - val_acc: 0.7434\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.64193 to 0.63976, saving model to best.model\n",
      "0s - loss: 0.7014 - acc: 0.7138 - val_loss: 0.6398 - val_acc: 0.7436\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.7015 - acc: 0.7130 - val_loss: 0.6403 - val_acc: 0.7420\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.63976 to 0.63830, saving model to best.model\n",
      "0s - loss: 0.7006 - acc: 0.7136 - val_loss: 0.6383 - val_acc: 0.7473\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.7007 - acc: 0.7137 - val_loss: 0.6407 - val_acc: 0.7457\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.63830 to 0.63678, saving model to best.model\n",
      "0s - loss: 0.7011 - acc: 0.7111 - val_loss: 0.6368 - val_acc: 0.7453\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.63678 to 0.63613, saving model to best.model\n",
      "0s - loss: 0.6999 - acc: 0.7119 - val_loss: 0.6361 - val_acc: 0.7430\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.63613 to 0.63013, saving model to best.model\n",
      "0s - loss: 0.6945 - acc: 0.7163 - val_loss: 0.6301 - val_acc: 0.7457\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6979 - acc: 0.7163 - val_loss: 0.6378 - val_acc: 0.7439\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6958 - acc: 0.7158 - val_loss: 0.6336 - val_acc: 0.7425\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.63013 to 0.62990, saving model to best.model\n",
      "1s - loss: 0.6939 - acc: 0.7164 - val_loss: 0.6299 - val_acc: 0.7487\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6938 - acc: 0.7181 - val_loss: 0.6317 - val_acc: 0.7446\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.62990 to 0.62909, saving model to best.model\n",
      "1s - loss: 0.6923 - acc: 0.7174 - val_loss: 0.6291 - val_acc: 0.7454\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6897 - acc: 0.7191 - val_loss: 0.6301 - val_acc: 0.7451\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.62909 to 0.62807, saving model to best.model\n",
      "1s - loss: 0.6923 - acc: 0.7156 - val_loss: 0.6281 - val_acc: 0.7495\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.6898 - acc: 0.7190 - val_loss: 0.6311 - val_acc: 0.7454\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6912 - acc: 0.7194 - val_loss: 0.6299 - val_acc: 0.7471\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.62807 to 0.62666, saving model to best.model\n",
      "0s - loss: 0.6885 - acc: 0.7196 - val_loss: 0.6267 - val_acc: 0.7508\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6912 - acc: 0.7196 - val_loss: 0.6300 - val_acc: 0.7484\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.62666 to 0.62333, saving model to best.model\n",
      "0s - loss: 0.6866 - acc: 0.7210 - val_loss: 0.6233 - val_acc: 0.7519\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6871 - acc: 0.7183 - val_loss: 0.6246 - val_acc: 0.7534\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.6845 - acc: 0.7198 - val_loss: 0.6246 - val_acc: 0.7471\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62333 to 0.62321, saving model to best.model\n",
      "0s - loss: 0.6872 - acc: 0.7196 - val_loss: 0.6232 - val_acc: 0.7506\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.62321 to 0.62283, saving model to best.model\n",
      "0s - loss: 0.6854 - acc: 0.7212 - val_loss: 0.6228 - val_acc: 0.7523\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.62283 to 0.61846, saving model to best.model\n",
      "0s - loss: 0.6849 - acc: 0.7191 - val_loss: 0.6185 - val_acc: 0.7534\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6828 - acc: 0.7224 - val_loss: 0.6207 - val_acc: 0.7494\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6832 - acc: 0.7193 - val_loss: 0.6219 - val_acc: 0.7475\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6822 - acc: 0.7200 - val_loss: 0.6202 - val_acc: 0.7573\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6796 - acc: 0.7214 - val_loss: 0.6195 - val_acc: 0.7496\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.61846 to 0.61844, saving model to best.model\n",
      "0s - loss: 0.6794 - acc: 0.7227 - val_loss: 0.6184 - val_acc: 0.7530\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6805 - acc: 0.7208 - val_loss: 0.6204 - val_acc: 0.7512\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.61844 to 0.61576, saving model to best.model\n",
      "0s - loss: 0.6806 - acc: 0.7197 - val_loss: 0.6158 - val_acc: 0.7554\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.61576 to 0.61474, saving model to best.model\n",
      "0s - loss: 0.6794 - acc: 0.7246 - val_loss: 0.6147 - val_acc: 0.7536\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6771 - acc: 0.7241 - val_loss: 0.6166 - val_acc: 0.7556\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6784 - acc: 0.7228 - val_loss: 0.6173 - val_acc: 0.7525\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6769 - acc: 0.7231 - val_loss: 0.6174 - val_acc: 0.7568\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.61474 to 0.61230, saving model to best.model\n",
      "0s - loss: 0.6750 - acc: 0.7257 - val_loss: 0.6123 - val_acc: 0.7560\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.61230 to 0.61196, saving model to best.model\n",
      "0s - loss: 0.6759 - acc: 0.7233 - val_loss: 0.6120 - val_acc: 0.7552\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.61196 to 0.60888, saving model to best.model\n",
      "0s - loss: 0.6756 - acc: 0.7252 - val_loss: 0.6089 - val_acc: 0.7582\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6730 - acc: 0.7236 - val_loss: 0.6126 - val_acc: 0.7583\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6757 - acc: 0.7229 - val_loss: 0.6119 - val_acc: 0.7559\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6735 - acc: 0.7262 - val_loss: 0.6115 - val_acc: 0.7547\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.60888 to 0.60633, saving model to best.model\n",
      "0s - loss: 0.6710 - acc: 0.7272 - val_loss: 0.6063 - val_acc: 0.7582\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6729 - acc: 0.7281 - val_loss: 0.6074 - val_acc: 0.7589\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6709 - acc: 0.7246 - val_loss: 0.6068 - val_acc: 0.7584\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6737 - acc: 0.7250 - val_loss: 0.6087 - val_acc: 0.7550\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6709 - acc: 0.7259 - val_loss: 0.6078 - val_acc: 0.7604\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6680 - acc: 0.7273 - val_loss: 0.6068 - val_acc: 0.7607\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6669 - acc: 0.7298 - val_loss: 0.6067 - val_acc: 0.7601\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.60633 to 0.60423, saving model to best.model\n",
      "0s - loss: 0.6687 - acc: 0.7273 - val_loss: 0.6042 - val_acc: 0.7601\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6721 - acc: 0.7258 - val_loss: 0.6082 - val_acc: 0.7618\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6701 - acc: 0.7268 - val_loss: 0.6045 - val_acc: 0.7618\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6679 - acc: 0.7272 - val_loss: 0.6058 - val_acc: 0.7621\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.60423 to 0.60309, saving model to best.model\n",
      "0s - loss: 0.6659 - acc: 0.7280 - val_loss: 0.6031 - val_acc: 0.7616\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.6642 - acc: 0.7289 - val_loss: 0.6033 - val_acc: 0.7609\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.60309 to 0.60077, saving model to best.model\n",
      "1s - loss: 0.6638 - acc: 0.7290 - val_loss: 0.6008 - val_acc: 0.7610\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.60077 to 0.60006, saving model to best.model\n",
      "0s - loss: 0.6668 - acc: 0.7283 - val_loss: 0.6001 - val_acc: 0.7612\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6672 - acc: 0.7258 - val_loss: 0.6025 - val_acc: 0.7628\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6648 - acc: 0.7272 - val_loss: 0.6001 - val_acc: 0.7602\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6623 - acc: 0.7289 - val_loss: 0.6002 - val_acc: 0.7593\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.60006 to 0.59789, saving model to best.model\n",
      "0s - loss: 0.6655 - acc: 0.7287 - val_loss: 0.5979 - val_acc: 0.7607\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.59789 to 0.59774, saving model to best.model\n",
      "0s - loss: 0.6623 - acc: 0.7314 - val_loss: 0.5977 - val_acc: 0.7632\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.59774 to 0.59551, saving model to best.model\n",
      "0s - loss: 0.6647 - acc: 0.7277 - val_loss: 0.5955 - val_acc: 0.7609\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6633 - acc: 0.7294 - val_loss: 0.5991 - val_acc: 0.7616\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6590 - acc: 0.7297 - val_loss: 0.5988 - val_acc: 0.7607\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6608 - acc: 0.7301 - val_loss: 0.5961 - val_acc: 0.7618\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6607 - acc: 0.7292 - val_loss: 0.5991 - val_acc: 0.7638\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6612 - acc: 0.7310 - val_loss: 0.5971 - val_acc: 0.7642\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.59551 to 0.59463, saving model to best.model\n",
      "0s - loss: 0.6582 - acc: 0.7333 - val_loss: 0.5946 - val_acc: 0.7630\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.59463 to 0.59320, saving model to best.model\n",
      "0s - loss: 0.6594 - acc: 0.7302 - val_loss: 0.5932 - val_acc: 0.7629\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6572 - acc: 0.7327 - val_loss: 0.5944 - val_acc: 0.7616\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6577 - acc: 0.7321 - val_loss: 0.5945 - val_acc: 0.7631\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6598 - acc: 0.7306 - val_loss: 0.5963 - val_acc: 0.7615\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.59320 to 0.59082, saving model to best.model\n",
      "0s - loss: 0.6561 - acc: 0.7322 - val_loss: 0.5908 - val_acc: 0.7617\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6564 - acc: 0.7320 - val_loss: 0.5924 - val_acc: 0.7636\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.6588 - acc: 0.7303 - val_loss: 0.5948 - val_acc: 0.7601\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6547 - acc: 0.7333 - val_loss: 0.5922 - val_acc: 0.7664\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.59082 to 0.58914, saving model to best.model\n",
      "0s - loss: 0.6519 - acc: 0.7340 - val_loss: 0.5891 - val_acc: 0.7663\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6529 - acc: 0.7330 - val_loss: 0.5901 - val_acc: 0.7618\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6548 - acc: 0.7350 - val_loss: 0.5893 - val_acc: 0.7639\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.58914 to 0.58835, saving model to best.model\n",
      "0s - loss: 0.6548 - acc: 0.7349 - val_loss: 0.5884 - val_acc: 0.7660\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6570 - acc: 0.7332 - val_loss: 0.5889 - val_acc: 0.7638\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.58835 to 0.58669, saving model to best.model\n",
      "0s - loss: 0.6520 - acc: 0.7364 - val_loss: 0.5867 - val_acc: 0.7656\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6561 - acc: 0.7337 - val_loss: 0.5888 - val_acc: 0.7652\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.58669 to 0.58654, saving model to best.model\n",
      "1s - loss: 0.6543 - acc: 0.7361 - val_loss: 0.5865 - val_acc: 0.7646\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.58654 to 0.58603, saving model to best.model\n",
      "0s - loss: 0.6513 - acc: 0.7357 - val_loss: 0.5860 - val_acc: 0.7648\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6507 - acc: 0.7340 - val_loss: 0.5864 - val_acc: 0.7671\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6517 - acc: 0.7348 - val_loss: 0.5877 - val_acc: 0.7669\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6510 - acc: 0.7352 - val_loss: 0.5872 - val_acc: 0.7631\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6504 - acc: 0.7315 - val_loss: 0.5865 - val_acc: 0.7658\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.58603 to 0.58475, saving model to best.model\n",
      "0s - loss: 0.6502 - acc: 0.7341 - val_loss: 0.5847 - val_acc: 0.7690\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.58475 to 0.58282, saving model to best.model\n",
      "0s - loss: 0.6480 - acc: 0.7358 - val_loss: 0.5828 - val_acc: 0.7667\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6472 - acc: 0.7362 - val_loss: 0.5839 - val_acc: 0.7659\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.58282 to 0.58276, saving model to best.model\n",
      "0s - loss: 0.6466 - acc: 0.7387 - val_loss: 0.5828 - val_acc: 0.7664\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6489 - acc: 0.7360 - val_loss: 0.5842 - val_acc: 0.7665\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.58276 to 0.57977, saving model to best.model\n",
      "0s - loss: 0.6457 - acc: 0.7376 - val_loss: 0.5798 - val_acc: 0.7696\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84594, saving model to best.model\n",
      "0s - loss: 0.8918 - acc: 0.6385 - val_loss: 0.8459 - val_acc: 0.6576\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84594 to 0.84423, saving model to best.model\n",
      "0s - loss: 0.8579 - acc: 0.6545 - val_loss: 0.8442 - val_acc: 0.6576\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84423 to 0.84197, saving model to best.model\n",
      "0s - loss: 0.8538 - acc: 0.6546 - val_loss: 0.8420 - val_acc: 0.6576\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84197 to 0.83376, saving model to best.model\n",
      "0s - loss: 0.8502 - acc: 0.6546 - val_loss: 0.8338 - val_acc: 0.6576\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83376 to 0.82750, saving model to best.model\n",
      "0s - loss: 0.8438 - acc: 0.6546 - val_loss: 0.8275 - val_acc: 0.6576\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82750 to 0.82416, saving model to best.model\n",
      "0s - loss: 0.8392 - acc: 0.6546 - val_loss: 0.8242 - val_acc: 0.6576\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82416 to 0.82316, saving model to best.model\n",
      "0s - loss: 0.8358 - acc: 0.6546 - val_loss: 0.8232 - val_acc: 0.6576\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82316 to 0.82183, saving model to best.model\n",
      "0s - loss: 0.8347 - acc: 0.6546 - val_loss: 0.8218 - val_acc: 0.6576\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82183 to 0.82133, saving model to best.model\n",
      "0s - loss: 0.8339 - acc: 0.6546 - val_loss: 0.8213 - val_acc: 0.6576\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82133 to 0.82022, saving model to best.model\n",
      "0s - loss: 0.8324 - acc: 0.6546 - val_loss: 0.8202 - val_acc: 0.6576\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82022 to 0.81896, saving model to best.model\n",
      "0s - loss: 0.8313 - acc: 0.6546 - val_loss: 0.8190 - val_acc: 0.6576\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81896 to 0.81881, saving model to best.model\n",
      "0s - loss: 0.8312 - acc: 0.6544 - val_loss: 0.8188 - val_acc: 0.6576\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81881 to 0.81870, saving model to best.model\n",
      "0s - loss: 0.8313 - acc: 0.6546 - val_loss: 0.8187 - val_acc: 0.6576\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81870 to 0.81750, saving model to best.model\n",
      "0s - loss: 0.8314 - acc: 0.6545 - val_loss: 0.8175 - val_acc: 0.6576\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81750 to 0.81739, saving model to best.model\n",
      "0s - loss: 0.8294 - acc: 0.6545 - val_loss: 0.8174 - val_acc: 0.6576\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81739 to 0.81602, saving model to best.model\n",
      "0s - loss: 0.8287 - acc: 0.6548 - val_loss: 0.8160 - val_acc: 0.6576\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8271 - acc: 0.6556 - val_loss: 0.8163 - val_acc: 0.6576\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81602 to 0.81544, saving model to best.model\n",
      "0s - loss: 0.8267 - acc: 0.6541 - val_loss: 0.8154 - val_acc: 0.6576\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81544 to 0.81295, saving model to best.model\n",
      "0s - loss: 0.8262 - acc: 0.6556 - val_loss: 0.8130 - val_acc: 0.6582\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.8235 - acc: 0.6565 - val_loss: 0.8138 - val_acc: 0.6576\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81295 to 0.81293, saving model to best.model\n",
      "0s - loss: 0.8248 - acc: 0.6557 - val_loss: 0.8129 - val_acc: 0.6581\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81293 to 0.81216, saving model to best.model\n",
      "0s - loss: 0.8245 - acc: 0.6560 - val_loss: 0.8122 - val_acc: 0.6595\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81216 to 0.80945, saving model to best.model\n",
      "0s - loss: 0.8214 - acc: 0.6577 - val_loss: 0.8094 - val_acc: 0.6602\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80945 to 0.80938, saving model to best.model\n",
      "0s - loss: 0.8223 - acc: 0.6569 - val_loss: 0.8094 - val_acc: 0.6603\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80938 to 0.80772, saving model to best.model\n",
      "0s - loss: 0.8216 - acc: 0.6573 - val_loss: 0.8077 - val_acc: 0.6600\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80772 to 0.80674, saving model to best.model\n",
      "0s - loss: 0.8206 - acc: 0.6571 - val_loss: 0.8067 - val_acc: 0.6614\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80674 to 0.80590, saving model to best.model\n",
      "0s - loss: 0.8190 - acc: 0.6579 - val_loss: 0.8059 - val_acc: 0.6646\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80590 to 0.80314, saving model to best.model\n",
      "0s - loss: 0.8177 - acc: 0.6593 - val_loss: 0.8031 - val_acc: 0.6645\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80314 to 0.80069, saving model to best.model\n",
      "0s - loss: 0.8163 - acc: 0.6596 - val_loss: 0.8007 - val_acc: 0.6672\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80069 to 0.80041, saving model to best.model\n",
      "0s - loss: 0.8149 - acc: 0.6599 - val_loss: 0.8004 - val_acc: 0.6644\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80041 to 0.79738, saving model to best.model\n",
      "0s - loss: 0.8139 - acc: 0.6610 - val_loss: 0.7974 - val_acc: 0.6684\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79738 to 0.79564, saving model to best.model\n",
      "0s - loss: 0.8123 - acc: 0.6608 - val_loss: 0.7956 - val_acc: 0.6704\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79564 to 0.79168, saving model to best.model\n",
      "0s - loss: 0.8096 - acc: 0.6632 - val_loss: 0.7917 - val_acc: 0.6706\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79168 to 0.79050, saving model to best.model\n",
      "0s - loss: 0.8094 - acc: 0.6613 - val_loss: 0.7905 - val_acc: 0.6707\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79050 to 0.79008, saving model to best.model\n",
      "0s - loss: 0.8064 - acc: 0.6619 - val_loss: 0.7901 - val_acc: 0.6674\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79008 to 0.78684, saving model to best.model\n",
      "0s - loss: 0.8041 - acc: 0.6643 - val_loss: 0.7868 - val_acc: 0.6719\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78684 to 0.78159, saving model to best.model\n",
      "0s - loss: 0.8028 - acc: 0.6654 - val_loss: 0.7816 - val_acc: 0.6737\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.8021 - acc: 0.6653 - val_loss: 0.7830 - val_acc: 0.6711\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78159 to 0.77920, saving model to best.model\n",
      "0s - loss: 0.7989 - acc: 0.6662 - val_loss: 0.7792 - val_acc: 0.6762\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.77920 to 0.77780, saving model to best.model\n",
      "0s - loss: 0.7995 - acc: 0.6669 - val_loss: 0.7778 - val_acc: 0.6711\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77780 to 0.77344, saving model to best.model\n",
      "0s - loss: 0.7970 - acc: 0.6665 - val_loss: 0.7734 - val_acc: 0.6749\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77344 to 0.77154, saving model to best.model\n",
      "0s - loss: 0.7958 - acc: 0.6683 - val_loss: 0.7715 - val_acc: 0.6778\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.7923 - acc: 0.6688 - val_loss: 0.7717 - val_acc: 0.6826\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77154 to 0.76618, saving model to best.model\n",
      "0s - loss: 0.7904 - acc: 0.6709 - val_loss: 0.7662 - val_acc: 0.6776\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.7916 - acc: 0.6695 - val_loss: 0.7672 - val_acc: 0.6796\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76618 to 0.76246, saving model to best.model\n",
      "1s - loss: 0.7892 - acc: 0.6697 - val_loss: 0.7625 - val_acc: 0.6775\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76246 to 0.75873, saving model to best.model\n",
      "1s - loss: 0.7856 - acc: 0.6700 - val_loss: 0.7587 - val_acc: 0.6841\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.75873 to 0.75772, saving model to best.model\n",
      "0s - loss: 0.7845 - acc: 0.6724 - val_loss: 0.7577 - val_acc: 0.6828\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.75772 to 0.75465, saving model to best.model\n",
      "0s - loss: 0.7838 - acc: 0.6730 - val_loss: 0.7547 - val_acc: 0.6830\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.75465 to 0.75090, saving model to best.model\n",
      "0s - loss: 0.7779 - acc: 0.6765 - val_loss: 0.7509 - val_acc: 0.6915\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75090 to 0.75041, saving model to best.model\n",
      "0s - loss: 0.7774 - acc: 0.6768 - val_loss: 0.7504 - val_acc: 0.6819\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75041 to 0.74609, saving model to best.model\n",
      "0s - loss: 0.7762 - acc: 0.6767 - val_loss: 0.7461 - val_acc: 0.6872\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.7770 - acc: 0.6751 - val_loss: 0.7465 - val_acc: 0.6813\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74609 to 0.74234, saving model to best.model\n",
      "0s - loss: 0.7727 - acc: 0.6757 - val_loss: 0.7423 - val_acc: 0.6923\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74234 to 0.73742, saving model to best.model\n",
      "0s - loss: 0.7718 - acc: 0.6782 - val_loss: 0.7374 - val_acc: 0.6959\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.73742 to 0.73618, saving model to best.model\n",
      "0s - loss: 0.7695 - acc: 0.6787 - val_loss: 0.7362 - val_acc: 0.6956\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.73618 to 0.73169, saving model to best.model\n",
      "0s - loss: 0.7648 - acc: 0.6790 - val_loss: 0.7317 - val_acc: 0.6917\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.7661 - acc: 0.6802 - val_loss: 0.7321 - val_acc: 0.6937\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73169 to 0.73063, saving model to best.model\n",
      "0s - loss: 0.7638 - acc: 0.6812 - val_loss: 0.7306 - val_acc: 0.6902\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73063 to 0.72763, saving model to best.model\n",
      "0s - loss: 0.7618 - acc: 0.6839 - val_loss: 0.7276 - val_acc: 0.6993\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.72763 to 0.71799, saving model to best.model\n",
      "0s - loss: 0.7605 - acc: 0.6827 - val_loss: 0.7180 - val_acc: 0.7014\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.7591 - acc: 0.6872 - val_loss: 0.7261 - val_acc: 0.7009\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.7585 - acc: 0.6838 - val_loss: 0.7184 - val_acc: 0.6996\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.71799 to 0.71611, saving model to best.model\n",
      "0s - loss: 0.7531 - acc: 0.6851 - val_loss: 0.7161 - val_acc: 0.6987\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.71611 to 0.71100, saving model to best.model\n",
      "0s - loss: 0.7524 - acc: 0.6854 - val_loss: 0.7110 - val_acc: 0.7037\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71100 to 0.70821, saving model to best.model\n",
      "0s - loss: 0.7516 - acc: 0.6889 - val_loss: 0.7082 - val_acc: 0.7032\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.70821 to 0.70527, saving model to best.model\n",
      "0s - loss: 0.7471 - acc: 0.6882 - val_loss: 0.7053 - val_acc: 0.7052\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.70527 to 0.70418, saving model to best.model\n",
      "0s - loss: 0.7477 - acc: 0.6878 - val_loss: 0.7042 - val_acc: 0.7062\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.70418 to 0.70225, saving model to best.model\n",
      "0s - loss: 0.7442 - acc: 0.6894 - val_loss: 0.7022 - val_acc: 0.7050\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70225 to 0.70127, saving model to best.model\n",
      "0s - loss: 0.7465 - acc: 0.6896 - val_loss: 0.7013 - val_acc: 0.7091\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.7437 - acc: 0.6911 - val_loss: 0.7034 - val_acc: 0.7069\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70127 to 0.69381, saving model to best.model\n",
      "0s - loss: 0.7416 - acc: 0.6919 - val_loss: 0.6938 - val_acc: 0.7117\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.7375 - acc: 0.6935 - val_loss: 0.6946 - val_acc: 0.7073\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.69381 to 0.68877, saving model to best.model\n",
      "0s - loss: 0.7396 - acc: 0.6917 - val_loss: 0.6888 - val_acc: 0.7142\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.7372 - acc: 0.6932 - val_loss: 0.6928 - val_acc: 0.7096\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.68877 to 0.68876, saving model to best.model\n",
      "0s - loss: 0.7336 - acc: 0.6943 - val_loss: 0.6888 - val_acc: 0.7142\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.68876 to 0.68663, saving model to best.model\n",
      "0s - loss: 0.7343 - acc: 0.6974 - val_loss: 0.6866 - val_acc: 0.7159\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.68663 to 0.68312, saving model to best.model\n",
      "0s - loss: 0.7311 - acc: 0.6959 - val_loss: 0.6831 - val_acc: 0.7153\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.68312 to 0.68142, saving model to best.model\n",
      "0s - loss: 0.7317 - acc: 0.6948 - val_loss: 0.6814 - val_acc: 0.7155\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.7323 - acc: 0.6952 - val_loss: 0.6830 - val_acc: 0.7141\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68142 to 0.67898, saving model to best.model\n",
      "0s - loss: 0.7292 - acc: 0.6965 - val_loss: 0.6790 - val_acc: 0.7165\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.7267 - acc: 0.6976 - val_loss: 0.6794 - val_acc: 0.7166\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.67898 to 0.67866, saving model to best.model\n",
      "0s - loss: 0.7289 - acc: 0.6977 - val_loss: 0.6787 - val_acc: 0.7141\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67866 to 0.67460, saving model to best.model\n",
      "0s - loss: 0.7249 - acc: 0.6979 - val_loss: 0.6746 - val_acc: 0.7163\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.67460 to 0.67285, saving model to best.model\n",
      "0s - loss: 0.7253 - acc: 0.6994 - val_loss: 0.6728 - val_acc: 0.7177\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67285 to 0.67045, saving model to best.model\n",
      "0s - loss: 0.7249 - acc: 0.7013 - val_loss: 0.6704 - val_acc: 0.7185\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7209 - acc: 0.7001 - val_loss: 0.6717 - val_acc: 0.7198\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67045 to 0.66798, saving model to best.model\n",
      "0s - loss: 0.7182 - acc: 0.7034 - val_loss: 0.6680 - val_acc: 0.7192\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.66798 to 0.66507, saving model to best.model\n",
      "0s - loss: 0.7172 - acc: 0.7018 - val_loss: 0.6651 - val_acc: 0.7199\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.66507 to 0.66197, saving model to best.model\n",
      "0s - loss: 0.7203 - acc: 0.7024 - val_loss: 0.6620 - val_acc: 0.7228\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.7190 - acc: 0.7021 - val_loss: 0.6647 - val_acc: 0.7215\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66197 to 0.66183, saving model to best.model\n",
      "0s - loss: 0.7171 - acc: 0.7018 - val_loss: 0.6618 - val_acc: 0.7230\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.7172 - acc: 0.7008 - val_loss: 0.6627 - val_acc: 0.7230\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66183 to 0.65806, saving model to best.model\n",
      "0s - loss: 0.7165 - acc: 0.7046 - val_loss: 0.6581 - val_acc: 0.7242\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.7119 - acc: 0.7055 - val_loss: 0.6604 - val_acc: 0.7245\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7137 - acc: 0.7035 - val_loss: 0.6618 - val_acc: 0.7251\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.65806 to 0.65698, saving model to best.model\n",
      "0s - loss: 0.7128 - acc: 0.7052 - val_loss: 0.6570 - val_acc: 0.7237\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.65698 to 0.65211, saving model to best.model\n",
      "0s - loss: 0.7099 - acc: 0.7083 - val_loss: 0.6521 - val_acc: 0.7279\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.7080 - acc: 0.7067 - val_loss: 0.6562 - val_acc: 0.7269\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7069 - acc: 0.7073 - val_loss: 0.6558 - val_acc: 0.7262\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7088 - acc: 0.7079 - val_loss: 0.6527 - val_acc: 0.7290\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.7057 - acc: 0.7088 - val_loss: 0.6526 - val_acc: 0.7288\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.65211 to 0.65199, saving model to best.model\n",
      "0s - loss: 0.7054 - acc: 0.7113 - val_loss: 0.6520 - val_acc: 0.7280\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65199 to 0.64629, saving model to best.model\n",
      "0s - loss: 0.6998 - acc: 0.7106 - val_loss: 0.6463 - val_acc: 0.7288\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.7027 - acc: 0.7105 - val_loss: 0.6485 - val_acc: 0.7287\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7035 - acc: 0.7078 - val_loss: 0.6484 - val_acc: 0.7296\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.7044 - acc: 0.7071 - val_loss: 0.6501 - val_acc: 0.7280\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.64629 to 0.64604, saving model to best.model\n",
      "0s - loss: 0.7001 - acc: 0.7109 - val_loss: 0.6460 - val_acc: 0.7296\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64604 to 0.64279, saving model to best.model\n",
      "0s - loss: 0.6992 - acc: 0.7129 - val_loss: 0.6428 - val_acc: 0.7282\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.6988 - acc: 0.7125 - val_loss: 0.6443 - val_acc: 0.7300\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.64279 to 0.64194, saving model to best.model\n",
      "0s - loss: 0.6993 - acc: 0.7112 - val_loss: 0.6419 - val_acc: 0.7302\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.64194 to 0.63716, saving model to best.model\n",
      "0s - loss: 0.6949 - acc: 0.7124 - val_loss: 0.6372 - val_acc: 0.7342\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6981 - acc: 0.7131 - val_loss: 0.6397 - val_acc: 0.7310\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.6960 - acc: 0.7128 - val_loss: 0.6386 - val_acc: 0.7323\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6961 - acc: 0.7135 - val_loss: 0.6379 - val_acc: 0.7335\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.63716 to 0.63441, saving model to best.model\n",
      "0s - loss: 0.6921 - acc: 0.7152 - val_loss: 0.6344 - val_acc: 0.7352\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.6923 - acc: 0.7139 - val_loss: 0.6387 - val_acc: 0.7299\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.6919 - acc: 0.7140 - val_loss: 0.6364 - val_acc: 0.7329\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6943 - acc: 0.7152 - val_loss: 0.6360 - val_acc: 0.7395\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.63441 to 0.63059, saving model to best.model\n",
      "0s - loss: 0.6920 - acc: 0.7167 - val_loss: 0.6306 - val_acc: 0.7377\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6901 - acc: 0.7180 - val_loss: 0.6318 - val_acc: 0.7369\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6914 - acc: 0.7157 - val_loss: 0.6349 - val_acc: 0.7382\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6903 - acc: 0.7155 - val_loss: 0.6311 - val_acc: 0.7370\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.63059 to 0.63031, saving model to best.model\n",
      "0s - loss: 0.6882 - acc: 0.7156 - val_loss: 0.6303 - val_acc: 0.7379\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6928 - acc: 0.7152 - val_loss: 0.6314 - val_acc: 0.7367\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63031 to 0.62961, saving model to best.model\n",
      "0s - loss: 0.6835 - acc: 0.7178 - val_loss: 0.6296 - val_acc: 0.7365\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62961 to 0.62878, saving model to best.model\n",
      "0s - loss: 0.6870 - acc: 0.7166 - val_loss: 0.6288 - val_acc: 0.7399\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.62878 to 0.62697, saving model to best.model\n",
      "0s - loss: 0.6865 - acc: 0.7190 - val_loss: 0.6270 - val_acc: 0.7388\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6856 - acc: 0.7173 - val_loss: 0.6285 - val_acc: 0.7367\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.62697 to 0.62339, saving model to best.model\n",
      "0s - loss: 0.6835 - acc: 0.7198 - val_loss: 0.6234 - val_acc: 0.7431\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6841 - acc: 0.7177 - val_loss: 0.6268 - val_acc: 0.7453\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6846 - acc: 0.7194 - val_loss: 0.6237 - val_acc: 0.7400\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.62339 to 0.62268, saving model to best.model\n",
      "0s - loss: 0.6841 - acc: 0.7183 - val_loss: 0.6227 - val_acc: 0.7433\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.62268 to 0.62228, saving model to best.model\n",
      "0s - loss: 0.6810 - acc: 0.7219 - val_loss: 0.6223 - val_acc: 0.7395\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.62228 to 0.62065, saving model to best.model\n",
      "0s - loss: 0.6817 - acc: 0.7200 - val_loss: 0.6206 - val_acc: 0.7411\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.6821 - acc: 0.7192 - val_loss: 0.6244 - val_acc: 0.7371\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6842 - acc: 0.7206 - val_loss: 0.6218 - val_acc: 0.7413\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62065 to 0.61791, saving model to best.model\n",
      "0s - loss: 0.6799 - acc: 0.7207 - val_loss: 0.6179 - val_acc: 0.7437\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6792 - acc: 0.7201 - val_loss: 0.6220 - val_acc: 0.7432\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6796 - acc: 0.7214 - val_loss: 0.6179 - val_acc: 0.7443\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.6783 - acc: 0.7205 - val_loss: 0.6202 - val_acc: 0.7434\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6783 - acc: 0.7221 - val_loss: 0.6188 - val_acc: 0.7399\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.61791 to 0.61682, saving model to best.model\n",
      "0s - loss: 0.6768 - acc: 0.7232 - val_loss: 0.6168 - val_acc: 0.7432\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6756 - acc: 0.7227 - val_loss: 0.6169 - val_acc: 0.7450\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.61682 to 0.61587, saving model to best.model\n",
      "0s - loss: 0.6772 - acc: 0.7205 - val_loss: 0.6159 - val_acc: 0.7438\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6773 - acc: 0.7217 - val_loss: 0.6160 - val_acc: 0.7426\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6756 - acc: 0.7223 - val_loss: 0.6160 - val_acc: 0.7448\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.61587 to 0.61516, saving model to best.model\n",
      "0s - loss: 0.6749 - acc: 0.7234 - val_loss: 0.6152 - val_acc: 0.7470\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.61516 to 0.61144, saving model to best.model\n",
      "0s - loss: 0.6740 - acc: 0.7256 - val_loss: 0.6114 - val_acc: 0.7452\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6732 - acc: 0.7253 - val_loss: 0.6126 - val_acc: 0.7461\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6705 - acc: 0.7244 - val_loss: 0.6131 - val_acc: 0.7464\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6711 - acc: 0.7253 - val_loss: 0.6137 - val_acc: 0.7424\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6752 - acc: 0.7239 - val_loss: 0.6150 - val_acc: 0.7437\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.61144 to 0.60838, saving model to best.model\n",
      "0s - loss: 0.6694 - acc: 0.7243 - val_loss: 0.6084 - val_acc: 0.7492\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6710 - acc: 0.7257 - val_loss: 0.6113 - val_acc: 0.7488\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6717 - acc: 0.7244 - val_loss: 0.6105 - val_acc: 0.7464\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6728 - acc: 0.7239 - val_loss: 0.6102 - val_acc: 0.7464\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6710 - acc: 0.7226 - val_loss: 0.6088 - val_acc: 0.7473\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7243 - val_loss: 0.6106 - val_acc: 0.7485\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.60838 to 0.60575, saving model to best.model\n",
      "0s - loss: 0.6679 - acc: 0.7272 - val_loss: 0.6058 - val_acc: 0.7479\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6710 - acc: 0.7241 - val_loss: 0.6084 - val_acc: 0.7493\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6683 - acc: 0.7258 - val_loss: 0.6073 - val_acc: 0.7479\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6673 - acc: 0.7262 - val_loss: 0.6076 - val_acc: 0.7473\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.60575 to 0.60172, saving model to best.model\n",
      "0s - loss: 0.6654 - acc: 0.7285 - val_loss: 0.6017 - val_acc: 0.7502\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6629 - acc: 0.7283 - val_loss: 0.6044 - val_acc: 0.7471\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6641 - acc: 0.7278 - val_loss: 0.6044 - val_acc: 0.7501\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6661 - acc: 0.7276 - val_loss: 0.6023 - val_acc: 0.7502\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6655 - acc: 0.7268 - val_loss: 0.6030 - val_acc: 0.7478\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6662 - acc: 0.7275 - val_loss: 0.6054 - val_acc: 0.7464\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6600 - acc: 0.7288 - val_loss: 0.6050 - val_acc: 0.7466\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6634 - acc: 0.7285 - val_loss: 0.6050 - val_acc: 0.7478\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6630 - acc: 0.7294 - val_loss: 0.6020 - val_acc: 0.7507\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.60172 to 0.60107, saving model to best.model\n",
      "0s - loss: 0.6634 - acc: 0.7281 - val_loss: 0.6011 - val_acc: 0.7491\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6633 - acc: 0.7304 - val_loss: 0.6047 - val_acc: 0.7453\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6599 - acc: 0.7299 - val_loss: 0.6036 - val_acc: 0.7472\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6608 - acc: 0.7273 - val_loss: 0.6011 - val_acc: 0.7495\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.60107 to 0.60052, saving model to best.model\n",
      "0s - loss: 0.6613 - acc: 0.7291 - val_loss: 0.6005 - val_acc: 0.7504\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.60052 to 0.59903, saving model to best.model\n",
      "0s - loss: 0.6629 - acc: 0.7302 - val_loss: 0.5990 - val_acc: 0.7501\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6587 - acc: 0.7303 - val_loss: 0.5996 - val_acc: 0.7520\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.6560 - acc: 0.7314 - val_loss: 0.5993 - val_acc: 0.7533\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.59903 to 0.59899, saving model to best.model\n",
      "0s - loss: 0.6591 - acc: 0.7293 - val_loss: 0.5990 - val_acc: 0.7498\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.59899 to 0.59656, saving model to best.model\n",
      "0s - loss: 0.6589 - acc: 0.7321 - val_loss: 0.5966 - val_acc: 0.7523\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6570 - acc: 0.7302 - val_loss: 0.5999 - val_acc: 0.7491\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.59656 to 0.59613, saving model to best.model\n",
      "0s - loss: 0.6558 - acc: 0.7314 - val_loss: 0.5961 - val_acc: 0.7514\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6552 - acc: 0.7321 - val_loss: 0.5967 - val_acc: 0.7505\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.59613 to 0.59605, saving model to best.model\n",
      "0s - loss: 0.6555 - acc: 0.7306 - val_loss: 0.5960 - val_acc: 0.7520\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59605 to 0.59549, saving model to best.model\n",
      "0s - loss: 0.6538 - acc: 0.7326 - val_loss: 0.5955 - val_acc: 0.7508\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6558 - acc: 0.7324 - val_loss: 0.5956 - val_acc: 0.7512\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6558 - acc: 0.7301 - val_loss: 0.5958 - val_acc: 0.7528\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.59549 to 0.59438, saving model to best.model\n",
      "0s - loss: 0.6554 - acc: 0.7321 - val_loss: 0.5944 - val_acc: 0.7519\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.59438 to 0.59417, saving model to best.model\n",
      "0s - loss: 0.6546 - acc: 0.7316 - val_loss: 0.5942 - val_acc: 0.7516\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6550 - acc: 0.7319 - val_loss: 0.5956 - val_acc: 0.7501\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6531 - acc: 0.7317 - val_loss: 0.5954 - val_acc: 0.7530\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6536 - acc: 0.7311 - val_loss: 0.5945 - val_acc: 0.7508\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.59417 to 0.59251, saving model to best.model\n",
      "0s - loss: 0.6537 - acc: 0.7343 - val_loss: 0.5925 - val_acc: 0.7535\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.59251 to 0.59174, saving model to best.model\n",
      "0s - loss: 0.6525 - acc: 0.7342 - val_loss: 0.5917 - val_acc: 0.7525\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.59174 to 0.59135, saving model to best.model\n",
      "0s - loss: 0.6525 - acc: 0.7331 - val_loss: 0.5913 - val_acc: 0.7542\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.59135 to 0.59001, saving model to best.model\n",
      "0s - loss: 0.6492 - acc: 0.7349 - val_loss: 0.5900 - val_acc: 0.7580\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.59001 to 0.58968, saving model to best.model\n",
      "0s - loss: 0.6509 - acc: 0.7341 - val_loss: 0.5897 - val_acc: 0.7552\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.58968 to 0.58741, saving model to best.model\n",
      "1s - loss: 0.6468 - acc: 0.7344 - val_loss: 0.5874 - val_acc: 0.7591\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84635, saving model to best.model\n",
      "0s - loss: 0.9332 - acc: 0.6185 - val_loss: 0.8464 - val_acc: 0.6581\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.8683 - acc: 0.6547 - val_loss: 0.8468 - val_acc: 0.6581\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84635 to 0.84623, saving model to best.model\n",
      "0s - loss: 0.8595 - acc: 0.6551 - val_loss: 0.8462 - val_acc: 0.6581\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.8576 - acc: 0.6551 - val_loss: 0.8466 - val_acc: 0.6581\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84623 to 0.83898, saving model to best.model\n",
      "0s - loss: 0.8538 - acc: 0.6551 - val_loss: 0.8390 - val_acc: 0.6581\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83898 to 0.83057, saving model to best.model\n",
      "0s - loss: 0.8467 - acc: 0.6551 - val_loss: 0.8306 - val_acc: 0.6581\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83057 to 0.82777, saving model to best.model\n",
      "0s - loss: 0.8422 - acc: 0.6551 - val_loss: 0.8278 - val_acc: 0.6581\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82777 to 0.82425, saving model to best.model\n",
      "0s - loss: 0.8380 - acc: 0.6551 - val_loss: 0.8243 - val_acc: 0.6581\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82425 to 0.82249, saving model to best.model\n",
      "0s - loss: 0.8378 - acc: 0.6551 - val_loss: 0.8225 - val_acc: 0.6581\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82249 to 0.82207, saving model to best.model\n",
      "0s - loss: 0.8373 - acc: 0.6551 - val_loss: 0.8221 - val_acc: 0.6581\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82207 to 0.82054, saving model to best.model\n",
      "0s - loss: 0.8334 - acc: 0.6551 - val_loss: 0.8205 - val_acc: 0.6581\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82054 to 0.82017, saving model to best.model\n",
      "0s - loss: 0.8332 - acc: 0.6550 - val_loss: 0.8202 - val_acc: 0.6581\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.8335 - acc: 0.6551 - val_loss: 0.8207 - val_acc: 0.6581\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82017 to 0.81959, saving model to best.model\n",
      "0s - loss: 0.8339 - acc: 0.6551 - val_loss: 0.8196 - val_acc: 0.6581\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81959 to 0.81908, saving model to best.model\n",
      "0s - loss: 0.8327 - acc: 0.6551 - val_loss: 0.8191 - val_acc: 0.6581\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.8326 - acc: 0.6552 - val_loss: 0.8195 - val_acc: 0.6581\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8299 - acc: 0.6550 - val_loss: 0.8196 - val_acc: 0.6581\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81908 to 0.81841, saving model to best.model\n",
      "0s - loss: 0.8312 - acc: 0.6552 - val_loss: 0.8184 - val_acc: 0.6581\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.8311 - acc: 0.6551 - val_loss: 0.8191 - val_acc: 0.6581\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81841 to 0.81697, saving model to best.model\n",
      "0s - loss: 0.8284 - acc: 0.6547 - val_loss: 0.8170 - val_acc: 0.6581\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81697 to 0.81581, saving model to best.model\n",
      "0s - loss: 0.8296 - acc: 0.6553 - val_loss: 0.8158 - val_acc: 0.6581\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81581 to 0.81540, saving model to best.model\n",
      "0s - loss: 0.8292 - acc: 0.6556 - val_loss: 0.8154 - val_acc: 0.6581\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81540 to 0.81350, saving model to best.model\n",
      "0s - loss: 0.8262 - acc: 0.6562 - val_loss: 0.8135 - val_acc: 0.6593\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81350 to 0.81298, saving model to best.model\n",
      "0s - loss: 0.8267 - acc: 0.6571 - val_loss: 0.8130 - val_acc: 0.6603\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81298 to 0.81166, saving model to best.model\n",
      "0s - loss: 0.8253 - acc: 0.6563 - val_loss: 0.8117 - val_acc: 0.6614\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81166 to 0.80997, saving model to best.model\n",
      "0s - loss: 0.8235 - acc: 0.6582 - val_loss: 0.8100 - val_acc: 0.6639\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.8239 - acc: 0.6577 - val_loss: 0.8101 - val_acc: 0.6617\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80997 to 0.80909, saving model to best.model\n",
      "0s - loss: 0.8229 - acc: 0.6585 - val_loss: 0.8091 - val_acc: 0.6623\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80909 to 0.80724, saving model to best.model\n",
      "0s - loss: 0.8221 - acc: 0.6580 - val_loss: 0.8072 - val_acc: 0.6650\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80724 to 0.80605, saving model to best.model\n",
      "0s - loss: 0.8211 - acc: 0.6600 - val_loss: 0.8060 - val_acc: 0.6649\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80605 to 0.80533, saving model to best.model\n",
      "0s - loss: 0.8195 - acc: 0.6613 - val_loss: 0.8053 - val_acc: 0.6676\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80533 to 0.80442, saving model to best.model\n",
      "0s - loss: 0.8170 - acc: 0.6615 - val_loss: 0.8044 - val_acc: 0.6638\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80442 to 0.80175, saving model to best.model\n",
      "0s - loss: 0.8164 - acc: 0.6606 - val_loss: 0.8018 - val_acc: 0.6674\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80175 to 0.79931, saving model to best.model\n",
      "0s - loss: 0.8139 - acc: 0.6610 - val_loss: 0.7993 - val_acc: 0.6691\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79931 to 0.79837, saving model to best.model\n",
      "0s - loss: 0.8125 - acc: 0.6610 - val_loss: 0.7984 - val_acc: 0.6683\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79837 to 0.79600, saving model to best.model\n",
      "0s - loss: 0.8101 - acc: 0.6628 - val_loss: 0.7960 - val_acc: 0.6687\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79600 to 0.79496, saving model to best.model\n",
      "0s - loss: 0.8104 - acc: 0.6615 - val_loss: 0.7950 - val_acc: 0.6698\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79496 to 0.79319, saving model to best.model\n",
      "0s - loss: 0.8093 - acc: 0.6640 - val_loss: 0.7932 - val_acc: 0.6696\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79319 to 0.79109, saving model to best.model\n",
      "0s - loss: 0.8081 - acc: 0.6628 - val_loss: 0.7911 - val_acc: 0.6692\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79109 to 0.78896, saving model to best.model\n",
      "0s - loss: 0.8060 - acc: 0.6653 - val_loss: 0.7890 - val_acc: 0.6724\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78896 to 0.78680, saving model to best.model\n",
      "0s - loss: 0.8061 - acc: 0.6654 - val_loss: 0.7868 - val_acc: 0.6722\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78680 to 0.78598, saving model to best.model\n",
      "0s - loss: 0.8017 - acc: 0.6671 - val_loss: 0.7860 - val_acc: 0.6756\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78598 to 0.78398, saving model to best.model\n",
      "0s - loss: 0.8000 - acc: 0.6665 - val_loss: 0.7840 - val_acc: 0.6722\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78398 to 0.78203, saving model to best.model\n",
      "0s - loss: 0.7989 - acc: 0.6678 - val_loss: 0.7820 - val_acc: 0.6749\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78203 to 0.77892, saving model to best.model\n",
      "0s - loss: 0.7988 - acc: 0.6669 - val_loss: 0.7789 - val_acc: 0.6770\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77892 to 0.77887, saving model to best.model\n",
      "0s - loss: 0.7972 - acc: 0.6675 - val_loss: 0.7789 - val_acc: 0.6782\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77887 to 0.77701, saving model to best.model\n",
      "0s - loss: 0.7939 - acc: 0.6702 - val_loss: 0.7770 - val_acc: 0.6806\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77701 to 0.77394, saving model to best.model\n",
      "0s - loss: 0.7935 - acc: 0.6698 - val_loss: 0.7739 - val_acc: 0.6795\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.7926 - acc: 0.6675 - val_loss: 0.7752 - val_acc: 0.6826\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77394 to 0.77112, saving model to best.model\n",
      "0s - loss: 0.7894 - acc: 0.6719 - val_loss: 0.7711 - val_acc: 0.6789\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77112 to 0.76838, saving model to best.model\n",
      "0s - loss: 0.7895 - acc: 0.6722 - val_loss: 0.7684 - val_acc: 0.6829\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76838 to 0.76711, saving model to best.model\n",
      "0s - loss: 0.7882 - acc: 0.6715 - val_loss: 0.7671 - val_acc: 0.6843\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76711 to 0.76545, saving model to best.model\n",
      "0s - loss: 0.7843 - acc: 0.6717 - val_loss: 0.7655 - val_acc: 0.6835\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76545 to 0.76248, saving model to best.model\n",
      "0s - loss: 0.7872 - acc: 0.6722 - val_loss: 0.7625 - val_acc: 0.6831\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76248 to 0.75694, saving model to best.model\n",
      "0s - loss: 0.7821 - acc: 0.6750 - val_loss: 0.7569 - val_acc: 0.6876\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.7813 - acc: 0.6759 - val_loss: 0.7570 - val_acc: 0.6848\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75694 to 0.75577, saving model to best.model\n",
      "0s - loss: 0.7791 - acc: 0.6761 - val_loss: 0.7558 - val_acc: 0.6863\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.7792 - acc: 0.6760 - val_loss: 0.7590 - val_acc: 0.6898\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75577 to 0.74932, saving model to best.model\n",
      "0s - loss: 0.7760 - acc: 0.6779 - val_loss: 0.7493 - val_acc: 0.6924\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.7739 - acc: 0.6765 - val_loss: 0.7510 - val_acc: 0.6858\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74932 to 0.74581, saving model to best.model\n",
      "0s - loss: 0.7724 - acc: 0.6784 - val_loss: 0.7458 - val_acc: 0.6926\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74581 to 0.74436, saving model to best.model\n",
      "0s - loss: 0.7718 - acc: 0.6806 - val_loss: 0.7444 - val_acc: 0.6945\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74436 to 0.74019, saving model to best.model\n",
      "0s - loss: 0.7686 - acc: 0.6826 - val_loss: 0.7402 - val_acc: 0.6929\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.74019 to 0.73862, saving model to best.model\n",
      "0s - loss: 0.7688 - acc: 0.6813 - val_loss: 0.7386 - val_acc: 0.6918\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73862 to 0.73329, saving model to best.model\n",
      "0s - loss: 0.7674 - acc: 0.6820 - val_loss: 0.7333 - val_acc: 0.6973\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73329 to 0.73149, saving model to best.model\n",
      "0s - loss: 0.7639 - acc: 0.6815 - val_loss: 0.7315 - val_acc: 0.6988\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.7640 - acc: 0.6825 - val_loss: 0.7325 - val_acc: 0.6958\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73149 to 0.72753, saving model to best.model\n",
      "0s - loss: 0.7601 - acc: 0.6854 - val_loss: 0.7275 - val_acc: 0.7004\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72753 to 0.72306, saving model to best.model\n",
      "0s - loss: 0.7558 - acc: 0.6862 - val_loss: 0.7231 - val_acc: 0.7048\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.7593 - acc: 0.6864 - val_loss: 0.7231 - val_acc: 0.6953\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72306 to 0.72022, saving model to best.model\n",
      "0s - loss: 0.7551 - acc: 0.6863 - val_loss: 0.7202 - val_acc: 0.7012\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72022 to 0.71875, saving model to best.model\n",
      "0s - loss: 0.7541 - acc: 0.6864 - val_loss: 0.7187 - val_acc: 0.7027\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71875 to 0.71523, saving model to best.model\n",
      "0s - loss: 0.7515 - acc: 0.6867 - val_loss: 0.7152 - val_acc: 0.7074\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71523 to 0.71081, saving model to best.model\n",
      "0s - loss: 0.7510 - acc: 0.6917 - val_loss: 0.7108 - val_acc: 0.7091\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.7480 - acc: 0.6893 - val_loss: 0.7113 - val_acc: 0.7093\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71081 to 0.70912, saving model to best.model\n",
      "0s - loss: 0.7458 - acc: 0.6936 - val_loss: 0.7091 - val_acc: 0.7150\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70912 to 0.70665, saving model to best.model\n",
      "0s - loss: 0.7437 - acc: 0.6943 - val_loss: 0.7067 - val_acc: 0.7136\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70665 to 0.70485, saving model to best.model\n",
      "0s - loss: 0.7431 - acc: 0.6939 - val_loss: 0.7049 - val_acc: 0.7158\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70485 to 0.70149, saving model to best.model\n",
      "0s - loss: 0.7408 - acc: 0.6947 - val_loss: 0.7015 - val_acc: 0.7129\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70149 to 0.69950, saving model to best.model\n",
      "0s - loss: 0.7423 - acc: 0.6922 - val_loss: 0.6995 - val_acc: 0.7150\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69950 to 0.69834, saving model to best.model\n",
      "0s - loss: 0.7393 - acc: 0.6956 - val_loss: 0.6983 - val_acc: 0.7149\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69834 to 0.69468, saving model to best.model\n",
      "0s - loss: 0.7352 - acc: 0.6966 - val_loss: 0.6947 - val_acc: 0.7157\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.7360 - acc: 0.6989 - val_loss: 0.6958 - val_acc: 0.7187\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69468 to 0.69257, saving model to best.model\n",
      "0s - loss: 0.7347 - acc: 0.6965 - val_loss: 0.6926 - val_acc: 0.7183\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69257 to 0.68603, saving model to best.model\n",
      "0s - loss: 0.7300 - acc: 0.6983 - val_loss: 0.6860 - val_acc: 0.7206\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.7331 - acc: 0.7009 - val_loss: 0.6873 - val_acc: 0.7218\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7323 - acc: 0.6969 - val_loss: 0.6898 - val_acc: 0.7164\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68603 to 0.68231, saving model to best.model\n",
      "0s - loss: 0.7301 - acc: 0.7000 - val_loss: 0.6823 - val_acc: 0.7212\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68231 to 0.68125, saving model to best.model\n",
      "0s - loss: 0.7284 - acc: 0.6997 - val_loss: 0.6813 - val_acc: 0.7213\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68125 to 0.67860, saving model to best.model\n",
      "0s - loss: 0.7253 - acc: 0.6994 - val_loss: 0.6786 - val_acc: 0.7225\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67860 to 0.67796, saving model to best.model\n",
      "0s - loss: 0.7258 - acc: 0.7011 - val_loss: 0.6780 - val_acc: 0.7219\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.7234 - acc: 0.7016 - val_loss: 0.6799 - val_acc: 0.7232\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67796 to 0.67385, saving model to best.model\n",
      "0s - loss: 0.7229 - acc: 0.7021 - val_loss: 0.6738 - val_acc: 0.7225\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67385 to 0.67132, saving model to best.model\n",
      "0s - loss: 0.7194 - acc: 0.7055 - val_loss: 0.6713 - val_acc: 0.7251\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67132 to 0.66917, saving model to best.model\n",
      "0s - loss: 0.7194 - acc: 0.7034 - val_loss: 0.6692 - val_acc: 0.7246\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7170 - acc: 0.7062 - val_loss: 0.6696 - val_acc: 0.7242\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.7179 - acc: 0.7042 - val_loss: 0.6713 - val_acc: 0.7245\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.7180 - acc: 0.7034 - val_loss: 0.6733 - val_acc: 0.7245\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66917 to 0.66893, saving model to best.model\n",
      "0s - loss: 0.7133 - acc: 0.7063 - val_loss: 0.6689 - val_acc: 0.7253\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66893 to 0.66424, saving model to best.model\n",
      "0s - loss: 0.7140 - acc: 0.7075 - val_loss: 0.6642 - val_acc: 0.7253\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7132 - acc: 0.7063 - val_loss: 0.6649 - val_acc: 0.7239\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.7097 - acc: 0.7083 - val_loss: 0.6651 - val_acc: 0.7245\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7093 - acc: 0.7094 - val_loss: 0.6643 - val_acc: 0.7273\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66424 to 0.66309, saving model to best.model\n",
      "0s - loss: 0.7101 - acc: 0.7074 - val_loss: 0.6631 - val_acc: 0.7262\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66309 to 0.66026, saving model to best.model\n",
      "0s - loss: 0.7112 - acc: 0.7059 - val_loss: 0.6603 - val_acc: 0.7265\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7063 - acc: 0.7103 - val_loss: 0.6603 - val_acc: 0.7283\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66026 to 0.65767, saving model to best.model\n",
      "0s - loss: 0.7056 - acc: 0.7084 - val_loss: 0.6577 - val_acc: 0.7282\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65767 to 0.65643, saving model to best.model\n",
      "0s - loss: 0.7044 - acc: 0.7101 - val_loss: 0.6564 - val_acc: 0.7285\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.7061 - acc: 0.7085 - val_loss: 0.6575 - val_acc: 0.7296\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65643 to 0.65420, saving model to best.model\n",
      "0s - loss: 0.7055 - acc: 0.7099 - val_loss: 0.6542 - val_acc: 0.7300\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.65420 to 0.65199, saving model to best.model\n",
      "0s - loss: 0.7010 - acc: 0.7128 - val_loss: 0.6520 - val_acc: 0.7309\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.7040 - acc: 0.7096 - val_loss: 0.6525 - val_acc: 0.7302\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65199 to 0.64828, saving model to best.model\n",
      "0s - loss: 0.7007 - acc: 0.7110 - val_loss: 0.6483 - val_acc: 0.7334\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.6994 - acc: 0.7128 - val_loss: 0.6521 - val_acc: 0.7299\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.64828 to 0.64609, saving model to best.model\n",
      "0s - loss: 0.6968 - acc: 0.7159 - val_loss: 0.6461 - val_acc: 0.7334\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.6958 - acc: 0.7138 - val_loss: 0.6493 - val_acc: 0.7333\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.6975 - acc: 0.7151 - val_loss: 0.6493 - val_acc: 0.7319\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.6975 - acc: 0.7136 - val_loss: 0.6466 - val_acc: 0.7331\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64609 to 0.64526, saving model to best.model\n",
      "0s - loss: 0.6957 - acc: 0.7149 - val_loss: 0.6453 - val_acc: 0.7326\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64526 to 0.64432, saving model to best.model\n",
      "0s - loss: 0.6954 - acc: 0.7139 - val_loss: 0.6443 - val_acc: 0.7321\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6970 - acc: 0.7119 - val_loss: 0.6451 - val_acc: 0.7328\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.64432 to 0.64216, saving model to best.model\n",
      "0s - loss: 0.6942 - acc: 0.7150 - val_loss: 0.6422 - val_acc: 0.7347\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6928 - acc: 0.7144 - val_loss: 0.6435 - val_acc: 0.7347\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64216 to 0.64108, saving model to best.model\n",
      "0s - loss: 0.6923 - acc: 0.7158 - val_loss: 0.6411 - val_acc: 0.7334\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6897 - acc: 0.7164 - val_loss: 0.6421 - val_acc: 0.7340\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.6910 - acc: 0.7164 - val_loss: 0.6444 - val_acc: 0.7355\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.64108 to 0.63706, saving model to best.model\n",
      "0s - loss: 0.6881 - acc: 0.7173 - val_loss: 0.6371 - val_acc: 0.7352\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6892 - acc: 0.7185 - val_loss: 0.6378 - val_acc: 0.7362\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6899 - acc: 0.7190 - val_loss: 0.6391 - val_acc: 0.7344\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63706 to 0.63139, saving model to best.model\n",
      "0s - loss: 0.6833 - acc: 0.7206 - val_loss: 0.6314 - val_acc: 0.7374\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6849 - acc: 0.7165 - val_loss: 0.6337 - val_acc: 0.7383\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6843 - acc: 0.7182 - val_loss: 0.6329 - val_acc: 0.7420\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6864 - acc: 0.7199 - val_loss: 0.6359 - val_acc: 0.7364\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6844 - acc: 0.7183 - val_loss: 0.6339 - val_acc: 0.7397\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6827 - acc: 0.7197 - val_loss: 0.6324 - val_acc: 0.7385\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63139 to 0.63048, saving model to best.model\n",
      "0s - loss: 0.6835 - acc: 0.7207 - val_loss: 0.6305 - val_acc: 0.7398\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6827 - acc: 0.7188 - val_loss: 0.6314 - val_acc: 0.7379\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6826 - acc: 0.7210 - val_loss: 0.6332 - val_acc: 0.7362\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.63048 to 0.62730, saving model to best.model\n",
      "0s - loss: 0.6826 - acc: 0.7203 - val_loss: 0.6273 - val_acc: 0.7392\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6763 - acc: 0.7220 - val_loss: 0.6293 - val_acc: 0.7391\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62730 to 0.62451, saving model to best.model\n",
      "0s - loss: 0.6790 - acc: 0.7223 - val_loss: 0.6245 - val_acc: 0.7446\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6763 - acc: 0.7236 - val_loss: 0.6261 - val_acc: 0.7438\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6781 - acc: 0.7205 - val_loss: 0.6262 - val_acc: 0.7409\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6760 - acc: 0.7246 - val_loss: 0.6259 - val_acc: 0.7424\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6776 - acc: 0.7203 - val_loss: 0.6259 - val_acc: 0.7389\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62451 to 0.62063, saving model to best.model\n",
      "0s - loss: 0.6737 - acc: 0.7257 - val_loss: 0.6206 - val_acc: 0.7458\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6757 - acc: 0.7216 - val_loss: 0.6223 - val_acc: 0.7418\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6726 - acc: 0.7254 - val_loss: 0.6218 - val_acc: 0.7439\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6724 - acc: 0.7231 - val_loss: 0.6222 - val_acc: 0.7425\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62063 to 0.61946, saving model to best.model\n",
      "0s - loss: 0.6691 - acc: 0.7259 - val_loss: 0.6195 - val_acc: 0.7444\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6727 - acc: 0.7232 - val_loss: 0.6217 - val_acc: 0.7412\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.61946 to 0.61911, saving model to best.model\n",
      "0s - loss: 0.6733 - acc: 0.7234 - val_loss: 0.6191 - val_acc: 0.7438\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.61911 to 0.61807, saving model to best.model\n",
      "1s - loss: 0.6686 - acc: 0.7265 - val_loss: 0.6181 - val_acc: 0.7443\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.6693 - acc: 0.7268 - val_loss: 0.6193 - val_acc: 0.7416\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6709 - acc: 0.7272 - val_loss: 0.6191 - val_acc: 0.7443\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.61807 to 0.61789, saving model to best.model\n",
      "0s - loss: 0.6684 - acc: 0.7245 - val_loss: 0.6179 - val_acc: 0.7450\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6693 - acc: 0.7261 - val_loss: 0.6181 - val_acc: 0.7430\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.61789 to 0.61753, saving model to best.model\n",
      "0s - loss: 0.6690 - acc: 0.7259 - val_loss: 0.6175 - val_acc: 0.7447\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61753 to 0.61685, saving model to best.model\n",
      "0s - loss: 0.6701 - acc: 0.7260 - val_loss: 0.6168 - val_acc: 0.7434\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.61685 to 0.61499, saving model to best.model\n",
      "0s - loss: 0.6680 - acc: 0.7256 - val_loss: 0.6150 - val_acc: 0.7453\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6686 - acc: 0.7229 - val_loss: 0.6168 - val_acc: 0.7430\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61499 to 0.61435, saving model to best.model\n",
      "0s - loss: 0.6668 - acc: 0.7277 - val_loss: 0.6144 - val_acc: 0.7471\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.61435 to 0.61164, saving model to best.model\n",
      "0s - loss: 0.6643 - acc: 0.7299 - val_loss: 0.6116 - val_acc: 0.7482\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6637 - acc: 0.7273 - val_loss: 0.6117 - val_acc: 0.7478\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6619 - acc: 0.7306 - val_loss: 0.6119 - val_acc: 0.7493\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6637 - acc: 0.7258 - val_loss: 0.6130 - val_acc: 0.7521\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61164 to 0.60901, saving model to best.model\n",
      "0s - loss: 0.6624 - acc: 0.7283 - val_loss: 0.6090 - val_acc: 0.7491\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6630 - acc: 0.7288 - val_loss: 0.6111 - val_acc: 0.7489\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6625 - acc: 0.7284 - val_loss: 0.6100 - val_acc: 0.7478\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6647 - acc: 0.7277 - val_loss: 0.6103 - val_acc: 0.7491\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.60901 to 0.60812, saving model to best.model\n",
      "0s - loss: 0.6590 - acc: 0.7287 - val_loss: 0.6081 - val_acc: 0.7478\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6599 - acc: 0.7299 - val_loss: 0.6106 - val_acc: 0.7475\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6613 - acc: 0.7310 - val_loss: 0.6108 - val_acc: 0.7485\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6608 - acc: 0.7311 - val_loss: 0.6104 - val_acc: 0.7475\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.60812 to 0.60743, saving model to best.model\n",
      "0s - loss: 0.6597 - acc: 0.7271 - val_loss: 0.6074 - val_acc: 0.7481\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6607 - acc: 0.7294 - val_loss: 0.6101 - val_acc: 0.7465\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.60743 to 0.60519, saving model to best.model\n",
      "0s - loss: 0.6574 - acc: 0.7292 - val_loss: 0.6052 - val_acc: 0.7500\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.60519 to 0.60368, saving model to best.model\n",
      "0s - loss: 0.6544 - acc: 0.7305 - val_loss: 0.6037 - val_acc: 0.7516\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6576 - acc: 0.7306 - val_loss: 0.6052 - val_acc: 0.7513\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.6550 - acc: 0.7324 - val_loss: 0.6060 - val_acc: 0.7515\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6581 - acc: 0.7310 - val_loss: 0.6058 - val_acc: 0.7515\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6570 - acc: 0.7311 - val_loss: 0.6055 - val_acc: 0.7529\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6549 - acc: 0.7317 - val_loss: 0.6053 - val_acc: 0.7484\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6560 - acc: 0.7319 - val_loss: 0.6042 - val_acc: 0.7485\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60368 to 0.60294, saving model to best.model\n",
      "0s - loss: 0.6555 - acc: 0.7298 - val_loss: 0.6029 - val_acc: 0.7509\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6531 - acc: 0.7311 - val_loss: 0.6063 - val_acc: 0.7505\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60294 to 0.60260, saving model to best.model\n",
      "0s - loss: 0.6555 - acc: 0.7329 - val_loss: 0.6026 - val_acc: 0.7518\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6550 - acc: 0.7321 - val_loss: 0.6027 - val_acc: 0.7493\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60260 to 0.59828, saving model to best.model\n",
      "0s - loss: 0.6493 - acc: 0.7349 - val_loss: 0.5983 - val_acc: 0.7548\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6500 - acc: 0.7351 - val_loss: 0.6023 - val_acc: 0.7504\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6508 - acc: 0.7346 - val_loss: 0.6009 - val_acc: 0.7514\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6510 - acc: 0.7340 - val_loss: 0.5993 - val_acc: 0.7533\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6528 - acc: 0.7318 - val_loss: 0.5993 - val_acc: 0.7521\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6508 - acc: 0.7345 - val_loss: 0.6015 - val_acc: 0.7518\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6521 - acc: 0.7325 - val_loss: 0.6005 - val_acc: 0.7533\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.59828 to 0.59709, saving model to best.model\n",
      "0s - loss: 0.6479 - acc: 0.7344 - val_loss: 0.5971 - val_acc: 0.7550\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.59709 to 0.59649, saving model to best.model\n",
      "0s - loss: 0.6487 - acc: 0.7361 - val_loss: 0.5965 - val_acc: 0.7554\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6480 - acc: 0.7356 - val_loss: 0.5969 - val_acc: 0.7533\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.59649 to 0.59390, saving model to best.model\n",
      "0s - loss: 0.6475 - acc: 0.7370 - val_loss: 0.5939 - val_acc: 0.7575\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6466 - acc: 0.7334 - val_loss: 0.5983 - val_acc: 0.7560\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84842, saving model to best.model\n",
      "0s - loss: 0.9325 - acc: 0.6212 - val_loss: 0.8484 - val_acc: 0.6529\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84842 to 0.84672, saving model to best.model\n",
      "0s - loss: 0.8592 - acc: 0.6598 - val_loss: 0.8467 - val_acc: 0.6529\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84672 to 0.84525, saving model to best.model\n",
      "0s - loss: 0.8529 - acc: 0.6611 - val_loss: 0.8452 - val_acc: 0.6529\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84525 to 0.84282, saving model to best.model\n",
      "0s - loss: 0.8464 - acc: 0.6612 - val_loss: 0.8428 - val_acc: 0.6529\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84282 to 0.83627, saving model to best.model\n",
      "0s - loss: 0.8418 - acc: 0.6612 - val_loss: 0.8363 - val_acc: 0.6529\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83627 to 0.83325, saving model to best.model\n",
      "0s - loss: 0.8367 - acc: 0.6612 - val_loss: 0.8333 - val_acc: 0.6529\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83325 to 0.82939, saving model to best.model\n",
      "0s - loss: 0.8342 - acc: 0.6611 - val_loss: 0.8294 - val_acc: 0.6529\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82939 to 0.82784, saving model to best.model\n",
      "0s - loss: 0.8312 - acc: 0.6611 - val_loss: 0.8278 - val_acc: 0.6529\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82784 to 0.82691, saving model to best.model\n",
      "0s - loss: 0.8305 - acc: 0.6612 - val_loss: 0.8269 - val_acc: 0.6529\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82691 to 0.82580, saving model to best.model\n",
      "0s - loss: 0.8277 - acc: 0.6611 - val_loss: 0.8258 - val_acc: 0.6529\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82580 to 0.82542, saving model to best.model\n",
      "0s - loss: 0.8258 - acc: 0.6613 - val_loss: 0.8254 - val_acc: 0.6529\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82542 to 0.82475, saving model to best.model\n",
      "0s - loss: 0.8253 - acc: 0.6612 - val_loss: 0.8248 - val_acc: 0.6529\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82475 to 0.82452, saving model to best.model\n",
      "0s - loss: 0.8239 - acc: 0.6611 - val_loss: 0.8245 - val_acc: 0.6529\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82452 to 0.82339, saving model to best.model\n",
      "0s - loss: 0.8227 - acc: 0.6611 - val_loss: 0.8234 - val_acc: 0.6529\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8235 - acc: 0.6608 - val_loss: 0.8244 - val_acc: 0.6529\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82339 to 0.82248, saving model to best.model\n",
      "0s - loss: 0.8218 - acc: 0.6614 - val_loss: 0.8225 - val_acc: 0.6529\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82248 to 0.82231, saving model to best.model\n",
      "1s - loss: 0.8215 - acc: 0.6609 - val_loss: 0.8223 - val_acc: 0.6529\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82231 to 0.82095, saving model to best.model\n",
      "0s - loss: 0.8204 - acc: 0.6609 - val_loss: 0.8209 - val_acc: 0.6529\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.82095 to 0.82022, saving model to best.model\n",
      "1s - loss: 0.8195 - acc: 0.6609 - val_loss: 0.8202 - val_acc: 0.6529\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82022 to 0.81985, saving model to best.model\n",
      "0s - loss: 0.8189 - acc: 0.6606 - val_loss: 0.8198 - val_acc: 0.6529\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81985 to 0.81845, saving model to best.model\n",
      "0s - loss: 0.8168 - acc: 0.6611 - val_loss: 0.8184 - val_acc: 0.6529\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81845 to 0.81763, saving model to best.model\n",
      "0s - loss: 0.8172 - acc: 0.6618 - val_loss: 0.8176 - val_acc: 0.6529\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.8160 - acc: 0.6618 - val_loss: 0.8177 - val_acc: 0.6529\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81763 to 0.81600, saving model to best.model\n",
      "0s - loss: 0.8153 - acc: 0.6620 - val_loss: 0.8160 - val_acc: 0.6532\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81600 to 0.81458, saving model to best.model\n",
      "0s - loss: 0.8145 - acc: 0.6615 - val_loss: 0.8146 - val_acc: 0.6537\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81458 to 0.81306, saving model to best.model\n",
      "0s - loss: 0.8130 - acc: 0.6627 - val_loss: 0.8131 - val_acc: 0.6547\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81306 to 0.81167, saving model to best.model\n",
      "0s - loss: 0.8118 - acc: 0.6631 - val_loss: 0.8117 - val_acc: 0.6569\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81167 to 0.81081, saving model to best.model\n",
      "0s - loss: 0.8098 - acc: 0.6650 - val_loss: 0.8108 - val_acc: 0.6573\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81081 to 0.80949, saving model to best.model\n",
      "0s - loss: 0.8093 - acc: 0.6635 - val_loss: 0.8095 - val_acc: 0.6585\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80949 to 0.80733, saving model to best.model\n",
      "0s - loss: 0.8072 - acc: 0.6650 - val_loss: 0.8073 - val_acc: 0.6611\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80733 to 0.80677, saving model to best.model\n",
      "0s - loss: 0.8084 - acc: 0.6643 - val_loss: 0.8068 - val_acc: 0.6631\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80677 to 0.80522, saving model to best.model\n",
      "0s - loss: 0.8070 - acc: 0.6653 - val_loss: 0.8052 - val_acc: 0.6626\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80522 to 0.80253, saving model to best.model\n",
      "0s - loss: 0.8039 - acc: 0.6658 - val_loss: 0.8025 - val_acc: 0.6648\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80253 to 0.80168, saving model to best.model\n",
      "0s - loss: 0.8034 - acc: 0.6656 - val_loss: 0.8017 - val_acc: 0.6639\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80168 to 0.79910, saving model to best.model\n",
      "0s - loss: 0.8015 - acc: 0.6662 - val_loss: 0.7991 - val_acc: 0.6652\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79910 to 0.79785, saving model to best.model\n",
      "0s - loss: 0.8013 - acc: 0.6673 - val_loss: 0.7979 - val_acc: 0.6663\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79785 to 0.79738, saving model to best.model\n",
      "0s - loss: 0.8002 - acc: 0.6663 - val_loss: 0.7974 - val_acc: 0.6625\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79738 to 0.79352, saving model to best.model\n",
      "1s - loss: 0.7983 - acc: 0.6665 - val_loss: 0.7935 - val_acc: 0.6680\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79352 to 0.79274, saving model to best.model\n",
      "0s - loss: 0.7963 - acc: 0.6668 - val_loss: 0.7927 - val_acc: 0.6662\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79274 to 0.79266, saving model to best.model\n",
      "0s - loss: 0.7950 - acc: 0.6667 - val_loss: 0.7927 - val_acc: 0.6642\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79266 to 0.78850, saving model to best.model\n",
      "0s - loss: 0.7923 - acc: 0.6699 - val_loss: 0.7885 - val_acc: 0.6701\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78850 to 0.78786, saving model to best.model\n",
      "0s - loss: 0.7920 - acc: 0.6675 - val_loss: 0.7879 - val_acc: 0.6657\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78786 to 0.78486, saving model to best.model\n",
      "0s - loss: 0.7901 - acc: 0.6695 - val_loss: 0.7849 - val_acc: 0.6712\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78486 to 0.78389, saving model to best.model\n",
      "0s - loss: 0.7915 - acc: 0.6696 - val_loss: 0.7839 - val_acc: 0.6698\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78389 to 0.78123, saving model to best.model\n",
      "0s - loss: 0.7894 - acc: 0.6688 - val_loss: 0.7812 - val_acc: 0.6727\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78123 to 0.77806, saving model to best.model\n",
      "0s - loss: 0.7859 - acc: 0.6708 - val_loss: 0.7781 - val_acc: 0.6745\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77806 to 0.77551, saving model to best.model\n",
      "1s - loss: 0.7872 - acc: 0.6722 - val_loss: 0.7755 - val_acc: 0.6779\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.7848 - acc: 0.6717 - val_loss: 0.7759 - val_acc: 0.6746\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77551 to 0.77365, saving model to best.model\n",
      "0s - loss: 0.7819 - acc: 0.6719 - val_loss: 0.7737 - val_acc: 0.6762\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77365 to 0.76785, saving model to best.model\n",
      "0s - loss: 0.7805 - acc: 0.6734 - val_loss: 0.7678 - val_acc: 0.6823\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76785 to 0.76716, saving model to best.model\n",
      "0s - loss: 0.7788 - acc: 0.6728 - val_loss: 0.7672 - val_acc: 0.6814\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.7788 - acc: 0.6722 - val_loss: 0.7697 - val_acc: 0.6746\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76716 to 0.76224, saving model to best.model\n",
      "0s - loss: 0.7748 - acc: 0.6764 - val_loss: 0.7622 - val_acc: 0.6850\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76224 to 0.76168, saving model to best.model\n",
      "0s - loss: 0.7734 - acc: 0.6766 - val_loss: 0.7617 - val_acc: 0.6864\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76168 to 0.75985, saving model to best.model\n",
      "0s - loss: 0.7698 - acc: 0.6781 - val_loss: 0.7598 - val_acc: 0.6817\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75985 to 0.75557, saving model to best.model\n",
      "0s - loss: 0.7711 - acc: 0.6787 - val_loss: 0.7556 - val_acc: 0.6886\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75557 to 0.75217, saving model to best.model\n",
      "0s - loss: 0.7679 - acc: 0.6800 - val_loss: 0.7522 - val_acc: 0.6927\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75217 to 0.74798, saving model to best.model\n",
      "0s - loss: 0.7668 - acc: 0.6809 - val_loss: 0.7480 - val_acc: 0.6932\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74798 to 0.74668, saving model to best.model\n",
      "0s - loss: 0.7644 - acc: 0.6797 - val_loss: 0.7467 - val_acc: 0.6944\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74668 to 0.74454, saving model to best.model\n",
      "0s - loss: 0.7650 - acc: 0.6813 - val_loss: 0.7445 - val_acc: 0.6957\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74454 to 0.74137, saving model to best.model\n",
      "0s - loss: 0.7615 - acc: 0.6852 - val_loss: 0.7414 - val_acc: 0.6959\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74137 to 0.74009, saving model to best.model\n",
      "0s - loss: 0.7589 - acc: 0.6847 - val_loss: 0.7401 - val_acc: 0.6959\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74009 to 0.73691, saving model to best.model\n",
      "0s - loss: 0.7597 - acc: 0.6835 - val_loss: 0.7369 - val_acc: 0.6996\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73691 to 0.73323, saving model to best.model\n",
      "0s - loss: 0.7561 - acc: 0.6869 - val_loss: 0.7332 - val_acc: 0.7023\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.7564 - acc: 0.6867 - val_loss: 0.7349 - val_acc: 0.6957\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73323 to 0.73111, saving model to best.model\n",
      "0s - loss: 0.7520 - acc: 0.6873 - val_loss: 0.7311 - val_acc: 0.6996\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73111 to 0.73020, saving model to best.model\n",
      "0s - loss: 0.7526 - acc: 0.6881 - val_loss: 0.7302 - val_acc: 0.7005\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73020 to 0.72691, saving model to best.model\n",
      "1s - loss: 0.7505 - acc: 0.6897 - val_loss: 0.7269 - val_acc: 0.7025\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72691 to 0.72575, saving model to best.model\n",
      "0s - loss: 0.7508 - acc: 0.6905 - val_loss: 0.7257 - val_acc: 0.7019\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72575 to 0.72050, saving model to best.model\n",
      "0s - loss: 0.7473 - acc: 0.6917 - val_loss: 0.7205 - val_acc: 0.7067\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72050 to 0.71989, saving model to best.model\n",
      "0s - loss: 0.7450 - acc: 0.6926 - val_loss: 0.7199 - val_acc: 0.7068\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71989 to 0.71646, saving model to best.model\n",
      "0s - loss: 0.7423 - acc: 0.6958 - val_loss: 0.7165 - val_acc: 0.7087\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.7411 - acc: 0.6946 - val_loss: 0.7175 - val_acc: 0.7084\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71646 to 0.71641, saving model to best.model\n",
      "0s - loss: 0.7385 - acc: 0.6967 - val_loss: 0.7164 - val_acc: 0.7100\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71641 to 0.71404, saving model to best.model\n",
      "0s - loss: 0.7382 - acc: 0.6978 - val_loss: 0.7140 - val_acc: 0.7094\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71404 to 0.71402, saving model to best.model\n",
      "0s - loss: 0.7388 - acc: 0.6965 - val_loss: 0.7140 - val_acc: 0.7088\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.71402 to 0.70915, saving model to best.model\n",
      "0s - loss: 0.7374 - acc: 0.6966 - val_loss: 0.7091 - val_acc: 0.7126\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.7358 - acc: 0.6963 - val_loss: 0.7102 - val_acc: 0.7097\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70915 to 0.70860, saving model to best.model\n",
      "0s - loss: 0.7339 - acc: 0.6993 - val_loss: 0.7086 - val_acc: 0.7126\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.7344 - acc: 0.6991 - val_loss: 0.7095 - val_acc: 0.7094\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70860 to 0.70575, saving model to best.model\n",
      "0s - loss: 0.7333 - acc: 0.6992 - val_loss: 0.7057 - val_acc: 0.7135\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.70575 to 0.70504, saving model to best.model\n",
      "0s - loss: 0.7300 - acc: 0.6984 - val_loss: 0.7050 - val_acc: 0.7118\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.70504 to 0.70181, saving model to best.model\n",
      "0s - loss: 0.7308 - acc: 0.6993 - val_loss: 0.7018 - val_acc: 0.7170\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.70181 to 0.69962, saving model to best.model\n",
      "0s - loss: 0.7317 - acc: 0.6991 - val_loss: 0.6996 - val_acc: 0.7165\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7281 - acc: 0.7022 - val_loss: 0.7007 - val_acc: 0.7179\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69962 to 0.69945, saving model to best.model\n",
      "0s - loss: 0.7256 - acc: 0.7036 - val_loss: 0.6995 - val_acc: 0.7149\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.69945 to 0.69601, saving model to best.model\n",
      "0s - loss: 0.7279 - acc: 0.7003 - val_loss: 0.6960 - val_acc: 0.7173\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.7242 - acc: 0.7028 - val_loss: 0.6964 - val_acc: 0.7172\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.69601 to 0.69198, saving model to best.model\n",
      "0s - loss: 0.7212 - acc: 0.7049 - val_loss: 0.6920 - val_acc: 0.7199\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69198 to 0.69023, saving model to best.model\n",
      "0s - loss: 0.7212 - acc: 0.7042 - val_loss: 0.6902 - val_acc: 0.7193\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.69023 to 0.68894, saving model to best.model\n",
      "0s - loss: 0.7232 - acc: 0.7033 - val_loss: 0.6889 - val_acc: 0.7194\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68894 to 0.68817, saving model to best.model\n",
      "0s - loss: 0.7199 - acc: 0.7046 - val_loss: 0.6882 - val_acc: 0.7200\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.7192 - acc: 0.7074 - val_loss: 0.6925 - val_acc: 0.7163\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68817 to 0.68582, saving model to best.model\n",
      "0s - loss: 0.7182 - acc: 0.7035 - val_loss: 0.6858 - val_acc: 0.7203\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.68582 to 0.68457, saving model to best.model\n",
      "0s - loss: 0.7173 - acc: 0.7089 - val_loss: 0.6846 - val_acc: 0.7201\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7156 - acc: 0.7080 - val_loss: 0.6851 - val_acc: 0.7194\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.68457 to 0.68290, saving model to best.model\n",
      "0s - loss: 0.7133 - acc: 0.7090 - val_loss: 0.6829 - val_acc: 0.7217\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.7132 - acc: 0.7080 - val_loss: 0.6838 - val_acc: 0.7208\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.68290 to 0.68043, saving model to best.model\n",
      "0s - loss: 0.7162 - acc: 0.7069 - val_loss: 0.6804 - val_acc: 0.7226\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.68043 to 0.67822, saving model to best.model\n",
      "0s - loss: 0.7127 - acc: 0.7069 - val_loss: 0.6782 - val_acc: 0.7231\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7133 - acc: 0.7106 - val_loss: 0.6788 - val_acc: 0.7220\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67822 to 0.67323, saving model to best.model\n",
      "0s - loss: 0.7086 - acc: 0.7112 - val_loss: 0.6732 - val_acc: 0.7262\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7105 - acc: 0.7084 - val_loss: 0.6755 - val_acc: 0.7238\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.67323 to 0.67309, saving model to best.model\n",
      "0s - loss: 0.7055 - acc: 0.7118 - val_loss: 0.6731 - val_acc: 0.7239\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.67309 to 0.67264, saving model to best.model\n",
      "0s - loss: 0.7058 - acc: 0.7131 - val_loss: 0.6726 - val_acc: 0.7260\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.67264 to 0.67193, saving model to best.model\n",
      "0s - loss: 0.7067 - acc: 0.7112 - val_loss: 0.6719 - val_acc: 0.7252\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.67193 to 0.67090, saving model to best.model\n",
      "0s - loss: 0.7040 - acc: 0.7143 - val_loss: 0.6709 - val_acc: 0.7249\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.67090 to 0.66621, saving model to best.model\n",
      "0s - loss: 0.7018 - acc: 0.7112 - val_loss: 0.6662 - val_acc: 0.7286\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.7020 - acc: 0.7140 - val_loss: 0.6709 - val_acc: 0.7228\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.7013 - acc: 0.7132 - val_loss: 0.6665 - val_acc: 0.7249\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.6993 - acc: 0.7139 - val_loss: 0.6674 - val_acc: 0.7270\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.66621 to 0.66286, saving model to best.model\n",
      "0s - loss: 0.6986 - acc: 0.7151 - val_loss: 0.6629 - val_acc: 0.7278\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6990 - acc: 0.7149 - val_loss: 0.6673 - val_acc: 0.7253\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.66286 to 0.66271, saving model to best.model\n",
      "0s - loss: 0.6976 - acc: 0.7146 - val_loss: 0.6627 - val_acc: 0.7266\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6976 - acc: 0.7175 - val_loss: 0.6629 - val_acc: 0.7279\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.66271 to 0.66066, saving model to best.model\n",
      "0s - loss: 0.6971 - acc: 0.7136 - val_loss: 0.6607 - val_acc: 0.7286\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.66066 to 0.66023, saving model to best.model\n",
      "0s - loss: 0.6957 - acc: 0.7152 - val_loss: 0.6602 - val_acc: 0.7285\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.66023 to 0.65543, saving model to best.model\n",
      "1s - loss: 0.6934 - acc: 0.7166 - val_loss: 0.6554 - val_acc: 0.7317\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6937 - acc: 0.7163 - val_loss: 0.6563 - val_acc: 0.7299\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6938 - acc: 0.7166 - val_loss: 0.6563 - val_acc: 0.7307\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65543 to 0.65365, saving model to best.model\n",
      "1s - loss: 0.6918 - acc: 0.7193 - val_loss: 0.6536 - val_acc: 0.7313\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.65365 to 0.65320, saving model to best.model\n",
      "0s - loss: 0.6935 - acc: 0.7161 - val_loss: 0.6532 - val_acc: 0.7327\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6920 - acc: 0.7178 - val_loss: 0.6552 - val_acc: 0.7288\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6898 - acc: 0.7191 - val_loss: 0.6541 - val_acc: 0.7290\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.65320 to 0.65207, saving model to best.model\n",
      "0s - loss: 0.6903 - acc: 0.7177 - val_loss: 0.6521 - val_acc: 0.7301\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.65207 to 0.65154, saving model to best.model\n",
      "0s - loss: 0.6874 - acc: 0.7209 - val_loss: 0.6515 - val_acc: 0.7343\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.6901 - acc: 0.7168 - val_loss: 0.6547 - val_acc: 0.7269\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.65154 to 0.64599, saving model to best.model\n",
      "0s - loss: 0.6863 - acc: 0.7220 - val_loss: 0.6460 - val_acc: 0.7365\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6814 - acc: 0.7205 - val_loss: 0.6481 - val_acc: 0.7324\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64599 to 0.64351, saving model to best.model\n",
      "0s - loss: 0.6862 - acc: 0.7199 - val_loss: 0.6435 - val_acc: 0.7367\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6870 - acc: 0.7201 - val_loss: 0.6470 - val_acc: 0.7334\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6834 - acc: 0.7204 - val_loss: 0.6437 - val_acc: 0.7354\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6831 - acc: 0.7197 - val_loss: 0.6438 - val_acc: 0.7350\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6817 - acc: 0.7209 - val_loss: 0.6460 - val_acc: 0.7319\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.64351 to 0.64308, saving model to best.model\n",
      "0s - loss: 0.6838 - acc: 0.7205 - val_loss: 0.6431 - val_acc: 0.7344\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64308 to 0.64191, saving model to best.model\n",
      "1s - loss: 0.6823 - acc: 0.7224 - val_loss: 0.6419 - val_acc: 0.7359\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.64191 to 0.63986, saving model to best.model\n",
      "0s - loss: 0.6805 - acc: 0.7205 - val_loss: 0.6399 - val_acc: 0.7386\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6824 - acc: 0.7215 - val_loss: 0.6439 - val_acc: 0.7320\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6801 - acc: 0.7224 - val_loss: 0.6414 - val_acc: 0.7392\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63986 to 0.63901, saving model to best.model\n",
      "0s - loss: 0.6794 - acc: 0.7221 - val_loss: 0.6390 - val_acc: 0.7367\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63901 to 0.63856, saving model to best.model\n",
      "0s - loss: 0.6810 - acc: 0.7198 - val_loss: 0.6386 - val_acc: 0.7368\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6808 - acc: 0.7208 - val_loss: 0.6392 - val_acc: 0.7368\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63856 to 0.63690, saving model to best.model\n",
      "0s - loss: 0.6786 - acc: 0.7237 - val_loss: 0.6369 - val_acc: 0.7411\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63690 to 0.63525, saving model to best.model\n",
      "0s - loss: 0.6779 - acc: 0.7212 - val_loss: 0.6353 - val_acc: 0.7382\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6785 - acc: 0.7219 - val_loss: 0.6385 - val_acc: 0.7386\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6797 - acc: 0.7234 - val_loss: 0.6386 - val_acc: 0.7348\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6763 - acc: 0.7215 - val_loss: 0.6374 - val_acc: 0.7349\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6766 - acc: 0.7213 - val_loss: 0.6383 - val_acc: 0.7365\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6761 - acc: 0.7223 - val_loss: 0.6357 - val_acc: 0.7358\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.63525 to 0.63489, saving model to best.model\n",
      "0s - loss: 0.6705 - acc: 0.7275 - val_loss: 0.6349 - val_acc: 0.7363\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.63489 to 0.63335, saving model to best.model\n",
      "0s - loss: 0.6732 - acc: 0.7259 - val_loss: 0.6333 - val_acc: 0.7375\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6742 - acc: 0.7249 - val_loss: 0.6347 - val_acc: 0.7368\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.63335 to 0.63296, saving model to best.model\n",
      "0s - loss: 0.6720 - acc: 0.7254 - val_loss: 0.6330 - val_acc: 0.7374\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.63296 to 0.63190, saving model to best.model\n",
      "0s - loss: 0.6709 - acc: 0.7239 - val_loss: 0.6319 - val_acc: 0.7371\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.63190 to 0.63028, saving model to best.model\n",
      "0s - loss: 0.6732 - acc: 0.7256 - val_loss: 0.6303 - val_acc: 0.7412\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6696 - acc: 0.7260 - val_loss: 0.6308 - val_acc: 0.7388\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6686 - acc: 0.7277 - val_loss: 0.6314 - val_acc: 0.7365\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.63028 to 0.62996, saving model to best.model\n",
      "0s - loss: 0.6703 - acc: 0.7248 - val_loss: 0.6300 - val_acc: 0.7377\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62996 to 0.62830, saving model to best.model\n",
      "0s - loss: 0.6681 - acc: 0.7261 - val_loss: 0.6283 - val_acc: 0.7392\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.62830 to 0.62774, saving model to best.model\n",
      "0s - loss: 0.6686 - acc: 0.7281 - val_loss: 0.6277 - val_acc: 0.7426\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62774 to 0.62624, saving model to best.model\n",
      "0s - loss: 0.6695 - acc: 0.7246 - val_loss: 0.6262 - val_acc: 0.7439\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6708 - acc: 0.7247 - val_loss: 0.6299 - val_acc: 0.7399\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6688 - acc: 0.7255 - val_loss: 0.6271 - val_acc: 0.7410\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7265 - val_loss: 0.6281 - val_acc: 0.7382\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.62624 to 0.62492, saving model to best.model\n",
      "0s - loss: 0.6656 - acc: 0.7283 - val_loss: 0.6249 - val_acc: 0.7418\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6667 - acc: 0.7250 - val_loss: 0.6268 - val_acc: 0.7407\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.62492 to 0.62452, saving model to best.model\n",
      "0s - loss: 0.6691 - acc: 0.7258 - val_loss: 0.6245 - val_acc: 0.7411\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.62452 to 0.62449, saving model to best.model\n",
      "0s - loss: 0.6646 - acc: 0.7259 - val_loss: 0.6245 - val_acc: 0.7432\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6642 - acc: 0.7265 - val_loss: 0.6259 - val_acc: 0.7406\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6649 - acc: 0.7256 - val_loss: 0.6274 - val_acc: 0.7388\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.62449 to 0.62234, saving model to best.model\n",
      "0s - loss: 0.6613 - acc: 0.7309 - val_loss: 0.6223 - val_acc: 0.7461\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6627 - acc: 0.7277 - val_loss: 0.6235 - val_acc: 0.7405\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6644 - acc: 0.7280 - val_loss: 0.6236 - val_acc: 0.7410\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.62234 to 0.62125, saving model to best.model\n",
      "0s - loss: 0.6641 - acc: 0.7285 - val_loss: 0.6212 - val_acc: 0.7443\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.62125 to 0.61932, saving model to best.model\n",
      "0s - loss: 0.6602 - acc: 0.7296 - val_loss: 0.6193 - val_acc: 0.7500\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6582 - acc: 0.7303 - val_loss: 0.6199 - val_acc: 0.7460\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6612 - acc: 0.7288 - val_loss: 0.6226 - val_acc: 0.7396\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.6620 - acc: 0.7304 - val_loss: 0.6204 - val_acc: 0.7427\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61932 to 0.61856, saving model to best.model\n",
      "0s - loss: 0.6609 - acc: 0.7301 - val_loss: 0.6186 - val_acc: 0.7466\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.6602 - acc: 0.7295 - val_loss: 0.6193 - val_acc: 0.7444\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.61856 to 0.61852, saving model to best.model\n",
      "0s - loss: 0.6605 - acc: 0.7317 - val_loss: 0.6185 - val_acc: 0.7447\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6574 - acc: 0.7316 - val_loss: 0.6191 - val_acc: 0.7438\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.61852 to 0.61565, saving model to best.model\n",
      "0s - loss: 0.6574 - acc: 0.7300 - val_loss: 0.6156 - val_acc: 0.7478\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6578 - acc: 0.7319 - val_loss: 0.6165 - val_acc: 0.7456\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6582 - acc: 0.7307 - val_loss: 0.6160 - val_acc: 0.7472\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6581 - acc: 0.7307 - val_loss: 0.6164 - val_acc: 0.7461\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.6576 - acc: 0.7307 - val_loss: 0.6179 - val_acc: 0.7444\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6594 - acc: 0.7287 - val_loss: 0.6163 - val_acc: 0.7451\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.61565 to 0.61394, saving model to best.model\n",
      "0s - loss: 0.6563 - acc: 0.7306 - val_loss: 0.6139 - val_acc: 0.7493\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6587 - acc: 0.7301 - val_loss: 0.6153 - val_acc: 0.7454\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6572 - acc: 0.7307 - val_loss: 0.6186 - val_acc: 0.7463\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.61394 to 0.61330, saving model to best.model\n",
      "0s - loss: 0.6531 - acc: 0.7336 - val_loss: 0.6133 - val_acc: 0.7489\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.61330 to 0.61264, saving model to best.model\n",
      "0s - loss: 0.6517 - acc: 0.7313 - val_loss: 0.6126 - val_acc: 0.7493\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.61264 to 0.61236, saving model to best.model\n",
      "0s - loss: 0.6573 - acc: 0.7314 - val_loss: 0.6124 - val_acc: 0.7486\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.61236 to 0.61023, saving model to best.model\n",
      "0s - loss: 0.6512 - acc: 0.7318 - val_loss: 0.6102 - val_acc: 0.7495\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6556 - acc: 0.7308 - val_loss: 0.6106 - val_acc: 0.7494\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.61023 to 0.60971, saving model to best.model\n",
      "0s - loss: 0.6517 - acc: 0.7323 - val_loss: 0.6097 - val_acc: 0.7511\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60971 to 0.60739, saving model to best.model\n",
      "0s - loss: 0.6523 - acc: 0.7316 - val_loss: 0.6074 - val_acc: 0.7542\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6500 - acc: 0.7338 - val_loss: 0.6095 - val_acc: 0.7509\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6519 - acc: 0.7310 - val_loss: 0.6095 - val_acc: 0.7509\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83501, saving model to best.model\n",
      "0s - loss: 0.9215 - acc: 0.6250 - val_loss: 0.8350 - val_acc: 0.6646\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.83501 to 0.83484, saving model to best.model\n",
      "0s - loss: 0.8569 - acc: 0.6597 - val_loss: 0.8348 - val_acc: 0.6646\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.83484 to 0.83370, saving model to best.model\n",
      "0s - loss: 0.8484 - acc: 0.6602 - val_loss: 0.8337 - val_acc: 0.6646\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83370 to 0.83037, saving model to best.model\n",
      "0s - loss: 0.8465 - acc: 0.6602 - val_loss: 0.8304 - val_acc: 0.6646\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83037 to 0.82251, saving model to best.model\n",
      "0s - loss: 0.8417 - acc: 0.6602 - val_loss: 0.8225 - val_acc: 0.6646\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82251 to 0.81896, saving model to best.model\n",
      "0s - loss: 0.8362 - acc: 0.6602 - val_loss: 0.8190 - val_acc: 0.6646\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.81896 to 0.81656, saving model to best.model\n",
      "0s - loss: 0.8336 - acc: 0.6602 - val_loss: 0.8166 - val_acc: 0.6646\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81656 to 0.81423, saving model to best.model\n",
      "0s - loss: 0.8308 - acc: 0.6602 - val_loss: 0.8142 - val_acc: 0.6646\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.8288 - acc: 0.6602 - val_loss: 0.8161 - val_acc: 0.6646\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81423 to 0.81293, saving model to best.model\n",
      "0s - loss: 0.8271 - acc: 0.6602 - val_loss: 0.8129 - val_acc: 0.6646\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81293 to 0.81189, saving model to best.model\n",
      "0s - loss: 0.8272 - acc: 0.6602 - val_loss: 0.8119 - val_acc: 0.6646\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.8263 - acc: 0.6602 - val_loss: 0.8127 - val_acc: 0.6646\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81189 to 0.81117, saving model to best.model\n",
      "0s - loss: 0.8249 - acc: 0.6602 - val_loss: 0.8112 - val_acc: 0.6646\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.8256 - acc: 0.6602 - val_loss: 0.8119 - val_acc: 0.6646\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8246 - acc: 0.6602 - val_loss: 0.8119 - val_acc: 0.6646\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8240 - acc: 0.6602 - val_loss: 0.8125 - val_acc: 0.6646\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8228 - acc: 0.6602 - val_loss: 0.8114 - val_acc: 0.6646\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81117 to 0.81072, saving model to best.model\n",
      "1s - loss: 0.8213 - acc: 0.6602 - val_loss: 0.8107 - val_acc: 0.6646\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81072 to 0.80991, saving model to best.model\n",
      "1s - loss: 0.8227 - acc: 0.6602 - val_loss: 0.8099 - val_acc: 0.6646\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.80991 to 0.80849, saving model to best.model\n",
      "1s - loss: 0.8207 - acc: 0.6603 - val_loss: 0.8085 - val_acc: 0.6646\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.80849 to 0.80822, saving model to best.model\n",
      "0s - loss: 0.8196 - acc: 0.6602 - val_loss: 0.8082 - val_acc: 0.6646\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.80822 to 0.80672, saving model to best.model\n",
      "0s - loss: 0.8193 - acc: 0.6605 - val_loss: 0.8067 - val_acc: 0.6646\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.8175 - acc: 0.6604 - val_loss: 0.8069 - val_acc: 0.6646\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80672 to 0.80516, saving model to best.model\n",
      "0s - loss: 0.8189 - acc: 0.6604 - val_loss: 0.8052 - val_acc: 0.6646\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80516 to 0.80422, saving model to best.model\n",
      "0s - loss: 0.8164 - acc: 0.6606 - val_loss: 0.8042 - val_acc: 0.6646\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80422 to 0.80299, saving model to best.model\n",
      "0s - loss: 0.8154 - acc: 0.6601 - val_loss: 0.8030 - val_acc: 0.6646\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.8147 - acc: 0.6617 - val_loss: 0.8031 - val_acc: 0.6648\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80299 to 0.80029, saving model to best.model\n",
      "0s - loss: 0.8137 - acc: 0.6605 - val_loss: 0.8003 - val_acc: 0.6649\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80029 to 0.79830, saving model to best.model\n",
      "0s - loss: 0.8124 - acc: 0.6617 - val_loss: 0.7983 - val_acc: 0.6663\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.79830 to 0.79638, saving model to best.model\n",
      "0s - loss: 0.8111 - acc: 0.6619 - val_loss: 0.7964 - val_acc: 0.6665\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79638 to 0.79380, saving model to best.model\n",
      "1s - loss: 0.8093 - acc: 0.6630 - val_loss: 0.7938 - val_acc: 0.6697\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79380 to 0.79175, saving model to best.model\n",
      "1s - loss: 0.8074 - acc: 0.6628 - val_loss: 0.7918 - val_acc: 0.6718\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79175 to 0.78912, saving model to best.model\n",
      "0s - loss: 0.8046 - acc: 0.6650 - val_loss: 0.7891 - val_acc: 0.6707\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.78912 to 0.78753, saving model to best.model\n",
      "1s - loss: 0.8032 - acc: 0.6648 - val_loss: 0.7875 - val_acc: 0.6745\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.78753 to 0.78493, saving model to best.model\n",
      "0s - loss: 0.8035 - acc: 0.6651 - val_loss: 0.7849 - val_acc: 0.6718\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.78493 to 0.78309, saving model to best.model\n",
      "0s - loss: 0.8009 - acc: 0.6659 - val_loss: 0.7831 - val_acc: 0.6707\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78309 to 0.78009, saving model to best.model\n",
      "0s - loss: 0.7975 - acc: 0.6675 - val_loss: 0.7801 - val_acc: 0.6748\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78009 to 0.77776, saving model to best.model\n",
      "0s - loss: 0.7968 - acc: 0.6664 - val_loss: 0.7778 - val_acc: 0.6749\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.77776 to 0.77556, saving model to best.model\n",
      "0s - loss: 0.7967 - acc: 0.6673 - val_loss: 0.7756 - val_acc: 0.6748\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.77556 to 0.77336, saving model to best.model\n",
      "0s - loss: 0.7929 - acc: 0.6703 - val_loss: 0.7734 - val_acc: 0.6782\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77336 to 0.77128, saving model to best.model\n",
      "0s - loss: 0.7919 - acc: 0.6677 - val_loss: 0.7713 - val_acc: 0.6801\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77128 to 0.76957, saving model to best.model\n",
      "0s - loss: 0.7907 - acc: 0.6692 - val_loss: 0.7696 - val_acc: 0.6769\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.76957 to 0.76793, saving model to best.model\n",
      "0s - loss: 0.7886 - acc: 0.6702 - val_loss: 0.7679 - val_acc: 0.6776\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.76793 to 0.76511, saving model to best.model\n",
      "1s - loss: 0.7867 - acc: 0.6724 - val_loss: 0.7651 - val_acc: 0.6814\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76511 to 0.76235, saving model to best.model\n",
      "0s - loss: 0.7861 - acc: 0.6715 - val_loss: 0.7623 - val_acc: 0.6869\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76235 to 0.76141, saving model to best.model\n",
      "0s - loss: 0.7849 - acc: 0.6718 - val_loss: 0.7614 - val_acc: 0.6847\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76141 to 0.75870, saving model to best.model\n",
      "0s - loss: 0.7824 - acc: 0.6736 - val_loss: 0.7587 - val_acc: 0.6827\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.75870 to 0.75780, saving model to best.model\n",
      "0s - loss: 0.7820 - acc: 0.6731 - val_loss: 0.7578 - val_acc: 0.6918\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.75780 to 0.75367, saving model to best.model\n",
      "0s - loss: 0.7778 - acc: 0.6730 - val_loss: 0.7537 - val_acc: 0.6925\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.75367 to 0.75200, saving model to best.model\n",
      "0s - loss: 0.7776 - acc: 0.6748 - val_loss: 0.7520 - val_acc: 0.6920\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75200 to 0.74959, saving model to best.model\n",
      "0s - loss: 0.7754 - acc: 0.6777 - val_loss: 0.7496 - val_acc: 0.6932\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.74959 to 0.74535, saving model to best.model\n",
      "0s - loss: 0.7720 - acc: 0.6785 - val_loss: 0.7454 - val_acc: 0.6945\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.74535 to 0.74433, saving model to best.model\n",
      "0s - loss: 0.7731 - acc: 0.6774 - val_loss: 0.7443 - val_acc: 0.6923\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74433 to 0.73949, saving model to best.model\n",
      "0s - loss: 0.7685 - acc: 0.6782 - val_loss: 0.7395 - val_acc: 0.6959\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "1s - loss: 0.7691 - acc: 0.6793 - val_loss: 0.7396 - val_acc: 0.6932\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.73949 to 0.73562, saving model to best.model\n",
      "0s - loss: 0.7672 - acc: 0.6787 - val_loss: 0.7356 - val_acc: 0.6974\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.73562 to 0.73356, saving model to best.model\n",
      "0s - loss: 0.7628 - acc: 0.6808 - val_loss: 0.7336 - val_acc: 0.6933\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.73356 to 0.72990, saving model to best.model\n",
      "0s - loss: 0.7619 - acc: 0.6819 - val_loss: 0.7299 - val_acc: 0.7008\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.72990 to 0.72629, saving model to best.model\n",
      "0s - loss: 0.7609 - acc: 0.6818 - val_loss: 0.7263 - val_acc: 0.7019\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.72629 to 0.72471, saving model to best.model\n",
      "0s - loss: 0.7559 - acc: 0.6848 - val_loss: 0.7247 - val_acc: 0.6960\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.72471 to 0.72086, saving model to best.model\n",
      "0s - loss: 0.7553 - acc: 0.6859 - val_loss: 0.7209 - val_acc: 0.7080\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72086 to 0.71856, saving model to best.model\n",
      "0s - loss: 0.7532 - acc: 0.6873 - val_loss: 0.7186 - val_acc: 0.7055\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.71856 to 0.71746, saving model to best.model\n",
      "0s - loss: 0.7503 - acc: 0.6882 - val_loss: 0.7175 - val_acc: 0.7069\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.71746 to 0.71455, saving model to best.model\n",
      "0s - loss: 0.7505 - acc: 0.6901 - val_loss: 0.7145 - val_acc: 0.7098\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.71455 to 0.70752, saving model to best.model\n",
      "0s - loss: 0.7464 - acc: 0.6909 - val_loss: 0.7075 - val_acc: 0.7097\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.7454 - acc: 0.6890 - val_loss: 0.7084 - val_acc: 0.7046\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.70752 to 0.70586, saving model to best.model\n",
      "0s - loss: 0.7474 - acc: 0.6893 - val_loss: 0.7059 - val_acc: 0.7103\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.70586 to 0.70506, saving model to best.model\n",
      "0s - loss: 0.7431 - acc: 0.6929 - val_loss: 0.7051 - val_acc: 0.7077\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.70506 to 0.70178, saving model to best.model\n",
      "0s - loss: 0.7408 - acc: 0.6930 - val_loss: 0.7018 - val_acc: 0.7138\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70178 to 0.69742, saving model to best.model\n",
      "0s - loss: 0.7400 - acc: 0.6931 - val_loss: 0.6974 - val_acc: 0.7155\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "0s - loss: 0.7385 - acc: 0.6940 - val_loss: 0.6978 - val_acc: 0.7173\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.69742 to 0.69103, saving model to best.model\n",
      "0s - loss: 0.7346 - acc: 0.6958 - val_loss: 0.6910 - val_acc: 0.7194\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.69103 to 0.68888, saving model to best.model\n",
      "0s - loss: 0.7324 - acc: 0.6965 - val_loss: 0.6889 - val_acc: 0.7179\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.7320 - acc: 0.6959 - val_loss: 0.6898 - val_acc: 0.7178\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.68888 to 0.68826, saving model to best.model\n",
      "0s - loss: 0.7316 - acc: 0.6989 - val_loss: 0.6883 - val_acc: 0.7224\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.68826 to 0.68541, saving model to best.model\n",
      "0s - loss: 0.7281 - acc: 0.6979 - val_loss: 0.6854 - val_acc: 0.7213\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.68541 to 0.68074, saving model to best.model\n",
      "0s - loss: 0.7270 - acc: 0.6986 - val_loss: 0.6807 - val_acc: 0.7219\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.7246 - acc: 0.6996 - val_loss: 0.6839 - val_acc: 0.7198\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.68074 to 0.67984, saving model to best.model\n",
      "0s - loss: 0.7263 - acc: 0.6988 - val_loss: 0.6798 - val_acc: 0.7247\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.67984 to 0.67734, saving model to best.model\n",
      "0s - loss: 0.7257 - acc: 0.7007 - val_loss: 0.6773 - val_acc: 0.7214\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.67734 to 0.67503, saving model to best.model\n",
      "0s - loss: 0.7233 - acc: 0.7016 - val_loss: 0.6750 - val_acc: 0.7226\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.7208 - acc: 0.7011 - val_loss: 0.6781 - val_acc: 0.7259\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.67503 to 0.67385, saving model to best.model\n",
      "0s - loss: 0.7238 - acc: 0.7012 - val_loss: 0.6739 - val_acc: 0.7228\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67385 to 0.67189, saving model to best.model\n",
      "0s - loss: 0.7203 - acc: 0.7046 - val_loss: 0.6719 - val_acc: 0.7248\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.67189 to 0.67151, saving model to best.model\n",
      "0s - loss: 0.7191 - acc: 0.7044 - val_loss: 0.6715 - val_acc: 0.7247\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67151 to 0.66858, saving model to best.model\n",
      "0s - loss: 0.7158 - acc: 0.7051 - val_loss: 0.6686 - val_acc: 0.7287\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.66858 to 0.66707, saving model to best.model\n",
      "0s - loss: 0.7178 - acc: 0.7056 - val_loss: 0.6671 - val_acc: 0.7262\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.66707 to 0.66617, saving model to best.model\n",
      "1s - loss: 0.7136 - acc: 0.7082 - val_loss: 0.6662 - val_acc: 0.7275\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.66617 to 0.66556, saving model to best.model\n",
      "0s - loss: 0.7134 - acc: 0.7047 - val_loss: 0.6656 - val_acc: 0.7296\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.66556 to 0.66300, saving model to best.model\n",
      "0s - loss: 0.7101 - acc: 0.7078 - val_loss: 0.6630 - val_acc: 0.7270\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.7099 - acc: 0.7079 - val_loss: 0.6636 - val_acc: 0.7279\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66300 to 0.66296, saving model to best.model\n",
      "0s - loss: 0.7085 - acc: 0.7092 - val_loss: 0.6630 - val_acc: 0.7306\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66296 to 0.66202, saving model to best.model\n",
      "0s - loss: 0.7094 - acc: 0.7097 - val_loss: 0.6620 - val_acc: 0.7314\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66202 to 0.65808, saving model to best.model\n",
      "0s - loss: 0.7097 - acc: 0.7079 - val_loss: 0.6581 - val_acc: 0.7310\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.65808 to 0.65614, saving model to best.model\n",
      "0s - loss: 0.7081 - acc: 0.7074 - val_loss: 0.6561 - val_acc: 0.7328\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7094 - acc: 0.7076 - val_loss: 0.6592 - val_acc: 0.7285\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.65614 to 0.65350, saving model to best.model\n",
      "0s - loss: 0.7056 - acc: 0.7090 - val_loss: 0.6535 - val_acc: 0.7330\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.65350 to 0.65234, saving model to best.model\n",
      "0s - loss: 0.7039 - acc: 0.7105 - val_loss: 0.6523 - val_acc: 0.7364\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.7008 - acc: 0.7120 - val_loss: 0.6535 - val_acc: 0.7343\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.65234 to 0.65048, saving model to best.model\n",
      "0s - loss: 0.7004 - acc: 0.7119 - val_loss: 0.6505 - val_acc: 0.7357\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.6996 - acc: 0.7137 - val_loss: 0.6518 - val_acc: 0.7322\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.65048 to 0.65030, saving model to best.model\n",
      "0s - loss: 0.7002 - acc: 0.7125 - val_loss: 0.6503 - val_acc: 0.7393\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.65030 to 0.64679, saving model to best.model\n",
      "0s - loss: 0.6973 - acc: 0.7129 - val_loss: 0.6468 - val_acc: 0.7386\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.6993 - acc: 0.7116 - val_loss: 0.6487 - val_acc: 0.7389\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.6967 - acc: 0.7138 - val_loss: 0.6482 - val_acc: 0.7354\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.64679 to 0.64346, saving model to best.model\n",
      "0s - loss: 0.6972 - acc: 0.7157 - val_loss: 0.6435 - val_acc: 0.7383\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.6966 - acc: 0.7141 - val_loss: 0.6446 - val_acc: 0.7370\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.64346 to 0.64215, saving model to best.model\n",
      "0s - loss: 0.6972 - acc: 0.7137 - val_loss: 0.6421 - val_acc: 0.7398\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64215 to 0.64125, saving model to best.model\n",
      "0s - loss: 0.6933 - acc: 0.7144 - val_loss: 0.6413 - val_acc: 0.7392\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.6919 - acc: 0.7166 - val_loss: 0.6414 - val_acc: 0.7374\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.6932 - acc: 0.7156 - val_loss: 0.6420 - val_acc: 0.7406\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.64125 to 0.63915, saving model to best.model\n",
      "0s - loss: 0.6905 - acc: 0.7178 - val_loss: 0.6391 - val_acc: 0.7392\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6903 - acc: 0.7164 - val_loss: 0.6399 - val_acc: 0.7386\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.6930 - acc: 0.7172 - val_loss: 0.6396 - val_acc: 0.7386\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6880 - acc: 0.7177 - val_loss: 0.6393 - val_acc: 0.7376\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.63915 to 0.63915, saving model to best.model\n",
      "0s - loss: 0.6906 - acc: 0.7145 - val_loss: 0.6391 - val_acc: 0.7393\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.63915 to 0.63746, saving model to best.model\n",
      "0s - loss: 0.6926 - acc: 0.7151 - val_loss: 0.6375 - val_acc: 0.7400\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.63746 to 0.63427, saving model to best.model\n",
      "0s - loss: 0.6896 - acc: 0.7191 - val_loss: 0.6343 - val_acc: 0.7424\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6861 - acc: 0.7198 - val_loss: 0.6344 - val_acc: 0.7410\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.6846 - acc: 0.7198 - val_loss: 0.6348 - val_acc: 0.7403\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.63427 to 0.63069, saving model to best.model\n",
      "0s - loss: 0.6806 - acc: 0.7218 - val_loss: 0.6307 - val_acc: 0.7425\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6850 - acc: 0.7191 - val_loss: 0.6328 - val_acc: 0.7418\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.63069 to 0.63011, saving model to best.model\n",
      "0s - loss: 0.6853 - acc: 0.7184 - val_loss: 0.6301 - val_acc: 0.7424\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6828 - acc: 0.7216 - val_loss: 0.6323 - val_acc: 0.7405\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.63011 to 0.62975, saving model to best.model\n",
      "0s - loss: 0.6810 - acc: 0.7219 - val_loss: 0.6298 - val_acc: 0.7410\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.62975 to 0.62905, saving model to best.model\n",
      "0s - loss: 0.6817 - acc: 0.7205 - val_loss: 0.6291 - val_acc: 0.7436\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62905 to 0.62849, saving model to best.model\n",
      "0s - loss: 0.6815 - acc: 0.7215 - val_loss: 0.6285 - val_acc: 0.7416\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6787 - acc: 0.7222 - val_loss: 0.6310 - val_acc: 0.7384\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.62849 to 0.62707, saving model to best.model\n",
      "0s - loss: 0.6807 - acc: 0.7206 - val_loss: 0.6271 - val_acc: 0.7413\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6790 - acc: 0.7228 - val_loss: 0.6287 - val_acc: 0.7447\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.62707 to 0.62551, saving model to best.model\n",
      "0s - loss: 0.6781 - acc: 0.7224 - val_loss: 0.6255 - val_acc: 0.7419\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6757 - acc: 0.7239 - val_loss: 0.6256 - val_acc: 0.7438\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6750 - acc: 0.7232 - val_loss: 0.6269 - val_acc: 0.7452\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6780 - acc: 0.7229 - val_loss: 0.6257 - val_acc: 0.7420\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.62551 to 0.62432, saving model to best.model\n",
      "0s - loss: 0.6727 - acc: 0.7274 - val_loss: 0.6243 - val_acc: 0.7416\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.62432 to 0.62323, saving model to best.model\n",
      "0s - loss: 0.6742 - acc: 0.7237 - val_loss: 0.6232 - val_acc: 0.7472\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6771 - acc: 0.7237 - val_loss: 0.6261 - val_acc: 0.7430\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62323 to 0.62129, saving model to best.model\n",
      "0s - loss: 0.6745 - acc: 0.7224 - val_loss: 0.6213 - val_acc: 0.7471\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62129 to 0.61995, saving model to best.model\n",
      "0s - loss: 0.6709 - acc: 0.7277 - val_loss: 0.6199 - val_acc: 0.7453\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6754 - acc: 0.7245 - val_loss: 0.6208 - val_acc: 0.7477\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.61995 to 0.61778, saving model to best.model\n",
      "0s - loss: 0.6709 - acc: 0.7252 - val_loss: 0.6178 - val_acc: 0.7473\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.61778 to 0.61663, saving model to best.model\n",
      "0s - loss: 0.6696 - acc: 0.7262 - val_loss: 0.6166 - val_acc: 0.7470\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6705 - acc: 0.7268 - val_loss: 0.6175 - val_acc: 0.7453\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6709 - acc: 0.7271 - val_loss: 0.6184 - val_acc: 0.7453\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.61663 to 0.61357, saving model to best.model\n",
      "0s - loss: 0.6670 - acc: 0.7295 - val_loss: 0.6136 - val_acc: 0.7494\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6660 - acc: 0.7279 - val_loss: 0.6139 - val_acc: 0.7501\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6684 - acc: 0.7261 - val_loss: 0.6160 - val_acc: 0.7480\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.61357 to 0.61188, saving model to best.model\n",
      "0s - loss: 0.6670 - acc: 0.7288 - val_loss: 0.6119 - val_acc: 0.7516\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6644 - acc: 0.7279 - val_loss: 0.6122 - val_acc: 0.7499\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6624 - acc: 0.7309 - val_loss: 0.6127 - val_acc: 0.7492\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.61188 to 0.61118, saving model to best.model\n",
      "0s - loss: 0.6652 - acc: 0.7276 - val_loss: 0.6112 - val_acc: 0.7492\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6658 - acc: 0.7271 - val_loss: 0.6142 - val_acc: 0.7488\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.61118 to 0.60838, saving model to best.model\n",
      "0s - loss: 0.6623 - acc: 0.7286 - val_loss: 0.6084 - val_acc: 0.7506\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.6679 - acc: 0.7269 - val_loss: 0.6133 - val_acc: 0.7491\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.60838 to 0.60817, saving model to best.model\n",
      "0s - loss: 0.6601 - acc: 0.7307 - val_loss: 0.6082 - val_acc: 0.7532\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6605 - acc: 0.7292 - val_loss: 0.6110 - val_acc: 0.7535\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6645 - acc: 0.7290 - val_loss: 0.6094 - val_acc: 0.7511\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6621 - acc: 0.7313 - val_loss: 0.6096 - val_acc: 0.7525\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.60817 to 0.60728, saving model to best.model\n",
      "0s - loss: 0.6583 - acc: 0.7301 - val_loss: 0.6073 - val_acc: 0.7521\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6593 - acc: 0.7288 - val_loss: 0.6085 - val_acc: 0.7541\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6619 - acc: 0.7315 - val_loss: 0.6086 - val_acc: 0.7511\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.60728 to 0.60272, saving model to best.model\n",
      "0s - loss: 0.6605 - acc: 0.7312 - val_loss: 0.6027 - val_acc: 0.7552\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6620 - acc: 0.7288 - val_loss: 0.6061 - val_acc: 0.7544\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6569 - acc: 0.7326 - val_loss: 0.6056 - val_acc: 0.7534\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6587 - acc: 0.7309 - val_loss: 0.6053 - val_acc: 0.7539\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6546 - acc: 0.7320 - val_loss: 0.6072 - val_acc: 0.7530\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6612 - acc: 0.7297 - val_loss: 0.6032 - val_acc: 0.7516\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6567 - acc: 0.7323 - val_loss: 0.6043 - val_acc: 0.7516\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.60272 to 0.60266, saving model to best.model\n",
      "0s - loss: 0.6569 - acc: 0.7328 - val_loss: 0.6027 - val_acc: 0.7515\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6572 - acc: 0.7294 - val_loss: 0.6036 - val_acc: 0.7570\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6585 - acc: 0.7321 - val_loss: 0.6028 - val_acc: 0.7570\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.60266 to 0.60165, saving model to best.model\n",
      "0s - loss: 0.6588 - acc: 0.7296 - val_loss: 0.6016 - val_acc: 0.7559\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6565 - acc: 0.7313 - val_loss: 0.6041 - val_acc: 0.7539\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6555 - acc: 0.7333 - val_loss: 0.6023 - val_acc: 0.7571\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.60165 to 0.60010, saving model to best.model\n",
      "0s - loss: 0.6564 - acc: 0.7289 - val_loss: 0.6001 - val_acc: 0.7553\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.60010 to 0.59882, saving model to best.model\n",
      "0s - loss: 0.6539 - acc: 0.7326 - val_loss: 0.5988 - val_acc: 0.7543\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6539 - acc: 0.7307 - val_loss: 0.6004 - val_acc: 0.7550\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.6529 - acc: 0.7335 - val_loss: 0.5989 - val_acc: 0.7552\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.59882 to 0.59719, saving model to best.model\n",
      "0s - loss: 0.6529 - acc: 0.7332 - val_loss: 0.5972 - val_acc: 0.7581\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.59719 to 0.59584, saving model to best.model\n",
      "0s - loss: 0.6488 - acc: 0.7347 - val_loss: 0.5958 - val_acc: 0.7577\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6501 - acc: 0.7329 - val_loss: 0.5964 - val_acc: 0.7571\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.59584 to 0.59518, saving model to best.model\n",
      "0s - loss: 0.6489 - acc: 0.7338 - val_loss: 0.5952 - val_acc: 0.7563\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6522 - acc: 0.7327 - val_loss: 0.5974 - val_acc: 0.7548\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.59518 to 0.59491, saving model to best.model\n",
      "0s - loss: 0.6515 - acc: 0.7360 - val_loss: 0.5949 - val_acc: 0.7585\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6487 - acc: 0.7350 - val_loss: 0.5955 - val_acc: 0.7584\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6472 - acc: 0.7347 - val_loss: 0.5971 - val_acc: 0.7582\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59491 to 0.59468, saving model to best.model\n",
      "0s - loss: 0.6479 - acc: 0.7346 - val_loss: 0.5947 - val_acc: 0.7562\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6487 - acc: 0.7350 - val_loss: 0.5963 - val_acc: 0.7595\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6488 - acc: 0.7338 - val_loss: 0.5948 - val_acc: 0.7561\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6480 - acc: 0.7352 - val_loss: 0.5954 - val_acc: 0.7576\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.59468 to 0.59257, saving model to best.model\n",
      "0s - loss: 0.6532 - acc: 0.7330 - val_loss: 0.5926 - val_acc: 0.7590\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.59257 to 0.59060, saving model to best.model\n",
      "0s - loss: 0.6447 - acc: 0.7360 - val_loss: 0.5906 - val_acc: 0.7598\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6434 - acc: 0.7380 - val_loss: 0.5946 - val_acc: 0.7574\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6471 - acc: 0.7358 - val_loss: 0.5919 - val_acc: 0.7580\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.59060 to 0.58989, saving model to best.model\n",
      "0s - loss: 0.6455 - acc: 0.7369 - val_loss: 0.5899 - val_acc: 0.7578\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6458 - acc: 0.7351 - val_loss: 0.5906 - val_acc: 0.7587\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.58989 to 0.58858, saving model to best.model\n",
      "0s - loss: 0.6414 - acc: 0.7382 - val_loss: 0.5886 - val_acc: 0.7595\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.58858 to 0.58852, saving model to best.model\n",
      "0s - loss: 0.6417 - acc: 0.7368 - val_loss: 0.5885 - val_acc: 0.7574\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.6418 - acc: 0.7382 - val_loss: 0.5918 - val_acc: 0.7580\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6428 - acc: 0.7383 - val_loss: 0.5902 - val_acc: 0.7569\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84171, saving model to best.model\n",
      "0s - loss: 0.9295 - acc: 0.6179 - val_loss: 0.8417 - val_acc: 0.6603\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84171 to 0.83995, saving model to best.model\n",
      "0s - loss: 0.8668 - acc: 0.6559 - val_loss: 0.8400 - val_acc: 0.6603\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.8570 - acc: 0.6568 - val_loss: 0.8418 - val_acc: 0.6603\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83995 to 0.83640, saving model to best.model\n",
      "0s - loss: 0.8530 - acc: 0.6568 - val_loss: 0.8364 - val_acc: 0.6603\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83640 to 0.83379, saving model to best.model\n",
      "0s - loss: 0.8492 - acc: 0.6568 - val_loss: 0.8338 - val_acc: 0.6603\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83379 to 0.82365, saving model to best.model\n",
      "0s - loss: 0.8414 - acc: 0.6568 - val_loss: 0.8236 - val_acc: 0.6603\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82365 to 0.82043, saving model to best.model\n",
      "1s - loss: 0.8389 - acc: 0.6568 - val_loss: 0.8204 - val_acc: 0.6603\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82043 to 0.81855, saving model to best.model\n",
      "0s - loss: 0.8370 - acc: 0.6568 - val_loss: 0.8185 - val_acc: 0.6603\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81855 to 0.81767, saving model to best.model\n",
      "0s - loss: 0.8341 - acc: 0.6569 - val_loss: 0.8177 - val_acc: 0.6603\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81767 to 0.81731, saving model to best.model\n",
      "0s - loss: 0.8328 - acc: 0.6567 - val_loss: 0.8173 - val_acc: 0.6603\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.8334 - acc: 0.6567 - val_loss: 0.8176 - val_acc: 0.6603\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81731 to 0.81696, saving model to best.model\n",
      "1s - loss: 0.8308 - acc: 0.6569 - val_loss: 0.8170 - val_acc: 0.6603\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.8300 - acc: 0.6569 - val_loss: 0.8181 - val_acc: 0.6603\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81696 to 0.81616, saving model to best.model\n",
      "1s - loss: 0.8295 - acc: 0.6567 - val_loss: 0.8162 - val_acc: 0.6603\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81616 to 0.81465, saving model to best.model\n",
      "0s - loss: 0.8290 - acc: 0.6569 - val_loss: 0.8146 - val_acc: 0.6603\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81465 to 0.81420, saving model to best.model\n",
      "0s - loss: 0.8276 - acc: 0.6564 - val_loss: 0.8142 - val_acc: 0.6603\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8284 - acc: 0.6567 - val_loss: 0.8168 - val_acc: 0.6603\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81420 to 0.81334, saving model to best.model\n",
      "0s - loss: 0.8276 - acc: 0.6568 - val_loss: 0.8133 - val_acc: 0.6603\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81334 to 0.81287, saving model to best.model\n",
      "0s - loss: 0.8269 - acc: 0.6574 - val_loss: 0.8129 - val_acc: 0.6603\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81287 to 0.81183, saving model to best.model\n",
      "1s - loss: 0.8258 - acc: 0.6564 - val_loss: 0.8118 - val_acc: 0.6603\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81183 to 0.81147, saving model to best.model\n",
      "0s - loss: 0.8251 - acc: 0.6562 - val_loss: 0.8115 - val_acc: 0.6603\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.8249 - acc: 0.6570 - val_loss: 0.8131 - val_acc: 0.6604\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81147 to 0.80939, saving model to best.model\n",
      "1s - loss: 0.8236 - acc: 0.6581 - val_loss: 0.8094 - val_acc: 0.6604\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.8231 - acc: 0.6573 - val_loss: 0.8096 - val_acc: 0.6604\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80939 to 0.80921, saving model to best.model\n",
      "1s - loss: 0.8217 - acc: 0.6587 - val_loss: 0.8092 - val_acc: 0.6623\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80921 to 0.80655, saving model to best.model\n",
      "0s - loss: 0.8210 - acc: 0.6583 - val_loss: 0.8065 - val_acc: 0.6624\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.8216 - acc: 0.6587 - val_loss: 0.8072 - val_acc: 0.6625\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.8204 - acc: 0.6604 - val_loss: 0.8073 - val_acc: 0.6632\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.8193 - acc: 0.6594 - val_loss: 0.8069 - val_acc: 0.6637\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80655 to 0.80399, saving model to best.model\n",
      "0s - loss: 0.8187 - acc: 0.6584 - val_loss: 0.8040 - val_acc: 0.6646\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80399 to 0.80329, saving model to best.model\n",
      "0s - loss: 0.8173 - acc: 0.6617 - val_loss: 0.8033 - val_acc: 0.6657\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80329 to 0.80243, saving model to best.model\n",
      "0s - loss: 0.8184 - acc: 0.6603 - val_loss: 0.8024 - val_acc: 0.6684\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.8153 - acc: 0.6613 - val_loss: 0.8027 - val_acc: 0.6674\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80243 to 0.80158, saving model to best.model\n",
      "0s - loss: 0.8145 - acc: 0.6625 - val_loss: 0.8016 - val_acc: 0.6681\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80158 to 0.79997, saving model to best.model\n",
      "0s - loss: 0.8141 - acc: 0.6628 - val_loss: 0.8000 - val_acc: 0.6708\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.8136 - acc: 0.6645 - val_loss: 0.8014 - val_acc: 0.6698\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79997 to 0.79699, saving model to best.model\n",
      "0s - loss: 0.8118 - acc: 0.6627 - val_loss: 0.7970 - val_acc: 0.6707\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79699 to 0.79515, saving model to best.model\n",
      "0s - loss: 0.8106 - acc: 0.6625 - val_loss: 0.7951 - val_acc: 0.6708\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79515 to 0.79274, saving model to best.model\n",
      "0s - loss: 0.8088 - acc: 0.6651 - val_loss: 0.7927 - val_acc: 0.6713\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.8098 - acc: 0.6640 - val_loss: 0.7940 - val_acc: 0.6713\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79274 to 0.78978, saving model to best.model\n",
      "0s - loss: 0.8063 - acc: 0.6653 - val_loss: 0.7898 - val_acc: 0.6727\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78978 to 0.78763, saving model to best.model\n",
      "0s - loss: 0.8056 - acc: 0.6649 - val_loss: 0.7876 - val_acc: 0.6732\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.8056 - acc: 0.6650 - val_loss: 0.7882 - val_acc: 0.6740\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78763 to 0.78425, saving model to best.model\n",
      "0s - loss: 0.8020 - acc: 0.6681 - val_loss: 0.7842 - val_acc: 0.6762\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78425 to 0.78177, saving model to best.model\n",
      "0s - loss: 0.8017 - acc: 0.6671 - val_loss: 0.7818 - val_acc: 0.6772\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78177 to 0.77821, saving model to best.model\n",
      "0s - loss: 0.7994 - acc: 0.6689 - val_loss: 0.7782 - val_acc: 0.6801\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77821 to 0.77724, saving model to best.model\n",
      "0s - loss: 0.7981 - acc: 0.6690 - val_loss: 0.7772 - val_acc: 0.6793\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77724 to 0.77449, saving model to best.model\n",
      "0s - loss: 0.7971 - acc: 0.6679 - val_loss: 0.7745 - val_acc: 0.6844\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77449 to 0.77091, saving model to best.model\n",
      "0s - loss: 0.7951 - acc: 0.6711 - val_loss: 0.7709 - val_acc: 0.6841\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.7941 - acc: 0.6708 - val_loss: 0.7709 - val_acc: 0.6843\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77091 to 0.76803, saving model to best.model\n",
      "0s - loss: 0.7919 - acc: 0.6712 - val_loss: 0.7680 - val_acc: 0.6861\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76803 to 0.76682, saving model to best.model\n",
      "0s - loss: 0.7892 - acc: 0.6741 - val_loss: 0.7668 - val_acc: 0.6876\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76682 to 0.75840, saving model to best.model\n",
      "0s - loss: 0.7873 - acc: 0.6727 - val_loss: 0.7584 - val_acc: 0.6902\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.7888 - acc: 0.6724 - val_loss: 0.7622 - val_acc: 0.6847\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.7858 - acc: 0.6734 - val_loss: 0.7593 - val_acc: 0.6885\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75840 to 0.75755, saving model to best.model\n",
      "0s - loss: 0.7817 - acc: 0.6762 - val_loss: 0.7576 - val_acc: 0.6900\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75755 to 0.74997, saving model to best.model\n",
      "0s - loss: 0.7785 - acc: 0.6794 - val_loss: 0.7500 - val_acc: 0.6941\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.7803 - acc: 0.6757 - val_loss: 0.7502 - val_acc: 0.6936\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74997 to 0.74888, saving model to best.model\n",
      "0s - loss: 0.7771 - acc: 0.6794 - val_loss: 0.7489 - val_acc: 0.6922\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74888 to 0.74616, saving model to best.model\n",
      "0s - loss: 0.7745 - acc: 0.6789 - val_loss: 0.7462 - val_acc: 0.6933\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74616 to 0.74430, saving model to best.model\n",
      "0s - loss: 0.7728 - acc: 0.6803 - val_loss: 0.7443 - val_acc: 0.6943\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74430 to 0.74142, saving model to best.model\n",
      "0s - loss: 0.7697 - acc: 0.6840 - val_loss: 0.7414 - val_acc: 0.6948\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74142 to 0.73900, saving model to best.model\n",
      "0s - loss: 0.7701 - acc: 0.6819 - val_loss: 0.7390 - val_acc: 0.7016\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73900 to 0.73480, saving model to best.model\n",
      "0s - loss: 0.7680 - acc: 0.6842 - val_loss: 0.7348 - val_acc: 0.7071\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73480 to 0.73167, saving model to best.model\n",
      "0s - loss: 0.7673 - acc: 0.6813 - val_loss: 0.7317 - val_acc: 0.7057\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.7660 - acc: 0.6838 - val_loss: 0.7347 - val_acc: 0.6995\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73167 to 0.72924, saving model to best.model\n",
      "0s - loss: 0.7646 - acc: 0.6850 - val_loss: 0.7292 - val_acc: 0.7109\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72924 to 0.72877, saving model to best.model\n",
      "0s - loss: 0.7623 - acc: 0.6869 - val_loss: 0.7288 - val_acc: 0.7023\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72877 to 0.72436, saving model to best.model\n",
      "0s - loss: 0.7580 - acc: 0.6898 - val_loss: 0.7244 - val_acc: 0.7082\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72436 to 0.71880, saving model to best.model\n",
      "0s - loss: 0.7567 - acc: 0.6888 - val_loss: 0.7188 - val_acc: 0.7156\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71880 to 0.71832, saving model to best.model\n",
      "0s - loss: 0.7556 - acc: 0.6892 - val_loss: 0.7183 - val_acc: 0.7131\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71832 to 0.71618, saving model to best.model\n",
      "0s - loss: 0.7524 - acc: 0.6921 - val_loss: 0.7162 - val_acc: 0.7138\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71618 to 0.71523, saving model to best.model\n",
      "0s - loss: 0.7545 - acc: 0.6904 - val_loss: 0.7152 - val_acc: 0.7177\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71523 to 0.71241, saving model to best.model\n",
      "0s - loss: 0.7536 - acc: 0.6882 - val_loss: 0.7124 - val_acc: 0.7208\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71241 to 0.71159, saving model to best.model\n",
      "1s - loss: 0.7498 - acc: 0.6920 - val_loss: 0.7116 - val_acc: 0.7181\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71159 to 0.71053, saving model to best.model\n",
      "1s - loss: 0.7497 - acc: 0.6940 - val_loss: 0.7105 - val_acc: 0.7160\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.71053 to 0.70650, saving model to best.model\n",
      "0s - loss: 0.7485 - acc: 0.6918 - val_loss: 0.7065 - val_acc: 0.7230\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70650 to 0.70248, saving model to best.model\n",
      "0s - loss: 0.7454 - acc: 0.6941 - val_loss: 0.7025 - val_acc: 0.7235\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.7449 - acc: 0.6937 - val_loss: 0.7032 - val_acc: 0.7214\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70248 to 0.69695, saving model to best.model\n",
      "0s - loss: 0.7407 - acc: 0.6970 - val_loss: 0.6969 - val_acc: 0.7285\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.7416 - acc: 0.6965 - val_loss: 0.6983 - val_acc: 0.7240\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.7406 - acc: 0.6964 - val_loss: 0.6980 - val_acc: 0.7249\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69695 to 0.69534, saving model to best.model\n",
      "0s - loss: 0.7395 - acc: 0.6977 - val_loss: 0.6953 - val_acc: 0.7281\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69534 to 0.69204, saving model to best.model\n",
      "0s - loss: 0.7366 - acc: 0.6989 - val_loss: 0.6920 - val_acc: 0.7315\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69204 to 0.69003, saving model to best.model\n",
      "0s - loss: 0.7336 - acc: 0.6991 - val_loss: 0.6900 - val_acc: 0.7288\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69003 to 0.68637, saving model to best.model\n",
      "0s - loss: 0.7324 - acc: 0.7018 - val_loss: 0.6864 - val_acc: 0.7285\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7341 - acc: 0.6983 - val_loss: 0.6894 - val_acc: 0.7272\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68637 to 0.68329, saving model to best.model\n",
      "0s - loss: 0.7327 - acc: 0.7002 - val_loss: 0.6833 - val_acc: 0.7337\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.7282 - acc: 0.7052 - val_loss: 0.6835 - val_acc: 0.7315\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68329 to 0.68134, saving model to best.model\n",
      "0s - loss: 0.7302 - acc: 0.7016 - val_loss: 0.6813 - val_acc: 0.7315\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.7269 - acc: 0.7031 - val_loss: 0.6814 - val_acc: 0.7328\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.7262 - acc: 0.7016 - val_loss: 0.6830 - val_acc: 0.7287\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68134 to 0.67552, saving model to best.model\n",
      "0s - loss: 0.7240 - acc: 0.7039 - val_loss: 0.6755 - val_acc: 0.7348\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.7227 - acc: 0.7051 - val_loss: 0.6792 - val_acc: 0.7308\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67552 to 0.67492, saving model to best.model\n",
      "0s - loss: 0.7234 - acc: 0.7023 - val_loss: 0.6749 - val_acc: 0.7344\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7220 - acc: 0.7032 - val_loss: 0.6755 - val_acc: 0.7347\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.67492 to 0.67234, saving model to best.model\n",
      "0s - loss: 0.7191 - acc: 0.7072 - val_loss: 0.6723 - val_acc: 0.7348\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67234 to 0.66970, saving model to best.model\n",
      "0s - loss: 0.7198 - acc: 0.7080 - val_loss: 0.6697 - val_acc: 0.7343\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66970 to 0.66790, saving model to best.model\n",
      "0s - loss: 0.7191 - acc: 0.7058 - val_loss: 0.6679 - val_acc: 0.7349\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66790 to 0.66646, saving model to best.model\n",
      "0s - loss: 0.7133 - acc: 0.7075 - val_loss: 0.6665 - val_acc: 0.7383\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7145 - acc: 0.7091 - val_loss: 0.6666 - val_acc: 0.7345\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66646 to 0.66553, saving model to best.model\n",
      "0s - loss: 0.7167 - acc: 0.7065 - val_loss: 0.6655 - val_acc: 0.7354\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7136 - acc: 0.7079 - val_loss: 0.6668 - val_acc: 0.7342\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66553 to 0.66493, saving model to best.model\n",
      "0s - loss: 0.7131 - acc: 0.7104 - val_loss: 0.6649 - val_acc: 0.7368\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66493 to 0.66301, saving model to best.model\n",
      "0s - loss: 0.7108 - acc: 0.7136 - val_loss: 0.6630 - val_acc: 0.7364\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.66301 to 0.66116, saving model to best.model\n",
      "0s - loss: 0.7120 - acc: 0.7078 - val_loss: 0.6612 - val_acc: 0.7392\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.7140 - acc: 0.7093 - val_loss: 0.6616 - val_acc: 0.7378\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.66116 to 0.65702, saving model to best.model\n",
      "0s - loss: 0.7092 - acc: 0.7111 - val_loss: 0.6570 - val_acc: 0.7402\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.7065 - acc: 0.7116 - val_loss: 0.6602 - val_acc: 0.7385\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65702 to 0.65404, saving model to best.model\n",
      "0s - loss: 0.7059 - acc: 0.7133 - val_loss: 0.6540 - val_acc: 0.7434\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.7051 - acc: 0.7134 - val_loss: 0.6583 - val_acc: 0.7404\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.7079 - acc: 0.7113 - val_loss: 0.6554 - val_acc: 0.7422\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65404 to 0.65159, saving model to best.model\n",
      "0s - loss: 0.7035 - acc: 0.7138 - val_loss: 0.6516 - val_acc: 0.7417\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.7071 - acc: 0.7122 - val_loss: 0.6585 - val_acc: 0.7375\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.7015 - acc: 0.7158 - val_loss: 0.6534 - val_acc: 0.7409\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65159 to 0.65123, saving model to best.model\n",
      "0s - loss: 0.7013 - acc: 0.7125 - val_loss: 0.6512 - val_acc: 0.7409\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.7038 - acc: 0.7111 - val_loss: 0.6562 - val_acc: 0.7350\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.7035 - acc: 0.7121 - val_loss: 0.6520 - val_acc: 0.7410\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.65123 to 0.65000, saving model to best.model\n",
      "0s - loss: 0.7009 - acc: 0.7141 - val_loss: 0.6500 - val_acc: 0.7440\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65000 to 0.64803, saving model to best.model\n",
      "0s - loss: 0.7009 - acc: 0.7149 - val_loss: 0.6480 - val_acc: 0.7398\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64803 to 0.64640, saving model to best.model\n",
      "0s - loss: 0.6993 - acc: 0.7164 - val_loss: 0.6464 - val_acc: 0.7456\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.64640 to 0.64635, saving model to best.model\n",
      "0s - loss: 0.6973 - acc: 0.7167 - val_loss: 0.6463 - val_acc: 0.7418\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6977 - acc: 0.7177 - val_loss: 0.6466 - val_acc: 0.7426\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64635 to 0.64277, saving model to best.model\n",
      "0s - loss: 0.6944 - acc: 0.7167 - val_loss: 0.6428 - val_acc: 0.7438\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6951 - acc: 0.7182 - val_loss: 0.6458 - val_acc: 0.7450\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.64277 to 0.64270, saving model to best.model\n",
      "0s - loss: 0.6948 - acc: 0.7182 - val_loss: 0.6427 - val_acc: 0.7438\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.64270 to 0.64236, saving model to best.model\n",
      "0s - loss: 0.6911 - acc: 0.7171 - val_loss: 0.6424 - val_acc: 0.7466\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6941 - acc: 0.7176 - val_loss: 0.6441 - val_acc: 0.7412\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.64236 to 0.64016, saving model to best.model\n",
      "0s - loss: 0.6947 - acc: 0.7155 - val_loss: 0.6402 - val_acc: 0.7450\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6954 - acc: 0.7150 - val_loss: 0.6454 - val_acc: 0.7411\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64016 to 0.63821, saving model to best.model\n",
      "0s - loss: 0.6887 - acc: 0.7210 - val_loss: 0.6382 - val_acc: 0.7468\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6908 - acc: 0.7198 - val_loss: 0.6412 - val_acc: 0.7420\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63821 to 0.63677, saving model to best.model\n",
      "0s - loss: 0.6908 - acc: 0.7195 - val_loss: 0.6368 - val_acc: 0.7457\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63677 to 0.63668, saving model to best.model\n",
      "0s - loss: 0.6900 - acc: 0.7199 - val_loss: 0.6367 - val_acc: 0.7445\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6880 - acc: 0.7204 - val_loss: 0.6389 - val_acc: 0.7438\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.6908 - acc: 0.7174 - val_loss: 0.6386 - val_acc: 0.7474\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63668 to 0.63333, saving model to best.model\n",
      "0s - loss: 0.6845 - acc: 0.7208 - val_loss: 0.6333 - val_acc: 0.7491\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63333 to 0.63316, saving model to best.model\n",
      "0s - loss: 0.6863 - acc: 0.7206 - val_loss: 0.6332 - val_acc: 0.7489\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.63316 to 0.63211, saving model to best.model\n",
      "0s - loss: 0.6878 - acc: 0.7184 - val_loss: 0.6321 - val_acc: 0.7477\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63211 to 0.63186, saving model to best.model\n",
      "0s - loss: 0.6848 - acc: 0.7206 - val_loss: 0.6319 - val_acc: 0.7486\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63186 to 0.63004, saving model to best.model\n",
      "1s - loss: 0.6835 - acc: 0.7232 - val_loss: 0.6300 - val_acc: 0.7494\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6857 - acc: 0.7213 - val_loss: 0.6323 - val_acc: 0.7482\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6850 - acc: 0.7210 - val_loss: 0.6335 - val_acc: 0.7481\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63004 to 0.62993, saving model to best.model\n",
      "1s - loss: 0.6812 - acc: 0.7213 - val_loss: 0.6299 - val_acc: 0.7470\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6861 - acc: 0.7207 - val_loss: 0.6304 - val_acc: 0.7492\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62993 to 0.62893, saving model to best.model\n",
      "1s - loss: 0.6808 - acc: 0.7231 - val_loss: 0.6289 - val_acc: 0.7479\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6782 - acc: 0.7226 - val_loss: 0.6296 - val_acc: 0.7477\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62893 to 0.62690, saving model to best.model\n",
      "1s - loss: 0.6788 - acc: 0.7256 - val_loss: 0.6269 - val_acc: 0.7504\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6810 - acc: 0.7228 - val_loss: 0.6300 - val_acc: 0.7488\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6789 - acc: 0.7229 - val_loss: 0.6270 - val_acc: 0.7508\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62690 to 0.62633, saving model to best.model\n",
      "1s - loss: 0.6809 - acc: 0.7230 - val_loss: 0.6263 - val_acc: 0.7502\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6774 - acc: 0.7233 - val_loss: 0.6269 - val_acc: 0.7478\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6778 - acc: 0.7255 - val_loss: 0.6266 - val_acc: 0.7492\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62633 to 0.62364, saving model to best.model\n",
      "1s - loss: 0.6784 - acc: 0.7246 - val_loss: 0.6236 - val_acc: 0.7520\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62364 to 0.62266, saving model to best.model\n",
      "0s - loss: 0.6766 - acc: 0.7250 - val_loss: 0.6227 - val_acc: 0.7500\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6744 - acc: 0.7261 - val_loss: 0.6232 - val_acc: 0.7511\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6790 - acc: 0.7236 - val_loss: 0.6232 - val_acc: 0.7502\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.62266 to 0.62078, saving model to best.model\n",
      "1s - loss: 0.6735 - acc: 0.7273 - val_loss: 0.6208 - val_acc: 0.7525\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.6772 - acc: 0.7251 - val_loss: 0.6224 - val_acc: 0.7509\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6741 - acc: 0.7254 - val_loss: 0.6221 - val_acc: 0.7516\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62078 to 0.62062, saving model to best.model\n",
      "0s - loss: 0.6719 - acc: 0.7264 - val_loss: 0.6206 - val_acc: 0.7518\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62062 to 0.62029, saving model to best.model\n",
      "0s - loss: 0.6706 - acc: 0.7283 - val_loss: 0.6203 - val_acc: 0.7527\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62029 to 0.62029, saving model to best.model\n",
      "0s - loss: 0.6737 - acc: 0.7240 - val_loss: 0.6203 - val_acc: 0.7530\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6719 - acc: 0.7263 - val_loss: 0.6204 - val_acc: 0.7539\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.62029 to 0.61864, saving model to best.model\n",
      "0s - loss: 0.6706 - acc: 0.7269 - val_loss: 0.6186 - val_acc: 0.7530\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61864 to 0.61605, saving model to best.model\n",
      "0s - loss: 0.6710 - acc: 0.7274 - val_loss: 0.6161 - val_acc: 0.7546\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6689 - acc: 0.7281 - val_loss: 0.6189 - val_acc: 0.7536\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6701 - acc: 0.7267 - val_loss: 0.6163 - val_acc: 0.7546\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7281 - val_loss: 0.6182 - val_acc: 0.7530\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6739 - acc: 0.7241 - val_loss: 0.6184 - val_acc: 0.7535\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61605 to 0.61601, saving model to best.model\n",
      "0s - loss: 0.6696 - acc: 0.7279 - val_loss: 0.6160 - val_acc: 0.7520\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61601 to 0.61234, saving model to best.model\n",
      "0s - loss: 0.6664 - acc: 0.7285 - val_loss: 0.6123 - val_acc: 0.7537\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6699 - acc: 0.7258 - val_loss: 0.6131 - val_acc: 0.7542\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6690 - acc: 0.7267 - val_loss: 0.6144 - val_acc: 0.7536\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61234 to 0.61194, saving model to best.model\n",
      "0s - loss: 0.6673 - acc: 0.7290 - val_loss: 0.6119 - val_acc: 0.7553\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6635 - acc: 0.7294 - val_loss: 0.6145 - val_acc: 0.7533\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6656 - acc: 0.7276 - val_loss: 0.6143 - val_acc: 0.7519\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "385s - loss: 0.6639 - acc: 0.7289 - val_loss: 0.6139 - val_acc: 0.7546\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61194 to 0.61014, saving model to best.model\n",
      "1s - loss: 0.6635 - acc: 0.7306 - val_loss: 0.6101 - val_acc: 0.7564\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6626 - acc: 0.7313 - val_loss: 0.6119 - val_acc: 0.7554\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6636 - acc: 0.7308 - val_loss: 0.6118 - val_acc: 0.7521\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.61014 to 0.60944, saving model to best.model\n",
      "1s - loss: 0.6648 - acc: 0.7295 - val_loss: 0.6094 - val_acc: 0.7555\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60944 to 0.60737, saving model to best.model\n",
      "1s - loss: 0.6639 - acc: 0.7304 - val_loss: 0.6074 - val_acc: 0.7564\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6613 - acc: 0.7301 - val_loss: 0.6085 - val_acc: 0.7563\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6630 - acc: 0.7303 - val_loss: 0.6099 - val_acc: 0.7537\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6626 - acc: 0.7297 - val_loss: 0.6076 - val_acc: 0.7582\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6624 - acc: 0.7337 - val_loss: 0.6079 - val_acc: 0.7546\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60737 to 0.60619, saving model to best.model\n",
      "1s - loss: 0.6641 - acc: 0.7292 - val_loss: 0.6062 - val_acc: 0.7571\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6617 - acc: 0.7328 - val_loss: 0.6084 - val_acc: 0.7550\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60619 to 0.60448, saving model to best.model\n",
      "2s - loss: 0.6604 - acc: 0.7323 - val_loss: 0.6045 - val_acc: 0.7571\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60448 to 0.60368, saving model to best.model\n",
      "2s - loss: 0.6603 - acc: 0.7320 - val_loss: 0.6037 - val_acc: 0.7576\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6591 - acc: 0.7338 - val_loss: 0.6042 - val_acc: 0.7589\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60368 to 0.60351, saving model to best.model\n",
      "1s - loss: 0.6583 - acc: 0.7335 - val_loss: 0.6035 - val_acc: 0.7569\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6577 - acc: 0.7332 - val_loss: 0.6040 - val_acc: 0.7562\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60351 to 0.60202, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7335 - val_loss: 0.6020 - val_acc: 0.7580\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7328 - val_loss: 0.6021 - val_acc: 0.7595\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6585 - acc: 0.7318 - val_loss: 0.6042 - val_acc: 0.7584\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6578 - acc: 0.7311 - val_loss: 0.6032 - val_acc: 0.7596\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60202 to 0.59988, saving model to best.model\n",
      "1s - loss: 0.6557 - acc: 0.7358 - val_loss: 0.5999 - val_acc: 0.7578\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6578 - acc: 0.7315 - val_loss: 0.6023 - val_acc: 0.7582\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83011, saving model to best.model\n",
      "1s - loss: 0.9004 - acc: 0.6358 - val_loss: 0.8301 - val_acc: 0.6678\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.83011 to 0.82732, saving model to best.model\n",
      "1s - loss: 0.8589 - acc: 0.6563 - val_loss: 0.8273 - val_acc: 0.6678\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.82732 to 0.82505, saving model to best.model\n",
      "1s - loss: 0.8519 - acc: 0.6564 - val_loss: 0.8251 - val_acc: 0.6678\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.82505 to 0.82074, saving model to best.model\n",
      "1s - loss: 0.8504 - acc: 0.6564 - val_loss: 0.8207 - val_acc: 0.6678\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.82074 to 0.81525, saving model to best.model\n",
      "1s - loss: 0.8437 - acc: 0.6564 - val_loss: 0.8152 - val_acc: 0.6678\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.81525 to 0.81222, saving model to best.model\n",
      "1s - loss: 0.8410 - acc: 0.6564 - val_loss: 0.8122 - val_acc: 0.6678\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.81222 to 0.80904, saving model to best.model\n",
      "1s - loss: 0.8366 - acc: 0.6564 - val_loss: 0.8090 - val_acc: 0.6678\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.80904 to 0.80848, saving model to best.model\n",
      "1s - loss: 0.8358 - acc: 0.6564 - val_loss: 0.8085 - val_acc: 0.6678\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "1s - loss: 0.8333 - acc: 0.6564 - val_loss: 0.8090 - val_acc: 0.6678\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.80848 to 0.80698, saving model to best.model\n",
      "1s - loss: 0.8322 - acc: 0.6565 - val_loss: 0.8070 - val_acc: 0.6678\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8308 - acc: 0.6564 - val_loss: 0.8084 - val_acc: 0.6678\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.80698 to 0.80575, saving model to best.model\n",
      "1s - loss: 0.8299 - acc: 0.6564 - val_loss: 0.8058 - val_acc: 0.6678\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.80575 to 0.80535, saving model to best.model\n",
      "1s - loss: 0.8295 - acc: 0.6565 - val_loss: 0.8053 - val_acc: 0.6678\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8287 - acc: 0.6564 - val_loss: 0.8062 - val_acc: 0.6678\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "1s - loss: 0.8298 - acc: 0.6565 - val_loss: 0.8060 - val_acc: 0.6678\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.80535 to 0.80361, saving model to best.model\n",
      "1s - loss: 0.8265 - acc: 0.6566 - val_loss: 0.8036 - val_acc: 0.6678\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.80361 to 0.80331, saving model to best.model\n",
      "1s - loss: 0.8262 - acc: 0.6564 - val_loss: 0.8033 - val_acc: 0.6678\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.80331 to 0.80321, saving model to best.model\n",
      "1s - loss: 0.8258 - acc: 0.6564 - val_loss: 0.8032 - val_acc: 0.6678\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.80321 to 0.80168, saving model to best.model\n",
      "1s - loss: 0.8250 - acc: 0.6579 - val_loss: 0.8017 - val_acc: 0.6678\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.80168 to 0.80051, saving model to best.model\n",
      "1230s - loss: 0.8238 - acc: 0.6576 - val_loss: 0.8005 - val_acc: 0.6704\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "1s - loss: 0.8226 - acc: 0.6584 - val_loss: 0.8019 - val_acc: 0.6700\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.80051 to 0.79996, saving model to best.model\n",
      "1s - loss: 0.8231 - acc: 0.6574 - val_loss: 0.8000 - val_acc: 0.6715\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.79996 to 0.79950, saving model to best.model\n",
      "1s - loss: 0.8218 - acc: 0.6579 - val_loss: 0.7995 - val_acc: 0.6685\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.79950 to 0.79761, saving model to best.model\n",
      "1s - loss: 0.8216 - acc: 0.6580 - val_loss: 0.7976 - val_acc: 0.6721\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.79761 to 0.79753, saving model to best.model\n",
      "1s - loss: 0.8201 - acc: 0.6586 - val_loss: 0.7975 - val_acc: 0.6717\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "1s - loss: 0.8184 - acc: 0.6590 - val_loss: 0.7978 - val_acc: 0.6710\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.79753 to 0.79544, saving model to best.model\n",
      "0s - loss: 0.8176 - acc: 0.6604 - val_loss: 0.7954 - val_acc: 0.6724\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.79544 to 0.79481, saving model to best.model\n",
      "1s - loss: 0.8175 - acc: 0.6600 - val_loss: 0.7948 - val_acc: 0.6725\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.8173 - acc: 0.6601 - val_loss: 0.7959 - val_acc: 0.6724\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.79481 to 0.79225, saving model to best.model\n",
      "1s - loss: 0.8164 - acc: 0.6607 - val_loss: 0.7922 - val_acc: 0.6721\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79225 to 0.79025, saving model to best.model\n",
      "1s - loss: 0.8136 - acc: 0.6622 - val_loss: 0.7903 - val_acc: 0.6756\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79025 to 0.78843, saving model to best.model\n",
      "2s - loss: 0.8134 - acc: 0.6616 - val_loss: 0.7884 - val_acc: 0.6742\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "2s - loss: 0.8125 - acc: 0.6623 - val_loss: 0.7894 - val_acc: 0.6763\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.78843 to 0.78526, saving model to best.model\n",
      "1s - loss: 0.8105 - acc: 0.6629 - val_loss: 0.7853 - val_acc: 0.6748\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.78526 to 0.78381, saving model to best.model\n",
      "1s - loss: 0.8099 - acc: 0.6633 - val_loss: 0.7838 - val_acc: 0.6746\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "1s - loss: 0.8080 - acc: 0.6632 - val_loss: 0.7843 - val_acc: 0.6754\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78381 to 0.78267, saving model to best.model\n",
      "1s - loss: 0.8058 - acc: 0.6661 - val_loss: 0.7827 - val_acc: 0.6749\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78267 to 0.77886, saving model to best.model\n",
      "1s - loss: 0.8045 - acc: 0.6656 - val_loss: 0.7789 - val_acc: 0.6780\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "1s - loss: 0.8039 - acc: 0.6665 - val_loss: 0.7799 - val_acc: 0.6744\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.77886 to 0.77341, saving model to best.model\n",
      "1s - loss: 0.8001 - acc: 0.6668 - val_loss: 0.7734 - val_acc: 0.6804\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77341 to 0.77142, saving model to best.model\n",
      "1s - loss: 0.8009 - acc: 0.6666 - val_loss: 0.7714 - val_acc: 0.6802\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77142 to 0.77122, saving model to best.model\n",
      "1s - loss: 0.7995 - acc: 0.6662 - val_loss: 0.7712 - val_acc: 0.6815\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77122 to 0.76933, saving model to best.model\n",
      "1s - loss: 0.7985 - acc: 0.6657 - val_loss: 0.7693 - val_acc: 0.6854\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.76933 to 0.76794, saving model to best.model\n",
      "1s - loss: 0.7959 - acc: 0.6676 - val_loss: 0.7679 - val_acc: 0.6817\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76794 to 0.76544, saving model to best.model\n",
      "1s - loss: 0.7958 - acc: 0.6694 - val_loss: 0.7654 - val_acc: 0.6835\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76544 to 0.76283, saving model to best.model\n",
      "1s - loss: 0.7948 - acc: 0.6693 - val_loss: 0.7628 - val_acc: 0.6874\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76283 to 0.75968, saving model to best.model\n",
      "1s - loss: 0.7909 - acc: 0.6688 - val_loss: 0.7597 - val_acc: 0.6875\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.75968 to 0.75899, saving model to best.model\n",
      "1s - loss: 0.7913 - acc: 0.6702 - val_loss: 0.7590 - val_acc: 0.6867\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "1s - loss: 0.7904 - acc: 0.6712 - val_loss: 0.7595 - val_acc: 0.6882\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.75899 to 0.75330, saving model to best.model\n",
      "1s - loss: 0.7882 - acc: 0.6706 - val_loss: 0.7533 - val_acc: 0.6890\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75330 to 0.75190, saving model to best.model\n",
      "1s - loss: 0.7860 - acc: 0.6734 - val_loss: 0.7519 - val_acc: 0.6927\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75190 to 0.74926, saving model to best.model\n",
      "1s - loss: 0.7868 - acc: 0.6709 - val_loss: 0.7493 - val_acc: 0.6905\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.74926 to 0.74905, saving model to best.model\n",
      "1s - loss: 0.7822 - acc: 0.6741 - val_loss: 0.7491 - val_acc: 0.6932\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74905 to 0.74531, saving model to best.model\n",
      "1s - loss: 0.7786 - acc: 0.6783 - val_loss: 0.7453 - val_acc: 0.6931\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74531 to 0.74496, saving model to best.model\n",
      "1s - loss: 0.7797 - acc: 0.6769 - val_loss: 0.7450 - val_acc: 0.6929\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74496 to 0.74264, saving model to best.model\n",
      "1s - loss: 0.7792 - acc: 0.6768 - val_loss: 0.7426 - val_acc: 0.6961\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74264 to 0.73895, saving model to best.model\n",
      "1s - loss: 0.7742 - acc: 0.6782 - val_loss: 0.7390 - val_acc: 0.6951\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.73895 to 0.73741, saving model to best.model\n",
      "1s - loss: 0.7747 - acc: 0.6783 - val_loss: 0.7374 - val_acc: 0.6991\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73741 to 0.73339, saving model to best.model\n",
      "1s - loss: 0.7728 - acc: 0.6793 - val_loss: 0.7334 - val_acc: 0.6967\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73339 to 0.73045, saving model to best.model\n",
      "1s - loss: 0.7726 - acc: 0.6794 - val_loss: 0.7304 - val_acc: 0.7012\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "1s - loss: 0.7723 - acc: 0.6798 - val_loss: 0.7312 - val_acc: 0.6975\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "1s - loss: 0.7709 - acc: 0.6811 - val_loss: 0.7317 - val_acc: 0.6963\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73045 to 0.72331, saving model to best.model\n",
      "1s - loss: 0.7644 - acc: 0.6832 - val_loss: 0.7233 - val_acc: 0.7063\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72331 to 0.72208, saving model to best.model\n",
      "1s - loss: 0.7631 - acc: 0.6836 - val_loss: 0.7221 - val_acc: 0.7015\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72208 to 0.71945, saving model to best.model\n",
      "1s - loss: 0.7629 - acc: 0.6851 - val_loss: 0.7194 - val_acc: 0.7039\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71945 to 0.71494, saving model to best.model\n",
      "62s - loss: 0.7617 - acc: 0.6846 - val_loss: 0.7149 - val_acc: 0.7096\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.7612 - acc: 0.6863 - val_loss: 0.7165 - val_acc: 0.7075\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71494 to 0.71493, saving model to best.model\n",
      "1s - loss: 0.7585 - acc: 0.6888 - val_loss: 0.7149 - val_acc: 0.7083\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71493 to 0.71281, saving model to best.model\n",
      "1s - loss: 0.7562 - acc: 0.6869 - val_loss: 0.7128 - val_acc: 0.7089\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71281 to 0.70834, saving model to best.model\n",
      "1s - loss: 0.7540 - acc: 0.6895 - val_loss: 0.7083 - val_acc: 0.7109\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70834 to 0.70433, saving model to best.model\n",
      "1s - loss: 0.7523 - acc: 0.6891 - val_loss: 0.7043 - val_acc: 0.7108\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.7501 - acc: 0.6924 - val_loss: 0.7044 - val_acc: 0.7150\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70433 to 0.70212, saving model to best.model\n",
      "1s - loss: 0.7479 - acc: 0.6921 - val_loss: 0.7021 - val_acc: 0.7174\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70212 to 0.69765, saving model to best.model\n",
      "1s - loss: 0.7479 - acc: 0.6929 - val_loss: 0.6977 - val_acc: 0.7166\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69765 to 0.69655, saving model to best.model\n",
      "1s - loss: 0.7446 - acc: 0.6933 - val_loss: 0.6966 - val_acc: 0.7183\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69655 to 0.69050, saving model to best.model\n",
      "1s - loss: 0.7444 - acc: 0.6933 - val_loss: 0.6905 - val_acc: 0.7207\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7445 - acc: 0.6929 - val_loss: 0.6923 - val_acc: 0.7172\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "1s - loss: 0.7382 - acc: 0.6960 - val_loss: 0.6912 - val_acc: 0.7215\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69050 to 0.68660, saving model to best.model\n",
      "1s - loss: 0.7379 - acc: 0.6959 - val_loss: 0.6866 - val_acc: 0.7254\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.68660 to 0.68178, saving model to best.model\n",
      "1s - loss: 0.7357 - acc: 0.6979 - val_loss: 0.6818 - val_acc: 0.7258\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7341 - acc: 0.6968 - val_loss: 0.6844 - val_acc: 0.7228\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7318 - acc: 0.6968 - val_loss: 0.6858 - val_acc: 0.7220\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68178 to 0.67693, saving model to best.model\n",
      "1s - loss: 0.7325 - acc: 0.6960 - val_loss: 0.6769 - val_acc: 0.7297\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7322 - acc: 0.6977 - val_loss: 0.6782 - val_acc: 0.7266\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.67693 to 0.67213, saving model to best.model\n",
      "1s - loss: 0.7290 - acc: 0.7012 - val_loss: 0.6721 - val_acc: 0.7267\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67213 to 0.67153, saving model to best.model\n",
      "1s - loss: 0.7270 - acc: 0.7019 - val_loss: 0.6715 - val_acc: 0.7281\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67153 to 0.67086, saving model to best.model\n",
      "1s - loss: 0.7275 - acc: 0.7019 - val_loss: 0.6709 - val_acc: 0.7296\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7257 - acc: 0.7012 - val_loss: 0.6722 - val_acc: 0.7321\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "146s - loss: 0.7228 - acc: 0.7032 - val_loss: 0.6714 - val_acc: 0.7282\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67086 to 0.66493, saving model to best.model\n",
      "1s - loss: 0.7207 - acc: 0.7037 - val_loss: 0.6649 - val_acc: 0.7317\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7202 - acc: 0.7046 - val_loss: 0.6676 - val_acc: 0.7307\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66493 to 0.66488, saving model to best.model\n",
      "1s - loss: 0.7209 - acc: 0.7018 - val_loss: 0.6649 - val_acc: 0.7313\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66488 to 0.66234, saving model to best.model\n",
      "1s - loss: 0.7202 - acc: 0.7036 - val_loss: 0.6623 - val_acc: 0.7322\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66234 to 0.65839, saving model to best.model\n",
      "1s - loss: 0.7185 - acc: 0.7021 - val_loss: 0.6584 - val_acc: 0.7347\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7148 - acc: 0.7064 - val_loss: 0.6598 - val_acc: 0.7358\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.65839 to 0.65651, saving model to best.model\n",
      "1s - loss: 0.7166 - acc: 0.7048 - val_loss: 0.6565 - val_acc: 0.7358\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.65651 to 0.65609, saving model to best.model\n",
      "1s - loss: 0.7166 - acc: 0.7051 - val_loss: 0.6561 - val_acc: 0.7379\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7117 - acc: 0.7058 - val_loss: 0.6572 - val_acc: 0.7384\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.65609 to 0.65067, saving model to best.model\n",
      "1s - loss: 0.7098 - acc: 0.7096 - val_loss: 0.6507 - val_acc: 0.7381\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7100 - acc: 0.7075 - val_loss: 0.6548 - val_acc: 0.7386\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.65067 to 0.65061, saving model to best.model\n",
      "1s - loss: 0.7078 - acc: 0.7104 - val_loss: 0.6506 - val_acc: 0.7396\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.65061 to 0.64994, saving model to best.model\n",
      "1s - loss: 0.7063 - acc: 0.7096 - val_loss: 0.6499 - val_acc: 0.7396\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7080 - acc: 0.7084 - val_loss: 0.6529 - val_acc: 0.7403\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.7087 - acc: 0.7098 - val_loss: 0.6540 - val_acc: 0.7381\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.64994 to 0.64603, saving model to best.model\n",
      "1s - loss: 0.7093 - acc: 0.7069 - val_loss: 0.6460 - val_acc: 0.7422\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.64603 to 0.64263, saving model to best.model\n",
      "1s - loss: 0.7035 - acc: 0.7112 - val_loss: 0.6426 - val_acc: 0.7413\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.64263 to 0.64255, saving model to best.model\n",
      "1s - loss: 0.7013 - acc: 0.7120 - val_loss: 0.6425 - val_acc: 0.7420\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.64255 to 0.63991, saving model to best.model\n",
      "1s - loss: 0.7010 - acc: 0.7120 - val_loss: 0.6399 - val_acc: 0.7438\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.7037 - acc: 0.7111 - val_loss: 0.6416 - val_acc: 0.7415\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.63991 to 0.63899, saving model to best.model\n",
      "1s - loss: 0.7003 - acc: 0.7119 - val_loss: 0.6390 - val_acc: 0.7422\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6999 - acc: 0.7126 - val_loss: 0.6399 - val_acc: 0.7433\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.63899 to 0.63533, saving model to best.model\n",
      "53s - loss: 0.6992 - acc: 0.7154 - val_loss: 0.6353 - val_acc: 0.7445\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6984 - acc: 0.7135 - val_loss: 0.6381 - val_acc: 0.7450\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6984 - acc: 0.7138 - val_loss: 0.6381 - val_acc: 0.7446\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.63533 to 0.63523, saving model to best.model\n",
      "1s - loss: 0.6942 - acc: 0.7144 - val_loss: 0.6352 - val_acc: 0.7447\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6953 - acc: 0.7153 - val_loss: 0.6383 - val_acc: 0.7443\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.63523 to 0.63286, saving model to best.model\n",
      "1s - loss: 0.6947 - acc: 0.7153 - val_loss: 0.6329 - val_acc: 0.7468\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.63286 to 0.63000, saving model to best.model\n",
      "1s - loss: 0.6911 - acc: 0.7165 - val_loss: 0.6300 - val_acc: 0.7486\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6927 - acc: 0.7141 - val_loss: 0.6318 - val_acc: 0.7444\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6920 - acc: 0.7152 - val_loss: 0.6359 - val_acc: 0.7450\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6929 - acc: 0.7153 - val_loss: 0.6304 - val_acc: 0.7485\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.63000 to 0.62897, saving model to best.model\n",
      "1s - loss: 0.6893 - acc: 0.7191 - val_loss: 0.6290 - val_acc: 0.7492\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6883 - acc: 0.7177 - val_loss: 0.6295 - val_acc: 0.7478\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "1s - loss: 0.6893 - acc: 0.7171 - val_loss: 0.6304 - val_acc: 0.7493\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.62897 to 0.62694, saving model to best.model\n",
      "1s - loss: 0.6868 - acc: 0.7188 - val_loss: 0.6269 - val_acc: 0.7473\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.62694 to 0.62589, saving model to best.model\n",
      "1s - loss: 0.6867 - acc: 0.7151 - val_loss: 0.6259 - val_acc: 0.7493\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62589 to 0.62359, saving model to best.model\n",
      "1s - loss: 0.6882 - acc: 0.7163 - val_loss: 0.6236 - val_acc: 0.7493\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6881 - acc: 0.7173 - val_loss: 0.6253 - val_acc: 0.7501\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.62359 to 0.62315, saving model to best.model\n",
      "1s - loss: 0.6831 - acc: 0.7192 - val_loss: 0.6232 - val_acc: 0.7508\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6815 - acc: 0.7188 - val_loss: 0.6236 - val_acc: 0.7498\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6825 - acc: 0.7220 - val_loss: 0.6236 - val_acc: 0.7485\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.62315 to 0.62209, saving model to best.model\n",
      "1s - loss: 0.6839 - acc: 0.7174 - val_loss: 0.6221 - val_acc: 0.7496\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6833 - acc: 0.7196 - val_loss: 0.6234 - val_acc: 0.7501\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.62209 to 0.61972, saving model to best.model\n",
      "1s - loss: 0.6776 - acc: 0.7232 - val_loss: 0.6197 - val_acc: 0.7508\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.61972 to 0.61792, saving model to best.model\n",
      "411s - loss: 0.6814 - acc: 0.7203 - val_loss: 0.6179 - val_acc: 0.7523\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6786 - acc: 0.7243 - val_loss: 0.6183 - val_acc: 0.7500\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6763 - acc: 0.7234 - val_loss: 0.6182 - val_acc: 0.7507\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.61792 to 0.61593, saving model to best.model\n",
      "1s - loss: 0.6756 - acc: 0.7224 - val_loss: 0.6159 - val_acc: 0.7522\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.61593 to 0.61470, saving model to best.model\n",
      "1s - loss: 0.6763 - acc: 0.7222 - val_loss: 0.6147 - val_acc: 0.7561\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6748 - acc: 0.7226 - val_loss: 0.6175 - val_acc: 0.7501\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6762 - acc: 0.7208 - val_loss: 0.6154 - val_acc: 0.7529\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6781 - acc: 0.7213 - val_loss: 0.6182 - val_acc: 0.7552\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6762 - acc: 0.7225 - val_loss: 0.6151 - val_acc: 0.7533\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.61470 to 0.61171, saving model to best.model\n",
      "1s - loss: 0.6712 - acc: 0.7245 - val_loss: 0.6117 - val_acc: 0.7552\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6734 - acc: 0.7226 - val_loss: 0.6123 - val_acc: 0.7555\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6722 - acc: 0.7233 - val_loss: 0.6162 - val_acc: 0.7548\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.61171 to 0.60796, saving model to best.model\n",
      "2s - loss: 0.6714 - acc: 0.7253 - val_loss: 0.6080 - val_acc: 0.7564\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "2s - loss: 0.6714 - acc: 0.7257 - val_loss: 0.6106 - val_acc: 0.7557\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6722 - acc: 0.7277 - val_loss: 0.6121 - val_acc: 0.7547\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.60796 to 0.60717, saving model to best.model\n",
      "1s - loss: 0.6681 - acc: 0.7258 - val_loss: 0.6072 - val_acc: 0.7570\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6718 - acc: 0.7246 - val_loss: 0.6083 - val_acc: 0.7554\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6712 - acc: 0.7252 - val_loss: 0.6076 - val_acc: 0.7573\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6691 - acc: 0.7272 - val_loss: 0.6082 - val_acc: 0.7566\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6668 - acc: 0.7264 - val_loss: 0.6077 - val_acc: 0.7550\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.60717 to 0.60684, saving model to best.model\n",
      "1s - loss: 0.6659 - acc: 0.7265 - val_loss: 0.6068 - val_acc: 0.7576\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.60684 to 0.60395, saving model to best.model\n",
      "1s - loss: 0.6683 - acc: 0.7270 - val_loss: 0.6040 - val_acc: 0.7575\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6682 - acc: 0.7274 - val_loss: 0.6045 - val_acc: 0.7600\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6678 - acc: 0.7274 - val_loss: 0.6072 - val_acc: 0.7555\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "176s - loss: 0.6643 - acc: 0.7299 - val_loss: 0.6074 - val_acc: 0.7553\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6643 - acc: 0.7281 - val_loss: 0.6054 - val_acc: 0.7563\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6667 - acc: 0.7272 - val_loss: 0.6043 - val_acc: 0.7576\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.60395 to 0.60075, saving model to best.model\n",
      "1s - loss: 0.6656 - acc: 0.7281 - val_loss: 0.6008 - val_acc: 0.7590\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6650 - acc: 0.7265 - val_loss: 0.6056 - val_acc: 0.7567\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6636 - acc: 0.7286 - val_loss: 0.6020 - val_acc: 0.7597\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7285 - val_loss: 0.6055 - val_acc: 0.7550\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6643 - acc: 0.7275 - val_loss: 0.6030 - val_acc: 0.7573\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.60075 to 0.59903, saving model to best.model\n",
      "1s - loss: 0.6583 - acc: 0.7307 - val_loss: 0.5990 - val_acc: 0.7604\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6643 - acc: 0.7287 - val_loss: 0.6002 - val_acc: 0.7626\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6601 - acc: 0.7308 - val_loss: 0.6007 - val_acc: 0.7557\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6603 - acc: 0.7300 - val_loss: 0.6015 - val_acc: 0.7560\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6617 - acc: 0.7278 - val_loss: 0.5998 - val_acc: 0.7575\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.59903 to 0.59804, saving model to best.model\n",
      "1s - loss: 0.6607 - acc: 0.7299 - val_loss: 0.5980 - val_acc: 0.7602\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6605 - acc: 0.7284 - val_loss: 0.5991 - val_acc: 0.7645\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.59804 to 0.59737, saving model to best.model\n",
      "1s - loss: 0.6595 - acc: 0.7312 - val_loss: 0.5974 - val_acc: 0.7639\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.59737 to 0.59550, saving model to best.model\n",
      "1s - loss: 0.6568 - acc: 0.7319 - val_loss: 0.5955 - val_acc: 0.7642\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6589 - acc: 0.7291 - val_loss: 0.5988 - val_acc: 0.7603\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6564 - acc: 0.7316 - val_loss: 0.5992 - val_acc: 0.7619\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6559 - acc: 0.7296 - val_loss: 0.5979 - val_acc: 0.7635\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.59550 to 0.59303, saving model to best.model\n",
      "1s - loss: 0.6558 - acc: 0.7327 - val_loss: 0.5930 - val_acc: 0.7648\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6570 - acc: 0.7310 - val_loss: 0.5968 - val_acc: 0.7638\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6568 - acc: 0.7318 - val_loss: 0.5964 - val_acc: 0.7633\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "82s - loss: 0.6530 - acc: 0.7338 - val_loss: 0.5934 - val_acc: 0.7653\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7333 - val_loss: 0.5954 - val_acc: 0.7616\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6581 - acc: 0.7311 - val_loss: 0.5944 - val_acc: 0.7650\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6521 - acc: 0.7310 - val_loss: 0.5935 - val_acc: 0.7685\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6524 - acc: 0.7330 - val_loss: 0.5934 - val_acc: 0.7632\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59303 to 0.59209, saving model to best.model\n",
      "1s - loss: 0.6525 - acc: 0.7316 - val_loss: 0.5921 - val_acc: 0.7649\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.59209 to 0.59054, saving model to best.model\n",
      "1s - loss: 0.6503 - acc: 0.7338 - val_loss: 0.5905 - val_acc: 0.7656\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6532 - acc: 0.7325 - val_loss: 0.5919 - val_acc: 0.7635\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6506 - acc: 0.7303 - val_loss: 0.5932 - val_acc: 0.7642\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6525 - acc: 0.7326 - val_loss: 0.5923 - val_acc: 0.7653\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6525 - acc: 0.7328 - val_loss: 0.5948 - val_acc: 0.7642\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6523 - acc: 0.7319 - val_loss: 0.5922 - val_acc: 0.7665\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.59054 to 0.59034, saving model to best.model\n",
      "2s - loss: 0.6492 - acc: 0.7341 - val_loss: 0.5903 - val_acc: 0.7666\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.59034 to 0.59016, saving model to best.model\n",
      "1s - loss: 0.6510 - acc: 0.7315 - val_loss: 0.5902 - val_acc: 0.7659\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6489 - acc: 0.7332 - val_loss: 0.5908 - val_acc: 0.7649\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6519 - acc: 0.7326 - val_loss: 0.5925 - val_acc: 0.7664\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.59016 to 0.58913, saving model to best.model\n",
      "1s - loss: 0.6489 - acc: 0.7347 - val_loss: 0.5891 - val_acc: 0.7646\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.58913 to 0.58797, saving model to best.model\n",
      "1s - loss: 0.6502 - acc: 0.7363 - val_loss: 0.5880 - val_acc: 0.7656\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.58797 to 0.58555, saving model to best.model\n",
      "1s - loss: 0.6457 - acc: 0.7361 - val_loss: 0.5856 - val_acc: 0.7687\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83796, saving model to best.model\n",
      "2s - loss: 0.9043 - acc: 0.6341 - val_loss: 0.8380 - val_acc: 0.6615\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.83796 to 0.83586, saving model to best.model\n",
      "61s - loss: 0.8571 - acc: 0.6592 - val_loss: 0.8359 - val_acc: 0.6615\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.83586 to 0.83314, saving model to best.model\n",
      "1s - loss: 0.8501 - acc: 0.6592 - val_loss: 0.8331 - val_acc: 0.6615\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83314 to 0.82667, saving model to best.model\n",
      "1s - loss: 0.8467 - acc: 0.6592 - val_loss: 0.8267 - val_acc: 0.6615\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.82667 to 0.82190, saving model to best.model\n",
      "1s - loss: 0.8388 - acc: 0.6592 - val_loss: 0.8219 - val_acc: 0.6615\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82190 to 0.81736, saving model to best.model\n",
      "1s - loss: 0.8340 - acc: 0.6592 - val_loss: 0.8174 - val_acc: 0.6615\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.81736 to 0.81505, saving model to best.model\n",
      "1s - loss: 0.8317 - acc: 0.6592 - val_loss: 0.8151 - val_acc: 0.6615\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81505 to 0.81330, saving model to best.model\n",
      "0s - loss: 0.8301 - acc: 0.6592 - val_loss: 0.8133 - val_acc: 0.6615\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81330 to 0.81315, saving model to best.model\n",
      "1s - loss: 0.8269 - acc: 0.6592 - val_loss: 0.8131 - val_acc: 0.6615\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81315 to 0.81283, saving model to best.model\n",
      "1s - loss: 0.8261 - acc: 0.6592 - val_loss: 0.8128 - val_acc: 0.6615\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81283 to 0.81226, saving model to best.model\n",
      "1s - loss: 0.8240 - acc: 0.6592 - val_loss: 0.8123 - val_acc: 0.6615\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8235 - acc: 0.6591 - val_loss: 0.8130 - val_acc: 0.6615\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81226 to 0.81185, saving model to best.model\n",
      "1s - loss: 0.8242 - acc: 0.6591 - val_loss: 0.8118 - val_acc: 0.6615\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81185 to 0.81050, saving model to best.model\n",
      "1s - loss: 0.8218 - acc: 0.6592 - val_loss: 0.8105 - val_acc: 0.6615\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "1s - loss: 0.8225 - acc: 0.6595 - val_loss: 0.8106 - val_acc: 0.6615\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81050 to 0.80949, saving model to best.model\n",
      "1s - loss: 0.8214 - acc: 0.6593 - val_loss: 0.8095 - val_acc: 0.6615\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.80949 to 0.80920, saving model to best.model\n",
      "1s - loss: 0.8208 - acc: 0.6590 - val_loss: 0.8092 - val_acc: 0.6615\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.80920 to 0.80800, saving model to best.model\n",
      "1s - loss: 0.8197 - acc: 0.6590 - val_loss: 0.8080 - val_acc: 0.6615\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "1s - loss: 0.8192 - acc: 0.6598 - val_loss: 0.8100 - val_acc: 0.6615\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.80800 to 0.80712, saving model to best.model\n",
      "1s - loss: 0.8181 - acc: 0.6585 - val_loss: 0.8071 - val_acc: 0.6616\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.80712 to 0.80597, saving model to best.model\n",
      "1s - loss: 0.8175 - acc: 0.6601 - val_loss: 0.8060 - val_acc: 0.6615\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "1s - loss: 0.8154 - acc: 0.6606 - val_loss: 0.8071 - val_acc: 0.6616\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.80597 to 0.80405, saving model to best.model\n",
      "1s - loss: 0.8161 - acc: 0.6607 - val_loss: 0.8041 - val_acc: 0.6628\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80405 to 0.80375, saving model to best.model\n",
      "1s - loss: 0.8158 - acc: 0.6615 - val_loss: 0.8038 - val_acc: 0.6631\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80375 to 0.80212, saving model to best.model\n",
      "1s - loss: 0.8136 - acc: 0.6613 - val_loss: 0.8021 - val_acc: 0.6658\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80212 to 0.80208, saving model to best.model\n",
      "1s - loss: 0.8125 - acc: 0.6627 - val_loss: 0.8021 - val_acc: 0.6641\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80208 to 0.79996, saving model to best.model\n",
      "1s - loss: 0.8124 - acc: 0.6629 - val_loss: 0.8000 - val_acc: 0.6684\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.79996 to 0.79958, saving model to best.model\n",
      "67s - loss: 0.8126 - acc: 0.6618 - val_loss: 0.7996 - val_acc: 0.6677\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.79958 to 0.79905, saving model to best.model\n",
      "1s - loss: 0.8105 - acc: 0.6629 - val_loss: 0.7991 - val_acc: 0.6685\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.79905 to 0.79705, saving model to best.model\n",
      "1s - loss: 0.8091 - acc: 0.6644 - val_loss: 0.7971 - val_acc: 0.6712\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79705 to 0.79509, saving model to best.model\n",
      "0s - loss: 0.8092 - acc: 0.6650 - val_loss: 0.7951 - val_acc: 0.6699\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79509 to 0.79404, saving model to best.model\n",
      "0s - loss: 0.8068 - acc: 0.6652 - val_loss: 0.7940 - val_acc: 0.6707\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79404 to 0.79293, saving model to best.model\n",
      "1s - loss: 0.8062 - acc: 0.6652 - val_loss: 0.7929 - val_acc: 0.6707\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79293 to 0.79077, saving model to best.model\n",
      "1s - loss: 0.8055 - acc: 0.6648 - val_loss: 0.7908 - val_acc: 0.6763\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79077 to 0.78897, saving model to best.model\n",
      "1s - loss: 0.8051 - acc: 0.6652 - val_loss: 0.7890 - val_acc: 0.6734\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.78897 to 0.78533, saving model to best.model\n",
      "1s - loss: 0.8016 - acc: 0.6668 - val_loss: 0.7853 - val_acc: 0.6746\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78533 to 0.78369, saving model to best.model\n",
      "1s - loss: 0.8001 - acc: 0.6664 - val_loss: 0.7837 - val_acc: 0.6741\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78369 to 0.78185, saving model to best.model\n",
      "1s - loss: 0.7995 - acc: 0.6673 - val_loss: 0.7819 - val_acc: 0.6746\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78185 to 0.78124, saving model to best.model\n",
      "1s - loss: 0.7973 - acc: 0.6698 - val_loss: 0.7812 - val_acc: 0.6755\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78124 to 0.77888, saving model to best.model\n",
      "1s - loss: 0.7968 - acc: 0.6693 - val_loss: 0.7789 - val_acc: 0.6741\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77888 to 0.77402, saving model to best.model\n",
      "1s - loss: 0.7944 - acc: 0.6684 - val_loss: 0.7740 - val_acc: 0.6781\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77402 to 0.77313, saving model to best.model\n",
      "2s - loss: 0.7932 - acc: 0.6701 - val_loss: 0.7731 - val_acc: 0.6850\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77313 to 0.76992, saving model to best.model\n",
      "2s - loss: 0.7914 - acc: 0.6710 - val_loss: 0.7699 - val_acc: 0.6810\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.76992 to 0.76911, saving model to best.model\n",
      "1s - loss: 0.7898 - acc: 0.6727 - val_loss: 0.7691 - val_acc: 0.6794\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76911 to 0.76530, saving model to best.model\n",
      "2s - loss: 0.7888 - acc: 0.6722 - val_loss: 0.7653 - val_acc: 0.6828\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76530 to 0.76318, saving model to best.model\n",
      "2s - loss: 0.7848 - acc: 0.6735 - val_loss: 0.7632 - val_acc: 0.6910\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76318 to 0.76288, saving model to best.model\n",
      "1s - loss: 0.7850 - acc: 0.6749 - val_loss: 0.7629 - val_acc: 0.6917\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76288 to 0.75899, saving model to best.model\n",
      "1s - loss: 0.7825 - acc: 0.6744 - val_loss: 0.7590 - val_acc: 0.6895\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.75899 to 0.75707, saving model to best.model\n",
      "1s - loss: 0.7829 - acc: 0.6765 - val_loss: 0.7571 - val_acc: 0.6884\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.75707 to 0.75482, saving model to best.model\n",
      "58s - loss: 0.7796 - acc: 0.6766 - val_loss: 0.7548 - val_acc: 0.6893\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75482 to 0.75317, saving model to best.model\n",
      "1s - loss: 0.7771 - acc: 0.6774 - val_loss: 0.7532 - val_acc: 0.6937\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75317 to 0.74973, saving model to best.model\n",
      "1s - loss: 0.7775 - acc: 0.6771 - val_loss: 0.7497 - val_acc: 0.6905\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.74973 to 0.74901, saving model to best.model\n",
      "1s - loss: 0.7755 - acc: 0.6794 - val_loss: 0.7490 - val_acc: 0.6907\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74901 to 0.74392, saving model to best.model\n",
      "1s - loss: 0.7728 - acc: 0.6796 - val_loss: 0.7439 - val_acc: 0.6957\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74392 to 0.74239, saving model to best.model\n",
      "1s - loss: 0.7716 - acc: 0.6810 - val_loss: 0.7424 - val_acc: 0.6986\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74239 to 0.73787, saving model to best.model\n",
      "1s - loss: 0.7700 - acc: 0.6811 - val_loss: 0.7379 - val_acc: 0.7015\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.73787 to 0.73566, saving model to best.model\n",
      "1s - loss: 0.7666 - acc: 0.6845 - val_loss: 0.7357 - val_acc: 0.7016\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "1s - loss: 0.7669 - acc: 0.6832 - val_loss: 0.7384 - val_acc: 0.6972\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73566 to 0.73144, saving model to best.model\n",
      "1s - loss: 0.7627 - acc: 0.6844 - val_loss: 0.7314 - val_acc: 0.7001\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73144 to 0.73065, saving model to best.model\n",
      "1s - loss: 0.7634 - acc: 0.6831 - val_loss: 0.7306 - val_acc: 0.7002\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73065 to 0.72284, saving model to best.model\n",
      "1s - loss: 0.7606 - acc: 0.6874 - val_loss: 0.7228 - val_acc: 0.7066\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72284 to 0.71945, saving model to best.model\n",
      "1s - loss: 0.7598 - acc: 0.6855 - val_loss: 0.7194 - val_acc: 0.7076\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.71945 to 0.71919, saving model to best.model\n",
      "1s - loss: 0.7574 - acc: 0.6867 - val_loss: 0.7192 - val_acc: 0.7104\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.71919 to 0.71427, saving model to best.model\n",
      "1s - loss: 0.7527 - acc: 0.6881 - val_loss: 0.7143 - val_acc: 0.7101\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "1s - loss: 0.7526 - acc: 0.6904 - val_loss: 0.7150 - val_acc: 0.7109\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71427 to 0.70976, saving model to best.model\n",
      "1s - loss: 0.7541 - acc: 0.6876 - val_loss: 0.7098 - val_acc: 0.7126\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.70976 to 0.70834, saving model to best.model\n",
      "1s - loss: 0.7478 - acc: 0.6898 - val_loss: 0.7083 - val_acc: 0.7117\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "1s - loss: 0.7487 - acc: 0.6896 - val_loss: 0.7086 - val_acc: 0.7108\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.70834 to 0.70528, saving model to best.model\n",
      "1s - loss: 0.7472 - acc: 0.6916 - val_loss: 0.7053 - val_acc: 0.7131\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70528 to 0.70308, saving model to best.model\n",
      "1s - loss: 0.7447 - acc: 0.6931 - val_loss: 0.7031 - val_acc: 0.7170\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70308 to 0.69831, saving model to best.model\n",
      "1s - loss: 0.7431 - acc: 0.6926 - val_loss: 0.6983 - val_acc: 0.7180\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.7416 - acc: 0.6932 - val_loss: 0.7010 - val_acc: 0.7141\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.69831 to 0.69430, saving model to best.model\n",
      "1s - loss: 0.7403 - acc: 0.6928 - val_loss: 0.6943 - val_acc: 0.7179\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.69430 to 0.68928, saving model to best.model\n",
      "737s - loss: 0.7375 - acc: 0.6955 - val_loss: 0.6893 - val_acc: 0.7219\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.7329 - acc: 0.6976 - val_loss: 0.6905 - val_acc: 0.7213\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.68928 to 0.68830, saving model to best.model\n",
      "1s - loss: 0.7374 - acc: 0.6959 - val_loss: 0.6883 - val_acc: 0.7186\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.68830 to 0.68815, saving model to best.model\n",
      "1s - loss: 0.7332 - acc: 0.6995 - val_loss: 0.6881 - val_acc: 0.7210\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.68815 to 0.68733, saving model to best.model\n",
      "1s - loss: 0.7320 - acc: 0.6984 - val_loss: 0.6873 - val_acc: 0.7194\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.68733 to 0.67929, saving model to best.model\n",
      "1s - loss: 0.7306 - acc: 0.6992 - val_loss: 0.6793 - val_acc: 0.7232\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.7322 - acc: 0.6986 - val_loss: 0.6833 - val_acc: 0.7206\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7305 - acc: 0.6979 - val_loss: 0.6827 - val_acc: 0.7227\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.67929 to 0.67875, saving model to best.model\n",
      "1s - loss: 0.7263 - acc: 0.7022 - val_loss: 0.6787 - val_acc: 0.7287\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.67875 to 0.67520, saving model to best.model\n",
      "1s - loss: 0.7255 - acc: 0.7026 - val_loss: 0.6752 - val_acc: 0.7254\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67520 to 0.67161, saving model to best.model\n",
      "1s - loss: 0.7239 - acc: 0.7009 - val_loss: 0.6716 - val_acc: 0.7245\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "2s - loss: 0.7257 - acc: 0.7002 - val_loss: 0.6729 - val_acc: 0.7259\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7237 - acc: 0.7020 - val_loss: 0.6722 - val_acc: 0.7303\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67161 to 0.66767, saving model to best.model\n",
      "2s - loss: 0.7236 - acc: 0.7008 - val_loss: 0.6677 - val_acc: 0.7310\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7205 - acc: 0.7058 - val_loss: 0.6690 - val_acc: 0.7290\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "2s - loss: 0.7174 - acc: 0.7061 - val_loss: 0.6679 - val_acc: 0.7280\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.66767 to 0.66727, saving model to best.model\n",
      "1s - loss: 0.7192 - acc: 0.7063 - val_loss: 0.6673 - val_acc: 0.7283\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.66727 to 0.66164, saving model to best.model\n",
      "1s - loss: 0.7179 - acc: 0.7074 - val_loss: 0.6616 - val_acc: 0.7338\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "1s - loss: 0.7169 - acc: 0.7043 - val_loss: 0.6629 - val_acc: 0.7315\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66164 to 0.66050, saving model to best.model\n",
      "1s - loss: 0.7128 - acc: 0.7075 - val_loss: 0.6605 - val_acc: 0.7345\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66050 to 0.66046, saving model to best.model\n",
      "1s - loss: 0.7134 - acc: 0.7080 - val_loss: 0.6605 - val_acc: 0.7327\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66046 to 0.65862, saving model to best.model\n",
      "1s - loss: 0.7143 - acc: 0.7080 - val_loss: 0.6586 - val_acc: 0.7357\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "32s - loss: 0.7136 - acc: 0.7063 - val_loss: 0.6588 - val_acc: 0.7336\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.65862 to 0.65398, saving model to best.model\n",
      "1s - loss: 0.7079 - acc: 0.7112 - val_loss: 0.6540 - val_acc: 0.7356\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7124 - acc: 0.7077 - val_loss: 0.6600 - val_acc: 0.7278\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7122 - acc: 0.7080 - val_loss: 0.6568 - val_acc: 0.7340\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.65398 to 0.65377, saving model to best.model\n",
      "1s - loss: 0.7112 - acc: 0.7093 - val_loss: 0.6538 - val_acc: 0.7336\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.65377 to 0.64955, saving model to best.model\n",
      "1s - loss: 0.7049 - acc: 0.7109 - val_loss: 0.6496 - val_acc: 0.7345\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.64955 to 0.64919, saving model to best.model\n",
      "1s - loss: 0.7062 - acc: 0.7115 - val_loss: 0.6492 - val_acc: 0.7364\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7044 - acc: 0.7120 - val_loss: 0.6505 - val_acc: 0.7336\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.64919 to 0.64759, saving model to best.model\n",
      "0s - loss: 0.7030 - acc: 0.7122 - val_loss: 0.6476 - val_acc: 0.7382\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.64759 to 0.64450, saving model to best.model\n",
      "0s - loss: 0.7020 - acc: 0.7148 - val_loss: 0.6445 - val_acc: 0.7379\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7042 - acc: 0.7108 - val_loss: 0.6454 - val_acc: 0.7369\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.64450 to 0.64181, saving model to best.model\n",
      "0s - loss: 0.7035 - acc: 0.7121 - val_loss: 0.6418 - val_acc: 0.7376\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7016 - acc: 0.7139 - val_loss: 0.6434 - val_acc: 0.7368\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6996 - acc: 0.7144 - val_loss: 0.6438 - val_acc: 0.7392\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.64181 to 0.63958, saving model to best.model\n",
      "1s - loss: 0.6977 - acc: 0.7139 - val_loss: 0.6396 - val_acc: 0.7412\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6975 - acc: 0.7135 - val_loss: 0.6409 - val_acc: 0.7393\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.63958 to 0.63873, saving model to best.model\n",
      "1s - loss: 0.6941 - acc: 0.7178 - val_loss: 0.6387 - val_acc: 0.7383\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6963 - acc: 0.7147 - val_loss: 0.6400 - val_acc: 0.7392\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.63873 to 0.63683, saving model to best.model\n",
      "1s - loss: 0.6955 - acc: 0.7183 - val_loss: 0.6368 - val_acc: 0.7395\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.63683 to 0.63664, saving model to best.model\n",
      "1s - loss: 0.6944 - acc: 0.7167 - val_loss: 0.6366 - val_acc: 0.7388\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.63664 to 0.63275, saving model to best.model\n",
      "1s - loss: 0.6931 - acc: 0.7179 - val_loss: 0.6327 - val_acc: 0.7418\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6941 - acc: 0.7158 - val_loss: 0.6387 - val_acc: 0.7386\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6923 - acc: 0.7144 - val_loss: 0.6332 - val_acc: 0.7456\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6934 - acc: 0.7177 - val_loss: 0.6330 - val_acc: 0.7431\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6902 - acc: 0.7183 - val_loss: 0.6368 - val_acc: 0.7379\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.63275 to 0.63011, saving model to best.model\n",
      "117s - loss: 0.6874 - acc: 0.7193 - val_loss: 0.6301 - val_acc: 0.7460\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6903 - acc: 0.7182 - val_loss: 0.6315 - val_acc: 0.7413\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.63011 to 0.63006, saving model to best.model\n",
      "1s - loss: 0.6895 - acc: 0.7166 - val_loss: 0.6301 - val_acc: 0.7450\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.63006 to 0.62502, saving model to best.model\n",
      "1s - loss: 0.6850 - acc: 0.7210 - val_loss: 0.6250 - val_acc: 0.7433\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6865 - acc: 0.7193 - val_loss: 0.6269 - val_acc: 0.7432\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6881 - acc: 0.7175 - val_loss: 0.6296 - val_acc: 0.7423\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62502 to 0.62368, saving model to best.model\n",
      "1s - loss: 0.6827 - acc: 0.7209 - val_loss: 0.6237 - val_acc: 0.7431\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6853 - acc: 0.7193 - val_loss: 0.6253 - val_acc: 0.7438\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6809 - acc: 0.7217 - val_loss: 0.6249 - val_acc: 0.7438\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.62368 to 0.62361, saving model to best.model\n",
      "1s - loss: 0.6836 - acc: 0.7217 - val_loss: 0.6236 - val_acc: 0.7438\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6851 - acc: 0.7213 - val_loss: 0.6262 - val_acc: 0.7432\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.62361 to 0.62083, saving model to best.model\n",
      "1s - loss: 0.6854 - acc: 0.7175 - val_loss: 0.6208 - val_acc: 0.7456\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.62083 to 0.61939, saving model to best.model\n",
      "2s - loss: 0.6806 - acc: 0.7223 - val_loss: 0.6194 - val_acc: 0.7471\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.61939 to 0.61628, saving model to best.model\n",
      "2s - loss: 0.6806 - acc: 0.7200 - val_loss: 0.6163 - val_acc: 0.7514\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "2s - loss: 0.6788 - acc: 0.7250 - val_loss: 0.6194 - val_acc: 0.7530\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6766 - acc: 0.7229 - val_loss: 0.6181 - val_acc: 0.7466\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6786 - acc: 0.7239 - val_loss: 0.6169 - val_acc: 0.7479\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6786 - acc: 0.7237 - val_loss: 0.6174 - val_acc: 0.7461\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.61628 to 0.61577, saving model to best.model\n",
      "1s - loss: 0.6766 - acc: 0.7241 - val_loss: 0.6158 - val_acc: 0.7512\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.61577 to 0.61460, saving model to best.model\n",
      "1s - loss: 0.6761 - acc: 0.7238 - val_loss: 0.6146 - val_acc: 0.7498\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6721 - acc: 0.7266 - val_loss: 0.6171 - val_acc: 0.7480\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6759 - acc: 0.7232 - val_loss: 0.6167 - val_acc: 0.7480\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.61460 to 0.61318, saving model to best.model\n",
      "1s - loss: 0.6735 - acc: 0.7244 - val_loss: 0.6132 - val_acc: 0.7475\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "57s - loss: 0.6755 - acc: 0.7263 - val_loss: 0.6156 - val_acc: 0.7460\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.61318 to 0.61007, saving model to best.model\n",
      "1s - loss: 0.6692 - acc: 0.7264 - val_loss: 0.6101 - val_acc: 0.7543\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6733 - acc: 0.7257 - val_loss: 0.6135 - val_acc: 0.7481\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6720 - acc: 0.7280 - val_loss: 0.6102 - val_acc: 0.7505\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6722 - acc: 0.7271 - val_loss: 0.6107 - val_acc: 0.7532\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.61007 to 0.60680, saving model to best.model\n",
      "1s - loss: 0.6677 - acc: 0.7289 - val_loss: 0.6068 - val_acc: 0.7566\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6693 - acc: 0.7264 - val_loss: 0.6080 - val_acc: 0.7539\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6697 - acc: 0.7277 - val_loss: 0.6086 - val_acc: 0.7508\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6709 - acc: 0.7268 - val_loss: 0.6078 - val_acc: 0.7546\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6720 - acc: 0.7256 - val_loss: 0.6080 - val_acc: 0.7539\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.60680 to 0.60514, saving model to best.model\n",
      "1s - loss: 0.6690 - acc: 0.7277 - val_loss: 0.6051 - val_acc: 0.7548\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.60514 to 0.60365, saving model to best.model\n",
      "1s - loss: 0.6690 - acc: 0.7275 - val_loss: 0.6036 - val_acc: 0.7569\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.60365 to 0.60333, saving model to best.model\n",
      "1s - loss: 0.6687 - acc: 0.7282 - val_loss: 0.6033 - val_acc: 0.7575\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6650 - acc: 0.7311 - val_loss: 0.6044 - val_acc: 0.7563\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6684 - acc: 0.7278 - val_loss: 0.6062 - val_acc: 0.7535\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.60333 to 0.60239, saving model to best.model\n",
      "1s - loss: 0.6670 - acc: 0.7255 - val_loss: 0.6024 - val_acc: 0.7546\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.60239 to 0.60194, saving model to best.model\n",
      "1s - loss: 0.6658 - acc: 0.7268 - val_loss: 0.6019 - val_acc: 0.7587\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6666 - acc: 0.7266 - val_loss: 0.6036 - val_acc: 0.7563\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.60194 to 0.60175, saving model to best.model\n",
      "1s - loss: 0.6633 - acc: 0.7308 - val_loss: 0.6018 - val_acc: 0.7573\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6674 - acc: 0.7270 - val_loss: 0.6029 - val_acc: 0.7564\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6630 - acc: 0.7275 - val_loss: 0.6036 - val_acc: 0.7544\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.60175 to 0.60063, saving model to best.model\n",
      "1s - loss: 0.6607 - acc: 0.7297 - val_loss: 0.6006 - val_acc: 0.7553\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.60063 to 0.59874, saving model to best.model\n",
      "1s - loss: 0.6638 - acc: 0.7299 - val_loss: 0.5987 - val_acc: 0.7576\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1460s - loss: 0.6623 - acc: 0.7290 - val_loss: 0.5999 - val_acc: 0.7571\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.59874 to 0.59840, saving model to best.model\n",
      "1s - loss: 0.6592 - acc: 0.7313 - val_loss: 0.5984 - val_acc: 0.7554\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.59840 to 0.59828, saving model to best.model\n",
      "1s - loss: 0.6581 - acc: 0.7329 - val_loss: 0.5983 - val_acc: 0.7570\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6596 - acc: 0.7311 - val_loss: 0.5985 - val_acc: 0.7566\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.59828 to 0.59595, saving model to best.model\n",
      "1s - loss: 0.6589 - acc: 0.7299 - val_loss: 0.5959 - val_acc: 0.7589\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6583 - acc: 0.7326 - val_loss: 0.5965 - val_acc: 0.7582\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6603 - acc: 0.7289 - val_loss: 0.5983 - val_acc: 0.7563\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.59595 to 0.59426, saving model to best.model\n",
      "0s - loss: 0.6589 - acc: 0.7307 - val_loss: 0.5943 - val_acc: 0.7607\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6578 - acc: 0.7326 - val_loss: 0.5979 - val_acc: 0.7626\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.59426 to 0.59322, saving model to best.model\n",
      "1s - loss: 0.6565 - acc: 0.7304 - val_loss: 0.5932 - val_acc: 0.7636\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7327 - val_loss: 0.5940 - val_acc: 0.7610\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6565 - acc: 0.7321 - val_loss: 0.5961 - val_acc: 0.7610\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.59322 to 0.59192, saving model to best.model\n",
      "1s - loss: 0.6606 - acc: 0.7294 - val_loss: 0.5919 - val_acc: 0.7644\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.59192 to 0.59095, saving model to best.model\n",
      "1s - loss: 0.6567 - acc: 0.7306 - val_loss: 0.5910 - val_acc: 0.7607\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.59095 to 0.59091, saving model to best.model\n",
      "1s - loss: 0.6525 - acc: 0.7353 - val_loss: 0.5909 - val_acc: 0.7630\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.59091 to 0.59091, saving model to best.model\n",
      "1s - loss: 0.6517 - acc: 0.7326 - val_loss: 0.5909 - val_acc: 0.7636\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6554 - acc: 0.7346 - val_loss: 0.5916 - val_acc: 0.7603\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.59091 to 0.58924, saving model to best.model\n",
      "1s - loss: 0.6537 - acc: 0.7335 - val_loss: 0.5892 - val_acc: 0.7664\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6525 - acc: 0.7329 - val_loss: 0.5922 - val_acc: 0.7587\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6533 - acc: 0.7334 - val_loss: 0.5893 - val_acc: 0.7602\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6550 - acc: 0.7335 - val_loss: 0.5899 - val_acc: 0.7646\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6515 - acc: 0.7339 - val_loss: 0.5937 - val_acc: 0.7658\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.58924 to 0.58823, saving model to best.model\n",
      "1s - loss: 0.6517 - acc: 0.7331 - val_loss: 0.5882 - val_acc: 0.7631\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6533 - acc: 0.7309 - val_loss: 0.5892 - val_acc: 0.7641\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.58823 to 0.58619, saving model to best.model\n",
      "116s - loss: 0.6485 - acc: 0.7364 - val_loss: 0.5862 - val_acc: 0.7672\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6507 - acc: 0.7333 - val_loss: 0.5866 - val_acc: 0.7683\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6502 - acc: 0.7352 - val_loss: 0.5886 - val_acc: 0.7633\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6513 - acc: 0.7334 - val_loss: 0.5894 - val_acc: 0.7632\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.58619 to 0.58554, saving model to best.model\n",
      "1s - loss: 0.6501 - acc: 0.7372 - val_loss: 0.5855 - val_acc: 0.7658\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.58554 to 0.58423, saving model to best.model\n",
      "1s - loss: 0.6473 - acc: 0.7364 - val_loss: 0.5842 - val_acc: 0.7648\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6477 - acc: 0.7354 - val_loss: 0.5854 - val_acc: 0.7664\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.58423 to 0.58401, saving model to best.model\n",
      "1s - loss: 0.6462 - acc: 0.7369 - val_loss: 0.5840 - val_acc: 0.7658\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.58401 to 0.58329, saving model to best.model\n",
      "1s - loss: 0.6460 - acc: 0.7369 - val_loss: 0.5833 - val_acc: 0.7649\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6476 - acc: 0.7375 - val_loss: 0.5839 - val_acc: 0.7684\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84423, saving model to best.model\n",
      "1s - loss: 0.8958 - acc: 0.6390 - val_loss: 0.8442 - val_acc: 0.6575\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84423 to 0.84233, saving model to best.model\n",
      "1s - loss: 0.8562 - acc: 0.6574 - val_loss: 0.8423 - val_acc: 0.6575\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84233 to 0.84047, saving model to best.model\n",
      "1s - loss: 0.8519 - acc: 0.6575 - val_loss: 0.8405 - val_acc: 0.6575\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84047 to 0.83489, saving model to best.model\n",
      "1s - loss: 0.8478 - acc: 0.6575 - val_loss: 0.8349 - val_acc: 0.6575\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83489 to 0.82848, saving model to best.model\n",
      "1s - loss: 0.8423 - acc: 0.6575 - val_loss: 0.8285 - val_acc: 0.6575\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "1s - loss: 0.8376 - acc: 0.6575 - val_loss: 0.8291 - val_acc: 0.6575\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82848 to 0.82267, saving model to best.model\n",
      "1s - loss: 0.8338 - acc: 0.6575 - val_loss: 0.8227 - val_acc: 0.6575\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82267 to 0.82264, saving model to best.model\n",
      "1s - loss: 0.8321 - acc: 0.6575 - val_loss: 0.8226 - val_acc: 0.6575\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "1s - loss: 0.8303 - acc: 0.6575 - val_loss: 0.8239 - val_acc: 0.6575\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82264 to 0.82173, saving model to best.model\n",
      "1s - loss: 0.8297 - acc: 0.6575 - val_loss: 0.8217 - val_acc: 0.6575\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82173 to 0.81959, saving model to best.model\n",
      "22s - loss: 0.8284 - acc: 0.6575 - val_loss: 0.8196 - val_acc: 0.6575\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8285 - acc: 0.6575 - val_loss: 0.8217 - val_acc: 0.6575\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8269 - acc: 0.6575 - val_loss: 0.8197 - val_acc: 0.6575\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81959 to 0.81853, saving model to best.model\n",
      "1s - loss: 0.8262 - acc: 0.6575 - val_loss: 0.8185 - val_acc: 0.6575\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "1s - loss: 0.8254 - acc: 0.6575 - val_loss: 0.8193 - val_acc: 0.6575\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81853 to 0.81810, saving model to best.model\n",
      "1s - loss: 0.8241 - acc: 0.6575 - val_loss: 0.8181 - val_acc: 0.6575\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "1s - loss: 0.8250 - acc: 0.6575 - val_loss: 0.8189 - val_acc: 0.6575\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81810 to 0.81695, saving model to best.model\n",
      "1s - loss: 0.8227 - acc: 0.6575 - val_loss: 0.8170 - val_acc: 0.6575\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81695 to 0.81662, saving model to best.model\n",
      "1s - loss: 0.8235 - acc: 0.6575 - val_loss: 0.8166 - val_acc: 0.6575\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "1s - loss: 0.8234 - acc: 0.6574 - val_loss: 0.8182 - val_acc: 0.6575\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81662 to 0.81587, saving model to best.model\n",
      "1s - loss: 0.8226 - acc: 0.6575 - val_loss: 0.8159 - val_acc: 0.6575\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81587 to 0.81432, saving model to best.model\n",
      "2s - loss: 0.8211 - acc: 0.6573 - val_loss: 0.8143 - val_acc: 0.6575\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "2s - loss: 0.8207 - acc: 0.6575 - val_loss: 0.8145 - val_acc: 0.6575\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81432 to 0.81371, saving model to best.model\n",
      "2s - loss: 0.8200 - acc: 0.6573 - val_loss: 0.8137 - val_acc: 0.6575\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81371 to 0.81234, saving model to best.model\n",
      "1s - loss: 0.8197 - acc: 0.6574 - val_loss: 0.8123 - val_acc: 0.6575\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81234 to 0.81006, saving model to best.model\n",
      "2s - loss: 0.8187 - acc: 0.6572 - val_loss: 0.8101 - val_acc: 0.6575\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81006 to 0.80888, saving model to best.model\n",
      "2s - loss: 0.8176 - acc: 0.6577 - val_loss: 0.8089 - val_acc: 0.6575\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80888 to 0.80749, saving model to best.model\n",
      "2s - loss: 0.8162 - acc: 0.6583 - val_loss: 0.8075 - val_acc: 0.6575\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80749 to 0.80585, saving model to best.model\n",
      "1s - loss: 0.8145 - acc: 0.6583 - val_loss: 0.8059 - val_acc: 0.6575\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80585 to 0.80453, saving model to best.model\n",
      "729s - loss: 0.8131 - acc: 0.6575 - val_loss: 0.8045 - val_acc: 0.6580\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80453 to 0.80126, saving model to best.model\n",
      "1s - loss: 0.8109 - acc: 0.6594 - val_loss: 0.8013 - val_acc: 0.6596\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80126 to 0.79963, saving model to best.model\n",
      "1s - loss: 0.8099 - acc: 0.6600 - val_loss: 0.7996 - val_acc: 0.6587\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79963 to 0.79622, saving model to best.model\n",
      "1s - loss: 0.8077 - acc: 0.6599 - val_loss: 0.7962 - val_acc: 0.6619\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79622 to 0.79406, saving model to best.model\n",
      "1s - loss: 0.8069 - acc: 0.6595 - val_loss: 0.7941 - val_acc: 0.6641\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79406 to 0.79337, saving model to best.model\n",
      "1s - loss: 0.8060 - acc: 0.6604 - val_loss: 0.7934 - val_acc: 0.6616\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79337 to 0.79004, saving model to best.model\n",
      "1s - loss: 0.8033 - acc: 0.6620 - val_loss: 0.7900 - val_acc: 0.6665\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "1s - loss: 0.8023 - acc: 0.6652 - val_loss: 0.7910 - val_acc: 0.6664\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79004 to 0.78782, saving model to best.model\n",
      "1s - loss: 0.8019 - acc: 0.6634 - val_loss: 0.7878 - val_acc: 0.6649\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78782 to 0.78394, saving model to best.model\n",
      "1s - loss: 0.7975 - acc: 0.6653 - val_loss: 0.7839 - val_acc: 0.6721\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78394 to 0.78316, saving model to best.model\n",
      "1s - loss: 0.7968 - acc: 0.6643 - val_loss: 0.7832 - val_acc: 0.6701\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78316 to 0.78309, saving model to best.model\n",
      "1s - loss: 0.7951 - acc: 0.6666 - val_loss: 0.7831 - val_acc: 0.6712\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78309 to 0.77960, saving model to best.model\n",
      "2s - loss: 0.7936 - acc: 0.6674 - val_loss: 0.7796 - val_acc: 0.6726\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "2s - loss: 0.7926 - acc: 0.6690 - val_loss: 0.7813 - val_acc: 0.6733\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77960 to 0.77824, saving model to best.model\n",
      "2s - loss: 0.7911 - acc: 0.6688 - val_loss: 0.7782 - val_acc: 0.6708\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77824 to 0.77458, saving model to best.model\n",
      "1s - loss: 0.7911 - acc: 0.6690 - val_loss: 0.7746 - val_acc: 0.6803\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "1s - loss: 0.7907 - acc: 0.6696 - val_loss: 0.7746 - val_acc: 0.6770\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77458 to 0.77335, saving model to best.model\n",
      "1s - loss: 0.7885 - acc: 0.6702 - val_loss: 0.7733 - val_acc: 0.6780\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77335 to 0.77130, saving model to best.model\n",
      "1s - loss: 0.7872 - acc: 0.6719 - val_loss: 0.7713 - val_acc: 0.6768\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77130 to 0.77014, saving model to best.model\n",
      "1s - loss: 0.7851 - acc: 0.6731 - val_loss: 0.7701 - val_acc: 0.6796\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77014 to 0.76537, saving model to best.model\n",
      "1s - loss: 0.7816 - acc: 0.6736 - val_loss: 0.7654 - val_acc: 0.6831\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "1s - loss: 0.7829 - acc: 0.6743 - val_loss: 0.7663 - val_acc: 0.6804\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76537 to 0.76087, saving model to best.model\n",
      "1s - loss: 0.7803 - acc: 0.6760 - val_loss: 0.7609 - val_acc: 0.6864\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "413s - loss: 0.7806 - acc: 0.6765 - val_loss: 0.7616 - val_acc: 0.6835\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76087 to 0.76059, saving model to best.model\n",
      "1s - loss: 0.7787 - acc: 0.6778 - val_loss: 0.7606 - val_acc: 0.6854\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76059 to 0.75903, saving model to best.model\n",
      "1s - loss: 0.7765 - acc: 0.6761 - val_loss: 0.7590 - val_acc: 0.6862\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75903 to 0.75291, saving model to best.model\n",
      "1s - loss: 0.7749 - acc: 0.6773 - val_loss: 0.7529 - val_acc: 0.6916\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75291 to 0.75195, saving model to best.model\n",
      "1s - loss: 0.7735 - acc: 0.6787 - val_loss: 0.7520 - val_acc: 0.6918\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75195 to 0.75194, saving model to best.model\n",
      "1s - loss: 0.7739 - acc: 0.6766 - val_loss: 0.7519 - val_acc: 0.6886\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75194 to 0.74622, saving model to best.model\n",
      "1s - loss: 0.7702 - acc: 0.6808 - val_loss: 0.7462 - val_acc: 0.6941\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "1s - loss: 0.7690 - acc: 0.6822 - val_loss: 0.7497 - val_acc: 0.6945\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74622 to 0.74076, saving model to best.model\n",
      "1s - loss: 0.7659 - acc: 0.6825 - val_loss: 0.7408 - val_acc: 0.6981\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74076 to 0.73991, saving model to best.model\n",
      "1s - loss: 0.7639 - acc: 0.6833 - val_loss: 0.7399 - val_acc: 0.6950\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73991 to 0.73775, saving model to best.model\n",
      "1s - loss: 0.7640 - acc: 0.6817 - val_loss: 0.7378 - val_acc: 0.6967\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73775 to 0.73581, saving model to best.model\n",
      "2s - loss: 0.7603 - acc: 0.6847 - val_loss: 0.7358 - val_acc: 0.6986\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73581 to 0.73264, saving model to best.model\n",
      "1s - loss: 0.7606 - acc: 0.6845 - val_loss: 0.7326 - val_acc: 0.7009\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73264 to 0.73040, saving model to best.model\n",
      "1s - loss: 0.7586 - acc: 0.6866 - val_loss: 0.7304 - val_acc: 0.7020\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73040 to 0.72726, saving model to best.model\n",
      "1s - loss: 0.7557 - acc: 0.6854 - val_loss: 0.7273 - val_acc: 0.7009\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "1s - loss: 0.7543 - acc: 0.6870 - val_loss: 0.7275 - val_acc: 0.7040\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72726 to 0.72690, saving model to best.model\n",
      "1s - loss: 0.7548 - acc: 0.6879 - val_loss: 0.7269 - val_acc: 0.7039\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72690 to 0.71970, saving model to best.model\n",
      "1s - loss: 0.7506 - acc: 0.6894 - val_loss: 0.7197 - val_acc: 0.7047\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "1s - loss: 0.7503 - acc: 0.6903 - val_loss: 0.7202 - val_acc: 0.7062\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71970 to 0.71911, saving model to best.model\n",
      "1s - loss: 0.7474 - acc: 0.6889 - val_loss: 0.7191 - val_acc: 0.7037\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71911 to 0.71388, saving model to best.model\n",
      "1s - loss: 0.7473 - acc: 0.6916 - val_loss: 0.7139 - val_acc: 0.7077\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71388 to 0.71172, saving model to best.model\n",
      "1s - loss: 0.7439 - acc: 0.6917 - val_loss: 0.7117 - val_acc: 0.7103\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71172 to 0.71023, saving model to best.model\n",
      "62s - loss: 0.7454 - acc: 0.6933 - val_loss: 0.7102 - val_acc: 0.7087\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71023 to 0.70649, saving model to best.model\n",
      "1s - loss: 0.7400 - acc: 0.6950 - val_loss: 0.7065 - val_acc: 0.7131\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7410 - acc: 0.6935 - val_loss: 0.7075 - val_acc: 0.7093\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70649 to 0.70442, saving model to best.model\n",
      "1s - loss: 0.7395 - acc: 0.6959 - val_loss: 0.7044 - val_acc: 0.7139\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7397 - acc: 0.6942 - val_loss: 0.7048 - val_acc: 0.7136\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70442 to 0.70308, saving model to best.model\n",
      "1s - loss: 0.7357 - acc: 0.6966 - val_loss: 0.7031 - val_acc: 0.7170\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70308 to 0.69802, saving model to best.model\n",
      "1s - loss: 0.7338 - acc: 0.6963 - val_loss: 0.6980 - val_acc: 0.7155\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7341 - acc: 0.6969 - val_loss: 0.6983 - val_acc: 0.7138\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69802 to 0.69425, saving model to best.model\n",
      "1s - loss: 0.7321 - acc: 0.6975 - val_loss: 0.6943 - val_acc: 0.7170\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7342 - acc: 0.6972 - val_loss: 0.6965 - val_acc: 0.7170\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69425 to 0.69401, saving model to best.model\n",
      "1s - loss: 0.7330 - acc: 0.6975 - val_loss: 0.6940 - val_acc: 0.7166\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69401 to 0.69138, saving model to best.model\n",
      "1s - loss: 0.7295 - acc: 0.6979 - val_loss: 0.6914 - val_acc: 0.7185\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "1s - loss: 0.7274 - acc: 0.7004 - val_loss: 0.6919 - val_acc: 0.7180\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.69138 to 0.68991, saving model to best.model\n",
      "1s - loss: 0.7279 - acc: 0.7005 - val_loss: 0.6899 - val_acc: 0.7228\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68991 to 0.68978, saving model to best.model\n",
      "2s - loss: 0.7272 - acc: 0.7004 - val_loss: 0.6898 - val_acc: 0.7204\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68978 to 0.68485, saving model to best.model\n",
      "2s - loss: 0.7249 - acc: 0.6989 - val_loss: 0.6849 - val_acc: 0.7220\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7240 - acc: 0.7009 - val_loss: 0.6876 - val_acc: 0.7162\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68485 to 0.68249, saving model to best.model\n",
      "1s - loss: 0.7210 - acc: 0.7027 - val_loss: 0.6825 - val_acc: 0.7224\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7209 - acc: 0.7008 - val_loss: 0.6836 - val_acc: 0.7193\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68249 to 0.67984, saving model to best.model\n",
      "1s - loss: 0.7208 - acc: 0.7039 - val_loss: 0.6798 - val_acc: 0.7239\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7172 - acc: 0.7044 - val_loss: 0.6819 - val_acc: 0.7272\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67984 to 0.67671, saving model to best.model\n",
      "1s - loss: 0.7176 - acc: 0.7059 - val_loss: 0.6767 - val_acc: 0.7225\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.67671 to 0.67649, saving model to best.model\n",
      "1s - loss: 0.7147 - acc: 0.7053 - val_loss: 0.6765 - val_acc: 0.7252\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67649 to 0.67588, saving model to best.model\n",
      "1s - loss: 0.7158 - acc: 0.7050 - val_loss: 0.6759 - val_acc: 0.7242\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67588 to 0.67435, saving model to best.model\n",
      "1s - loss: 0.7111 - acc: 0.7078 - val_loss: 0.6743 - val_acc: 0.7245\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.67435 to 0.67407, saving model to best.model\n",
      "1s - loss: 0.7147 - acc: 0.7045 - val_loss: 0.6741 - val_acc: 0.7249\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.67407 to 0.67095, saving model to best.model\n",
      "1s - loss: 0.7134 - acc: 0.7070 - val_loss: 0.6710 - val_acc: 0.7279\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7106 - acc: 0.7075 - val_loss: 0.6719 - val_acc: 0.7252\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.67095 to 0.67026, saving model to best.model\n",
      "1s - loss: 0.7126 - acc: 0.7054 - val_loss: 0.6703 - val_acc: 0.7258\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.67026 to 0.66811, saving model to best.model\n",
      "1s - loss: 0.7097 - acc: 0.7085 - val_loss: 0.6681 - val_acc: 0.7302\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66811 to 0.66693, saving model to best.model\n",
      "1s - loss: 0.7098 - acc: 0.7066 - val_loss: 0.6669 - val_acc: 0.7309\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.66693 to 0.66505, saving model to best.model\n",
      "1s - loss: 0.7087 - acc: 0.7082 - val_loss: 0.6651 - val_acc: 0.7290\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.7089 - acc: 0.7112 - val_loss: 0.6663 - val_acc: 0.7309\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.66505 to 0.66337, saving model to best.model\n",
      "1s - loss: 0.7047 - acc: 0.7099 - val_loss: 0.6634 - val_acc: 0.7343\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.7069 - acc: 0.7068 - val_loss: 0.6635 - val_acc: 0.7283\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66337 to 0.66070, saving model to best.model\n",
      "1s - loss: 0.7024 - acc: 0.7109 - val_loss: 0.6607 - val_acc: 0.7344\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.66070 to 0.65991, saving model to best.model\n",
      "1s - loss: 0.7027 - acc: 0.7079 - val_loss: 0.6599 - val_acc: 0.7356\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65991 to 0.65901, saving model to best.model\n",
      "1s - loss: 0.7037 - acc: 0.7069 - val_loss: 0.6590 - val_acc: 0.7352\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65901 to 0.65780, saving model to best.model\n",
      "1s - loss: 0.7039 - acc: 0.7118 - val_loss: 0.6578 - val_acc: 0.7362\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65780 to 0.65724, saving model to best.model\n",
      "45s - loss: 0.6994 - acc: 0.7138 - val_loss: 0.6572 - val_acc: 0.7335\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6978 - acc: 0.7133 - val_loss: 0.6585 - val_acc: 0.7372\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65724 to 0.65549, saving model to best.model\n",
      "1s - loss: 0.6954 - acc: 0.7168 - val_loss: 0.6555 - val_acc: 0.7384\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6986 - acc: 0.7125 - val_loss: 0.6570 - val_acc: 0.7359\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65549 to 0.65233, saving model to best.model\n",
      "1s - loss: 0.6972 - acc: 0.7132 - val_loss: 0.6523 - val_acc: 0.7370\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6967 - acc: 0.7112 - val_loss: 0.6538 - val_acc: 0.7376\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65233 to 0.65139, saving model to best.model\n",
      "1s - loss: 0.6982 - acc: 0.7126 - val_loss: 0.6514 - val_acc: 0.7381\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65139 to 0.64951, saving model to best.model\n",
      "1s - loss: 0.6927 - acc: 0.7126 - val_loss: 0.6495 - val_acc: 0.7369\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6935 - acc: 0.7152 - val_loss: 0.6519 - val_acc: 0.7347\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6950 - acc: 0.7145 - val_loss: 0.6513 - val_acc: 0.7374\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64951 to 0.64775, saving model to best.model\n",
      "1s - loss: 0.6896 - acc: 0.7157 - val_loss: 0.6477 - val_acc: 0.7384\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6887 - acc: 0.7164 - val_loss: 0.6496 - val_acc: 0.7397\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "2s - loss: 0.6914 - acc: 0.7155 - val_loss: 0.6493 - val_acc: 0.7386\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6907 - acc: 0.7167 - val_loss: 0.6516 - val_acc: 0.7390\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64775 to 0.64638, saving model to best.model\n",
      "1s - loss: 0.6877 - acc: 0.7172 - val_loss: 0.6464 - val_acc: 0.7393\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6895 - acc: 0.7154 - val_loss: 0.6494 - val_acc: 0.7404\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6882 - acc: 0.7182 - val_loss: 0.6487 - val_acc: 0.7340\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64638 to 0.64249, saving model to best.model\n",
      "1s - loss: 0.6868 - acc: 0.7185 - val_loss: 0.6425 - val_acc: 0.7391\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.64249 to 0.64187, saving model to best.model\n",
      "1s - loss: 0.6859 - acc: 0.7197 - val_loss: 0.6419 - val_acc: 0.7393\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6859 - acc: 0.7174 - val_loss: 0.6425 - val_acc: 0.7379\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.64187 to 0.64067, saving model to best.model\n",
      "1s - loss: 0.6879 - acc: 0.7173 - val_loss: 0.6407 - val_acc: 0.7403\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6823 - acc: 0.7200 - val_loss: 0.6433 - val_acc: 0.7397\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6863 - acc: 0.7185 - val_loss: 0.6435 - val_acc: 0.7378\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6829 - acc: 0.7201 - val_loss: 0.6443 - val_acc: 0.7374\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6835 - acc: 0.7198 - val_loss: 0.6428 - val_acc: 0.7378\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.64067 to 0.63868, saving model to best.model\n",
      "1s - loss: 0.6810 - acc: 0.7198 - val_loss: 0.6387 - val_acc: 0.7413\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6812 - acc: 0.7193 - val_loss: 0.6407 - val_acc: 0.7406\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6804 - acc: 0.7191 - val_loss: 0.6409 - val_acc: 0.7406\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6813 - acc: 0.7188 - val_loss: 0.6404 - val_acc: 0.7415\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63868 to 0.63724, saving model to best.model\n",
      "1s - loss: 0.6805 - acc: 0.7224 - val_loss: 0.6372 - val_acc: 0.7420\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63724 to 0.63696, saving model to best.model\n",
      "2s - loss: 0.6826 - acc: 0.7207 - val_loss: 0.6370 - val_acc: 0.7423\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6783 - acc: 0.7209 - val_loss: 0.6384 - val_acc: 0.7411\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6772 - acc: 0.7224 - val_loss: 0.6379 - val_acc: 0.7416\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63696 to 0.63680, saving model to best.model\n",
      "1s - loss: 0.6776 - acc: 0.7236 - val_loss: 0.6368 - val_acc: 0.7416\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.63680 to 0.63152, saving model to best.model\n",
      "1s - loss: 0.6717 - acc: 0.7253 - val_loss: 0.6315 - val_acc: 0.7424\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6747 - acc: 0.7236 - val_loss: 0.6336 - val_acc: 0.7417\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6754 - acc: 0.7229 - val_loss: 0.6333 - val_acc: 0.7412\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6726 - acc: 0.7242 - val_loss: 0.6324 - val_acc: 0.7425\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6740 - acc: 0.7221 - val_loss: 0.6325 - val_acc: 0.7418\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.63152 to 0.62982, saving model to best.model\n",
      "1s - loss: 0.6739 - acc: 0.7231 - val_loss: 0.6298 - val_acc: 0.7431\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "85s - loss: 0.6768 - acc: 0.7227 - val_loss: 0.6328 - val_acc: 0.7425\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6734 - acc: 0.7240 - val_loss: 0.6302 - val_acc: 0.7427\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62982 to 0.62944, saving model to best.model\n",
      "1s - loss: 0.6714 - acc: 0.7249 - val_loss: 0.6294 - val_acc: 0.7431\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.62944 to 0.62934, saving model to best.model\n",
      "1s - loss: 0.6726 - acc: 0.7252 - val_loss: 0.6293 - val_acc: 0.7439\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.62934 to 0.62910, saving model to best.model\n",
      "0s - loss: 0.6707 - acc: 0.7237 - val_loss: 0.6291 - val_acc: 0.7441\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62910 to 0.62907, saving model to best.model\n",
      "1s - loss: 0.6712 - acc: 0.7248 - val_loss: 0.6291 - val_acc: 0.7451\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.62907 to 0.62727, saving model to best.model\n",
      "1s - loss: 0.6709 - acc: 0.7212 - val_loss: 0.6273 - val_acc: 0.7440\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6695 - acc: 0.7254 - val_loss: 0.6275 - val_acc: 0.7445\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62727 to 0.62493, saving model to best.model\n",
      "1s - loss: 0.6667 - acc: 0.7265 - val_loss: 0.6249 - val_acc: 0.7459\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6682 - acc: 0.7244 - val_loss: 0.6252 - val_acc: 0.7453\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6709 - acc: 0.7235 - val_loss: 0.6287 - val_acc: 0.7433\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.62493 to 0.62392, saving model to best.model\n",
      "1s - loss: 0.6672 - acc: 0.7270 - val_loss: 0.6239 - val_acc: 0.7448\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "2s - loss: 0.6661 - acc: 0.7259 - val_loss: 0.6253 - val_acc: 0.7441\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "2s - loss: 0.6662 - acc: 0.7283 - val_loss: 0.6264 - val_acc: 0.7434\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.62392 to 0.62224, saving model to best.model\n",
      "2s - loss: 0.6633 - acc: 0.7285 - val_loss: 0.6222 - val_acc: 0.7452\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6665 - acc: 0.7253 - val_loss: 0.6230 - val_acc: 0.7465\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6649 - acc: 0.7268 - val_loss: 0.6224 - val_acc: 0.7459\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.62224 to 0.62149, saving model to best.model\n",
      "1s - loss: 0.6638 - acc: 0.7284 - val_loss: 0.6215 - val_acc: 0.7461\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.62149 to 0.61990, saving model to best.model\n",
      "1s - loss: 0.6607 - acc: 0.7283 - val_loss: 0.6199 - val_acc: 0.7471\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6647 - acc: 0.7263 - val_loss: 0.6215 - val_acc: 0.7458\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.61990 to 0.61916, saving model to best.model\n",
      "1s - loss: 0.6604 - acc: 0.7299 - val_loss: 0.6192 - val_acc: 0.7484\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6602 - acc: 0.7278 - val_loss: 0.6198 - val_acc: 0.7495\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6597 - acc: 0.7308 - val_loss: 0.6201 - val_acc: 0.7487\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61916 to 0.61678, saving model to best.model\n",
      "1s - loss: 0.6600 - acc: 0.7289 - val_loss: 0.6168 - val_acc: 0.7479\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6631 - acc: 0.7277 - val_loss: 0.6182 - val_acc: 0.7480\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61678 to 0.61624, saving model to best.model\n",
      "1s - loss: 0.6616 - acc: 0.7276 - val_loss: 0.6162 - val_acc: 0.7494\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6624 - acc: 0.7299 - val_loss: 0.6182 - val_acc: 0.7480\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6577 - acc: 0.7303 - val_loss: 0.6188 - val_acc: 0.7486\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6581 - acc: 0.7302 - val_loss: 0.6169 - val_acc: 0.7506\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6569 - acc: 0.7324 - val_loss: 0.6174 - val_acc: 0.7502\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6590 - acc: 0.7287 - val_loss: 0.6167 - val_acc: 0.7489\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61624 to 0.61443, saving model to best.model\n",
      "1s - loss: 0.6587 - acc: 0.7288 - val_loss: 0.6144 - val_acc: 0.7516\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.61443 to 0.61441, saving model to best.model\n",
      "1s - loss: 0.6553 - acc: 0.7305 - val_loss: 0.6144 - val_acc: 0.7506\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.61441 to 0.61357, saving model to best.model\n",
      "1s - loss: 0.6556 - acc: 0.7333 - val_loss: 0.6136 - val_acc: 0.7502\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.61357 to 0.61209, saving model to best.model\n",
      "1s - loss: 0.6555 - acc: 0.7285 - val_loss: 0.6121 - val_acc: 0.7522\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6570 - acc: 0.7329 - val_loss: 0.6135 - val_acc: 0.7518\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6518 - acc: 0.7346 - val_loss: 0.6127 - val_acc: 0.7512\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6535 - acc: 0.7314 - val_loss: 0.6125 - val_acc: 0.7515\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6531 - acc: 0.7304 - val_loss: 0.6128 - val_acc: 0.7509\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6554 - acc: 0.7304 - val_loss: 0.6131 - val_acc: 0.7523\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.61209 to 0.61146, saving model to best.model\n",
      "1s - loss: 0.6519 - acc: 0.7312 - val_loss: 0.6115 - val_acc: 0.7532\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.61146 to 0.61004, saving model to best.model\n",
      "1s - loss: 0.6518 - acc: 0.7320 - val_loss: 0.6100 - val_acc: 0.7520\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.61004 to 0.60900, saving model to best.model\n",
      "1s - loss: 0.6500 - acc: 0.7352 - val_loss: 0.6090 - val_acc: 0.7536\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6518 - acc: 0.7328 - val_loss: 0.6103 - val_acc: 0.7544\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6501 - acc: 0.7345 - val_loss: 0.6093 - val_acc: 0.7542\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60900 to 0.60874, saving model to best.model\n",
      "46s - loss: 0.6498 - acc: 0.7328 - val_loss: 0.6087 - val_acc: 0.7540\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6532 - acc: 0.7347 - val_loss: 0.6094 - val_acc: 0.7533\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83330, saving model to best.model\n",
      "1s - loss: 0.9076 - acc: 0.6329 - val_loss: 0.8333 - val_acc: 0.6643\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8560 - acc: 0.6593 - val_loss: 0.8338 - val_acc: 0.6643\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.83330 to 0.83171, saving model to best.model\n",
      "1s - loss: 0.8514 - acc: 0.6595 - val_loss: 0.8317 - val_acc: 0.6643\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83171 to 0.82470, saving model to best.model\n",
      "1s - loss: 0.8469 - acc: 0.6595 - val_loss: 0.8247 - val_acc: 0.6643\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.82470 to 0.82197, saving model to best.model\n",
      "0s - loss: 0.8407 - acc: 0.6595 - val_loss: 0.8220 - val_acc: 0.6643\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82197 to 0.81676, saving model to best.model\n",
      "0s - loss: 0.8333 - acc: 0.6595 - val_loss: 0.8168 - val_acc: 0.6643\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.81676 to 0.81243, saving model to best.model\n",
      "1s - loss: 0.8312 - acc: 0.6595 - val_loss: 0.8124 - val_acc: 0.6643\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81243 to 0.81156, saving model to best.model\n",
      "1s - loss: 0.8285 - acc: 0.6595 - val_loss: 0.8116 - val_acc: 0.6643\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81156 to 0.81016, saving model to best.model\n",
      "1s - loss: 0.8255 - acc: 0.6596 - val_loss: 0.8102 - val_acc: 0.6643\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81016 to 0.80931, saving model to best.model\n",
      "1s - loss: 0.8252 - acc: 0.6594 - val_loss: 0.8093 - val_acc: 0.6643\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8254 - acc: 0.6595 - val_loss: 0.8105 - val_acc: 0.6643\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.80931 to 0.80801, saving model to best.model\n",
      "1s - loss: 0.8232 - acc: 0.6595 - val_loss: 0.8080 - val_acc: 0.6643\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.80801 to 0.80690, saving model to best.model\n",
      "1s - loss: 0.8238 - acc: 0.6595 - val_loss: 0.8069 - val_acc: 0.6643\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8227 - acc: 0.6594 - val_loss: 0.8069 - val_acc: 0.6643\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.80690 to 0.80670, saving model to best.model\n",
      "1s - loss: 0.8210 - acc: 0.6596 - val_loss: 0.8067 - val_acc: 0.6643\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.80670 to 0.80537, saving model to best.model\n",
      "1s - loss: 0.8218 - acc: 0.6596 - val_loss: 0.8054 - val_acc: 0.6643\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.80537 to 0.80459, saving model to best.model\n",
      "1s - loss: 0.8202 - acc: 0.6596 - val_loss: 0.8046 - val_acc: 0.6643\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "1s - loss: 0.8196 - acc: 0.6595 - val_loss: 0.8052 - val_acc: 0.6643\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "1s - loss: 0.8174 - acc: 0.6598 - val_loss: 0.8049 - val_acc: 0.6644\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.80459 to 0.80341, saving model to best.model\n",
      "24s - loss: 0.8160 - acc: 0.6601 - val_loss: 0.8034 - val_acc: 0.6644\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.80341 to 0.80249, saving model to best.model\n",
      "1s - loss: 0.8164 - acc: 0.6605 - val_loss: 0.8025 - val_acc: 0.6646\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.80249 to 0.80209, saving model to best.model\n",
      "1s - loss: 0.8144 - acc: 0.6630 - val_loss: 0.8021 - val_acc: 0.6667\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.80209 to 0.80150, saving model to best.model\n",
      "1s - loss: 0.8149 - acc: 0.6621 - val_loss: 0.8015 - val_acc: 0.6657\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80150 to 0.80102, saving model to best.model\n",
      "0s - loss: 0.8129 - acc: 0.6624 - val_loss: 0.8010 - val_acc: 0.6670\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80102 to 0.79832, saving model to best.model\n",
      "1s - loss: 0.8118 - acc: 0.6636 - val_loss: 0.7983 - val_acc: 0.6681\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.79832 to 0.79612, saving model to best.model\n",
      "0s - loss: 0.8124 - acc: 0.6641 - val_loss: 0.7961 - val_acc: 0.6697\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.79612 to 0.79508, saving model to best.model\n",
      "0s - loss: 0.8102 - acc: 0.6641 - val_loss: 0.7951 - val_acc: 0.6698\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.79508 to 0.79380, saving model to best.model\n",
      "1s - loss: 0.8096 - acc: 0.6633 - val_loss: 0.7938 - val_acc: 0.6707\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.79380 to 0.79222, saving model to best.model\n",
      "1s - loss: 0.8082 - acc: 0.6653 - val_loss: 0.7922 - val_acc: 0.6722\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.79222 to 0.79108, saving model to best.model\n",
      "1s - loss: 0.8062 - acc: 0.6665 - val_loss: 0.7911 - val_acc: 0.6710\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79108 to 0.78933, saving model to best.model\n",
      "1s - loss: 0.8051 - acc: 0.6659 - val_loss: 0.7893 - val_acc: 0.6718\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.78933 to 0.78778, saving model to best.model\n",
      "1s - loss: 0.8040 - acc: 0.6660 - val_loss: 0.7878 - val_acc: 0.6719\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.78778 to 0.78562, saving model to best.model\n",
      "2s - loss: 0.8023 - acc: 0.6674 - val_loss: 0.7856 - val_acc: 0.6720\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.78562 to 0.78431, saving model to best.model\n",
      "1s - loss: 0.8007 - acc: 0.6673 - val_loss: 0.7843 - val_acc: 0.6718\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "1s - loss: 0.7991 - acc: 0.6690 - val_loss: 0.7846 - val_acc: 0.6722\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.78431 to 0.78041, saving model to best.model\n",
      "1s - loss: 0.7971 - acc: 0.6688 - val_loss: 0.7804 - val_acc: 0.6732\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78041 to 0.77797, saving model to best.model\n",
      "1s - loss: 0.7968 - acc: 0.6683 - val_loss: 0.7780 - val_acc: 0.6749\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.77797 to 0.77502, saving model to best.model\n",
      "1s - loss: 0.7941 - acc: 0.6703 - val_loss: 0.7750 - val_acc: 0.6766\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "1s - loss: 0.7938 - acc: 0.6684 - val_loss: 0.7760 - val_acc: 0.6730\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.77502 to 0.77280, saving model to best.model\n",
      "1s - loss: 0.7908 - acc: 0.6704 - val_loss: 0.7728 - val_acc: 0.6774\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77280 to 0.76929, saving model to best.model\n",
      "1s - loss: 0.7899 - acc: 0.6692 - val_loss: 0.7693 - val_acc: 0.6793\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.76929 to 0.76585, saving model to best.model\n",
      "1s - loss: 0.7880 - acc: 0.6734 - val_loss: 0.7659 - val_acc: 0.6811\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.76585 to 0.76466, saving model to best.model\n",
      "26s - loss: 0.7853 - acc: 0.6725 - val_loss: 0.7647 - val_acc: 0.6830\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.76466 to 0.76430, saving model to best.model\n",
      "1s - loss: 0.7857 - acc: 0.6724 - val_loss: 0.7643 - val_acc: 0.6801\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76430 to 0.76070, saving model to best.model\n",
      "1s - loss: 0.7839 - acc: 0.6730 - val_loss: 0.7607 - val_acc: 0.6849\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76070 to 0.76031, saving model to best.model\n",
      "1s - loss: 0.7832 - acc: 0.6743 - val_loss: 0.7603 - val_acc: 0.6863\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76031 to 0.75687, saving model to best.model\n",
      "1s - loss: 0.7802 - acc: 0.6751 - val_loss: 0.7569 - val_acc: 0.6861\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.75687 to 0.75438, saving model to best.model\n",
      "1s - loss: 0.7790 - acc: 0.6750 - val_loss: 0.7544 - val_acc: 0.6862\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.75438 to 0.75111, saving model to best.model\n",
      "1s - loss: 0.7772 - acc: 0.6799 - val_loss: 0.7511 - val_acc: 0.6870\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "1s - loss: 0.7758 - acc: 0.6797 - val_loss: 0.7533 - val_acc: 0.6882\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75111 to 0.74810, saving model to best.model\n",
      "1s - loss: 0.7722 - acc: 0.6789 - val_loss: 0.7481 - val_acc: 0.6896\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "1s - loss: 0.7727 - acc: 0.6782 - val_loss: 0.7483 - val_acc: 0.6896\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.74810 to 0.74136, saving model to best.model\n",
      "1s - loss: 0.7695 - acc: 0.6801 - val_loss: 0.7414 - val_acc: 0.6906\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74136 to 0.74091, saving model to best.model\n",
      "1s - loss: 0.7690 - acc: 0.6801 - val_loss: 0.7409 - val_acc: 0.6918\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74091 to 0.73702, saving model to best.model\n",
      "2s - loss: 0.7681 - acc: 0.6813 - val_loss: 0.7370 - val_acc: 0.6932\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.73702 to 0.73491, saving model to best.model\n",
      "2s - loss: 0.7646 - acc: 0.6815 - val_loss: 0.7349 - val_acc: 0.6925\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.73491 to 0.73166, saving model to best.model\n",
      "2s - loss: 0.7642 - acc: 0.6826 - val_loss: 0.7317 - val_acc: 0.6963\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.73166 to 0.72665, saving model to best.model\n",
      "2s - loss: 0.7606 - acc: 0.6837 - val_loss: 0.7266 - val_acc: 0.6993\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.72665 to 0.72657, saving model to best.model\n",
      "2s - loss: 0.7582 - acc: 0.6859 - val_loss: 0.7266 - val_acc: 0.6995\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.72657 to 0.72631, saving model to best.model\n",
      "1s - loss: 0.7582 - acc: 0.6871 - val_loss: 0.7263 - val_acc: 0.6975\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.72631 to 0.71913, saving model to best.model\n",
      "1s - loss: 0.7546 - acc: 0.6897 - val_loss: 0.7191 - val_acc: 0.7013\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "1s - loss: 0.7543 - acc: 0.6869 - val_loss: 0.7196 - val_acc: 0.6992\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.71913 to 0.71383, saving model to best.model\n",
      "2s - loss: 0.7514 - acc: 0.6910 - val_loss: 0.7138 - val_acc: 0.7037\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.71383 to 0.71216, saving model to best.model\n",
      "1s - loss: 0.7510 - acc: 0.6905 - val_loss: 0.7122 - val_acc: 0.7060\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.71216 to 0.70917, saving model to best.model\n",
      "1s - loss: 0.7480 - acc: 0.6903 - val_loss: 0.7092 - val_acc: 0.7055\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.70917 to 0.70872, saving model to best.model\n",
      "1s - loss: 0.7447 - acc: 0.6921 - val_loss: 0.7087 - val_acc: 0.7052\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.70872 to 0.70197, saving model to best.model\n",
      "1s - loss: 0.7404 - acc: 0.6932 - val_loss: 0.7020 - val_acc: 0.7078\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "1s - loss: 0.7402 - acc: 0.6947 - val_loss: 0.7031 - val_acc: 0.7071\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.70197 to 0.70086, saving model to best.model\n",
      "1s - loss: 0.7390 - acc: 0.6960 - val_loss: 0.7009 - val_acc: 0.7077\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70086 to 0.69648, saving model to best.model\n",
      "1s - loss: 0.7387 - acc: 0.6940 - val_loss: 0.6965 - val_acc: 0.7115\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.69648 to 0.69117, saving model to best.model\n",
      "1s - loss: 0.7368 - acc: 0.6965 - val_loss: 0.6912 - val_acc: 0.7135\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.69117 to 0.69069, saving model to best.model\n",
      "1s - loss: 0.7365 - acc: 0.6950 - val_loss: 0.6907 - val_acc: 0.7158\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7333 - acc: 0.6980 - val_loss: 0.6913 - val_acc: 0.7138\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.69069 to 0.68536, saving model to best.model\n",
      "1s - loss: 0.7305 - acc: 0.6974 - val_loss: 0.6854 - val_acc: 0.7178\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.7291 - acc: 0.7006 - val_loss: 0.6871 - val_acc: 0.7178\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "1s - loss: 0.7269 - acc: 0.7029 - val_loss: 0.6854 - val_acc: 0.7171\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.68536 to 0.68439, saving model to best.model\n",
      "1s - loss: 0.7296 - acc: 0.6997 - val_loss: 0.6844 - val_acc: 0.7164\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.68439 to 0.67534, saving model to best.model\n",
      "1s - loss: 0.7255 - acc: 0.6998 - val_loss: 0.6753 - val_acc: 0.7214\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7244 - acc: 0.7030 - val_loss: 0.6775 - val_acc: 0.7184\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.67534 to 0.67409, saving model to best.model\n",
      "1s - loss: 0.7194 - acc: 0.7045 - val_loss: 0.6741 - val_acc: 0.7249\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.67409 to 0.67105, saving model to best.model\n",
      "35s - loss: 0.7178 - acc: 0.7047 - val_loss: 0.6710 - val_acc: 0.7241\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.67105 to 0.67084, saving model to best.model\n",
      "1s - loss: 0.7161 - acc: 0.7025 - val_loss: 0.6708 - val_acc: 0.7222\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.67084 to 0.67034, saving model to best.model\n",
      "1s - loss: 0.7189 - acc: 0.7048 - val_loss: 0.6703 - val_acc: 0.7215\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67034 to 0.66784, saving model to best.model\n",
      "1s - loss: 0.7177 - acc: 0.7048 - val_loss: 0.6678 - val_acc: 0.7222\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7127 - acc: 0.7060 - val_loss: 0.6697 - val_acc: 0.7196\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7128 - acc: 0.7063 - val_loss: 0.6706 - val_acc: 0.7186\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.66784 to 0.66668, saving model to best.model\n",
      "1s - loss: 0.7120 - acc: 0.7067 - val_loss: 0.6667 - val_acc: 0.7200\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.66668 to 0.66568, saving model to best.model\n",
      "1s - loss: 0.7115 - acc: 0.7080 - val_loss: 0.6657 - val_acc: 0.7198\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.66568 to 0.66155, saving model to best.model\n",
      "1s - loss: 0.7109 - acc: 0.7069 - val_loss: 0.6616 - val_acc: 0.7267\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.66155 to 0.65602, saving model to best.model\n",
      "1s - loss: 0.7058 - acc: 0.7111 - val_loss: 0.6560 - val_acc: 0.7311\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7079 - acc: 0.7089 - val_loss: 0.6586 - val_acc: 0.7321\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "1s - loss: 0.7076 - acc: 0.7090 - val_loss: 0.6588 - val_acc: 0.7265\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.65602 to 0.65214, saving model to best.model\n",
      "1s - loss: 0.7025 - acc: 0.7125 - val_loss: 0.6521 - val_acc: 0.7348\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7017 - acc: 0.7109 - val_loss: 0.6540 - val_acc: 0.7283\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7017 - acc: 0.7139 - val_loss: 0.6522 - val_acc: 0.7300\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "2s - loss: 0.7006 - acc: 0.7123 - val_loss: 0.6543 - val_acc: 0.7286\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.65214 to 0.65132, saving model to best.model\n",
      "1s - loss: 0.7014 - acc: 0.7125 - val_loss: 0.6513 - val_acc: 0.7307\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.65132 to 0.64870, saving model to best.model\n",
      "1s - loss: 0.6970 - acc: 0.7135 - val_loss: 0.6487 - val_acc: 0.7316\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.64870 to 0.64866, saving model to best.model\n",
      "1s - loss: 0.6972 - acc: 0.7162 - val_loss: 0.6487 - val_acc: 0.7316\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.64866 to 0.64626, saving model to best.model\n",
      "1s - loss: 0.6972 - acc: 0.7169 - val_loss: 0.6463 - val_acc: 0.7359\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.64626 to 0.64520, saving model to best.model\n",
      "1s - loss: 0.6979 - acc: 0.7142 - val_loss: 0.6452 - val_acc: 0.7341\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.6958 - acc: 0.7128 - val_loss: 0.6455 - val_acc: 0.7354\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.64520 to 0.64277, saving model to best.model\n",
      "1s - loss: 0.6941 - acc: 0.7133 - val_loss: 0.6428 - val_acc: 0.7368\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "332s - loss: 0.6911 - acc: 0.7171 - val_loss: 0.6441 - val_acc: 0.7371\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.64277 to 0.64172, saving model to best.model\n",
      "1s - loss: 0.6933 - acc: 0.7173 - val_loss: 0.6417 - val_acc: 0.7389\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.64172 to 0.64101, saving model to best.model\n",
      "0s - loss: 0.6916 - acc: 0.7172 - val_loss: 0.6410 - val_acc: 0.7385\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.6919 - acc: 0.7155 - val_loss: 0.6429 - val_acc: 0.7370\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.64101 to 0.64087, saving model to best.model\n",
      "0s - loss: 0.6931 - acc: 0.7172 - val_loss: 0.6409 - val_acc: 0.7369\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64087 to 0.63830, saving model to best.model\n",
      "1s - loss: 0.6914 - acc: 0.7168 - val_loss: 0.6383 - val_acc: 0.7379\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6860 - acc: 0.7186 - val_loss: 0.6390 - val_acc: 0.7369\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.63830 to 0.63742, saving model to best.model\n",
      "1s - loss: 0.6870 - acc: 0.7189 - val_loss: 0.6374 - val_acc: 0.7393\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.63742 to 0.63686, saving model to best.model\n",
      "1s - loss: 0.6869 - acc: 0.7212 - val_loss: 0.6369 - val_acc: 0.7431\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.63686 to 0.63435, saving model to best.model\n",
      "1s - loss: 0.6876 - acc: 0.7167 - val_loss: 0.6343 - val_acc: 0.7404\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6875 - acc: 0.7186 - val_loss: 0.6356 - val_acc: 0.7410\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6877 - acc: 0.7207 - val_loss: 0.6362 - val_acc: 0.7381\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.63435 to 0.63361, saving model to best.model\n",
      "1s - loss: 0.6827 - acc: 0.7205 - val_loss: 0.6336 - val_acc: 0.7384\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.63361 to 0.63228, saving model to best.model\n",
      "1s - loss: 0.6824 - acc: 0.7217 - val_loss: 0.6323 - val_acc: 0.7419\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6811 - acc: 0.7220 - val_loss: 0.6333 - val_acc: 0.7427\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6830 - acc: 0.7212 - val_loss: 0.6335 - val_acc: 0.7423\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.63228 to 0.62890, saving model to best.model\n",
      "1s - loss: 0.6797 - acc: 0.7235 - val_loss: 0.6289 - val_acc: 0.7453\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.62890 to 0.62821, saving model to best.model\n",
      "1s - loss: 0.6783 - acc: 0.7222 - val_loss: 0.6282 - val_acc: 0.7471\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.62821 to 0.62704, saving model to best.model\n",
      "1s - loss: 0.6791 - acc: 0.7224 - val_loss: 0.6270 - val_acc: 0.7457\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6796 - acc: 0.7218 - val_loss: 0.6308 - val_acc: 0.7417\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "1s - loss: 0.6792 - acc: 0.7232 - val_loss: 0.6279 - val_acc: 0.7429\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6794 - acc: 0.7223 - val_loss: 0.6295 - val_acc: 0.7427\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6752 - acc: 0.7242 - val_loss: 0.6277 - val_acc: 0.7432\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62704 to 0.62369, saving model to best.model\n",
      "1s - loss: 0.6758 - acc: 0.7240 - val_loss: 0.6237 - val_acc: 0.7454\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "119s - loss: 0.6723 - acc: 0.7256 - val_loss: 0.6266 - val_acc: 0.7436\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6740 - acc: 0.7260 - val_loss: 0.6239 - val_acc: 0.7451\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.62369 to 0.62228, saving model to best.model\n",
      "1s - loss: 0.6738 - acc: 0.7230 - val_loss: 0.6223 - val_acc: 0.7458\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6754 - acc: 0.7250 - val_loss: 0.6223 - val_acc: 0.7468\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.62228 to 0.61906, saving model to best.model\n",
      "1s - loss: 0.6722 - acc: 0.7253 - val_loss: 0.6191 - val_acc: 0.7471\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6723 - acc: 0.7245 - val_loss: 0.6199 - val_acc: 0.7482\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6711 - acc: 0.7280 - val_loss: 0.6222 - val_acc: 0.7461\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6717 - acc: 0.7241 - val_loss: 0.6205 - val_acc: 0.7470\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.61906 to 0.61800, saving model to best.model\n",
      "1s - loss: 0.6687 - acc: 0.7245 - val_loss: 0.6180 - val_acc: 0.7495\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.61800 to 0.61579, saving model to best.model\n",
      "1s - loss: 0.6676 - acc: 0.7283 - val_loss: 0.6158 - val_acc: 0.7516\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6670 - acc: 0.7279 - val_loss: 0.6199 - val_acc: 0.7457\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6658 - acc: 0.7270 - val_loss: 0.6170 - val_acc: 0.7482\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6680 - acc: 0.7278 - val_loss: 0.6174 - val_acc: 0.7478\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.61579 to 0.61253, saving model to best.model\n",
      "2s - loss: 0.6634 - acc: 0.7297 - val_loss: 0.6125 - val_acc: 0.7532\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6654 - acc: 0.7269 - val_loss: 0.6146 - val_acc: 0.7493\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6655 - acc: 0.7302 - val_loss: 0.6156 - val_acc: 0.7482\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6657 - acc: 0.7279 - val_loss: 0.6139 - val_acc: 0.7488\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "2s - loss: 0.6635 - acc: 0.7299 - val_loss: 0.6136 - val_acc: 0.7505\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.61253 to 0.61146, saving model to best.model\n",
      "1s - loss: 0.6633 - acc: 0.7289 - val_loss: 0.6115 - val_acc: 0.7532\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6631 - acc: 0.7284 - val_loss: 0.6130 - val_acc: 0.7491\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6609 - acc: 0.7293 - val_loss: 0.6117 - val_acc: 0.7523\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.61146 to 0.60880, saving model to best.model\n",
      "1s - loss: 0.6613 - acc: 0.7289 - val_loss: 0.6088 - val_acc: 0.7527\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7294 - val_loss: 0.6111 - val_acc: 0.7530\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "316s - loss: 0.6626 - acc: 0.7290 - val_loss: 0.6098 - val_acc: 0.7548\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6595 - acc: 0.7300 - val_loss: 0.6119 - val_acc: 0.7499\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6582 - acc: 0.7325 - val_loss: 0.6098 - val_acc: 0.7520\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.60880 to 0.60541, saving model to best.model\n",
      "1s - loss: 0.6598 - acc: 0.7302 - val_loss: 0.6054 - val_acc: 0.7549\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6626 - acc: 0.7305 - val_loss: 0.6117 - val_acc: 0.7532\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6590 - acc: 0.7333 - val_loss: 0.6064 - val_acc: 0.7527\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6582 - acc: 0.7313 - val_loss: 0.6074 - val_acc: 0.7527\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.60541 to 0.60191, saving model to best.model\n",
      "0s - loss: 0.6565 - acc: 0.7304 - val_loss: 0.6019 - val_acc: 0.7582\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6536 - acc: 0.7340 - val_loss: 0.6053 - val_acc: 0.7550\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6568 - acc: 0.7310 - val_loss: 0.6067 - val_acc: 0.7526\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6552 - acc: 0.7326 - val_loss: 0.6043 - val_acc: 0.7547\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6562 - acc: 0.7331 - val_loss: 0.6043 - val_acc: 0.7574\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "2s - loss: 0.6526 - acc: 0.7333 - val_loss: 0.6048 - val_acc: 0.7549\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6531 - acc: 0.7331 - val_loss: 0.6038 - val_acc: 0.7585\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6552 - acc: 0.7338 - val_loss: 0.6075 - val_acc: 0.7527\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6519 - acc: 0.7342 - val_loss: 0.6044 - val_acc: 0.7543\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.60191 to 0.60032, saving model to best.model\n",
      "1s - loss: 0.6537 - acc: 0.7331 - val_loss: 0.6003 - val_acc: 0.7585\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6491 - acc: 0.7359 - val_loss: 0.6017 - val_acc: 0.7580\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.60032 to 0.59910, saving model to best.model\n",
      "1s - loss: 0.6511 - acc: 0.7355 - val_loss: 0.5991 - val_acc: 0.7577\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6506 - acc: 0.7346 - val_loss: 0.6052 - val_acc: 0.7548\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6489 - acc: 0.7366 - val_loss: 0.6043 - val_acc: 0.7518\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.59910 to 0.59878, saving model to best.model\n",
      "1s - loss: 0.6483 - acc: 0.7348 - val_loss: 0.5988 - val_acc: 0.7594\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.59878 to 0.59794, saving model to best.model\n",
      "1s - loss: 0.6498 - acc: 0.7343 - val_loss: 0.5979 - val_acc: 0.7573\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6504 - acc: 0.7345 - val_loss: 0.5995 - val_acc: 0.7573\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "68s - loss: 0.6484 - acc: 0.7369 - val_loss: 0.5980 - val_acc: 0.7582\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6485 - acc: 0.7344 - val_loss: 0.5983 - val_acc: 0.7596\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6472 - acc: 0.7354 - val_loss: 0.5994 - val_acc: 0.7590\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6484 - acc: 0.7347 - val_loss: 0.5986 - val_acc: 0.7589\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.59794 to 0.59421, saving model to best.model\n",
      "1s - loss: 0.6441 - acc: 0.7380 - val_loss: 0.5942 - val_acc: 0.7596\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6456 - acc: 0.7371 - val_loss: 0.5973 - val_acc: 0.7589\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6456 - acc: 0.7383 - val_loss: 0.5954 - val_acc: 0.7609\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6421 - acc: 0.7396 - val_loss: 0.5956 - val_acc: 0.7591\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6448 - acc: 0.7369 - val_loss: 0.5957 - val_acc: 0.7584\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6429 - acc: 0.7393 - val_loss: 0.5953 - val_acc: 0.7575\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.59421 to 0.59265, saving model to best.model\n",
      "1s - loss: 0.6435 - acc: 0.7371 - val_loss: 0.5927 - val_acc: 0.7612\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6435 - acc: 0.7364 - val_loss: 0.5930 - val_acc: 0.7625\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59265 to 0.58978, saving model to best.model\n",
      "2s - loss: 0.6413 - acc: 0.7376 - val_loss: 0.5898 - val_acc: 0.7624\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "2s - loss: 0.6408 - acc: 0.7380 - val_loss: 0.5930 - val_acc: 0.7622\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.58978 to 0.58827, saving model to best.model\n",
      "2s - loss: 0.6379 - acc: 0.7400 - val_loss: 0.5883 - val_acc: 0.7626\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6408 - acc: 0.7381 - val_loss: 0.5913 - val_acc: 0.7617\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6401 - acc: 0.7403 - val_loss: 0.5921 - val_acc: 0.7609\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6423 - acc: 0.7374 - val_loss: 0.5904 - val_acc: 0.7615\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6413 - acc: 0.7386 - val_loss: 0.5887 - val_acc: 0.7643\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6371 - acc: 0.7397 - val_loss: 0.5890 - val_acc: 0.7628\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6362 - acc: 0.7432 - val_loss: 0.5909 - val_acc: 0.7603\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.58827 to 0.58789, saving model to best.model\n",
      "1s - loss: 0.6361 - acc: 0.7416 - val_loss: 0.5879 - val_acc: 0.7641\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6377 - acc: 0.7399 - val_loss: 0.5901 - val_acc: 0.7629\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6365 - acc: 0.7370 - val_loss: 0.5885 - val_acc: 0.7641\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6396 - acc: 0.7386 - val_loss: 0.5887 - val_acc: 0.7641\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.58789 to 0.58738, saving model to best.model\n",
      "1s - loss: 0.6392 - acc: 0.7371 - val_loss: 0.5874 - val_acc: 0.7648\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84557, saving model to best.model\n",
      "1s - loss: 0.9148 - acc: 0.6265 - val_loss: 0.8456 - val_acc: 0.6559\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84557 to 0.84553, saving model to best.model\n",
      "1s - loss: 0.8615 - acc: 0.6565 - val_loss: 0.8455 - val_acc: 0.6559\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84553 to 0.84517, saving model to best.model\n",
      "1s - loss: 0.8549 - acc: 0.6570 - val_loss: 0.8452 - val_acc: 0.6559\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84517 to 0.84436, saving model to best.model\n",
      "1445s - loss: 0.8528 - acc: 0.6570 - val_loss: 0.8444 - val_acc: 0.6559\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84436 to 0.83345, saving model to best.model\n",
      "1s - loss: 0.8469 - acc: 0.6570 - val_loss: 0.8334 - val_acc: 0.6559\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83345 to 0.82836, saving model to best.model\n",
      "1s - loss: 0.8418 - acc: 0.6570 - val_loss: 0.8284 - val_acc: 0.6559\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "1s - loss: 0.8364 - acc: 0.6570 - val_loss: 0.8296 - val_acc: 0.6559\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82836 to 0.82535, saving model to best.model\n",
      "1s - loss: 0.8372 - acc: 0.6570 - val_loss: 0.8254 - val_acc: 0.6559\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82535 to 0.82342, saving model to best.model\n",
      "1s - loss: 0.8350 - acc: 0.6570 - val_loss: 0.8234 - val_acc: 0.6559\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82342 to 0.82235, saving model to best.model\n",
      "1s - loss: 0.8318 - acc: 0.6570 - val_loss: 0.8223 - val_acc: 0.6559\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.8318 - acc: 0.6571 - val_loss: 0.8224 - val_acc: 0.6559\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8323 - acc: 0.6570 - val_loss: 0.8238 - val_acc: 0.6559\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82235 to 0.82142, saving model to best.model\n",
      "1s - loss: 0.8317 - acc: 0.6570 - val_loss: 0.8214 - val_acc: 0.6559\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8298 - acc: 0.6570 - val_loss: 0.8222 - val_acc: 0.6559\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82142 to 0.82130, saving model to best.model\n",
      "2s - loss: 0.8296 - acc: 0.6571 - val_loss: 0.8213 - val_acc: 0.6559\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82130 to 0.82064, saving model to best.model\n",
      "2s - loss: 0.8290 - acc: 0.6570 - val_loss: 0.8206 - val_acc: 0.6559\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82064 to 0.81985, saving model to best.model\n",
      "1s - loss: 0.8282 - acc: 0.6570 - val_loss: 0.8199 - val_acc: 0.6559\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81985 to 0.81818, saving model to best.model\n",
      "1s - loss: 0.8275 - acc: 0.6574 - val_loss: 0.8182 - val_acc: 0.6559\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81818 to 0.81751, saving model to best.model\n",
      "1s - loss: 0.8256 - acc: 0.6569 - val_loss: 0.8175 - val_acc: 0.6559\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "1s - loss: 0.8263 - acc: 0.6573 - val_loss: 0.8191 - val_acc: 0.6556\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81751 to 0.81733, saving model to best.model\n",
      "1s - loss: 0.8253 - acc: 0.6575 - val_loss: 0.8173 - val_acc: 0.6559\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81733 to 0.81556, saving model to best.model\n",
      "1s - loss: 0.8240 - acc: 0.6571 - val_loss: 0.8156 - val_acc: 0.6561\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81556 to 0.81503, saving model to best.model\n",
      "1s - loss: 0.8231 - acc: 0.6571 - val_loss: 0.8150 - val_acc: 0.6559\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81503 to 0.81491, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6573 - val_loss: 0.8149 - val_acc: 0.6577\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81491 to 0.81389, saving model to best.model\n",
      "1s - loss: 0.8220 - acc: 0.6590 - val_loss: 0.8139 - val_acc: 0.6563\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81389 to 0.81250, saving model to best.model\n",
      "1s - loss: 0.8212 - acc: 0.6597 - val_loss: 0.8125 - val_acc: 0.6567\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81250 to 0.81198, saving model to best.model\n",
      "1s - loss: 0.8209 - acc: 0.6577 - val_loss: 0.8120 - val_acc: 0.6580\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "1s - loss: 0.8191 - acc: 0.6589 - val_loss: 0.8124 - val_acc: 0.6571\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81198 to 0.80955, saving model to best.model\n",
      "576s - loss: 0.8190 - acc: 0.6600 - val_loss: 0.8096 - val_acc: 0.6597\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80955 to 0.80873, saving model to best.model\n",
      "1s - loss: 0.8176 - acc: 0.6609 - val_loss: 0.8087 - val_acc: 0.6605\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80873 to 0.80778, saving model to best.model\n",
      "1s - loss: 0.8176 - acc: 0.6605 - val_loss: 0.8078 - val_acc: 0.6608\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80778 to 0.80644, saving model to best.model\n",
      "1s - loss: 0.8145 - acc: 0.6623 - val_loss: 0.8064 - val_acc: 0.6608\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80644 to 0.80563, saving model to best.model\n",
      "1s - loss: 0.8140 - acc: 0.6616 - val_loss: 0.8056 - val_acc: 0.6604\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80563 to 0.80165, saving model to best.model\n",
      "0s - loss: 0.8119 - acc: 0.6622 - val_loss: 0.8016 - val_acc: 0.6644\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "1s - loss: 0.8112 - acc: 0.6631 - val_loss: 0.8018 - val_acc: 0.6623\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80165 to 0.79903, saving model to best.model\n",
      "0s - loss: 0.8099 - acc: 0.6653 - val_loss: 0.7990 - val_acc: 0.6646\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79903 to 0.79660, saving model to best.model\n",
      "0s - loss: 0.8082 - acc: 0.6623 - val_loss: 0.7966 - val_acc: 0.6672\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79660 to 0.79524, saving model to best.model\n",
      "1s - loss: 0.8066 - acc: 0.6646 - val_loss: 0.7952 - val_acc: 0.6638\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79524 to 0.79413, saving model to best.model\n",
      "7206s - loss: 0.8052 - acc: 0.6659 - val_loss: 0.7941 - val_acc: 0.6693\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79413 to 0.79114, saving model to best.model\n",
      "2s - loss: 0.8050 - acc: 0.6645 - val_loss: 0.7911 - val_acc: 0.6686\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79114 to 0.78984, saving model to best.model\n",
      "1s - loss: 0.8039 - acc: 0.6666 - val_loss: 0.7898 - val_acc: 0.6697\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78984 to 0.78765, saving model to best.model\n",
      "1s - loss: 0.8016 - acc: 0.6671 - val_loss: 0.7877 - val_acc: 0.6731\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78765 to 0.78651, saving model to best.model\n",
      "0s - loss: 0.7998 - acc: 0.6679 - val_loss: 0.7865 - val_acc: 0.6687\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78651 to 0.78254, saving model to best.model\n",
      "1s - loss: 0.7998 - acc: 0.6672 - val_loss: 0.7825 - val_acc: 0.6758\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "1s - loss: 0.7975 - acc: 0.6669 - val_loss: 0.7867 - val_acc: 0.6727\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78254 to 0.77983, saving model to best.model\n",
      "1s - loss: 0.7967 - acc: 0.6682 - val_loss: 0.7798 - val_acc: 0.6726\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77983 to 0.77781, saving model to best.model\n",
      "1s - loss: 0.7937 - acc: 0.6704 - val_loss: 0.7778 - val_acc: 0.6748\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77781 to 0.77297, saving model to best.model\n",
      "1s - loss: 0.7916 - acc: 0.6704 - val_loss: 0.7730 - val_acc: 0.6794\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77297 to 0.77113, saving model to best.model\n",
      "7203s - loss: 0.7905 - acc: 0.6701 - val_loss: 0.7711 - val_acc: 0.6789\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77113 to 0.76946, saving model to best.model\n",
      "1s - loss: 0.7882 - acc: 0.6706 - val_loss: 0.7695 - val_acc: 0.6806\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76946 to 0.76878, saving model to best.model\n",
      "1s - loss: 0.7880 - acc: 0.6724 - val_loss: 0.7688 - val_acc: 0.6815\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76878 to 0.76521, saving model to best.model\n",
      "1s - loss: 0.7854 - acc: 0.6729 - val_loss: 0.7652 - val_acc: 0.6849\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.7849 - acc: 0.6724 - val_loss: 0.7653 - val_acc: 0.6836\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76521 to 0.76131, saving model to best.model\n",
      "1s - loss: 0.7833 - acc: 0.6736 - val_loss: 0.7613 - val_acc: 0.6831\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76131 to 0.75654, saving model to best.model\n",
      "1s - loss: 0.7791 - acc: 0.6781 - val_loss: 0.7565 - val_acc: 0.6890\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75654 to 0.75374, saving model to best.model\n",
      "1s - loss: 0.7770 - acc: 0.6782 - val_loss: 0.7537 - val_acc: 0.6916\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75374 to 0.75009, saving model to best.model\n",
      "1s - loss: 0.7767 - acc: 0.6758 - val_loss: 0.7501 - val_acc: 0.6916\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75009 to 0.74875, saving model to best.model\n",
      "1s - loss: 0.7747 - acc: 0.6769 - val_loss: 0.7488 - val_acc: 0.6920\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74875 to 0.74763, saving model to best.model\n",
      "1s - loss: 0.7736 - acc: 0.6774 - val_loss: 0.7476 - val_acc: 0.6929\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74763 to 0.74268, saving model to best.model\n",
      "1s - loss: 0.7682 - acc: 0.6801 - val_loss: 0.7427 - val_acc: 0.6943\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74268 to 0.73995, saving model to best.model\n",
      "7203s - loss: 0.7702 - acc: 0.6792 - val_loss: 0.7399 - val_acc: 0.6948\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73995 to 0.73556, saving model to best.model\n",
      "1s - loss: 0.7682 - acc: 0.6796 - val_loss: 0.7356 - val_acc: 0.6974\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73556 to 0.73474, saving model to best.model\n",
      "1s - loss: 0.7658 - acc: 0.6815 - val_loss: 0.7347 - val_acc: 0.6996\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73474 to 0.73019, saving model to best.model\n",
      "1s - loss: 0.7612 - acc: 0.6839 - val_loss: 0.7302 - val_acc: 0.6981\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73019 to 0.72759, saving model to best.model\n",
      "0s - loss: 0.7597 - acc: 0.6872 - val_loss: 0.7276 - val_acc: 0.7041\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72759 to 0.72639, saving model to best.model\n",
      "1s - loss: 0.7590 - acc: 0.6866 - val_loss: 0.7264 - val_acc: 0.7029\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72639 to 0.72150, saving model to best.model\n",
      "1s - loss: 0.7595 - acc: 0.6847 - val_loss: 0.7215 - val_acc: 0.7041\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72150 to 0.72029, saving model to best.model\n",
      "1s - loss: 0.7548 - acc: 0.6899 - val_loss: 0.7203 - val_acc: 0.7016\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72029 to 0.71479, saving model to best.model\n",
      "1s - loss: 0.7493 - acc: 0.6893 - val_loss: 0.7148 - val_acc: 0.7053\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71479 to 0.71164, saving model to best.model\n",
      "1s - loss: 0.7499 - acc: 0.6892 - val_loss: 0.7116 - val_acc: 0.7071\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "1s - loss: 0.7473 - acc: 0.6890 - val_loss: 0.7117 - val_acc: 0.7078\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71164 to 0.71019, saving model to best.model\n",
      "1s - loss: 0.7477 - acc: 0.6901 - val_loss: 0.7102 - val_acc: 0.7108\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71019 to 0.70628, saving model to best.model\n",
      "817s - loss: 0.7473 - acc: 0.6906 - val_loss: 0.7063 - val_acc: 0.7136\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70628 to 0.70117, saving model to best.model\n",
      "1s - loss: 0.7409 - acc: 0.6932 - val_loss: 0.7012 - val_acc: 0.7112\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70117 to 0.69953, saving model to best.model\n",
      "1s - loss: 0.7398 - acc: 0.6944 - val_loss: 0.6995 - val_acc: 0.7130\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69953 to 0.69777, saving model to best.model\n",
      "1s - loss: 0.7387 - acc: 0.6935 - val_loss: 0.6978 - val_acc: 0.7141\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7397 - acc: 0.6933 - val_loss: 0.6981 - val_acc: 0.7162\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69777 to 0.69644, saving model to best.model\n",
      "1s - loss: 0.7361 - acc: 0.6969 - val_loss: 0.6964 - val_acc: 0.7148\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69644 to 0.69409, saving model to best.model\n",
      "1s - loss: 0.7362 - acc: 0.6954 - val_loss: 0.6941 - val_acc: 0.7111\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69409 to 0.68979, saving model to best.model\n",
      "1s - loss: 0.7341 - acc: 0.6967 - val_loss: 0.6898 - val_acc: 0.7169\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68979 to 0.68733, saving model to best.model\n",
      "1s - loss: 0.7321 - acc: 0.6966 - val_loss: 0.6873 - val_acc: 0.7174\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7307 - acc: 0.6977 - val_loss: 0.6891 - val_acc: 0.7193\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68733 to 0.68261, saving model to best.model\n",
      "1s - loss: 0.7302 - acc: 0.6977 - val_loss: 0.6826 - val_acc: 0.7221\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68261 to 0.68154, saving model to best.model\n",
      "1s - loss: 0.7267 - acc: 0.7009 - val_loss: 0.6815 - val_acc: 0.7208\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7286 - acc: 0.6997 - val_loss: 0.6822 - val_acc: 0.7173\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68154 to 0.67963, saving model to best.model\n",
      "1s - loss: 0.7261 - acc: 0.7005 - val_loss: 0.6796 - val_acc: 0.7219\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67963 to 0.67873, saving model to best.model\n",
      "1s - loss: 0.7232 - acc: 0.7025 - val_loss: 0.6787 - val_acc: 0.7217\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67873 to 0.67695, saving model to best.model\n",
      "1s - loss: 0.7233 - acc: 0.7013 - val_loss: 0.6770 - val_acc: 0.7187\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67695 to 0.67503, saving model to best.model\n",
      "1s - loss: 0.7240 - acc: 0.7023 - val_loss: 0.6750 - val_acc: 0.7220\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67503 to 0.67277, saving model to best.model\n",
      "1s - loss: 0.7190 - acc: 0.7050 - val_loss: 0.6728 - val_acc: 0.7203\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7225 - acc: 0.7033 - val_loss: 0.6744 - val_acc: 0.7220\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67277 to 0.67073, saving model to best.model\n",
      "1s - loss: 0.7217 - acc: 0.7025 - val_loss: 0.6707 - val_acc: 0.7218\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67073 to 0.66806, saving model to best.model\n",
      "1s - loss: 0.7164 - acc: 0.7044 - val_loss: 0.6681 - val_acc: 0.7261\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7142 - acc: 0.7054 - val_loss: 0.6698 - val_acc: 0.7220\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66806 to 0.66699, saving model to best.model\n",
      "1s - loss: 0.7132 - acc: 0.7047 - val_loss: 0.6670 - val_acc: 0.7274\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7151 - acc: 0.7071 - val_loss: 0.6695 - val_acc: 0.7226\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66699 to 0.66343, saving model to best.model\n",
      "1s - loss: 0.7141 - acc: 0.7066 - val_loss: 0.6634 - val_acc: 0.7283\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7083 - acc: 0.7090 - val_loss: 0.6643 - val_acc: 0.7258\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66343 to 0.66229, saving model to best.model\n",
      "1s - loss: 0.7138 - acc: 0.7085 - val_loss: 0.6623 - val_acc: 0.7255\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66229 to 0.66086, saving model to best.model\n",
      "1s - loss: 0.7110 - acc: 0.7100 - val_loss: 0.6609 - val_acc: 0.7276\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66086 to 0.65982, saving model to best.model\n",
      "1s - loss: 0.7092 - acc: 0.7071 - val_loss: 0.6598 - val_acc: 0.7283\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.65982 to 0.65888, saving model to best.model\n",
      "1s - loss: 0.7078 - acc: 0.7079 - val_loss: 0.6589 - val_acc: 0.7276\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7077 - acc: 0.7101 - val_loss: 0.6597 - val_acc: 0.7251\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65888 to 0.65498, saving model to best.model\n",
      "1s - loss: 0.7032 - acc: 0.7106 - val_loss: 0.6550 - val_acc: 0.7355\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65498 to 0.65483, saving model to best.model\n",
      "1s - loss: 0.7042 - acc: 0.7097 - val_loss: 0.6548 - val_acc: 0.7315\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7029 - acc: 0.7109 - val_loss: 0.6600 - val_acc: 0.7222\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65483 to 0.65379, saving model to best.model\n",
      "1s - loss: 0.7023 - acc: 0.7091 - val_loss: 0.6538 - val_acc: 0.7328\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65379 to 0.65262, saving model to best.model\n",
      "1s - loss: 0.6990 - acc: 0.7132 - val_loss: 0.6526 - val_acc: 0.7331\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65262 to 0.65119, saving model to best.model\n",
      "1s - loss: 0.7008 - acc: 0.7116 - val_loss: 0.6512 - val_acc: 0.7317\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6986 - acc: 0.7144 - val_loss: 0.6517 - val_acc: 0.7323\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.7018 - acc: 0.7109 - val_loss: 0.6519 - val_acc: 0.7286\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65119 to 0.64717, saving model to best.model\n",
      "1s - loss: 0.7001 - acc: 0.7119 - val_loss: 0.6472 - val_acc: 0.7344\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6969 - acc: 0.7103 - val_loss: 0.6481 - val_acc: 0.7311\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.64717 to 0.64589, saving model to best.model\n",
      "1s - loss: 0.6970 - acc: 0.7160 - val_loss: 0.6459 - val_acc: 0.7340\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.64589 to 0.64370, saving model to best.model\n",
      "1s - loss: 0.6934 - acc: 0.7149 - val_loss: 0.6437 - val_acc: 0.7340\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6951 - acc: 0.7140 - val_loss: 0.6477 - val_acc: 0.7327\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64370 to 0.64052, saving model to best.model\n",
      "1s - loss: 0.6914 - acc: 0.7181 - val_loss: 0.6405 - val_acc: 0.7359\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6936 - acc: 0.7172 - val_loss: 0.6470 - val_acc: 0.7368\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6922 - acc: 0.7167 - val_loss: 0.6453 - val_acc: 0.7326\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6931 - acc: 0.7142 - val_loss: 0.6428 - val_acc: 0.7329\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64052 to 0.63899, saving model to best.model\n",
      "1s - loss: 0.6900 - acc: 0.7169 - val_loss: 0.6390 - val_acc: 0.7351\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6905 - acc: 0.7173 - val_loss: 0.6426 - val_acc: 0.7356\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.63899 to 0.63889, saving model to best.model\n",
      "1s - loss: 0.6887 - acc: 0.7181 - val_loss: 0.6389 - val_acc: 0.7357\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.63889 to 0.63842, saving model to best.model\n",
      "1s - loss: 0.6888 - acc: 0.7152 - val_loss: 0.6384 - val_acc: 0.7338\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6879 - acc: 0.7170 - val_loss: 0.6394 - val_acc: 0.7355\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63842 to 0.63447, saving model to best.model\n",
      "1s - loss: 0.6887 - acc: 0.7176 - val_loss: 0.6345 - val_acc: 0.7364\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6838 - acc: 0.7189 - val_loss: 0.6354 - val_acc: 0.7351\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.63447 to 0.63378, saving model to best.model\n",
      "1s - loss: 0.6864 - acc: 0.7161 - val_loss: 0.6338 - val_acc: 0.7365\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6874 - acc: 0.7171 - val_loss: 0.6366 - val_acc: 0.7340\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6838 - acc: 0.7197 - val_loss: 0.6359 - val_acc: 0.7355\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6852 - acc: 0.7200 - val_loss: 0.6365 - val_acc: 0.7352\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.63378 to 0.63284, saving model to best.model\n",
      "1s - loss: 0.6849 - acc: 0.7180 - val_loss: 0.6328 - val_acc: 0.7377\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63284 to 0.63201, saving model to best.model\n",
      "1s - loss: 0.6831 - acc: 0.7187 - val_loss: 0.6320 - val_acc: 0.7344\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6839 - acc: 0.7191 - val_loss: 0.6331 - val_acc: 0.7375\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.63201 to 0.63198, saving model to best.model\n",
      "1s - loss: 0.6821 - acc: 0.7208 - val_loss: 0.6320 - val_acc: 0.7359\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.6826 - acc: 0.7212 - val_loss: 0.6340 - val_acc: 0.7335\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63198 to 0.63057, saving model to best.model\n",
      "1s - loss: 0.6806 - acc: 0.7241 - val_loss: 0.6306 - val_acc: 0.7365\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63057 to 0.62851, saving model to best.model\n",
      "1s - loss: 0.6800 - acc: 0.7214 - val_loss: 0.6285 - val_acc: 0.7385\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62851 to 0.62589, saving model to best.model\n",
      "1s - loss: 0.6764 - acc: 0.7231 - val_loss: 0.6259 - val_acc: 0.7396\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.62589 to 0.62372, saving model to best.model\n",
      "1s - loss: 0.6764 - acc: 0.7208 - val_loss: 0.6237 - val_acc: 0.7443\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6748 - acc: 0.7229 - val_loss: 0.6281 - val_acc: 0.7363\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6761 - acc: 0.7211 - val_loss: 0.6242 - val_acc: 0.7405\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6750 - acc: 0.7239 - val_loss: 0.6248 - val_acc: 0.7423\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62372 to 0.62289, saving model to best.model\n",
      "1s - loss: 0.6739 - acc: 0.7224 - val_loss: 0.6229 - val_acc: 0.7424\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.62289 to 0.62229, saving model to best.model\n",
      "1s - loss: 0.6727 - acc: 0.7254 - val_loss: 0.6223 - val_acc: 0.7419\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6749 - acc: 0.7232 - val_loss: 0.6229 - val_acc: 0.7417\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6746 - acc: 0.7247 - val_loss: 0.6241 - val_acc: 0.7370\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6771 - acc: 0.7220 - val_loss: 0.6263 - val_acc: 0.7396\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6707 - acc: 0.7261 - val_loss: 0.6247 - val_acc: 0.7405\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6749 - acc: 0.7242 - val_loss: 0.6279 - val_acc: 0.7386\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62229 to 0.62196, saving model to best.model\n",
      "1s - loss: 0.6702 - acc: 0.7246 - val_loss: 0.6220 - val_acc: 0.7425\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6697 - acc: 0.7259 - val_loss: 0.6230 - val_acc: 0.7390\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62196 to 0.61989, saving model to best.model\n",
      "1s - loss: 0.6708 - acc: 0.7235 - val_loss: 0.6199 - val_acc: 0.7403\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6708 - acc: 0.7250 - val_loss: 0.6200 - val_acc: 0.7437\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6710 - acc: 0.7212 - val_loss: 0.6205 - val_acc: 0.7404\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.61989 to 0.61657, saving model to best.model\n",
      "1s - loss: 0.6660 - acc: 0.7265 - val_loss: 0.6166 - val_acc: 0.7460\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6684 - acc: 0.7258 - val_loss: 0.6217 - val_acc: 0.7433\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6681 - acc: 0.7263 - val_loss: 0.6166 - val_acc: 0.7454\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61657 to 0.61552, saving model to best.model\n",
      "1s - loss: 0.6683 - acc: 0.7265 - val_loss: 0.6155 - val_acc: 0.7440\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6648 - acc: 0.7272 - val_loss: 0.6171 - val_acc: 0.7430\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.61552 to 0.61499, saving model to best.model\n",
      "1s - loss: 0.6677 - acc: 0.7263 - val_loss: 0.6150 - val_acc: 0.7415\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6683 - acc: 0.7253 - val_loss: 0.6165 - val_acc: 0.7447\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6649 - acc: 0.7283 - val_loss: 0.6157 - val_acc: 0.7410\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61499 to 0.61223, saving model to best.model\n",
      "0s - loss: 0.6638 - acc: 0.7264 - val_loss: 0.6122 - val_acc: 0.7479\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6641 - acc: 0.7279 - val_loss: 0.6136 - val_acc: 0.7434\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6645 - acc: 0.7269 - val_loss: 0.6128 - val_acc: 0.7467\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6638 - acc: 0.7271 - val_loss: 0.6127 - val_acc: 0.7452\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6611 - acc: 0.7287 - val_loss: 0.6145 - val_acc: 0.7416\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.61223 to 0.61220, saving model to best.model\n",
      "1s - loss: 0.6623 - acc: 0.7263 - val_loss: 0.6122 - val_acc: 0.7461\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61220 to 0.61035, saving model to best.model\n",
      "1s - loss: 0.6643 - acc: 0.7282 - val_loss: 0.6103 - val_acc: 0.7461\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6626 - acc: 0.7277 - val_loss: 0.6139 - val_acc: 0.7413\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6595 - acc: 0.7289 - val_loss: 0.6104 - val_acc: 0.7430\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6584 - acc: 0.7300 - val_loss: 0.6112 - val_acc: 0.7433\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6601 - acc: 0.7287 - val_loss: 0.6111 - val_acc: 0.7425\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61035 to 0.60948, saving model to best.model\n",
      "1s - loss: 0.6571 - acc: 0.7295 - val_loss: 0.6095 - val_acc: 0.7445\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6605 - acc: 0.7297 - val_loss: 0.6098 - val_acc: 0.7437\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.60948 to 0.60922, saving model to best.model\n",
      "1s - loss: 0.6595 - acc: 0.7291 - val_loss: 0.6092 - val_acc: 0.7425\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.60922 to 0.60732, saving model to best.model\n",
      "1s - loss: 0.6592 - acc: 0.7308 - val_loss: 0.6073 - val_acc: 0.7474\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6594 - acc: 0.7317 - val_loss: 0.6097 - val_acc: 0.7448\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6583 - acc: 0.7298 - val_loss: 0.6077 - val_acc: 0.7460\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60732 to 0.60625, saving model to best.model\n",
      "0s - loss: 0.6560 - acc: 0.7293 - val_loss: 0.6063 - val_acc: 0.7487\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60625 to 0.60374, saving model to best.model\n",
      "0s - loss: 0.6561 - acc: 0.7313 - val_loss: 0.6037 - val_acc: 0.7508\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6560 - acc: 0.7303 - val_loss: 0.6059 - val_acc: 0.7472\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6565 - acc: 0.7309 - val_loss: 0.6071 - val_acc: 0.7460\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60374 to 0.60373, saving model to best.model\n",
      "0s - loss: 0.6561 - acc: 0.7285 - val_loss: 0.6037 - val_acc: 0.7478\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6568 - acc: 0.7333 - val_loss: 0.6046 - val_acc: 0.7488\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.6542 - acc: 0.7335 - val_loss: 0.6053 - val_acc: 0.7477\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6539 - acc: 0.7322 - val_loss: 0.6048 - val_acc: 0.7460\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60373 to 0.60135, saving model to best.model\n",
      "0s - loss: 0.6540 - acc: 0.7327 - val_loss: 0.6013 - val_acc: 0.7505\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6565 - acc: 0.7322 - val_loss: 0.6045 - val_acc: 0.7459\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6553 - acc: 0.7323 - val_loss: 0.6053 - val_acc: 0.7460\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6530 - acc: 0.7318 - val_loss: 0.6020 - val_acc: 0.7504\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60135 to 0.60053, saving model to best.model\n",
      "1s - loss: 0.6544 - acc: 0.7299 - val_loss: 0.6005 - val_acc: 0.7501\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6493 - acc: 0.7338 - val_loss: 0.6009 - val_acc: 0.7504\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60053 to 0.59956, saving model to best.model\n",
      "1s - loss: 0.6503 - acc: 0.7337 - val_loss: 0.5996 - val_acc: 0.7518\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.59956 to 0.59798, saving model to best.model\n",
      "1s - loss: 0.6500 - acc: 0.7345 - val_loss: 0.5980 - val_acc: 0.7506\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6472 - acc: 0.7331 - val_loss: 0.5983 - val_acc: 0.7509\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6485 - acc: 0.7322 - val_loss: 0.5990 - val_acc: 0.7520\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.59798 to 0.59742, saving model to best.model\n",
      "1s - loss: 0.6500 - acc: 0.7357 - val_loss: 0.5974 - val_acc: 0.7536\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6469 - acc: 0.7352 - val_loss: 0.5985 - val_acc: 0.7513\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84416, saving model to best.model\n",
      "1s - loss: 0.8994 - acc: 0.6389 - val_loss: 0.8442 - val_acc: 0.6588\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8612 - acc: 0.6554 - val_loss: 0.8446 - val_acc: 0.6588\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84416 to 0.84187, saving model to best.model\n",
      "1s - loss: 0.8549 - acc: 0.6554 - val_loss: 0.8419 - val_acc: 0.6588\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84187 to 0.83023, saving model to best.model\n",
      "1s - loss: 0.8494 - acc: 0.6554 - val_loss: 0.8302 - val_acc: 0.6588\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83023 to 0.82782, saving model to best.model\n",
      "1s - loss: 0.8429 - acc: 0.6554 - val_loss: 0.8278 - val_acc: 0.6588\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82782 to 0.82339, saving model to best.model\n",
      "1s - loss: 0.8396 - acc: 0.6554 - val_loss: 0.8234 - val_acc: 0.6588\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82339 to 0.82119, saving model to best.model\n",
      "1s - loss: 0.8368 - acc: 0.6554 - val_loss: 0.8212 - val_acc: 0.6588\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.8346 - acc: 0.6554 - val_loss: 0.8212 - val_acc: 0.6588\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.8353 - acc: 0.6554 - val_loss: 0.8244 - val_acc: 0.6588\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82119 to 0.82116, saving model to best.model\n",
      "1s - loss: 0.8327 - acc: 0.6554 - val_loss: 0.8212 - val_acc: 0.6588\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82116 to 0.82113, saving model to best.model\n",
      "1s - loss: 0.8326 - acc: 0.6554 - val_loss: 0.8211 - val_acc: 0.6588\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82113 to 0.82014, saving model to best.model\n",
      "1s - loss: 0.8325 - acc: 0.6554 - val_loss: 0.8201 - val_acc: 0.6588\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82014 to 0.81807, saving model to best.model\n",
      "1s - loss: 0.8302 - acc: 0.6555 - val_loss: 0.8181 - val_acc: 0.6588\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8307 - acc: 0.6556 - val_loss: 0.8194 - val_acc: 0.6588\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81807 to 0.81770, saving model to best.model\n",
      "1s - loss: 0.8291 - acc: 0.6553 - val_loss: 0.8177 - val_acc: 0.6588\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81770 to 0.81678, saving model to best.model\n",
      "1s - loss: 0.8279 - acc: 0.6555 - val_loss: 0.8168 - val_acc: 0.6588\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81678 to 0.81613, saving model to best.model\n",
      "1s - loss: 0.8277 - acc: 0.6553 - val_loss: 0.8161 - val_acc: 0.6588\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.8263 - acc: 0.6553 - val_loss: 0.8164 - val_acc: 0.6593\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81613 to 0.81587, saving model to best.model\n",
      "0s - loss: 0.8255 - acc: 0.6557 - val_loss: 0.8159 - val_acc: 0.6595\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81587 to 0.81421, saving model to best.model\n",
      "1s - loss: 0.8257 - acc: 0.6564 - val_loss: 0.8142 - val_acc: 0.6594\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81421 to 0.81373, saving model to best.model\n",
      "1s - loss: 0.8248 - acc: 0.6557 - val_loss: 0.8137 - val_acc: 0.6603\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81373 to 0.81282, saving model to best.model\n",
      "1s - loss: 0.8237 - acc: 0.6557 - val_loss: 0.8128 - val_acc: 0.6604\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81282 to 0.81263, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6587 - val_loss: 0.8126 - val_acc: 0.6616\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81263 to 0.81146, saving model to best.model\n",
      "1s - loss: 0.8233 - acc: 0.6580 - val_loss: 0.8115 - val_acc: 0.6624\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "1s - loss: 0.8216 - acc: 0.6584 - val_loss: 0.8129 - val_acc: 0.6643\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "1s - loss: 0.8202 - acc: 0.6597 - val_loss: 0.8115 - val_acc: 0.6628\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81146 to 0.81097, saving model to best.model\n",
      "1s - loss: 0.8205 - acc: 0.6574 - val_loss: 0.8110 - val_acc: 0.6615\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81097 to 0.80893, saving model to best.model\n",
      "1s - loss: 0.8193 - acc: 0.6581 - val_loss: 0.8089 - val_acc: 0.6637\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80893 to 0.80705, saving model to best.model\n",
      "1s - loss: 0.8185 - acc: 0.6614 - val_loss: 0.8070 - val_acc: 0.6641\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80705 to 0.80679, saving model to best.model\n",
      "1s - loss: 0.8175 - acc: 0.6611 - val_loss: 0.8068 - val_acc: 0.6635\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80679 to 0.80458, saving model to best.model\n",
      "1s - loss: 0.8159 - acc: 0.6605 - val_loss: 0.8046 - val_acc: 0.6659\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80458 to 0.80278, saving model to best.model\n",
      "1s - loss: 0.8141 - acc: 0.6615 - val_loss: 0.8028 - val_acc: 0.6665\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80278 to 0.80235, saving model to best.model\n",
      "1s - loss: 0.8129 - acc: 0.6609 - val_loss: 0.8023 - val_acc: 0.6672\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80235 to 0.79993, saving model to best.model\n",
      "1s - loss: 0.8116 - acc: 0.6626 - val_loss: 0.7999 - val_acc: 0.6672\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79993 to 0.79802, saving model to best.model\n",
      "1s - loss: 0.8092 - acc: 0.6636 - val_loss: 0.7980 - val_acc: 0.6670\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79802 to 0.79570, saving model to best.model\n",
      "1s - loss: 0.8082 - acc: 0.6630 - val_loss: 0.7957 - val_acc: 0.6690\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79570 to 0.79414, saving model to best.model\n",
      "1s - loss: 0.8069 - acc: 0.6642 - val_loss: 0.7941 - val_acc: 0.6712\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79414 to 0.79285, saving model to best.model\n",
      "1s - loss: 0.8058 - acc: 0.6641 - val_loss: 0.7929 - val_acc: 0.6681\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79285 to 0.79100, saving model to best.model\n",
      "1s - loss: 0.8034 - acc: 0.6644 - val_loss: 0.7910 - val_acc: 0.6674\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "1s - loss: 0.8045 - acc: 0.6650 - val_loss: 0.7912 - val_acc: 0.6663\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79100 to 0.78755, saving model to best.model\n",
      "1s - loss: 0.8018 - acc: 0.6660 - val_loss: 0.7876 - val_acc: 0.6704\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78755 to 0.78617, saving model to best.model\n",
      "1s - loss: 0.8019 - acc: 0.6650 - val_loss: 0.7862 - val_acc: 0.6727\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78617 to 0.78392, saving model to best.model\n",
      "1s - loss: 0.7986 - acc: 0.6683 - val_loss: 0.7839 - val_acc: 0.6746\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78392 to 0.78116, saving model to best.model\n",
      "1s - loss: 0.7968 - acc: 0.6674 - val_loss: 0.7812 - val_acc: 0.6753\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78116 to 0.78051, saving model to best.model\n",
      "1s - loss: 0.7960 - acc: 0.6683 - val_loss: 0.7805 - val_acc: 0.6744\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78051 to 0.77791, saving model to best.model\n",
      "1s - loss: 0.7943 - acc: 0.6690 - val_loss: 0.7779 - val_acc: 0.6748\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.7930 - acc: 0.6695 - val_loss: 0.7784 - val_acc: 0.6727\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77791 to 0.77296, saving model to best.model\n",
      "0s - loss: 0.7912 - acc: 0.6698 - val_loss: 0.7730 - val_acc: 0.6785\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.7906 - acc: 0.6692 - val_loss: 0.7733 - val_acc: 0.6752\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77296 to 0.77160, saving model to best.model\n",
      "1s - loss: 0.7887 - acc: 0.6713 - val_loss: 0.7716 - val_acc: 0.6749\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77160 to 0.76830, saving model to best.model\n",
      "1s - loss: 0.7861 - acc: 0.6705 - val_loss: 0.7683 - val_acc: 0.6775\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76830 to 0.76814, saving model to best.model\n",
      "1s - loss: 0.7845 - acc: 0.6734 - val_loss: 0.7681 - val_acc: 0.6776\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76814 to 0.76359, saving model to best.model\n",
      "1s - loss: 0.7840 - acc: 0.6732 - val_loss: 0.7636 - val_acc: 0.6793\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76359 to 0.76031, saving model to best.model\n",
      "1s - loss: 0.7799 - acc: 0.6742 - val_loss: 0.7603 - val_acc: 0.6807\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76031 to 0.75810, saving model to best.model\n",
      "1s - loss: 0.7806 - acc: 0.6734 - val_loss: 0.7581 - val_acc: 0.6820\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75810 to 0.75667, saving model to best.model\n",
      "1s - loss: 0.7793 - acc: 0.6754 - val_loss: 0.7567 - val_acc: 0.6797\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75667 to 0.75268, saving model to best.model\n",
      "1s - loss: 0.7765 - acc: 0.6750 - val_loss: 0.7527 - val_acc: 0.6829\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75268 to 0.74893, saving model to best.model\n",
      "1s - loss: 0.7738 - acc: 0.6772 - val_loss: 0.7489 - val_acc: 0.6847\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74893 to 0.74786, saving model to best.model\n",
      "1s - loss: 0.7730 - acc: 0.6775 - val_loss: 0.7479 - val_acc: 0.6849\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74786 to 0.74656, saving model to best.model\n",
      "1s - loss: 0.7709 - acc: 0.6775 - val_loss: 0.7466 - val_acc: 0.6854\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74656 to 0.74265, saving model to best.model\n",
      "1s - loss: 0.7688 - acc: 0.6789 - val_loss: 0.7427 - val_acc: 0.6924\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74265 to 0.74211, saving model to best.model\n",
      "1s - loss: 0.7680 - acc: 0.6797 - val_loss: 0.7421 - val_acc: 0.6852\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74211 to 0.73678, saving model to best.model\n",
      "1s - loss: 0.7640 - acc: 0.6822 - val_loss: 0.7368 - val_acc: 0.6893\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73678 to 0.73365, saving model to best.model\n",
      "1s - loss: 0.7646 - acc: 0.6815 - val_loss: 0.7337 - val_acc: 0.6950\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73365 to 0.72958, saving model to best.model\n",
      "1s - loss: 0.7609 - acc: 0.6847 - val_loss: 0.7296 - val_acc: 0.6977\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72958 to 0.72778, saving model to best.model\n",
      "1s - loss: 0.7601 - acc: 0.6834 - val_loss: 0.7278 - val_acc: 0.6975\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.7584 - acc: 0.6846 - val_loss: 0.7278 - val_acc: 0.6936\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72778 to 0.72492, saving model to best.model\n",
      "1s - loss: 0.7565 - acc: 0.6867 - val_loss: 0.7249 - val_acc: 0.6994\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72492 to 0.71978, saving model to best.model\n",
      "1s - loss: 0.7525 - acc: 0.6880 - val_loss: 0.7198 - val_acc: 0.7011\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "1s - loss: 0.7537 - acc: 0.6861 - val_loss: 0.7250 - val_acc: 0.6985\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71978 to 0.71812, saving model to best.model\n",
      "1s - loss: 0.7500 - acc: 0.6891 - val_loss: 0.7181 - val_acc: 0.6998\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71812 to 0.71594, saving model to best.model\n",
      "1s - loss: 0.7487 - acc: 0.6892 - val_loss: 0.7159 - val_acc: 0.7030\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7460 - acc: 0.6922 - val_loss: 0.7165 - val_acc: 0.6987\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71594 to 0.70958, saving model to best.model\n",
      "1s - loss: 0.7444 - acc: 0.6910 - val_loss: 0.7096 - val_acc: 0.7076\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70958 to 0.70669, saving model to best.model\n",
      "1s - loss: 0.7458 - acc: 0.6917 - val_loss: 0.7067 - val_acc: 0.7108\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70669 to 0.70485, saving model to best.model\n",
      "1s - loss: 0.7398 - acc: 0.6947 - val_loss: 0.7049 - val_acc: 0.7112\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7396 - acc: 0.6923 - val_loss: 0.7065 - val_acc: 0.7039\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70485 to 0.70375, saving model to best.model\n",
      "1s - loss: 0.7389 - acc: 0.6927 - val_loss: 0.7037 - val_acc: 0.7116\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70375 to 0.70236, saving model to best.model\n",
      "1s - loss: 0.7405 - acc: 0.6911 - val_loss: 0.7024 - val_acc: 0.7101\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70236 to 0.69920, saving model to best.model\n",
      "1s - loss: 0.7334 - acc: 0.6986 - val_loss: 0.6992 - val_acc: 0.7137\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69920 to 0.69874, saving model to best.model\n",
      "1s - loss: 0.7354 - acc: 0.6949 - val_loss: 0.6987 - val_acc: 0.7141\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69874 to 0.69369, saving model to best.model\n",
      "1s - loss: 0.7332 - acc: 0.6971 - val_loss: 0.6937 - val_acc: 0.7164\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "1s - loss: 0.7317 - acc: 0.6982 - val_loss: 0.6952 - val_acc: 0.7139\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69369 to 0.69217, saving model to best.model\n",
      "1s - loss: 0.7285 - acc: 0.6984 - val_loss: 0.6922 - val_acc: 0.7141\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69217 to 0.69078, saving model to best.model\n",
      "1s - loss: 0.7305 - acc: 0.6988 - val_loss: 0.6908 - val_acc: 0.7156\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7276 - acc: 0.6991 - val_loss: 0.6932 - val_acc: 0.7111\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "1s - loss: 0.7278 - acc: 0.7005 - val_loss: 0.6914 - val_acc: 0.7153\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.69078 to 0.68761, saving model to best.model\n",
      "1s - loss: 0.7257 - acc: 0.7013 - val_loss: 0.6876 - val_acc: 0.7157\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68761 to 0.68532, saving model to best.model\n",
      "1s - loss: 0.7238 - acc: 0.7023 - val_loss: 0.6853 - val_acc: 0.7159\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68532 to 0.68428, saving model to best.model\n",
      "1s - loss: 0.7237 - acc: 0.7022 - val_loss: 0.6843 - val_acc: 0.7183\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68428 to 0.68081, saving model to best.model\n",
      "1s - loss: 0.7206 - acc: 0.7033 - val_loss: 0.6808 - val_acc: 0.7193\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "1s - loss: 0.7214 - acc: 0.7036 - val_loss: 0.6850 - val_acc: 0.7180\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68081 to 0.67614, saving model to best.model\n",
      "1s - loss: 0.7205 - acc: 0.7047 - val_loss: 0.6761 - val_acc: 0.7219\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7188 - acc: 0.7036 - val_loss: 0.6800 - val_acc: 0.7211\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7169 - acc: 0.7051 - val_loss: 0.6804 - val_acc: 0.7193\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7186 - acc: 0.7022 - val_loss: 0.6773 - val_acc: 0.7222\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.67614 to 0.67476, saving model to best.model\n",
      "1s - loss: 0.7160 - acc: 0.7050 - val_loss: 0.6748 - val_acc: 0.7234\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67476 to 0.67367, saving model to best.model\n",
      "1s - loss: 0.7156 - acc: 0.7053 - val_loss: 0.6737 - val_acc: 0.7212\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7156 - acc: 0.7063 - val_loss: 0.6751 - val_acc: 0.7208\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7134 - acc: 0.7063 - val_loss: 0.6738 - val_acc: 0.7247\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.67367 to 0.66925, saving model to best.model\n",
      "0s - loss: 0.7095 - acc: 0.7093 - val_loss: 0.6693 - val_acc: 0.7248\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66925 to 0.66812, saving model to best.model\n",
      "0s - loss: 0.7089 - acc: 0.7102 - val_loss: 0.6681 - val_acc: 0.7260\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66812 to 0.66664, saving model to best.model\n",
      "1s - loss: 0.7107 - acc: 0.7077 - val_loss: 0.6666 - val_acc: 0.7263\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.7083 - acc: 0.7104 - val_loss: 0.6685 - val_acc: 0.7244\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7075 - acc: 0.7113 - val_loss: 0.6670 - val_acc: 0.7268\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.66664 to 0.66621, saving model to best.model\n",
      "1s - loss: 0.7086 - acc: 0.7097 - val_loss: 0.6662 - val_acc: 0.7263\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.7049 - acc: 0.7104 - val_loss: 0.6664 - val_acc: 0.7247\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.66621 to 0.66353, saving model to best.model\n",
      "1s - loss: 0.7046 - acc: 0.7111 - val_loss: 0.6635 - val_acc: 0.7299\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66353 to 0.66249, saving model to best.model\n",
      "1s - loss: 0.7032 - acc: 0.7084 - val_loss: 0.6625 - val_acc: 0.7294\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66249 to 0.66120, saving model to best.model\n",
      "1s - loss: 0.7017 - acc: 0.7107 - val_loss: 0.6612 - val_acc: 0.7307\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.7030 - acc: 0.7106 - val_loss: 0.6621 - val_acc: 0.7283\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.66120 to 0.65956, saving model to best.model\n",
      "1s - loss: 0.7019 - acc: 0.7131 - val_loss: 0.6596 - val_acc: 0.7304\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65956 to 0.65918, saving model to best.model\n",
      "1s - loss: 0.7039 - acc: 0.7103 - val_loss: 0.6592 - val_acc: 0.7288\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65918 to 0.65586, saving model to best.model\n",
      "1s - loss: 0.7019 - acc: 0.7131 - val_loss: 0.6559 - val_acc: 0.7324\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.7007 - acc: 0.7117 - val_loss: 0.6577 - val_acc: 0.7307\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6984 - acc: 0.7129 - val_loss: 0.6560 - val_acc: 0.7316\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.65586 to 0.65567, saving model to best.model\n",
      "1s - loss: 0.6970 - acc: 0.7163 - val_loss: 0.6557 - val_acc: 0.7309\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65567 to 0.65509, saving model to best.model\n",
      "1s - loss: 0.6998 - acc: 0.7109 - val_loss: 0.6551 - val_acc: 0.7307\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6961 - acc: 0.7138 - val_loss: 0.6563 - val_acc: 0.7282\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65509 to 0.65179, saving model to best.model\n",
      "1s - loss: 0.6938 - acc: 0.7152 - val_loss: 0.6518 - val_acc: 0.7322\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6952 - acc: 0.7160 - val_loss: 0.6535 - val_acc: 0.7301\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6933 - acc: 0.7146 - val_loss: 0.6542 - val_acc: 0.7295\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.65179 to 0.65087, saving model to best.model\n",
      "1s - loss: 0.6961 - acc: 0.7151 - val_loss: 0.6509 - val_acc: 0.7329\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.65087 to 0.64799, saving model to best.model\n",
      "1s - loss: 0.6932 - acc: 0.7141 - val_loss: 0.6480 - val_acc: 0.7359\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6925 - acc: 0.7141 - val_loss: 0.6495 - val_acc: 0.7329\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6924 - acc: 0.7162 - val_loss: 0.6500 - val_acc: 0.7340\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6898 - acc: 0.7179 - val_loss: 0.6507 - val_acc: 0.7350\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64799 to 0.64649, saving model to best.model\n",
      "1s - loss: 0.6875 - acc: 0.7187 - val_loss: 0.6465 - val_acc: 0.7367\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.64649 to 0.64499, saving model to best.model\n",
      "1s - loss: 0.6870 - acc: 0.7181 - val_loss: 0.6450 - val_acc: 0.7361\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64499 to 0.64356, saving model to best.model\n",
      "1s - loss: 0.6867 - acc: 0.7186 - val_loss: 0.6436 - val_acc: 0.7383\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6865 - acc: 0.7184 - val_loss: 0.6455 - val_acc: 0.7344\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.64356 to 0.64047, saving model to best.model\n",
      "0s - loss: 0.6816 - acc: 0.7210 - val_loss: 0.6405 - val_acc: 0.7377\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6884 - acc: 0.7180 - val_loss: 0.6444 - val_acc: 0.7362\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6880 - acc: 0.7169 - val_loss: 0.6418 - val_acc: 0.7388\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6853 - acc: 0.7192 - val_loss: 0.6433 - val_acc: 0.7359\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64047 to 0.64034, saving model to best.model\n",
      "0s - loss: 0.6840 - acc: 0.7196 - val_loss: 0.6403 - val_acc: 0.7384\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.64034 to 0.63758, saving model to best.model\n",
      "1s - loss: 0.6816 - acc: 0.7208 - val_loss: 0.6376 - val_acc: 0.7383\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63758 to 0.63726, saving model to best.model\n",
      "1s - loss: 0.6840 - acc: 0.7201 - val_loss: 0.6373 - val_acc: 0.7400\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6829 - acc: 0.7187 - val_loss: 0.6374 - val_acc: 0.7395\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63726 to 0.63682, saving model to best.model\n",
      "0s - loss: 0.6798 - acc: 0.7219 - val_loss: 0.6368 - val_acc: 0.7386\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63682 to 0.63327, saving model to best.model\n",
      "1s - loss: 0.6831 - acc: 0.7214 - val_loss: 0.6333 - val_acc: 0.7399\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6793 - acc: 0.7212 - val_loss: 0.6346 - val_acc: 0.7399\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6791 - acc: 0.7199 - val_loss: 0.6342 - val_acc: 0.7395\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6794 - acc: 0.7207 - val_loss: 0.6346 - val_acc: 0.7397\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63327 to 0.63127, saving model to best.model\n",
      "1s - loss: 0.6765 - acc: 0.7212 - val_loss: 0.6313 - val_acc: 0.7403\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6741 - acc: 0.7236 - val_loss: 0.6332 - val_acc: 0.7390\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63127 to 0.63018, saving model to best.model\n",
      "1s - loss: 0.6759 - acc: 0.7230 - val_loss: 0.6302 - val_acc: 0.7415\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6752 - acc: 0.7216 - val_loss: 0.6306 - val_acc: 0.7403\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63018 to 0.62806, saving model to best.model\n",
      "0s - loss: 0.6724 - acc: 0.7243 - val_loss: 0.6281 - val_acc: 0.7407\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62806 to 0.62701, saving model to best.model\n",
      "0s - loss: 0.6732 - acc: 0.7243 - val_loss: 0.6270 - val_acc: 0.7420\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6733 - acc: 0.7245 - val_loss: 0.6308 - val_acc: 0.7413\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6746 - acc: 0.7233 - val_loss: 0.6298 - val_acc: 0.7418\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6724 - acc: 0.7247 - val_loss: 0.6290 - val_acc: 0.7410\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6740 - acc: 0.7239 - val_loss: 0.6296 - val_acc: 0.7402\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6749 - acc: 0.7235 - val_loss: 0.6271 - val_acc: 0.7426\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62701 to 0.62553, saving model to best.model\n",
      "1s - loss: 0.6699 - acc: 0.7253 - val_loss: 0.6255 - val_acc: 0.7430\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6699 - acc: 0.7273 - val_loss: 0.6275 - val_acc: 0.7425\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6708 - acc: 0.7254 - val_loss: 0.6262 - val_acc: 0.7419\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6699 - acc: 0.7244 - val_loss: 0.6269 - val_acc: 0.7400\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.62553 to 0.62471, saving model to best.model\n",
      "1s - loss: 0.6737 - acc: 0.7245 - val_loss: 0.6247 - val_acc: 0.7434\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62471 to 0.62410, saving model to best.model\n",
      "0s - loss: 0.6716 - acc: 0.7265 - val_loss: 0.6241 - val_acc: 0.7423\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62410 to 0.62214, saving model to best.model\n",
      "1s - loss: 0.6677 - acc: 0.7267 - val_loss: 0.6221 - val_acc: 0.7437\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62214 to 0.62090, saving model to best.model\n",
      "1s - loss: 0.6666 - acc: 0.7282 - val_loss: 0.6209 - val_acc: 0.7439\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.62090 to 0.62027, saving model to best.model\n",
      "1s - loss: 0.6679 - acc: 0.7244 - val_loss: 0.6203 - val_acc: 0.7454\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6683 - acc: 0.7257 - val_loss: 0.6208 - val_acc: 0.7446\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6675 - acc: 0.7256 - val_loss: 0.6209 - val_acc: 0.7443\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6658 - acc: 0.7269 - val_loss: 0.6205 - val_acc: 0.7434\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.62027 to 0.61911, saving model to best.model\n",
      "0s - loss: 0.6658 - acc: 0.7267 - val_loss: 0.6191 - val_acc: 0.7448\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6654 - acc: 0.7271 - val_loss: 0.6203 - val_acc: 0.7452\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61911 to 0.61817, saving model to best.model\n",
      "0s - loss: 0.6668 - acc: 0.7264 - val_loss: 0.6182 - val_acc: 0.7438\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61817 to 0.61758, saving model to best.model\n",
      "0s - loss: 0.6665 - acc: 0.7275 - val_loss: 0.6176 - val_acc: 0.7447\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61758 to 0.61601, saving model to best.model\n",
      "1s - loss: 0.6656 - acc: 0.7275 - val_loss: 0.6160 - val_acc: 0.7448\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61601 to 0.61562, saving model to best.model\n",
      "1s - loss: 0.6624 - acc: 0.7274 - val_loss: 0.6156 - val_acc: 0.7450\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6652 - acc: 0.7286 - val_loss: 0.6181 - val_acc: 0.7452\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6649 - acc: 0.7262 - val_loss: 0.6171 - val_acc: 0.7461\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7294 - val_loss: 0.6176 - val_acc: 0.7456\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6611 - acc: 0.7278 - val_loss: 0.6170 - val_acc: 0.7436\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61562 to 0.61397, saving model to best.model\n",
      "1s - loss: 0.6595 - acc: 0.7297 - val_loss: 0.6140 - val_acc: 0.7466\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6587 - acc: 0.7309 - val_loss: 0.6152 - val_acc: 0.7472\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61397 to 0.61339, saving model to best.model\n",
      "1s - loss: 0.6573 - acc: 0.7295 - val_loss: 0.6134 - val_acc: 0.7453\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6597 - acc: 0.7305 - val_loss: 0.6138 - val_acc: 0.7464\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.61339 to 0.61169, saving model to best.model\n",
      "1s - loss: 0.6569 - acc: 0.7330 - val_loss: 0.6117 - val_acc: 0.7468\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6589 - acc: 0.7312 - val_loss: 0.6133 - val_acc: 0.7448\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6594 - acc: 0.7288 - val_loss: 0.6121 - val_acc: 0.7458\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61169 to 0.61009, saving model to best.model\n",
      "1s - loss: 0.6558 - acc: 0.7303 - val_loss: 0.6101 - val_acc: 0.7468\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6589 - acc: 0.7294 - val_loss: 0.6102 - val_acc: 0.7470\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6570 - acc: 0.7290 - val_loss: 0.6145 - val_acc: 0.7441\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.61009 to 0.60893, saving model to best.model\n",
      "1s - loss: 0.6571 - acc: 0.7314 - val_loss: 0.6089 - val_acc: 0.7475\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6568 - acc: 0.7313 - val_loss: 0.6112 - val_acc: 0.7458\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60893 to 0.60797, saving model to best.model\n",
      "1s - loss: 0.6565 - acc: 0.7297 - val_loss: 0.6080 - val_acc: 0.7471\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6570 - acc: 0.7306 - val_loss: 0.6116 - val_acc: 0.7463\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6567 - acc: 0.7297 - val_loss: 0.6136 - val_acc: 0.7438\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60797 to 0.60718, saving model to best.model\n",
      "0s - loss: 0.6543 - acc: 0.7323 - val_loss: 0.6072 - val_acc: 0.7475\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60718 to 0.60625, saving model to best.model\n",
      "0s - loss: 0.6536 - acc: 0.7304 - val_loss: 0.6063 - val_acc: 0.7489\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60625 to 0.60601, saving model to best.model\n",
      "1s - loss: 0.6548 - acc: 0.7325 - val_loss: 0.6060 - val_acc: 0.7499\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6552 - acc: 0.7310 - val_loss: 0.6064 - val_acc: 0.7496\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60601 to 0.60408, saving model to best.model\n",
      "1s - loss: 0.6521 - acc: 0.7321 - val_loss: 0.6041 - val_acc: 0.7504\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6534 - acc: 0.7317 - val_loss: 0.6054 - val_acc: 0.7508\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7320 - val_loss: 0.6050 - val_acc: 0.7498\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6539 - acc: 0.7318 - val_loss: 0.6057 - val_acc: 0.7481\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83782, saving model to best.model\n",
      "1s - loss: 0.9184 - acc: 0.6281 - val_loss: 0.8378 - val_acc: 0.6656\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8615 - acc: 0.6578 - val_loss: 0.8402 - val_acc: 0.6656\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.83782 to 0.83698, saving model to best.model\n",
      "1s - loss: 0.8535 - acc: 0.6586 - val_loss: 0.8370 - val_acc: 0.6656\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83698 to 0.83580, saving model to best.model\n",
      "1s - loss: 0.8522 - acc: 0.6586 - val_loss: 0.8358 - val_acc: 0.6656\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83580 to 0.82554, saving model to best.model\n",
      "1s - loss: 0.8455 - acc: 0.6586 - val_loss: 0.8255 - val_acc: 0.6656\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82554 to 0.81785, saving model to best.model\n",
      "1s - loss: 0.8387 - acc: 0.6586 - val_loss: 0.8179 - val_acc: 0.6656\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "1s - loss: 0.8341 - acc: 0.6586 - val_loss: 0.8199 - val_acc: 0.6656\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81785 to 0.81661, saving model to best.model\n",
      "1s - loss: 0.8328 - acc: 0.6586 - val_loss: 0.8166 - val_acc: 0.6656\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81661 to 0.81411, saving model to best.model\n",
      "1s - loss: 0.8315 - acc: 0.6586 - val_loss: 0.8141 - val_acc: 0.6656\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81411 to 0.81389, saving model to best.model\n",
      "1s - loss: 0.8297 - acc: 0.6586 - val_loss: 0.8139 - val_acc: 0.6656\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81389 to 0.81179, saving model to best.model\n",
      "1s - loss: 0.8286 - acc: 0.6585 - val_loss: 0.8118 - val_acc: 0.6656\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8276 - acc: 0.6586 - val_loss: 0.8120 - val_acc: 0.6656\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.8252 - acc: 0.6585 - val_loss: 0.8118 - val_acc: 0.6656\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81179 to 0.81061, saving model to best.model\n",
      "0s - loss: 0.8260 - acc: 0.6586 - val_loss: 0.8106 - val_acc: 0.6656\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "1s - loss: 0.8255 - acc: 0.6586 - val_loss: 0.8111 - val_acc: 0.6656\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8250 - acc: 0.6587 - val_loss: 0.8107 - val_acc: 0.6656\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81061 to 0.80890, saving model to best.model\n",
      "1s - loss: 0.8238 - acc: 0.6587 - val_loss: 0.8089 - val_acc: 0.6656\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.80890 to 0.80846, saving model to best.model\n",
      "1s - loss: 0.8241 - acc: 0.6582 - val_loss: 0.8085 - val_acc: 0.6656\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.80846 to 0.80775, saving model to best.model\n",
      "1s - loss: 0.8238 - acc: 0.6586 - val_loss: 0.8078 - val_acc: 0.6656\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.80775 to 0.80731, saving model to best.model\n",
      "1s - loss: 0.8224 - acc: 0.6588 - val_loss: 0.8073 - val_acc: 0.6656\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.80731 to 0.80668, saving model to best.model\n",
      "1s - loss: 0.8205 - acc: 0.6588 - val_loss: 0.8067 - val_acc: 0.6656\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "1s - loss: 0.8196 - acc: 0.6583 - val_loss: 0.8069 - val_acc: 0.6656\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.80668 to 0.80604, saving model to best.model\n",
      "1s - loss: 0.8193 - acc: 0.6588 - val_loss: 0.8060 - val_acc: 0.6659\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80604 to 0.80481, saving model to best.model\n",
      "1s - loss: 0.8177 - acc: 0.6590 - val_loss: 0.8048 - val_acc: 0.6674\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80481 to 0.80335, saving model to best.model\n",
      "1s - loss: 0.8174 - acc: 0.6595 - val_loss: 0.8033 - val_acc: 0.6684\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "1s - loss: 0.8161 - acc: 0.6600 - val_loss: 0.8053 - val_acc: 0.6692\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80335 to 0.80116, saving model to best.model\n",
      "1s - loss: 0.8141 - acc: 0.6619 - val_loss: 0.8012 - val_acc: 0.6684\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80116 to 0.80071, saving model to best.model\n",
      "1s - loss: 0.8140 - acc: 0.6609 - val_loss: 0.8007 - val_acc: 0.6708\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80071 to 0.79913, saving model to best.model\n",
      "1s - loss: 0.8131 - acc: 0.6632 - val_loss: 0.7991 - val_acc: 0.6731\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "1s - loss: 0.8114 - acc: 0.6616 - val_loss: 0.7993 - val_acc: 0.6737\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79913 to 0.79728, saving model to best.model\n",
      "1s - loss: 0.8107 - acc: 0.6632 - val_loss: 0.7973 - val_acc: 0.6718\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79728 to 0.79565, saving model to best.model\n",
      "1s - loss: 0.8089 - acc: 0.6616 - val_loss: 0.7956 - val_acc: 0.6737\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79565 to 0.79452, saving model to best.model\n",
      "1s - loss: 0.8081 - acc: 0.6640 - val_loss: 0.7945 - val_acc: 0.6746\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79452 to 0.79382, saving model to best.model\n",
      "1s - loss: 0.8058 - acc: 0.6649 - val_loss: 0.7938 - val_acc: 0.6740\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79382 to 0.78948, saving model to best.model\n",
      "1s - loss: 0.8048 - acc: 0.6650 - val_loss: 0.7895 - val_acc: 0.6774\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.78948 to 0.78823, saving model to best.model\n",
      "1s - loss: 0.8035 - acc: 0.6658 - val_loss: 0.7882 - val_acc: 0.6778\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78823 to 0.78622, saving model to best.model\n",
      "1s - loss: 0.8024 - acc: 0.6658 - val_loss: 0.7862 - val_acc: 0.6781\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78622 to 0.78550, saving model to best.model\n",
      "1s - loss: 0.8015 - acc: 0.6641 - val_loss: 0.7855 - val_acc: 0.6783\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78550 to 0.78463, saving model to best.model\n",
      "1s - loss: 0.7990 - acc: 0.6688 - val_loss: 0.7846 - val_acc: 0.6783\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78463 to 0.78243, saving model to best.model\n",
      "1s - loss: 0.7979 - acc: 0.6682 - val_loss: 0.7824 - val_acc: 0.6766\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78243 to 0.77829, saving model to best.model\n",
      "1s - loss: 0.7956 - acc: 0.6681 - val_loss: 0.7783 - val_acc: 0.6792\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77829 to 0.77690, saving model to best.model\n",
      "1s - loss: 0.7963 - acc: 0.6690 - val_loss: 0.7769 - val_acc: 0.6807\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77690 to 0.77584, saving model to best.model\n",
      "1s - loss: 0.7951 - acc: 0.6678 - val_loss: 0.7758 - val_acc: 0.6776\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77584 to 0.77105, saving model to best.model\n",
      "1s - loss: 0.7921 - acc: 0.6710 - val_loss: 0.7711 - val_acc: 0.6824\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77105 to 0.77034, saving model to best.model\n",
      "0s - loss: 0.7916 - acc: 0.6699 - val_loss: 0.7703 - val_acc: 0.6821\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77034 to 0.76687, saving model to best.model\n",
      "0s - loss: 0.7886 - acc: 0.6708 - val_loss: 0.7669 - val_acc: 0.6836\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76687 to 0.76655, saving model to best.model\n",
      "0s - loss: 0.7869 - acc: 0.6722 - val_loss: 0.7665 - val_acc: 0.6841\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76655 to 0.76377, saving model to best.model\n",
      "0s - loss: 0.7849 - acc: 0.6727 - val_loss: 0.7638 - val_acc: 0.6854\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76377 to 0.76155, saving model to best.model\n",
      "1s - loss: 0.7840 - acc: 0.6731 - val_loss: 0.7615 - val_acc: 0.6888\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76155 to 0.75891, saving model to best.model\n",
      "1s - loss: 0.7808 - acc: 0.6767 - val_loss: 0.7589 - val_acc: 0.6852\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75891 to 0.75508, saving model to best.model\n",
      "1s - loss: 0.7792 - acc: 0.6770 - val_loss: 0.7551 - val_acc: 0.6867\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75508 to 0.75386, saving model to best.model\n",
      "1s - loss: 0.7784 - acc: 0.6775 - val_loss: 0.7539 - val_acc: 0.6877\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75386 to 0.74827, saving model to best.model\n",
      "0s - loss: 0.7752 - acc: 0.6773 - val_loss: 0.7483 - val_acc: 0.6898\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.7730 - acc: 0.6810 - val_loss: 0.7484 - val_acc: 0.6966\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74827 to 0.74437, saving model to best.model\n",
      "0s - loss: 0.7745 - acc: 0.6781 - val_loss: 0.7444 - val_acc: 0.6909\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74437 to 0.74074, saving model to best.model\n",
      "1s - loss: 0.7688 - acc: 0.6802 - val_loss: 0.7407 - val_acc: 0.6964\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74074 to 0.73639, saving model to best.model\n",
      "1s - loss: 0.7673 - acc: 0.6804 - val_loss: 0.7364 - val_acc: 0.6996\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.73639 to 0.73406, saving model to best.model\n",
      "1s - loss: 0.7648 - acc: 0.6846 - val_loss: 0.7341 - val_acc: 0.6964\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73406 to 0.73371, saving model to best.model\n",
      "1s - loss: 0.7630 - acc: 0.6823 - val_loss: 0.7337 - val_acc: 0.6977\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73371 to 0.72745, saving model to best.model\n",
      "1s - loss: 0.7590 - acc: 0.6865 - val_loss: 0.7275 - val_acc: 0.7009\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "1s - loss: 0.7589 - acc: 0.6867 - val_loss: 0.7278 - val_acc: 0.6986\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72745 to 0.72324, saving model to best.model\n",
      "1s - loss: 0.7561 - acc: 0.6882 - val_loss: 0.7232 - val_acc: 0.7044\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.72324 to 0.71872, saving model to best.model\n",
      "1s - loss: 0.7551 - acc: 0.6884 - val_loss: 0.7187 - val_acc: 0.7023\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "1s - loss: 0.7527 - acc: 0.6897 - val_loss: 0.7192 - val_acc: 0.7053\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "1s - loss: 0.7522 - acc: 0.6902 - val_loss: 0.7191 - val_acc: 0.7047\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71872 to 0.71058, saving model to best.model\n",
      "1s - loss: 0.7478 - acc: 0.6909 - val_loss: 0.7106 - val_acc: 0.7070\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.71058 to 0.70921, saving model to best.model\n",
      "1s - loss: 0.7457 - acc: 0.6926 - val_loss: 0.7092 - val_acc: 0.7111\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.70921 to 0.70552, saving model to best.model\n",
      "1s - loss: 0.7441 - acc: 0.6948 - val_loss: 0.7055 - val_acc: 0.7107\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.70552 to 0.70470, saving model to best.model\n",
      "1s - loss: 0.7415 - acc: 0.6951 - val_loss: 0.7047 - val_acc: 0.7112\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70470 to 0.70468, saving model to best.model\n",
      "0s - loss: 0.7423 - acc: 0.6920 - val_loss: 0.7047 - val_acc: 0.7067\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70468 to 0.70005, saving model to best.model\n",
      "0s - loss: 0.7382 - acc: 0.6962 - val_loss: 0.7000 - val_acc: 0.7163\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70005 to 0.69472, saving model to best.model\n",
      "0s - loss: 0.7355 - acc: 0.6997 - val_loss: 0.6947 - val_acc: 0.7157\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.7338 - acc: 0.6994 - val_loss: 0.6949 - val_acc: 0.7190\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.69472 to 0.69026, saving model to best.model\n",
      "0s - loss: 0.7349 - acc: 0.6970 - val_loss: 0.6903 - val_acc: 0.7172\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.7319 - acc: 0.7005 - val_loss: 0.6940 - val_acc: 0.7174\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.7302 - acc: 0.7025 - val_loss: 0.6905 - val_acc: 0.7164\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69026 to 0.68734, saving model to best.model\n",
      "1s - loss: 0.7272 - acc: 0.7009 - val_loss: 0.6873 - val_acc: 0.7171\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.68734 to 0.68535, saving model to best.model\n",
      "1s - loss: 0.7298 - acc: 0.6981 - val_loss: 0.6854 - val_acc: 0.7190\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7265 - acc: 0.7010 - val_loss: 0.6854 - val_acc: 0.7191\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.68535 to 0.68534, saving model to best.model\n",
      "1s - loss: 0.7256 - acc: 0.7029 - val_loss: 0.6853 - val_acc: 0.7172\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68534 to 0.68439, saving model to best.model\n",
      "1s - loss: 0.7249 - acc: 0.7023 - val_loss: 0.6844 - val_acc: 0.7173\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.68439 to 0.68255, saving model to best.model\n",
      "1s - loss: 0.7244 - acc: 0.7036 - val_loss: 0.6826 - val_acc: 0.7237\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68255 to 0.67814, saving model to best.model\n",
      "1s - loss: 0.7256 - acc: 0.7023 - val_loss: 0.6781 - val_acc: 0.7204\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7217 - acc: 0.7067 - val_loss: 0.6796 - val_acc: 0.7245\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7184 - acc: 0.7046 - val_loss: 0.6795 - val_acc: 0.7197\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67814 to 0.67494, saving model to best.model\n",
      "1s - loss: 0.7161 - acc: 0.7084 - val_loss: 0.6749 - val_acc: 0.7232\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "1s - loss: 0.7159 - acc: 0.7065 - val_loss: 0.6768 - val_acc: 0.7233\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67494 to 0.67346, saving model to best.model\n",
      "1s - loss: 0.7150 - acc: 0.7082 - val_loss: 0.6735 - val_acc: 0.7242\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67346 to 0.66929, saving model to best.model\n",
      "1s - loss: 0.7162 - acc: 0.7065 - val_loss: 0.6693 - val_acc: 0.7249\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.66929 to 0.66766, saving model to best.model\n",
      "1s - loss: 0.7138 - acc: 0.7095 - val_loss: 0.6677 - val_acc: 0.7245\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7128 - acc: 0.7083 - val_loss: 0.6687 - val_acc: 0.7240\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66766 to 0.66513, saving model to best.model\n",
      "1s - loss: 0.7098 - acc: 0.7086 - val_loss: 0.6651 - val_acc: 0.7263\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7086 - acc: 0.7111 - val_loss: 0.6683 - val_acc: 0.7247\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7112 - acc: 0.7103 - val_loss: 0.6665 - val_acc: 0.7261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66513 to 0.66436, saving model to best.model\n",
      "1s - loss: 0.7069 - acc: 0.7098 - val_loss: 0.6644 - val_acc: 0.7262\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7077 - acc: 0.7104 - val_loss: 0.6668 - val_acc: 0.7274\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66436 to 0.66292, saving model to best.model\n",
      "1s - loss: 0.7072 - acc: 0.7108 - val_loss: 0.6629 - val_acc: 0.7275\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7047 - acc: 0.7110 - val_loss: 0.6668 - val_acc: 0.7274\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66292 to 0.65744, saving model to best.model\n",
      "1s - loss: 0.7026 - acc: 0.7152 - val_loss: 0.6574 - val_acc: 0.7286\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7049 - acc: 0.7123 - val_loss: 0.6592 - val_acc: 0.7293\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7028 - acc: 0.7129 - val_loss: 0.6619 - val_acc: 0.7283\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.65744 to 0.65687, saving model to best.model\n",
      "1s - loss: 0.7024 - acc: 0.7112 - val_loss: 0.6569 - val_acc: 0.7289\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.65687 to 0.65531, saving model to best.model\n",
      "1s - loss: 0.7035 - acc: 0.7112 - val_loss: 0.6553 - val_acc: 0.7310\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.6988 - acc: 0.7143 - val_loss: 0.6553 - val_acc: 0.7314\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.6985 - acc: 0.7151 - val_loss: 0.6556 - val_acc: 0.7297\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7008 - acc: 0.7142 - val_loss: 0.6573 - val_acc: 0.7292\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65531 to 0.65343, saving model to best.model\n",
      "1s - loss: 0.6976 - acc: 0.7152 - val_loss: 0.6534 - val_acc: 0.7306\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65343 to 0.64936, saving model to best.model\n",
      "1s - loss: 0.6957 - acc: 0.7164 - val_loss: 0.6494 - val_acc: 0.7300\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6991 - acc: 0.7151 - val_loss: 0.6509 - val_acc: 0.7320\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6968 - acc: 0.7157 - val_loss: 0.6509 - val_acc: 0.7311\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.6952 - acc: 0.7153 - val_loss: 0.6544 - val_acc: 0.7309\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.6903 - acc: 0.7194 - val_loss: 0.6496 - val_acc: 0.7323\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.64936 to 0.64835, saving model to best.model\n",
      "0s - loss: 0.6952 - acc: 0.7167 - val_loss: 0.6483 - val_acc: 0.7348\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.6939 - acc: 0.7168 - val_loss: 0.6484 - val_acc: 0.7328\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.64835 to 0.64730, saving model to best.model\n",
      "0s - loss: 0.6893 - acc: 0.7195 - val_loss: 0.6473 - val_acc: 0.7331\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.64730 to 0.64517, saving model to best.model\n",
      "0s - loss: 0.6877 - acc: 0.7176 - val_loss: 0.6452 - val_acc: 0.7331\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64517 to 0.64442, saving model to best.model\n",
      "0s - loss: 0.6902 - acc: 0.7172 - val_loss: 0.6444 - val_acc: 0.7330\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.6901 - acc: 0.7195 - val_loss: 0.6463 - val_acc: 0.7333\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64442 to 0.64200, saving model to best.model\n",
      "0s - loss: 0.6872 - acc: 0.7199 - val_loss: 0.6420 - val_acc: 0.7337\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.6917 - acc: 0.7164 - val_loss: 0.6443 - val_acc: 0.7345\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64200 to 0.64186, saving model to best.model\n",
      "0s - loss: 0.6867 - acc: 0.7201 - val_loss: 0.6419 - val_acc: 0.7341\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6881 - acc: 0.7188 - val_loss: 0.6420 - val_acc: 0.7349\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64186 to 0.64102, saving model to best.model\n",
      "1s - loss: 0.6845 - acc: 0.7192 - val_loss: 0.6410 - val_acc: 0.7344\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64102 to 0.63773, saving model to best.model\n",
      "1s - loss: 0.6886 - acc: 0.7197 - val_loss: 0.6377 - val_acc: 0.7354\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6851 - acc: 0.7221 - val_loss: 0.6412 - val_acc: 0.7344\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63773 to 0.63712, saving model to best.model\n",
      "1s - loss: 0.6842 - acc: 0.7205 - val_loss: 0.6371 - val_acc: 0.7348\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.63712 to 0.63461, saving model to best.model\n",
      "1s - loss: 0.6849 - acc: 0.7196 - val_loss: 0.6346 - val_acc: 0.7349\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6817 - acc: 0.7217 - val_loss: 0.6377 - val_acc: 0.7344\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6826 - acc: 0.7210 - val_loss: 0.6349 - val_acc: 0.7361\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6814 - acc: 0.7224 - val_loss: 0.6352 - val_acc: 0.7354\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6838 - acc: 0.7217 - val_loss: 0.6394 - val_acc: 0.7357\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.63461 to 0.63359, saving model to best.model\n",
      "1s - loss: 0.6768 - acc: 0.7244 - val_loss: 0.6336 - val_acc: 0.7376\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63359 to 0.63290, saving model to best.model\n",
      "1s - loss: 0.6798 - acc: 0.7216 - val_loss: 0.6329 - val_acc: 0.7378\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63290 to 0.63089, saving model to best.model\n",
      "1s - loss: 0.6811 - acc: 0.7231 - val_loss: 0.6309 - val_acc: 0.7370\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6762 - acc: 0.7243 - val_loss: 0.6338 - val_acc: 0.7368\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63089 to 0.62950, saving model to best.model\n",
      "1s - loss: 0.6779 - acc: 0.7237 - val_loss: 0.6295 - val_acc: 0.7372\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6764 - acc: 0.7234 - val_loss: 0.6325 - val_acc: 0.7378\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62950 to 0.62940, saving model to best.model\n",
      "1s - loss: 0.6757 - acc: 0.7217 - val_loss: 0.6294 - val_acc: 0.7378\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62940 to 0.62899, saving model to best.model\n",
      "1s - loss: 0.6742 - acc: 0.7238 - val_loss: 0.6290 - val_acc: 0.7381\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6767 - acc: 0.7224 - val_loss: 0.6316 - val_acc: 0.7385\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62899 to 0.62768, saving model to best.model\n",
      "0s - loss: 0.6742 - acc: 0.7253 - val_loss: 0.6277 - val_acc: 0.7386\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6767 - acc: 0.7221 - val_loss: 0.6306 - val_acc: 0.7393\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.62768 to 0.62702, saving model to best.model\n",
      "1s - loss: 0.6753 - acc: 0.7235 - val_loss: 0.6270 - val_acc: 0.7384\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62702 to 0.62605, saving model to best.model\n",
      "0s - loss: 0.6713 - acc: 0.7280 - val_loss: 0.6261 - val_acc: 0.7395\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.62605 to 0.62583, saving model to best.model\n",
      "0s - loss: 0.6702 - acc: 0.7281 - val_loss: 0.6258 - val_acc: 0.7409\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6703 - acc: 0.7280 - val_loss: 0.6307 - val_acc: 0.7386\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6730 - acc: 0.7285 - val_loss: 0.6258 - val_acc: 0.7392\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62583 to 0.62539, saving model to best.model\n",
      "0s - loss: 0.6698 - acc: 0.7253 - val_loss: 0.6254 - val_acc: 0.7404\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.62539 to 0.62441, saving model to best.model\n",
      "0s - loss: 0.6686 - acc: 0.7279 - val_loss: 0.6244 - val_acc: 0.7399\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62441 to 0.62303, saving model to best.model\n",
      "0s - loss: 0.6654 - acc: 0.7282 - val_loss: 0.6230 - val_acc: 0.7398\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62303 to 0.62299, saving model to best.model\n",
      "0s - loss: 0.6684 - acc: 0.7285 - val_loss: 0.6230 - val_acc: 0.7413\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62299 to 0.62179, saving model to best.model\n",
      "0s - loss: 0.6671 - acc: 0.7282 - val_loss: 0.6218 - val_acc: 0.7410\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6678 - acc: 0.7254 - val_loss: 0.6234 - val_acc: 0.7403\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62179 to 0.62102, saving model to best.model\n",
      "1s - loss: 0.6678 - acc: 0.7267 - val_loss: 0.6210 - val_acc: 0.7423\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62102 to 0.61990, saving model to best.model\n",
      "1s - loss: 0.6632 - acc: 0.7289 - val_loss: 0.6199 - val_acc: 0.7423\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6669 - acc: 0.7285 - val_loss: 0.6222 - val_acc: 0.7399\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6656 - acc: 0.7286 - val_loss: 0.6208 - val_acc: 0.7422\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.61990 to 0.61904, saving model to best.model\n",
      "1s - loss: 0.6676 - acc: 0.7268 - val_loss: 0.6190 - val_acc: 0.7451\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6647 - acc: 0.7280 - val_loss: 0.6195 - val_acc: 0.7429\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6645 - acc: 0.7288 - val_loss: 0.6197 - val_acc: 0.7426\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6666 - acc: 0.7294 - val_loss: 0.6203 - val_acc: 0.7436\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6621 - acc: 0.7294 - val_loss: 0.6212 - val_acc: 0.7434\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6631 - acc: 0.7299 - val_loss: 0.6196 - val_acc: 0.7438\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61904 to 0.61631, saving model to best.model\n",
      "0s - loss: 0.6604 - acc: 0.7290 - val_loss: 0.6163 - val_acc: 0.7433\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6610 - acc: 0.7278 - val_loss: 0.6168 - val_acc: 0.7444\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61631 to 0.61538, saving model to best.model\n",
      "1s - loss: 0.6636 - acc: 0.7277 - val_loss: 0.6154 - val_acc: 0.7459\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6626 - acc: 0.7301 - val_loss: 0.6157 - val_acc: 0.7454\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6600 - acc: 0.7305 - val_loss: 0.6168 - val_acc: 0.7454\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.61538 to 0.61537, saving model to best.model\n",
      "1s - loss: 0.6606 - acc: 0.7295 - val_loss: 0.6154 - val_acc: 0.7444\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6600 - acc: 0.7297 - val_loss: 0.6173 - val_acc: 0.7441\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6613 - acc: 0.7293 - val_loss: 0.6169 - val_acc: 0.7451\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61537 to 0.61536, saving model to best.model\n",
      "1s - loss: 0.6574 - acc: 0.7311 - val_loss: 0.6154 - val_acc: 0.7444\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61536 to 0.61382, saving model to best.model\n",
      "1s - loss: 0.6584 - acc: 0.7316 - val_loss: 0.6138 - val_acc: 0.7457\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.61382 to 0.61331, saving model to best.model\n",
      "1s - loss: 0.6567 - acc: 0.7318 - val_loss: 0.6133 - val_acc: 0.7478\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61331 to 0.61115, saving model to best.model\n",
      "1s - loss: 0.6565 - acc: 0.7309 - val_loss: 0.6111 - val_acc: 0.7468\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.61115 to 0.60964, saving model to best.model\n",
      "1s - loss: 0.6577 - acc: 0.7322 - val_loss: 0.6096 - val_acc: 0.7485\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6592 - acc: 0.7290 - val_loss: 0.6121 - val_acc: 0.7493\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6538 - acc: 0.7320 - val_loss: 0.6122 - val_acc: 0.7459\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6574 - acc: 0.7315 - val_loss: 0.6101 - val_acc: 0.7500\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6570 - acc: 0.7315 - val_loss: 0.6099 - val_acc: 0.7488\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60964 to 0.60938, saving model to best.model\n",
      "1s - loss: 0.6580 - acc: 0.7312 - val_loss: 0.6094 - val_acc: 0.7491\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60938 to 0.60830, saving model to best.model\n",
      "1s - loss: 0.6529 - acc: 0.7328 - val_loss: 0.6083 - val_acc: 0.7499\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6536 - acc: 0.7325 - val_loss: 0.6096 - val_acc: 0.7467\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60830 to 0.60614, saving model to best.model\n",
      "1s - loss: 0.6543 - acc: 0.7328 - val_loss: 0.6061 - val_acc: 0.7485\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6530 - acc: 0.7334 - val_loss: 0.6096 - val_acc: 0.7494\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7333 - val_loss: 0.6086 - val_acc: 0.7493\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6526 - acc: 0.7335 - val_loss: 0.6090 - val_acc: 0.7489\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6469 - acc: 0.7358 - val_loss: 0.6063 - val_acc: 0.7499\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60614 to 0.60562, saving model to best.model\n",
      "0s - loss: 0.6524 - acc: 0.7341 - val_loss: 0.6056 - val_acc: 0.7494\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60562 to 0.60509, saving model to best.model\n",
      "1s - loss: 0.6501 - acc: 0.7345 - val_loss: 0.6051 - val_acc: 0.7499\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6499 - acc: 0.7344 - val_loss: 0.6053 - val_acc: 0.7506\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.60509 to 0.60320, saving model to best.model\n",
      "1s - loss: 0.6516 - acc: 0.7348 - val_loss: 0.6032 - val_acc: 0.7504\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6480 - acc: 0.7330 - val_loss: 0.6065 - val_acc: 0.7506\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6499 - acc: 0.7343 - val_loss: 0.6046 - val_acc: 0.7514\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6521 - acc: 0.7351 - val_loss: 0.6037 - val_acc: 0.7511\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6501 - acc: 0.7334 - val_loss: 0.6038 - val_acc: 0.7504\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60320 to 0.60217, saving model to best.model\n",
      "1s - loss: 0.6512 - acc: 0.7339 - val_loss: 0.6022 - val_acc: 0.7516\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60217 to 0.60202, saving model to best.model\n",
      "1s - loss: 0.6466 - acc: 0.7351 - val_loss: 0.6020 - val_acc: 0.7535\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6491 - acc: 0.7330 - val_loss: 0.6022 - val_acc: 0.7501\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6472 - acc: 0.7335 - val_loss: 0.6056 - val_acc: 0.7516\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84270, saving model to best.model\n",
      "1s - loss: 0.9336 - acc: 0.6193 - val_loss: 0.8427 - val_acc: 0.6559\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84270 to 0.84256, saving model to best.model\n",
      "1s - loss: 0.8613 - acc: 0.6589 - val_loss: 0.8426 - val_acc: 0.6559\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84256 to 0.84180, saving model to best.model\n",
      "1s - loss: 0.8514 - acc: 0.6600 - val_loss: 0.8418 - val_acc: 0.6559\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84180 to 0.83932, saving model to best.model\n",
      "1s - loss: 0.8492 - acc: 0.6600 - val_loss: 0.8393 - val_acc: 0.6559\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83932 to 0.83299, saving model to best.model\n",
      "1s - loss: 0.8433 - acc: 0.6600 - val_loss: 0.8330 - val_acc: 0.6559\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83299 to 0.82884, saving model to best.model\n",
      "1s - loss: 0.8389 - acc: 0.6600 - val_loss: 0.8288 - val_acc: 0.6559\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82884 to 0.82595, saving model to best.model\n",
      "1s - loss: 0.8342 - acc: 0.6600 - val_loss: 0.8260 - val_acc: 0.6559\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "1s - loss: 0.8314 - acc: 0.6600 - val_loss: 0.8268 - val_acc: 0.6559\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82595 to 0.82320, saving model to best.model\n",
      "1s - loss: 0.8298 - acc: 0.6600 - val_loss: 0.8232 - val_acc: 0.6559\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82320 to 0.82229, saving model to best.model\n",
      "1s - loss: 0.8266 - acc: 0.6600 - val_loss: 0.8223 - val_acc: 0.6559\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8274 - acc: 0.6599 - val_loss: 0.8223 - val_acc: 0.6559\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82229 to 0.82126, saving model to best.model\n",
      "1s - loss: 0.8260 - acc: 0.6599 - val_loss: 0.8213 - val_acc: 0.6559\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8257 - acc: 0.6600 - val_loss: 0.8226 - val_acc: 0.6559\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8253 - acc: 0.6601 - val_loss: 0.8213 - val_acc: 0.6559\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "1s - loss: 0.8259 - acc: 0.6599 - val_loss: 0.8216 - val_acc: 0.6559\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82126 to 0.81980, saving model to best.model\n",
      "0s - loss: 0.8237 - acc: 0.6599 - val_loss: 0.8198 - val_acc: 0.6559\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81980 to 0.81920, saving model to best.model\n",
      "0s - loss: 0.8215 - acc: 0.6603 - val_loss: 0.8192 - val_acc: 0.6559\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.8220 - acc: 0.6598 - val_loss: 0.8205 - val_acc: 0.6559\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.8224 - acc: 0.6600 - val_loss: 0.8201 - val_acc: 0.6559\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.8211 - acc: 0.6600 - val_loss: 0.8203 - val_acc: 0.6559\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81920 to 0.81699, saving model to best.model\n",
      "1s - loss: 0.8208 - acc: 0.6600 - val_loss: 0.8170 - val_acc: 0.6559\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.8176 - acc: 0.6603 - val_loss: 0.8171 - val_acc: 0.6559\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81699 to 0.81506, saving model to best.model\n",
      "1s - loss: 0.8187 - acc: 0.6606 - val_loss: 0.8151 - val_acc: 0.6559\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81506 to 0.81399, saving model to best.model\n",
      "1s - loss: 0.8181 - acc: 0.6617 - val_loss: 0.8140 - val_acc: 0.6561\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81399 to 0.81300, saving model to best.model\n",
      "0s - loss: 0.8161 - acc: 0.6609 - val_loss: 0.8130 - val_acc: 0.6560\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81300 to 0.81062, saving model to best.model\n",
      "0s - loss: 0.8150 - acc: 0.6606 - val_loss: 0.8106 - val_acc: 0.6559\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.8135 - acc: 0.6621 - val_loss: 0.8111 - val_acc: 0.6562\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81062 to 0.80808, saving model to best.model\n",
      "0s - loss: 0.8134 - acc: 0.6620 - val_loss: 0.8081 - val_acc: 0.6581\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.8137 - acc: 0.6625 - val_loss: 0.8087 - val_acc: 0.6570\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80808 to 0.80541, saving model to best.model\n",
      "0s - loss: 0.8104 - acc: 0.6636 - val_loss: 0.8054 - val_acc: 0.6635\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80541 to 0.80487, saving model to best.model\n",
      "0s - loss: 0.8110 - acc: 0.6631 - val_loss: 0.8049 - val_acc: 0.6583\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80487 to 0.80318, saving model to best.model\n",
      "0s - loss: 0.8088 - acc: 0.6644 - val_loss: 0.8032 - val_acc: 0.6602\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80318 to 0.80031, saving model to best.model\n",
      "1s - loss: 0.8072 - acc: 0.6641 - val_loss: 0.8003 - val_acc: 0.6625\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80031 to 0.79941, saving model to best.model\n",
      "1s - loss: 0.8065 - acc: 0.6653 - val_loss: 0.7994 - val_acc: 0.6619\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79941 to 0.79726, saving model to best.model\n",
      "1s - loss: 0.8039 - acc: 0.6670 - val_loss: 0.7973 - val_acc: 0.6646\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79726 to 0.79488, saving model to best.model\n",
      "1s - loss: 0.8023 - acc: 0.6659 - val_loss: 0.7949 - val_acc: 0.6639\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79488 to 0.79430, saving model to best.model\n",
      "1s - loss: 0.8023 - acc: 0.6669 - val_loss: 0.7943 - val_acc: 0.6610\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79430 to 0.79094, saving model to best.model\n",
      "1s - loss: 0.7983 - acc: 0.6692 - val_loss: 0.7909 - val_acc: 0.6673\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79094 to 0.78842, saving model to best.model\n",
      "1s - loss: 0.7955 - acc: 0.6697 - val_loss: 0.7884 - val_acc: 0.6721\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78842 to 0.78530, saving model to best.model\n",
      "1s - loss: 0.7958 - acc: 0.6686 - val_loss: 0.7853 - val_acc: 0.6730\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78530 to 0.78443, saving model to best.model\n",
      "1s - loss: 0.7947 - acc: 0.6686 - val_loss: 0.7844 - val_acc: 0.6687\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78443 to 0.78352, saving model to best.model\n",
      "1s - loss: 0.7925 - acc: 0.6693 - val_loss: 0.7835 - val_acc: 0.6653\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78352 to 0.78350, saving model to best.model\n",
      "1s - loss: 0.7908 - acc: 0.6706 - val_loss: 0.7835 - val_acc: 0.6697\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78350 to 0.77767, saving model to best.model\n",
      "1s - loss: 0.7895 - acc: 0.6717 - val_loss: 0.7777 - val_acc: 0.6748\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77767 to 0.77727, saving model to best.model\n",
      "1s - loss: 0.7874 - acc: 0.6725 - val_loss: 0.7773 - val_acc: 0.6733\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77727 to 0.77392, saving model to best.model\n",
      "0s - loss: 0.7843 - acc: 0.6740 - val_loss: 0.7739 - val_acc: 0.6747\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77392 to 0.77162, saving model to best.model\n",
      "0s - loss: 0.7845 - acc: 0.6740 - val_loss: 0.7716 - val_acc: 0.6775\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77162 to 0.77100, saving model to best.model\n",
      "0s - loss: 0.7819 - acc: 0.6752 - val_loss: 0.7710 - val_acc: 0.6781\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77100 to 0.76628, saving model to best.model\n",
      "0s - loss: 0.7778 - acc: 0.6770 - val_loss: 0.7663 - val_acc: 0.6824\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76628 to 0.76267, saving model to best.model\n",
      "0s - loss: 0.7790 - acc: 0.6765 - val_loss: 0.7627 - val_acc: 0.6841\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.7766 - acc: 0.6777 - val_loss: 0.7647 - val_acc: 0.6762\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76267 to 0.76156, saving model to best.model\n",
      "0s - loss: 0.7761 - acc: 0.6786 - val_loss: 0.7616 - val_acc: 0.6841\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76156 to 0.75628, saving model to best.model\n",
      "1s - loss: 0.7744 - acc: 0.6765 - val_loss: 0.7563 - val_acc: 0.6879\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75628 to 0.75105, saving model to best.model\n",
      "1s - loss: 0.7706 - acc: 0.6801 - val_loss: 0.7510 - val_acc: 0.6899\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "1s - loss: 0.7686 - acc: 0.6820 - val_loss: 0.7513 - val_acc: 0.6892\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75105 to 0.74748, saving model to best.model\n",
      "1s - loss: 0.7697 - acc: 0.6814 - val_loss: 0.7475 - val_acc: 0.6917\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74748 to 0.74610, saving model to best.model\n",
      "1s - loss: 0.7680 - acc: 0.6807 - val_loss: 0.7461 - val_acc: 0.6922\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74610 to 0.74315, saving model to best.model\n",
      "1s - loss: 0.7652 - acc: 0.6817 - val_loss: 0.7432 - val_acc: 0.6929\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "1s - loss: 0.7634 - acc: 0.6832 - val_loss: 0.7441 - val_acc: 0.6875\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74315 to 0.73513, saving model to best.model\n",
      "1s - loss: 0.7584 - acc: 0.6851 - val_loss: 0.7351 - val_acc: 0.6978\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73513 to 0.73357, saving model to best.model\n",
      "1s - loss: 0.7567 - acc: 0.6875 - val_loss: 0.7336 - val_acc: 0.6968\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73357 to 0.72952, saving model to best.model\n",
      "1s - loss: 0.7567 - acc: 0.6877 - val_loss: 0.7295 - val_acc: 0.7016\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.72952 to 0.72924, saving model to best.model\n",
      "1s - loss: 0.7556 - acc: 0.6867 - val_loss: 0.7292 - val_acc: 0.6971\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72924 to 0.72628, saving model to best.model\n",
      "1s - loss: 0.7517 - acc: 0.6884 - val_loss: 0.7263 - val_acc: 0.7034\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72628 to 0.72550, saving model to best.model\n",
      "1s - loss: 0.7518 - acc: 0.6910 - val_loss: 0.7255 - val_acc: 0.7056\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72550 to 0.71839, saving model to best.model\n",
      "1s - loss: 0.7468 - acc: 0.6917 - val_loss: 0.7184 - val_acc: 0.7063\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.7490 - acc: 0.6915 - val_loss: 0.7198 - val_acc: 0.7076\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.7456 - acc: 0.6945 - val_loss: 0.7191 - val_acc: 0.7032\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71839 to 0.71401, saving model to best.model\n",
      "0s - loss: 0.7446 - acc: 0.6926 - val_loss: 0.7140 - val_acc: 0.7064\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71401 to 0.71158, saving model to best.model\n",
      "1s - loss: 0.7442 - acc: 0.6926 - val_loss: 0.7116 - val_acc: 0.7111\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "1s - loss: 0.7405 - acc: 0.6960 - val_loss: 0.7119 - val_acc: 0.7063\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71158 to 0.70800, saving model to best.model\n",
      "1s - loss: 0.7400 - acc: 0.6949 - val_loss: 0.7080 - val_acc: 0.7100\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7384 - acc: 0.6961 - val_loss: 0.7085 - val_acc: 0.7083\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70800 to 0.70799, saving model to best.model\n",
      "1s - loss: 0.7368 - acc: 0.6967 - val_loss: 0.7080 - val_acc: 0.7116\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70799 to 0.70212, saving model to best.model\n",
      "1s - loss: 0.7344 - acc: 0.6992 - val_loss: 0.7021 - val_acc: 0.7131\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.7352 - acc: 0.6971 - val_loss: 0.7035 - val_acc: 0.7105\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70212 to 0.69630, saving model to best.model\n",
      "0s - loss: 0.7309 - acc: 0.6983 - val_loss: 0.6963 - val_acc: 0.7167\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "1s - loss: 0.7326 - acc: 0.7006 - val_loss: 0.6965 - val_acc: 0.7186\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69630 to 0.69236, saving model to best.model\n",
      "1s - loss: 0.7297 - acc: 0.6992 - val_loss: 0.6924 - val_acc: 0.7191\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7279 - acc: 0.7015 - val_loss: 0.6935 - val_acc: 0.7183\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7277 - acc: 0.7020 - val_loss: 0.6929 - val_acc: 0.7119\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69236 to 0.68881, saving model to best.model\n",
      "1s - loss: 0.7258 - acc: 0.7030 - val_loss: 0.6888 - val_acc: 0.7159\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68881 to 0.68494, saving model to best.model\n",
      "1s - loss: 0.7215 - acc: 0.7044 - val_loss: 0.6849 - val_acc: 0.7221\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7234 - acc: 0.7059 - val_loss: 0.6864 - val_acc: 0.7192\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.68494 to 0.68298, saving model to best.model\n",
      "1s - loss: 0.7218 - acc: 0.7061 - val_loss: 0.6830 - val_acc: 0.7228\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7202 - acc: 0.7057 - val_loss: 0.6832 - val_acc: 0.7171\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68298 to 0.67751, saving model to best.model\n",
      "1s - loss: 0.7203 - acc: 0.7031 - val_loss: 0.6775 - val_acc: 0.7259\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67751 to 0.67702, saving model to best.model\n",
      "1s - loss: 0.7154 - acc: 0.7085 - val_loss: 0.6770 - val_acc: 0.7233\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.7188 - acc: 0.7076 - val_loss: 0.6802 - val_acc: 0.7200\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "1s - loss: 0.7156 - acc: 0.7074 - val_loss: 0.6780 - val_acc: 0.7260\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67702 to 0.67534, saving model to best.model\n",
      "1s - loss: 0.7126 - acc: 0.7077 - val_loss: 0.6753 - val_acc: 0.7253\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67534 to 0.67059, saving model to best.model\n",
      "1s - loss: 0.7118 - acc: 0.7093 - val_loss: 0.6706 - val_acc: 0.7296\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7115 - acc: 0.7094 - val_loss: 0.6731 - val_acc: 0.7292\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67059 to 0.67005, saving model to best.model\n",
      "1s - loss: 0.7106 - acc: 0.7092 - val_loss: 0.6701 - val_acc: 0.7261\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67005 to 0.66742, saving model to best.model\n",
      "1s - loss: 0.7116 - acc: 0.7099 - val_loss: 0.6674 - val_acc: 0.7287\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7101 - acc: 0.7105 - val_loss: 0.6702 - val_acc: 0.7242\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7054 - acc: 0.7109 - val_loss: 0.6691 - val_acc: 0.7238\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.66742 to 0.66525, saving model to best.model\n",
      "0s - loss: 0.7080 - acc: 0.7090 - val_loss: 0.6653 - val_acc: 0.7310\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.7063 - acc: 0.7097 - val_loss: 0.6691 - val_acc: 0.7246\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7064 - acc: 0.7118 - val_loss: 0.6717 - val_acc: 0.7166\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66525 to 0.65970, saving model to best.model\n",
      "1s - loss: 0.7004 - acc: 0.7145 - val_loss: 0.6597 - val_acc: 0.7320\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7026 - acc: 0.7129 - val_loss: 0.6602 - val_acc: 0.7304\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7046 - acc: 0.7128 - val_loss: 0.6612 - val_acc: 0.7294\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65970 to 0.65782, saving model to best.model\n",
      "0s - loss: 0.6994 - acc: 0.7160 - val_loss: 0.6578 - val_acc: 0.7324\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65782 to 0.65460, saving model to best.model\n",
      "0s - loss: 0.6984 - acc: 0.7138 - val_loss: 0.6546 - val_acc: 0.7334\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65460 to 0.65460, saving model to best.model\n",
      "0s - loss: 0.6973 - acc: 0.7152 - val_loss: 0.6546 - val_acc: 0.7351\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.6958 - acc: 0.7142 - val_loss: 0.6579 - val_acc: 0.7308\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65460 to 0.65455, saving model to best.model\n",
      "0s - loss: 0.6959 - acc: 0.7167 - val_loss: 0.6545 - val_acc: 0.7349\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.6975 - acc: 0.7167 - val_loss: 0.6547 - val_acc: 0.7330\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65455 to 0.65361, saving model to best.model\n",
      "1s - loss: 0.6976 - acc: 0.7145 - val_loss: 0.6536 - val_acc: 0.7322\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6960 - acc: 0.7147 - val_loss: 0.6565 - val_acc: 0.7254\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6942 - acc: 0.7186 - val_loss: 0.6547 - val_acc: 0.7276\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65361 to 0.64827, saving model to best.model\n",
      "1s - loss: 0.6920 - acc: 0.7186 - val_loss: 0.6483 - val_acc: 0.7368\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.64827 to 0.64827, saving model to best.model\n",
      "1s - loss: 0.6891 - acc: 0.7192 - val_loss: 0.6483 - val_acc: 0.7343\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6919 - acc: 0.7172 - val_loss: 0.6500 - val_acc: 0.7340\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.64827 to 0.64789, saving model to best.model\n",
      "1s - loss: 0.6919 - acc: 0.7189 - val_loss: 0.6479 - val_acc: 0.7361\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64789 to 0.64781, saving model to best.model\n",
      "1s - loss: 0.6909 - acc: 0.7164 - val_loss: 0.6478 - val_acc: 0.7356\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.64781 to 0.64778, saving model to best.model\n",
      "1s - loss: 0.6906 - acc: 0.7181 - val_loss: 0.6478 - val_acc: 0.7329\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64778 to 0.64490, saving model to best.model\n",
      "1s - loss: 0.6879 - acc: 0.7199 - val_loss: 0.6449 - val_acc: 0.7382\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64490 to 0.64288, saving model to best.model\n",
      "1s - loss: 0.6864 - acc: 0.7205 - val_loss: 0.6429 - val_acc: 0.7367\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6881 - acc: 0.7180 - val_loss: 0.6433 - val_acc: 0.7389\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6876 - acc: 0.7204 - val_loss: 0.6458 - val_acc: 0.7333\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6843 - acc: 0.7191 - val_loss: 0.6461 - val_acc: 0.7330\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64288 to 0.63825, saving model to best.model\n",
      "1s - loss: 0.6825 - acc: 0.7217 - val_loss: 0.6382 - val_acc: 0.7379\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.63825 to 0.63599, saving model to best.model\n",
      "1s - loss: 0.6848 - acc: 0.7194 - val_loss: 0.6360 - val_acc: 0.7381\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6822 - acc: 0.7231 - val_loss: 0.6410 - val_acc: 0.7349\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6837 - acc: 0.7215 - val_loss: 0.6409 - val_acc: 0.7342\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6803 - acc: 0.7227 - val_loss: 0.6379 - val_acc: 0.7381\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6790 - acc: 0.7215 - val_loss: 0.6368 - val_acc: 0.7379\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6795 - acc: 0.7220 - val_loss: 0.6367 - val_acc: 0.7389\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.63599 to 0.63404, saving model to best.model\n",
      "1s - loss: 0.6799 - acc: 0.7215 - val_loss: 0.6340 - val_acc: 0.7384\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6760 - acc: 0.7269 - val_loss: 0.6360 - val_acc: 0.7369\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63404 to 0.63220, saving model to best.model\n",
      "0s - loss: 0.6756 - acc: 0.7243 - val_loss: 0.6322 - val_acc: 0.7429\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6781 - acc: 0.7239 - val_loss: 0.6337 - val_acc: 0.7389\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6769 - acc: 0.7218 - val_loss: 0.6326 - val_acc: 0.7393\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63220 to 0.63071, saving model to best.model\n",
      "0s - loss: 0.6738 - acc: 0.7240 - val_loss: 0.6307 - val_acc: 0.7397\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63071 to 0.62880, saving model to best.model\n",
      "0s - loss: 0.6761 - acc: 0.7235 - val_loss: 0.6288 - val_acc: 0.7429\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6739 - acc: 0.7263 - val_loss: 0.6336 - val_acc: 0.7363\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6723 - acc: 0.7262 - val_loss: 0.6300 - val_acc: 0.7404\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6739 - acc: 0.7240 - val_loss: 0.6290 - val_acc: 0.7415\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62880 to 0.62855, saving model to best.model\n",
      "1s - loss: 0.6717 - acc: 0.7262 - val_loss: 0.6285 - val_acc: 0.7390\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6735 - acc: 0.7254 - val_loss: 0.6306 - val_acc: 0.7374\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6726 - acc: 0.7247 - val_loss: 0.6302 - val_acc: 0.7389\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6725 - acc: 0.7263 - val_loss: 0.6288 - val_acc: 0.7388\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6699 - acc: 0.7261 - val_loss: 0.6295 - val_acc: 0.7397\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62855 to 0.62512, saving model to best.model\n",
      "0s - loss: 0.6666 - acc: 0.7277 - val_loss: 0.6251 - val_acc: 0.7416\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6697 - acc: 0.7270 - val_loss: 0.6266 - val_acc: 0.7404\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6670 - acc: 0.7280 - val_loss: 0.6260 - val_acc: 0.7378\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.62512 to 0.61982, saving model to best.model\n",
      "1s - loss: 0.6678 - acc: 0.7266 - val_loss: 0.6198 - val_acc: 0.7453\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6702 - acc: 0.7266 - val_loss: 0.6246 - val_acc: 0.7412\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6659 - acc: 0.7264 - val_loss: 0.6232 - val_acc: 0.7456\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.61982 to 0.61958, saving model to best.model\n",
      "1s - loss: 0.6666 - acc: 0.7278 - val_loss: 0.6196 - val_acc: 0.7436\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6659 - acc: 0.7275 - val_loss: 0.6208 - val_acc: 0.7451\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6633 - acc: 0.7315 - val_loss: 0.6203 - val_acc: 0.7433\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6648 - acc: 0.7289 - val_loss: 0.6201 - val_acc: 0.7432\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.61958 to 0.61937, saving model to best.model\n",
      "1s - loss: 0.6635 - acc: 0.7270 - val_loss: 0.6194 - val_acc: 0.7430\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6630 - acc: 0.7300 - val_loss: 0.6201 - val_acc: 0.7424\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.61937 to 0.61679, saving model to best.model\n",
      "1s - loss: 0.6648 - acc: 0.7273 - val_loss: 0.6168 - val_acc: 0.7466\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61679 to 0.61561, saving model to best.model\n",
      "1s - loss: 0.6626 - acc: 0.7282 - val_loss: 0.6156 - val_acc: 0.7478\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6604 - acc: 0.7294 - val_loss: 0.6177 - val_acc: 0.7448\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6600 - acc: 0.7284 - val_loss: 0.6162 - val_acc: 0.7471\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61561 to 0.61413, saving model to best.model\n",
      "0s - loss: 0.6617 - acc: 0.7286 - val_loss: 0.6141 - val_acc: 0.7472\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6596 - acc: 0.7302 - val_loss: 0.6156 - val_acc: 0.7443\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6598 - acc: 0.7303 - val_loss: 0.6161 - val_acc: 0.7447\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6599 - acc: 0.7316 - val_loss: 0.6159 - val_acc: 0.7473\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6589 - acc: 0.7306 - val_loss: 0.6149 - val_acc: 0.7453\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6561 - acc: 0.7337 - val_loss: 0.6144 - val_acc: 0.7464\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61413 to 0.61045, saving model to best.model\n",
      "0s - loss: 0.6573 - acc: 0.7311 - val_loss: 0.6105 - val_acc: 0.7470\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6597 - acc: 0.7291 - val_loss: 0.6157 - val_acc: 0.7441\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6577 - acc: 0.7304 - val_loss: 0.6132 - val_acc: 0.7463\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61045 to 0.60907, saving model to best.model\n",
      "0s - loss: 0.6569 - acc: 0.7325 - val_loss: 0.6091 - val_acc: 0.7479\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6599 - acc: 0.7283 - val_loss: 0.6109 - val_acc: 0.7470\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.60907 to 0.60801, saving model to best.model\n",
      "0s - loss: 0.6578 - acc: 0.7309 - val_loss: 0.6080 - val_acc: 0.7487\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6550 - acc: 0.7301 - val_loss: 0.6116 - val_acc: 0.7456\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.60801 to 0.60622, saving model to best.model\n",
      "0s - loss: 0.6521 - acc: 0.7333 - val_loss: 0.6062 - val_acc: 0.7501\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6525 - acc: 0.7325 - val_loss: 0.6103 - val_acc: 0.7468\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6524 - acc: 0.7334 - val_loss: 0.6081 - val_acc: 0.7485\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6522 - acc: 0.7318 - val_loss: 0.6087 - val_acc: 0.7498\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6543 - acc: 0.7346 - val_loss: 0.6073 - val_acc: 0.7500\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.60622 to 0.60473, saving model to best.model\n",
      "1s - loss: 0.6509 - acc: 0.7337 - val_loss: 0.6047 - val_acc: 0.7502\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6517 - acc: 0.7331 - val_loss: 0.6053 - val_acc: 0.7493\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6526 - acc: 0.7327 - val_loss: 0.6076 - val_acc: 0.7460\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60473 to 0.60365, saving model to best.model\n",
      "1s - loss: 0.6504 - acc: 0.7331 - val_loss: 0.6036 - val_acc: 0.7516\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6486 - acc: 0.7353 - val_loss: 0.6049 - val_acc: 0.7506\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6538 - acc: 0.7354 - val_loss: 0.6041 - val_acc: 0.7491\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6485 - acc: 0.7337 - val_loss: 0.6048 - val_acc: 0.7515\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60365 to 0.60205, saving model to best.model\n",
      "1s - loss: 0.6519 - acc: 0.7340 - val_loss: 0.6021 - val_acc: 0.7521\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60205 to 0.60182, saving model to best.model\n",
      "1s - loss: 0.6488 - acc: 0.7330 - val_loss: 0.6018 - val_acc: 0.7518\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6487 - acc: 0.7355 - val_loss: 0.6031 - val_acc: 0.7519\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6519 - acc: 0.7330 - val_loss: 0.6042 - val_acc: 0.7500\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6525 - acc: 0.7330 - val_loss: 0.6037 - val_acc: 0.7512\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.60182 to 0.60089, saving model to best.model\n",
      "0s - loss: 0.6473 - acc: 0.7361 - val_loss: 0.6009 - val_acc: 0.7519\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60089 to 0.60019, saving model to best.model\n",
      "0s - loss: 0.6468 - acc: 0.7356 - val_loss: 0.6002 - val_acc: 0.7522\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6471 - acc: 0.7363 - val_loss: 0.6010 - val_acc: 0.7521\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6478 - acc: 0.7359 - val_loss: 0.6011 - val_acc: 0.7502\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.60019 to 0.59973, saving model to best.model\n",
      "0s - loss: 0.6472 - acc: 0.7345 - val_loss: 0.5997 - val_acc: 0.7548\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.59973 to 0.59903, saving model to best.model\n",
      "0s - loss: 0.6434 - acc: 0.7352 - val_loss: 0.5990 - val_acc: 0.7523\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.59903 to 0.59821, saving model to best.model\n",
      "0s - loss: 0.6445 - acc: 0.7383 - val_loss: 0.5982 - val_acc: 0.7521\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.59821 to 0.59813, saving model to best.model\n",
      "0s - loss: 0.6425 - acc: 0.7375 - val_loss: 0.5981 - val_acc: 0.7525\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6462 - acc: 0.7352 - val_loss: 0.5992 - val_acc: 0.7501\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84538, saving model to best.model\n",
      "0s - loss: 0.9394 - acc: 0.6165 - val_loss: 0.8454 - val_acc: 0.6587\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84538 to 0.84428, saving model to best.model\n",
      "1s - loss: 0.8642 - acc: 0.6562 - val_loss: 0.8443 - val_acc: 0.6587\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84428 to 0.84391, saving model to best.model\n",
      "1s - loss: 0.8567 - acc: 0.6573 - val_loss: 0.8439 - val_acc: 0.6587\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84391 to 0.84229, saving model to best.model\n",
      "1s - loss: 0.8531 - acc: 0.6573 - val_loss: 0.8423 - val_acc: 0.6587\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84229 to 0.83756, saving model to best.model\n",
      "1s - loss: 0.8496 - acc: 0.6573 - val_loss: 0.8376 - val_acc: 0.6587\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83756 to 0.83145, saving model to best.model\n",
      "1s - loss: 0.8456 - acc: 0.6573 - val_loss: 0.8314 - val_acc: 0.6587\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83145 to 0.82572, saving model to best.model\n",
      "1s - loss: 0.8416 - acc: 0.6573 - val_loss: 0.8257 - val_acc: 0.6587\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82572 to 0.82273, saving model to best.model\n",
      "1s - loss: 0.8367 - acc: 0.6574 - val_loss: 0.8227 - val_acc: 0.6587\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "1s - loss: 0.8340 - acc: 0.6573 - val_loss: 0.8228 - val_acc: 0.6587\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82273 to 0.81971, saving model to best.model\n",
      "1s - loss: 0.8342 - acc: 0.6573 - val_loss: 0.8197 - val_acc: 0.6587\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81971 to 0.81913, saving model to best.model\n",
      "1s - loss: 0.8310 - acc: 0.6573 - val_loss: 0.8191 - val_acc: 0.6587\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81913 to 0.81742, saving model to best.model\n",
      "1s - loss: 0.8317 - acc: 0.6574 - val_loss: 0.8174 - val_acc: 0.6587\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81742 to 0.81711, saving model to best.model\n",
      "1s - loss: 0.8301 - acc: 0.6574 - val_loss: 0.8171 - val_acc: 0.6587\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8295 - acc: 0.6573 - val_loss: 0.8175 - val_acc: 0.6587\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81711 to 0.81580, saving model to best.model\n",
      "1s - loss: 0.8293 - acc: 0.6575 - val_loss: 0.8158 - val_acc: 0.6587\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81580 to 0.81459, saving model to best.model\n",
      "1s - loss: 0.8275 - acc: 0.6574 - val_loss: 0.8146 - val_acc: 0.6587\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81459 to 0.81338, saving model to best.model\n",
      "1s - loss: 0.8259 - acc: 0.6572 - val_loss: 0.8134 - val_acc: 0.6587\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "1s - loss: 0.8253 - acc: 0.6570 - val_loss: 0.8143 - val_acc: 0.6587\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81338 to 0.81269, saving model to best.model\n",
      "0s - loss: 0.8251 - acc: 0.6573 - val_loss: 0.8127 - val_acc: 0.6587\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81269 to 0.81252, saving model to best.model\n",
      "0s - loss: 0.8239 - acc: 0.6568 - val_loss: 0.8125 - val_acc: 0.6587\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.8225 - acc: 0.6577 - val_loss: 0.8138 - val_acc: 0.6588\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81252 to 0.80894, saving model to best.model\n",
      "0s - loss: 0.8224 - acc: 0.6579 - val_loss: 0.8089 - val_acc: 0.6587\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.80894 to 0.80861, saving model to best.model\n",
      "0s - loss: 0.8216 - acc: 0.6584 - val_loss: 0.8086 - val_acc: 0.6587\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.8193 - acc: 0.6588 - val_loss: 0.8087 - val_acc: 0.6616\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80861 to 0.80720, saving model to best.model\n",
      "0s - loss: 0.8195 - acc: 0.6597 - val_loss: 0.8072 - val_acc: 0.6603\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80720 to 0.80466, saving model to best.model\n",
      "0s - loss: 0.8181 - acc: 0.6599 - val_loss: 0.8047 - val_acc: 0.6623\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80466 to 0.80363, saving model to best.model\n",
      "0s - loss: 0.8176 - acc: 0.6609 - val_loss: 0.8036 - val_acc: 0.6632\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.8161 - acc: 0.6611 - val_loss: 0.8039 - val_acc: 0.6623\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80363 to 0.80235, saving model to best.model\n",
      "1s - loss: 0.8152 - acc: 0.6604 - val_loss: 0.8023 - val_acc: 0.6655\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80235 to 0.80035, saving model to best.model\n",
      "1s - loss: 0.8130 - acc: 0.6607 - val_loss: 0.8003 - val_acc: 0.6662\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80035 to 0.80023, saving model to best.model\n",
      "1s - loss: 0.8140 - acc: 0.6617 - val_loss: 0.8002 - val_acc: 0.6660\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80023 to 0.79824, saving model to best.model\n",
      "1s - loss: 0.8114 - acc: 0.6616 - val_loss: 0.7982 - val_acc: 0.6696\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79824 to 0.79564, saving model to best.model\n",
      "1s - loss: 0.8104 - acc: 0.6634 - val_loss: 0.7956 - val_acc: 0.6706\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79564 to 0.79497, saving model to best.model\n",
      "1s - loss: 0.8071 - acc: 0.6652 - val_loss: 0.7950 - val_acc: 0.6719\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79497 to 0.79347, saving model to best.model\n",
      "1s - loss: 0.8081 - acc: 0.6642 - val_loss: 0.7935 - val_acc: 0.6696\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79347 to 0.79228, saving model to best.model\n",
      "1s - loss: 0.8076 - acc: 0.6645 - val_loss: 0.7923 - val_acc: 0.6717\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79228 to 0.78972, saving model to best.model\n",
      "1s - loss: 0.8060 - acc: 0.6660 - val_loss: 0.7897 - val_acc: 0.6713\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78972 to 0.78736, saving model to best.model\n",
      "1s - loss: 0.8043 - acc: 0.6654 - val_loss: 0.7874 - val_acc: 0.6714\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78736 to 0.78423, saving model to best.model\n",
      "1s - loss: 0.7991 - acc: 0.6674 - val_loss: 0.7842 - val_acc: 0.6753\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "1s - loss: 0.8008 - acc: 0.6657 - val_loss: 0.7843 - val_acc: 0.6715\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78423 to 0.78074, saving model to best.model\n",
      "1s - loss: 0.7982 - acc: 0.6687 - val_loss: 0.7807 - val_acc: 0.6733\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78074 to 0.77981, saving model to best.model\n",
      "1s - loss: 0.7971 - acc: 0.6673 - val_loss: 0.7798 - val_acc: 0.6741\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77981 to 0.77769, saving model to best.model\n",
      "1s - loss: 0.7952 - acc: 0.6702 - val_loss: 0.7777 - val_acc: 0.6766\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77769 to 0.77433, saving model to best.model\n",
      "1s - loss: 0.7926 - acc: 0.6690 - val_loss: 0.7743 - val_acc: 0.6786\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77433 to 0.77234, saving model to best.model\n",
      "1s - loss: 0.7909 - acc: 0.6685 - val_loss: 0.7723 - val_acc: 0.6806\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77234 to 0.77229, saving model to best.model\n",
      "1s - loss: 0.7899 - acc: 0.6699 - val_loss: 0.7723 - val_acc: 0.6795\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77229 to 0.76945, saving model to best.model\n",
      "1s - loss: 0.7879 - acc: 0.6723 - val_loss: 0.7695 - val_acc: 0.6792\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76945 to 0.76611, saving model to best.model\n",
      "1s - loss: 0.7881 - acc: 0.6712 - val_loss: 0.7661 - val_acc: 0.6836\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76611 to 0.76411, saving model to best.model\n",
      "1s - loss: 0.7859 - acc: 0.6735 - val_loss: 0.7641 - val_acc: 0.6868\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76411 to 0.76092, saving model to best.model\n",
      "1s - loss: 0.7847 - acc: 0.6720 - val_loss: 0.7609 - val_acc: 0.6840\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76092 to 0.75881, saving model to best.model\n",
      "1s - loss: 0.7811 - acc: 0.6742 - val_loss: 0.7588 - val_acc: 0.6844\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75881 to 0.75658, saving model to best.model\n",
      "1s - loss: 0.7807 - acc: 0.6750 - val_loss: 0.7566 - val_acc: 0.6856\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75658 to 0.75399, saving model to best.model\n",
      "1s - loss: 0.7782 - acc: 0.6755 - val_loss: 0.7540 - val_acc: 0.6852\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75399 to 0.75243, saving model to best.model\n",
      "1s - loss: 0.7771 - acc: 0.6768 - val_loss: 0.7524 - val_acc: 0.6869\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75243 to 0.75012, saving model to best.model\n",
      "1s - loss: 0.7740 - acc: 0.6781 - val_loss: 0.7501 - val_acc: 0.6868\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75012 to 0.74868, saving model to best.model\n",
      "1s - loss: 0.7717 - acc: 0.6780 - val_loss: 0.7487 - val_acc: 0.6859\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74868 to 0.74355, saving model to best.model\n",
      "1s - loss: 0.7695 - acc: 0.6796 - val_loss: 0.7435 - val_acc: 0.6899\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74355 to 0.74243, saving model to best.model\n",
      "1s - loss: 0.7683 - acc: 0.6805 - val_loss: 0.7424 - val_acc: 0.6888\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74243 to 0.74181, saving model to best.model\n",
      "1s - loss: 0.7703 - acc: 0.6784 - val_loss: 0.7418 - val_acc: 0.6878\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74181 to 0.73723, saving model to best.model\n",
      "1s - loss: 0.7666 - acc: 0.6812 - val_loss: 0.7372 - val_acc: 0.6946\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73723 to 0.73477, saving model to best.model\n",
      "1s - loss: 0.7637 - acc: 0.6834 - val_loss: 0.7348 - val_acc: 0.6958\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73477 to 0.73247, saving model to best.model\n",
      "1s - loss: 0.7635 - acc: 0.6834 - val_loss: 0.7325 - val_acc: 0.6965\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73247 to 0.73180, saving model to best.model\n",
      "1s - loss: 0.7626 - acc: 0.6829 - val_loss: 0.7318 - val_acc: 0.6948\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73180 to 0.72647, saving model to best.model\n",
      "1s - loss: 0.7594 - acc: 0.6850 - val_loss: 0.7265 - val_acc: 0.7040\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72647 to 0.72487, saving model to best.model\n",
      "1s - loss: 0.7576 - acc: 0.6877 - val_loss: 0.7249 - val_acc: 0.7053\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72487 to 0.71903, saving model to best.model\n",
      "1s - loss: 0.7556 - acc: 0.6860 - val_loss: 0.7190 - val_acc: 0.7074\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.7535 - acc: 0.6881 - val_loss: 0.7208 - val_acc: 0.7040\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "1s - loss: 0.7538 - acc: 0.6852 - val_loss: 0.7196 - val_acc: 0.7020\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71903 to 0.71369, saving model to best.model\n",
      "1s - loss: 0.7493 - acc: 0.6901 - val_loss: 0.7137 - val_acc: 0.7067\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "1s - loss: 0.7493 - acc: 0.6906 - val_loss: 0.7140 - val_acc: 0.7009\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71369 to 0.70760, saving model to best.model\n",
      "1s - loss: 0.7442 - acc: 0.6910 - val_loss: 0.7076 - val_acc: 0.7084\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70760 to 0.70358, saving model to best.model\n",
      "1s - loss: 0.7420 - acc: 0.6900 - val_loss: 0.7036 - val_acc: 0.7111\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7437 - acc: 0.6914 - val_loss: 0.7037 - val_acc: 0.7105\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.7393 - acc: 0.6945 - val_loss: 0.7050 - val_acc: 0.7064\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70358 to 0.70160, saving model to best.model\n",
      "1s - loss: 0.7407 - acc: 0.6930 - val_loss: 0.7016 - val_acc: 0.7111\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.7393 - acc: 0.6929 - val_loss: 0.7041 - val_acc: 0.7093\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70160 to 0.69942, saving model to best.model\n",
      "0s - loss: 0.7379 - acc: 0.6955 - val_loss: 0.6994 - val_acc: 0.7105\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69942 to 0.69274, saving model to best.model\n",
      "0s - loss: 0.7364 - acc: 0.6946 - val_loss: 0.6927 - val_acc: 0.7173\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7341 - acc: 0.6948 - val_loss: 0.6932 - val_acc: 0.7138\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69274 to 0.69259, saving model to best.model\n",
      "1s - loss: 0.7351 - acc: 0.6957 - val_loss: 0.6926 - val_acc: 0.7162\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69259 to 0.68605, saving model to best.model\n",
      "1s - loss: 0.7335 - acc: 0.6949 - val_loss: 0.6861 - val_acc: 0.7173\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7320 - acc: 0.6968 - val_loss: 0.6904 - val_acc: 0.7142\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68605 to 0.68515, saving model to best.model\n",
      "1s - loss: 0.7294 - acc: 0.6984 - val_loss: 0.6851 - val_acc: 0.7156\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68515 to 0.68378, saving model to best.model\n",
      "1s - loss: 0.7288 - acc: 0.6999 - val_loss: 0.6838 - val_acc: 0.7181\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.68378 to 0.68292, saving model to best.model\n",
      "1s - loss: 0.7284 - acc: 0.6997 - val_loss: 0.6829 - val_acc: 0.7186\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68292 to 0.68217, saving model to best.model\n",
      "1s - loss: 0.7255 - acc: 0.6997 - val_loss: 0.6822 - val_acc: 0.7163\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68217 to 0.67935, saving model to best.model\n",
      "1s - loss: 0.7269 - acc: 0.6996 - val_loss: 0.6794 - val_acc: 0.7200\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7203 - acc: 0.7019 - val_loss: 0.6801 - val_acc: 0.7193\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67935 to 0.67417, saving model to best.model\n",
      "1s - loss: 0.7217 - acc: 0.7035 - val_loss: 0.6742 - val_acc: 0.7215\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67417 to 0.67368, saving model to best.model\n",
      "1s - loss: 0.7224 - acc: 0.7029 - val_loss: 0.6737 - val_acc: 0.7215\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67368 to 0.67146, saving model to best.model\n",
      "1s - loss: 0.7201 - acc: 0.7032 - val_loss: 0.6715 - val_acc: 0.7288\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67146 to 0.66962, saving model to best.model\n",
      "1s - loss: 0.7173 - acc: 0.7042 - val_loss: 0.6696 - val_acc: 0.7247\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66962 to 0.66936, saving model to best.model\n",
      "1s - loss: 0.7168 - acc: 0.7054 - val_loss: 0.6694 - val_acc: 0.7225\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7172 - acc: 0.7037 - val_loss: 0.6698 - val_acc: 0.7245\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66936 to 0.66892, saving model to best.model\n",
      "1s - loss: 0.7159 - acc: 0.7030 - val_loss: 0.6689 - val_acc: 0.7249\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.66892 to 0.66870, saving model to best.model\n",
      "1s - loss: 0.7121 - acc: 0.7054 - val_loss: 0.6687 - val_acc: 0.7232\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66870 to 0.66625, saving model to best.model\n",
      "1s - loss: 0.7126 - acc: 0.7063 - val_loss: 0.6662 - val_acc: 0.7275\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.66625 to 0.66194, saving model to best.model\n",
      "1s - loss: 0.7097 - acc: 0.7091 - val_loss: 0.6619 - val_acc: 0.7270\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7103 - acc: 0.7085 - val_loss: 0.6637 - val_acc: 0.7287\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66194 to 0.65811, saving model to best.model\n",
      "1s - loss: 0.7083 - acc: 0.7081 - val_loss: 0.6581 - val_acc: 0.7282\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7087 - acc: 0.7102 - val_loss: 0.6622 - val_acc: 0.7251\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7091 - acc: 0.7072 - val_loss: 0.6599 - val_acc: 0.7255\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7092 - acc: 0.7077 - val_loss: 0.6588 - val_acc: 0.7293\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65811 to 0.65639, saving model to best.model\n",
      "1s - loss: 0.7065 - acc: 0.7107 - val_loss: 0.6564 - val_acc: 0.7297\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65639 to 0.65622, saving model to best.model\n",
      "0s - loss: 0.7067 - acc: 0.7088 - val_loss: 0.6562 - val_acc: 0.7301\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7049 - acc: 0.7100 - val_loss: 0.6578 - val_acc: 0.7263\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65622 to 0.65615, saving model to best.model\n",
      "0s - loss: 0.7046 - acc: 0.7101 - val_loss: 0.6561 - val_acc: 0.7287\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65615 to 0.65146, saving model to best.model\n",
      "0s - loss: 0.7031 - acc: 0.7096 - val_loss: 0.6515 - val_acc: 0.7314\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65146 to 0.65028, saving model to best.model\n",
      "1s - loss: 0.7016 - acc: 0.7124 - val_loss: 0.6503 - val_acc: 0.7313\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65028 to 0.64873, saving model to best.model\n",
      "1s - loss: 0.6971 - acc: 0.7135 - val_loss: 0.6487 - val_acc: 0.7309\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.7008 - acc: 0.7152 - val_loss: 0.6528 - val_acc: 0.7289\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "0s - loss: 0.6997 - acc: 0.7128 - val_loss: 0.6511 - val_acc: 0.7297\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6977 - acc: 0.7116 - val_loss: 0.6488 - val_acc: 0.7320\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6978 - acc: 0.7132 - val_loss: 0.6496 - val_acc: 0.7315\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.64873 to 0.64596, saving model to best.model\n",
      "1s - loss: 0.6943 - acc: 0.7160 - val_loss: 0.6460 - val_acc: 0.7326\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6973 - acc: 0.7134 - val_loss: 0.6486 - val_acc: 0.7311\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64596 to 0.64242, saving model to best.model\n",
      "1s - loss: 0.6959 - acc: 0.7138 - val_loss: 0.6424 - val_acc: 0.7368\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6937 - acc: 0.7159 - val_loss: 0.6464 - val_acc: 0.7322\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6960 - acc: 0.7152 - val_loss: 0.6455 - val_acc: 0.7310\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.6919 - acc: 0.7145 - val_loss: 0.6433 - val_acc: 0.7348\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64242 to 0.63863, saving model to best.model\n",
      "0s - loss: 0.6867 - acc: 0.7194 - val_loss: 0.6386 - val_acc: 0.7368\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6946 - acc: 0.7135 - val_loss: 0.6418 - val_acc: 0.7355\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.63863 to 0.63763, saving model to best.model\n",
      "0s - loss: 0.6913 - acc: 0.7172 - val_loss: 0.6376 - val_acc: 0.7369\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6926 - acc: 0.7155 - val_loss: 0.6405 - val_acc: 0.7336\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6904 - acc: 0.7168 - val_loss: 0.6391 - val_acc: 0.7358\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.6864 - acc: 0.7164 - val_loss: 0.6394 - val_acc: 0.7337\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.6888 - acc: 0.7185 - val_loss: 0.6377 - val_acc: 0.7348\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6860 - acc: 0.7177 - val_loss: 0.6378 - val_acc: 0.7351\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.63763 to 0.63415, saving model to best.model\n",
      "1s - loss: 0.6840 - acc: 0.7200 - val_loss: 0.6341 - val_acc: 0.7367\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6843 - acc: 0.7210 - val_loss: 0.6374 - val_acc: 0.7415\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.63415 to 0.63354, saving model to best.model\n",
      "1s - loss: 0.6843 - acc: 0.7191 - val_loss: 0.6335 - val_acc: 0.7426\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6857 - acc: 0.7199 - val_loss: 0.6368 - val_acc: 0.7374\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6863 - acc: 0.7176 - val_loss: 0.6367 - val_acc: 0.7361\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63354 to 0.63264, saving model to best.model\n",
      "1s - loss: 0.6817 - acc: 0.7204 - val_loss: 0.6326 - val_acc: 0.7385\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6812 - acc: 0.7186 - val_loss: 0.6340 - val_acc: 0.7365\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63264 to 0.63062, saving model to best.model\n",
      "0s - loss: 0.6808 - acc: 0.7217 - val_loss: 0.6306 - val_acc: 0.7395\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63062 to 0.63033, saving model to best.model\n",
      "0s - loss: 0.6796 - acc: 0.7222 - val_loss: 0.6303 - val_acc: 0.7397\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63033 to 0.62828, saving model to best.model\n",
      "0s - loss: 0.6790 - acc: 0.7209 - val_loss: 0.6283 - val_acc: 0.7384\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62828 to 0.62573, saving model to best.model\n",
      "1s - loss: 0.6792 - acc: 0.7222 - val_loss: 0.6257 - val_acc: 0.7434\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6798 - acc: 0.7220 - val_loss: 0.6308 - val_acc: 0.7392\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6778 - acc: 0.7213 - val_loss: 0.6258 - val_acc: 0.7405\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6784 - acc: 0.7217 - val_loss: 0.6264 - val_acc: 0.7446\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.62573 to 0.62498, saving model to best.model\n",
      "1s - loss: 0.6737 - acc: 0.7230 - val_loss: 0.6250 - val_acc: 0.7412\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6730 - acc: 0.7227 - val_loss: 0.6259 - val_acc: 0.7438\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6760 - acc: 0.7228 - val_loss: 0.6281 - val_acc: 0.7406\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6764 - acc: 0.7224 - val_loss: 0.6257 - val_acc: 0.7405\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.62498 to 0.62166, saving model to best.model\n",
      "1s - loss: 0.6720 - acc: 0.7238 - val_loss: 0.6217 - val_acc: 0.7437\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62166 to 0.62030, saving model to best.model\n",
      "1s - loss: 0.6758 - acc: 0.7218 - val_loss: 0.6203 - val_acc: 0.7456\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6744 - acc: 0.7222 - val_loss: 0.6216 - val_acc: 0.7440\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6729 - acc: 0.7227 - val_loss: 0.6218 - val_acc: 0.7422\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62030 to 0.61883, saving model to best.model\n",
      "1s - loss: 0.6724 - acc: 0.7266 - val_loss: 0.6188 - val_acc: 0.7448\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6728 - acc: 0.7239 - val_loss: 0.6209 - val_acc: 0.7432\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6725 - acc: 0.7257 - val_loss: 0.6201 - val_acc: 0.7437\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.61883 to 0.61804, saving model to best.model\n",
      "1s - loss: 0.6674 - acc: 0.7258 - val_loss: 0.6180 - val_acc: 0.7454\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.61804 to 0.61794, saving model to best.model\n",
      "1s - loss: 0.6687 - acc: 0.7248 - val_loss: 0.6179 - val_acc: 0.7451\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6721 - acc: 0.7244 - val_loss: 0.6205 - val_acc: 0.7471\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61794 to 0.61715, saving model to best.model\n",
      "1s - loss: 0.6671 - acc: 0.7261 - val_loss: 0.6171 - val_acc: 0.7439\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6680 - acc: 0.7262 - val_loss: 0.6197 - val_acc: 0.7464\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61715 to 0.61314, saving model to best.model\n",
      "1s - loss: 0.6640 - acc: 0.7268 - val_loss: 0.6131 - val_acc: 0.7499\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6675 - acc: 0.7277 - val_loss: 0.6190 - val_acc: 0.7451\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6647 - acc: 0.7258 - val_loss: 0.6157 - val_acc: 0.7458\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6669 - acc: 0.7256 - val_loss: 0.6153 - val_acc: 0.7464\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6657 - acc: 0.7254 - val_loss: 0.6163 - val_acc: 0.7448\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6626 - acc: 0.7300 - val_loss: 0.6132 - val_acc: 0.7472\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6601 - acc: 0.7304 - val_loss: 0.6141 - val_acc: 0.7474\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61314 to 0.61305, saving model to best.model\n",
      "1s - loss: 0.6646 - acc: 0.7287 - val_loss: 0.6131 - val_acc: 0.7487\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6649 - acc: 0.7278 - val_loss: 0.6131 - val_acc: 0.7481\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61305 to 0.60899, saving model to best.model\n",
      "1s - loss: 0.6620 - acc: 0.7294 - val_loss: 0.6090 - val_acc: 0.7512\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6649 - acc: 0.7293 - val_loss: 0.6127 - val_acc: 0.7468\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6588 - acc: 0.7301 - val_loss: 0.6100 - val_acc: 0.7473\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.60899 to 0.60781, saving model to best.model\n",
      "1s - loss: 0.6619 - acc: 0.7294 - val_loss: 0.6078 - val_acc: 0.7512\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6613 - acc: 0.7280 - val_loss: 0.6082 - val_acc: 0.7516\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6605 - acc: 0.7301 - val_loss: 0.6090 - val_acc: 0.7533\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6620 - acc: 0.7291 - val_loss: 0.6102 - val_acc: 0.7482\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6606 - acc: 0.7290 - val_loss: 0.6143 - val_acc: 0.7432\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.60781 to 0.60622, saving model to best.model\n",
      "1s - loss: 0.6570 - acc: 0.7288 - val_loss: 0.6062 - val_acc: 0.7535\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6594 - acc: 0.7295 - val_loss: 0.6091 - val_acc: 0.7486\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6599 - acc: 0.7305 - val_loss: 0.6068 - val_acc: 0.7513\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6597 - acc: 0.7310 - val_loss: 0.6092 - val_acc: 0.7495\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6583 - acc: 0.7289 - val_loss: 0.6084 - val_acc: 0.7496\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7305 - val_loss: 0.6081 - val_acc: 0.7489\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60622 to 0.60529, saving model to best.model\n",
      "1s - loss: 0.6558 - acc: 0.7312 - val_loss: 0.6053 - val_acc: 0.7520\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6572 - acc: 0.7318 - val_loss: 0.6066 - val_acc: 0.7505\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6551 - acc: 0.7337 - val_loss: 0.6059 - val_acc: 0.7507\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60529 to 0.60427, saving model to best.model\n",
      "1s - loss: 0.6573 - acc: 0.7301 - val_loss: 0.6043 - val_acc: 0.7535\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.60427 to 0.60318, saving model to best.model\n",
      "1s - loss: 0.6554 - acc: 0.7309 - val_loss: 0.6032 - val_acc: 0.7513\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60318 to 0.59908, saving model to best.model\n",
      "1s - loss: 0.6520 - acc: 0.7328 - val_loss: 0.5991 - val_acc: 0.7570\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6524 - acc: 0.7329 - val_loss: 0.6020 - val_acc: 0.7521\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6560 - acc: 0.7286 - val_loss: 0.6035 - val_acc: 0.7518\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6522 - acc: 0.7343 - val_loss: 0.6040 - val_acc: 0.7514\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6505 - acc: 0.7345 - val_loss: 0.5994 - val_acc: 0.7563\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.59908 to 0.59883, saving model to best.model\n",
      "0s - loss: 0.6523 - acc: 0.7307 - val_loss: 0.5988 - val_acc: 0.7569\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6512 - acc: 0.7338 - val_loss: 0.5992 - val_acc: 0.7589\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.59883 to 0.59798, saving model to best.model\n",
      "0s - loss: 0.6499 - acc: 0.7318 - val_loss: 0.5980 - val_acc: 0.7566\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6491 - acc: 0.7333 - val_loss: 0.5995 - val_acc: 0.7535\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.59798 to 0.59614, saving model to best.model\n",
      "0s - loss: 0.6526 - acc: 0.7333 - val_loss: 0.5961 - val_acc: 0.7581\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6518 - acc: 0.7343 - val_loss: 0.5983 - val_acc: 0.7574\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.6505 - acc: 0.7350 - val_loss: 0.5975 - val_acc: 0.7574\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.6507 - acc: 0.7330 - val_loss: 0.5971 - val_acc: 0.7581\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6483 - acc: 0.7365 - val_loss: 0.5986 - val_acc: 0.7564\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83580, saving model to best.model\n",
      "0s - loss: 0.9107 - acc: 0.6308 - val_loss: 0.8358 - val_acc: 0.6616\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.83580 to 0.83561, saving model to best.model\n",
      "0s - loss: 0.8582 - acc: 0.6594 - val_loss: 0.8356 - val_acc: 0.6616\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.83561 to 0.83492, saving model to best.model\n",
      "0s - loss: 0.8502 - acc: 0.6600 - val_loss: 0.8349 - val_acc: 0.6616\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83492 to 0.83112, saving model to best.model\n",
      "0s - loss: 0.8450 - acc: 0.6600 - val_loss: 0.8311 - val_acc: 0.6616\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83112 to 0.82130, saving model to best.model\n",
      "0s - loss: 0.8395 - acc: 0.6600 - val_loss: 0.8213 - val_acc: 0.6616\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82130 to 0.81871, saving model to best.model\n",
      "0s - loss: 0.8331 - acc: 0.6599 - val_loss: 0.8187 - val_acc: 0.6616\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.81871 to 0.81768, saving model to best.model\n",
      "1s - loss: 0.8309 - acc: 0.6599 - val_loss: 0.8177 - val_acc: 0.6616\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81768 to 0.81570, saving model to best.model\n",
      "1s - loss: 0.8287 - acc: 0.6599 - val_loss: 0.8157 - val_acc: 0.6616\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81570 to 0.81485, saving model to best.model\n",
      "1s - loss: 0.8283 - acc: 0.6600 - val_loss: 0.8148 - val_acc: 0.6616\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81485 to 0.81455, saving model to best.model\n",
      "1s - loss: 0.8261 - acc: 0.6598 - val_loss: 0.8145 - val_acc: 0.6616\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81455 to 0.81395, saving model to best.model\n",
      "1s - loss: 0.8247 - acc: 0.6599 - val_loss: 0.8140 - val_acc: 0.6616\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81395 to 0.81287, saving model to best.model\n",
      "1s - loss: 0.8253 - acc: 0.6598 - val_loss: 0.8129 - val_acc: 0.6616\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81287 to 0.81279, saving model to best.model\n",
      "1s - loss: 0.8242 - acc: 0.6599 - val_loss: 0.8128 - val_acc: 0.6616\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8229 - acc: 0.6597 - val_loss: 0.8131 - val_acc: 0.6616\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81279 to 0.81187, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6599 - val_loss: 0.8119 - val_acc: 0.6616\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81187 to 0.81130, saving model to best.model\n",
      "1s - loss: 0.8226 - acc: 0.6599 - val_loss: 0.8113 - val_acc: 0.6616\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "1s - loss: 0.8201 - acc: 0.6596 - val_loss: 0.8120 - val_acc: 0.6616\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81130 to 0.81063, saving model to best.model\n",
      "1s - loss: 0.8192 - acc: 0.6599 - val_loss: 0.8106 - val_acc: 0.6616\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81063 to 0.81020, saving model to best.model\n",
      "0s - loss: 0.8184 - acc: 0.6600 - val_loss: 0.8102 - val_acc: 0.6616\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81020 to 0.81017, saving model to best.model\n",
      "0s - loss: 0.8186 - acc: 0.6606 - val_loss: 0.8102 - val_acc: 0.6616\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81017 to 0.80808, saving model to best.model\n",
      "0s - loss: 0.8164 - acc: 0.6611 - val_loss: 0.8081 - val_acc: 0.6631\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.8163 - acc: 0.6605 - val_loss: 0.8084 - val_acc: 0.6635\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.80808 to 0.80757, saving model to best.model\n",
      "0s - loss: 0.8150 - acc: 0.6612 - val_loss: 0.8076 - val_acc: 0.6638\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80757 to 0.80694, saving model to best.model\n",
      "0s - loss: 0.8147 - acc: 0.6614 - val_loss: 0.8069 - val_acc: 0.6644\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80694 to 0.80508, saving model to best.model\n",
      "0s - loss: 0.8137 - acc: 0.6617 - val_loss: 0.8051 - val_acc: 0.6650\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80508 to 0.80466, saving model to best.model\n",
      "0s - loss: 0.8110 - acc: 0.6630 - val_loss: 0.8047 - val_acc: 0.6650\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80466 to 0.80295, saving model to best.model\n",
      "0s - loss: 0.8103 - acc: 0.6625 - val_loss: 0.8029 - val_acc: 0.6639\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80295 to 0.80249, saving model to best.model\n",
      "0s - loss: 0.8098 - acc: 0.6633 - val_loss: 0.8025 - val_acc: 0.6644\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80249 to 0.80118, saving model to best.model\n",
      "0s - loss: 0.8091 - acc: 0.6637 - val_loss: 0.8012 - val_acc: 0.6639\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80118 to 0.79950, saving model to best.model\n",
      "1s - loss: 0.8063 - acc: 0.6647 - val_loss: 0.7995 - val_acc: 0.6649\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.79950 to 0.79848, saving model to best.model\n",
      "1s - loss: 0.8054 - acc: 0.6651 - val_loss: 0.7985 - val_acc: 0.6651\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79848 to 0.79766, saving model to best.model\n",
      "1s - loss: 0.8039 - acc: 0.6663 - val_loss: 0.7977 - val_acc: 0.6659\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79766 to 0.79545, saving model to best.model\n",
      "1s - loss: 0.8032 - acc: 0.6654 - val_loss: 0.7955 - val_acc: 0.6662\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79545 to 0.79225, saving model to best.model\n",
      "1s - loss: 0.8009 - acc: 0.6673 - val_loss: 0.7922 - val_acc: 0.6674\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79225 to 0.79012, saving model to best.model\n",
      "1s - loss: 0.7984 - acc: 0.6693 - val_loss: 0.7901 - val_acc: 0.6692\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79012 to 0.78794, saving model to best.model\n",
      "1s - loss: 0.7969 - acc: 0.6691 - val_loss: 0.7879 - val_acc: 0.6693\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78794 to 0.78590, saving model to best.model\n",
      "1s - loss: 0.7953 - acc: 0.6700 - val_loss: 0.7859 - val_acc: 0.6699\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78590 to 0.78451, saving model to best.model\n",
      "1s - loss: 0.7949 - acc: 0.6695 - val_loss: 0.7845 - val_acc: 0.6707\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.7933 - acc: 0.6709 - val_loss: 0.7846 - val_acc: 0.6699\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78451 to 0.78002, saving model to best.model\n",
      "1s - loss: 0.7909 - acc: 0.6715 - val_loss: 0.7800 - val_acc: 0.6720\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78002 to 0.77675, saving model to best.model\n",
      "1s - loss: 0.7886 - acc: 0.6731 - val_loss: 0.7768 - val_acc: 0.6740\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77675 to 0.77553, saving model to best.model\n",
      "1s - loss: 0.7878 - acc: 0.6712 - val_loss: 0.7755 - val_acc: 0.6747\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77553 to 0.77391, saving model to best.model\n",
      "1s - loss: 0.7878 - acc: 0.6714 - val_loss: 0.7739 - val_acc: 0.6744\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77391 to 0.77032, saving model to best.model\n",
      "1s - loss: 0.7843 - acc: 0.6752 - val_loss: 0.7703 - val_acc: 0.6765\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77032 to 0.76823, saving model to best.model\n",
      "1s - loss: 0.7834 - acc: 0.6748 - val_loss: 0.7682 - val_acc: 0.6770\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.76823 to 0.76618, saving model to best.model\n",
      "1s - loss: 0.7812 - acc: 0.6766 - val_loss: 0.7662 - val_acc: 0.6793\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76618 to 0.76333, saving model to best.model\n",
      "1s - loss: 0.7784 - acc: 0.6781 - val_loss: 0.7633 - val_acc: 0.6800\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76333 to 0.76218, saving model to best.model\n",
      "1s - loss: 0.7802 - acc: 0.6750 - val_loss: 0.7622 - val_acc: 0.6844\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76218 to 0.76126, saving model to best.model\n",
      "1s - loss: 0.7779 - acc: 0.6782 - val_loss: 0.7613 - val_acc: 0.6813\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76126 to 0.75895, saving model to best.model\n",
      "1s - loss: 0.7725 - acc: 0.6801 - val_loss: 0.7590 - val_acc: 0.6862\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75895 to 0.75320, saving model to best.model\n",
      "1s - loss: 0.7714 - acc: 0.6805 - val_loss: 0.7532 - val_acc: 0.6876\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75320 to 0.74967, saving model to best.model\n",
      "1s - loss: 0.7695 - acc: 0.6808 - val_loss: 0.7497 - val_acc: 0.6878\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.74967 to 0.74823, saving model to best.model\n",
      "1s - loss: 0.7676 - acc: 0.6815 - val_loss: 0.7482 - val_acc: 0.6898\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74823 to 0.74611, saving model to best.model\n",
      "1s - loss: 0.7669 - acc: 0.6830 - val_loss: 0.7461 - val_acc: 0.6897\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74611 to 0.74394, saving model to best.model\n",
      "1s - loss: 0.7663 - acc: 0.6842 - val_loss: 0.7439 - val_acc: 0.6930\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74394 to 0.73869, saving model to best.model\n",
      "1s - loss: 0.7593 - acc: 0.6871 - val_loss: 0.7387 - val_acc: 0.6960\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.73869 to 0.73698, saving model to best.model\n",
      "1s - loss: 0.7579 - acc: 0.6895 - val_loss: 0.7370 - val_acc: 0.6947\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.73698 to 0.73583, saving model to best.model\n",
      "1s - loss: 0.7611 - acc: 0.6879 - val_loss: 0.7358 - val_acc: 0.6957\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73583 to 0.73033, saving model to best.model\n",
      "1s - loss: 0.7566 - acc: 0.6908 - val_loss: 0.7303 - val_acc: 0.7009\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "1s - loss: 0.7539 - acc: 0.6905 - val_loss: 0.7317 - val_acc: 0.7001\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73033 to 0.72801, saving model to best.model\n",
      "1s - loss: 0.7517 - acc: 0.6923 - val_loss: 0.7280 - val_acc: 0.6992\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72801 to 0.72397, saving model to best.model\n",
      "1s - loss: 0.7509 - acc: 0.6907 - val_loss: 0.7240 - val_acc: 0.7032\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.72397 to 0.72223, saving model to best.model\n",
      "1s - loss: 0.7483 - acc: 0.6954 - val_loss: 0.7222 - val_acc: 0.7042\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72223 to 0.71943, saving model to best.model\n",
      "1s - loss: 0.7461 - acc: 0.6960 - val_loss: 0.7194 - val_acc: 0.7043\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.71943 to 0.71639, saving model to best.model\n",
      "1s - loss: 0.7438 - acc: 0.6949 - val_loss: 0.7164 - val_acc: 0.7059\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71639 to 0.71539, saving model to best.model\n",
      "1s - loss: 0.7423 - acc: 0.6956 - val_loss: 0.7154 - val_acc: 0.7073\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.71539 to 0.71108, saving model to best.model\n",
      "1s - loss: 0.7399 - acc: 0.6968 - val_loss: 0.7111 - val_acc: 0.7078\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "1s - loss: 0.7374 - acc: 0.6986 - val_loss: 0.7119 - val_acc: 0.7061\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71108 to 0.70932, saving model to best.model\n",
      "1s - loss: 0.7359 - acc: 0.6974 - val_loss: 0.7093 - val_acc: 0.7105\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70932 to 0.70599, saving model to best.model\n",
      "1s - loss: 0.7350 - acc: 0.7000 - val_loss: 0.7060 - val_acc: 0.7097\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70599 to 0.69798, saving model to best.model\n",
      "1s - loss: 0.7326 - acc: 0.7030 - val_loss: 0.6980 - val_acc: 0.7135\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.7291 - acc: 0.7027 - val_loss: 0.6991 - val_acc: 0.7133\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.7304 - acc: 0.7025 - val_loss: 0.6989 - val_acc: 0.7133\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.69798 to 0.69565, saving model to best.model\n",
      "0s - loss: 0.7297 - acc: 0.7034 - val_loss: 0.6956 - val_acc: 0.7160\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69565 to 0.69504, saving model to best.model\n",
      "0s - loss: 0.7267 - acc: 0.7037 - val_loss: 0.6950 - val_acc: 0.7143\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69504 to 0.69381, saving model to best.model\n",
      "0s - loss: 0.7263 - acc: 0.7056 - val_loss: 0.6938 - val_acc: 0.7159\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69381 to 0.69221, saving model to best.model\n",
      "0s - loss: 0.7239 - acc: 0.7048 - val_loss: 0.6922 - val_acc: 0.7171\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69221 to 0.69125, saving model to best.model\n",
      "0s - loss: 0.7213 - acc: 0.7065 - val_loss: 0.6912 - val_acc: 0.7186\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69125 to 0.68912, saving model to best.model\n",
      "0s - loss: 0.7200 - acc: 0.7080 - val_loss: 0.6891 - val_acc: 0.7156\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.68912 to 0.68314, saving model to best.model\n",
      "1s - loss: 0.7198 - acc: 0.7097 - val_loss: 0.6831 - val_acc: 0.7203\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7225 - acc: 0.7039 - val_loss: 0.6844 - val_acc: 0.7211\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.68314 to 0.68173, saving model to best.model\n",
      "1s - loss: 0.7169 - acc: 0.7111 - val_loss: 0.6817 - val_acc: 0.7211\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68173 to 0.67699, saving model to best.model\n",
      "1s - loss: 0.7154 - acc: 0.7087 - val_loss: 0.6770 - val_acc: 0.7215\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67699 to 0.67623, saving model to best.model\n",
      "1s - loss: 0.7131 - acc: 0.7119 - val_loss: 0.6762 - val_acc: 0.7221\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.67623 to 0.67541, saving model to best.model\n",
      "1s - loss: 0.7126 - acc: 0.7130 - val_loss: 0.6754 - val_acc: 0.7247\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67541 to 0.67367, saving model to best.model\n",
      "1s - loss: 0.7116 - acc: 0.7119 - val_loss: 0.6737 - val_acc: 0.7220\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "1s - loss: 0.7118 - acc: 0.7120 - val_loss: 0.6744 - val_acc: 0.7220\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67367 to 0.67252, saving model to best.model\n",
      "1s - loss: 0.7089 - acc: 0.7147 - val_loss: 0.6725 - val_acc: 0.7239\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67252 to 0.67126, saving model to best.model\n",
      "1s - loss: 0.7082 - acc: 0.7116 - val_loss: 0.6713 - val_acc: 0.7256\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67126 to 0.66961, saving model to best.model\n",
      "1s - loss: 0.7071 - acc: 0.7131 - val_loss: 0.6696 - val_acc: 0.7278\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7081 - acc: 0.7135 - val_loss: 0.6702 - val_acc: 0.7249\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66961 to 0.66586, saving model to best.model\n",
      "1s - loss: 0.7040 - acc: 0.7149 - val_loss: 0.6659 - val_acc: 0.7268\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7056 - acc: 0.7126 - val_loss: 0.6660 - val_acc: 0.7265\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66586 to 0.66346, saving model to best.model\n",
      "1s - loss: 0.7008 - acc: 0.7159 - val_loss: 0.6635 - val_acc: 0.7285\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7041 - acc: 0.7145 - val_loss: 0.6647 - val_acc: 0.7279\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.66346 to 0.66210, saving model to best.model\n",
      "1s - loss: 0.7016 - acc: 0.7173 - val_loss: 0.6621 - val_acc: 0.7273\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66210 to 0.65930, saving model to best.model\n",
      "1s - loss: 0.7002 - acc: 0.7143 - val_loss: 0.6593 - val_acc: 0.7309\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.6974 - acc: 0.7191 - val_loss: 0.6618 - val_acc: 0.7279\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.65930 to 0.65853, saving model to best.model\n",
      "1s - loss: 0.6961 - acc: 0.7180 - val_loss: 0.6585 - val_acc: 0.7316\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.65853 to 0.65620, saving model to best.model\n",
      "1s - loss: 0.6968 - acc: 0.7196 - val_loss: 0.6562 - val_acc: 0.7315\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.65620 to 0.65616, saving model to best.model\n",
      "1s - loss: 0.6938 - acc: 0.7203 - val_loss: 0.6562 - val_acc: 0.7313\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.6944 - acc: 0.7174 - val_loss: 0.6584 - val_acc: 0.7269\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.65616 to 0.65280, saving model to best.model\n",
      "1s - loss: 0.6942 - acc: 0.7184 - val_loss: 0.6528 - val_acc: 0.7341\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65280 to 0.65280, saving model to best.model\n",
      "0s - loss: 0.6957 - acc: 0.7190 - val_loss: 0.6528 - val_acc: 0.7343\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65280 to 0.65083, saving model to best.model\n",
      "0s - loss: 0.6915 - acc: 0.7199 - val_loss: 0.6508 - val_acc: 0.7331\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.6957 - acc: 0.7171 - val_loss: 0.6554 - val_acc: 0.7301\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.6914 - acc: 0.7207 - val_loss: 0.6521 - val_acc: 0.7337\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65083 to 0.64988, saving model to best.model\n",
      "1s - loss: 0.6920 - acc: 0.7207 - val_loss: 0.6499 - val_acc: 0.7354\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64988 to 0.64917, saving model to best.model\n",
      "1s - loss: 0.6896 - acc: 0.7210 - val_loss: 0.6492 - val_acc: 0.7370\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.64917 to 0.64827, saving model to best.model\n",
      "1s - loss: 0.6902 - acc: 0.7215 - val_loss: 0.6483 - val_acc: 0.7371\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.64827 to 0.64797, saving model to best.model\n",
      "1s - loss: 0.6904 - acc: 0.7193 - val_loss: 0.6480 - val_acc: 0.7334\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6853 - acc: 0.7234 - val_loss: 0.6484 - val_acc: 0.7333\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.64797 to 0.64584, saving model to best.model\n",
      "1s - loss: 0.6875 - acc: 0.7217 - val_loss: 0.6458 - val_acc: 0.7369\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.64584 to 0.64274, saving model to best.model\n",
      "1s - loss: 0.6882 - acc: 0.7210 - val_loss: 0.6427 - val_acc: 0.7390\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6858 - acc: 0.7219 - val_loss: 0.6463 - val_acc: 0.7343\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6856 - acc: 0.7222 - val_loss: 0.6433 - val_acc: 0.7375\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64274 to 0.63938, saving model to best.model\n",
      "1s - loss: 0.6804 - acc: 0.7241 - val_loss: 0.6394 - val_acc: 0.7397\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6818 - acc: 0.7239 - val_loss: 0.6444 - val_acc: 0.7425\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6823 - acc: 0.7267 - val_loss: 0.6456 - val_acc: 0.7321\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6831 - acc: 0.7235 - val_loss: 0.6408 - val_acc: 0.7410\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.63938 to 0.63876, saving model to best.model\n",
      "1s - loss: 0.6817 - acc: 0.7273 - val_loss: 0.6388 - val_acc: 0.7395\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.63876 to 0.63800, saving model to best.model\n",
      "1s - loss: 0.6831 - acc: 0.7259 - val_loss: 0.6380 - val_acc: 0.7391\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.63800 to 0.63561, saving model to best.model\n",
      "1s - loss: 0.6798 - acc: 0.7241 - val_loss: 0.6356 - val_acc: 0.7409\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.63561 to 0.63527, saving model to best.model\n",
      "1s - loss: 0.6801 - acc: 0.7249 - val_loss: 0.6353 - val_acc: 0.7432\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6780 - acc: 0.7280 - val_loss: 0.6356 - val_acc: 0.7420\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63527 to 0.63464, saving model to best.model\n",
      "1s - loss: 0.6769 - acc: 0.7273 - val_loss: 0.6346 - val_acc: 0.7416\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6788 - acc: 0.7266 - val_loss: 0.6368 - val_acc: 0.7419\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6764 - acc: 0.7274 - val_loss: 0.6359 - val_acc: 0.7419\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6782 - acc: 0.7257 - val_loss: 0.6355 - val_acc: 0.7402\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63464 to 0.63029, saving model to best.model\n",
      "1s - loss: 0.6732 - acc: 0.7262 - val_loss: 0.6303 - val_acc: 0.7432\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6780 - acc: 0.7253 - val_loss: 0.6325 - val_acc: 0.7407\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6743 - acc: 0.7281 - val_loss: 0.6332 - val_acc: 0.7399\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6731 - acc: 0.7278 - val_loss: 0.6328 - val_acc: 0.7419\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63029 to 0.63024, saving model to best.model\n",
      "1s - loss: 0.6737 - acc: 0.7277 - val_loss: 0.6302 - val_acc: 0.7440\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.63024 to 0.62841, saving model to best.model\n",
      "0s - loss: 0.6699 - acc: 0.7309 - val_loss: 0.6284 - val_acc: 0.7438\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.6734 - acc: 0.7299 - val_loss: 0.6300 - val_acc: 0.7425\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6696 - acc: 0.7326 - val_loss: 0.6300 - val_acc: 0.7411\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6731 - acc: 0.7293 - val_loss: 0.6298 - val_acc: 0.7403\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6714 - acc: 0.7301 - val_loss: 0.6303 - val_acc: 0.7412\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6695 - acc: 0.7301 - val_loss: 0.6301 - val_acc: 0.7402\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62841 to 0.62551, saving model to best.model\n",
      "0s - loss: 0.6698 - acc: 0.7310 - val_loss: 0.6255 - val_acc: 0.7454\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6694 - acc: 0.7315 - val_loss: 0.6281 - val_acc: 0.7436\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6654 - acc: 0.7321 - val_loss: 0.6263 - val_acc: 0.7450\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62551 to 0.62431, saving model to best.model\n",
      "0s - loss: 0.6677 - acc: 0.7307 - val_loss: 0.6243 - val_acc: 0.7454\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6696 - acc: 0.7292 - val_loss: 0.6243 - val_acc: 0.7448\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6654 - acc: 0.7323 - val_loss: 0.6260 - val_acc: 0.7444\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.62431 to 0.62408, saving model to best.model\n",
      "0s - loss: 0.6651 - acc: 0.7323 - val_loss: 0.6241 - val_acc: 0.7450\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62408 to 0.62340, saving model to best.model\n",
      "0s - loss: 0.6652 - acc: 0.7320 - val_loss: 0.6234 - val_acc: 0.7467\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6645 - acc: 0.7324 - val_loss: 0.6241 - val_acc: 0.7447\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62340 to 0.62119, saving model to best.model\n",
      "0s - loss: 0.6637 - acc: 0.7324 - val_loss: 0.6212 - val_acc: 0.7446\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62119 to 0.62096, saving model to best.model\n",
      "0s - loss: 0.6625 - acc: 0.7332 - val_loss: 0.6210 - val_acc: 0.7464\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62096 to 0.62089, saving model to best.model\n",
      "0s - loss: 0.6638 - acc: 0.7316 - val_loss: 0.6209 - val_acc: 0.7467\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62089 to 0.62008, saving model to best.model\n",
      "1s - loss: 0.6636 - acc: 0.7318 - val_loss: 0.6201 - val_acc: 0.7446\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62008 to 0.61891, saving model to best.model\n",
      "1s - loss: 0.6612 - acc: 0.7346 - val_loss: 0.6189 - val_acc: 0.7465\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.61891 to 0.61801, saving model to best.model\n",
      "1s - loss: 0.6627 - acc: 0.7316 - val_loss: 0.6180 - val_acc: 0.7480\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6619 - acc: 0.7345 - val_loss: 0.6182 - val_acc: 0.7487\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61801 to 0.61795, saving model to best.model\n",
      "1s - loss: 0.6617 - acc: 0.7319 - val_loss: 0.6179 - val_acc: 0.7486\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.61795 to 0.61433, saving model to best.model\n",
      "1s - loss: 0.6591 - acc: 0.7340 - val_loss: 0.6143 - val_acc: 0.7529\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6608 - acc: 0.7354 - val_loss: 0.6171 - val_acc: 0.7514\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6600 - acc: 0.7329 - val_loss: 0.6163 - val_acc: 0.7480\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6587 - acc: 0.7347 - val_loss: 0.6167 - val_acc: 0.7514\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6607 - acc: 0.7340 - val_loss: 0.6172 - val_acc: 0.7444\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.61433 to 0.61394, saving model to best.model\n",
      "1s - loss: 0.6572 - acc: 0.7344 - val_loss: 0.6139 - val_acc: 0.7506\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61394 to 0.61383, saving model to best.model\n",
      "1s - loss: 0.6595 - acc: 0.7336 - val_loss: 0.6138 - val_acc: 0.7523\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.61383 to 0.61188, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7340 - val_loss: 0.6119 - val_acc: 0.7543\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6578 - acc: 0.7343 - val_loss: 0.6131 - val_acc: 0.7488\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6551 - acc: 0.7349 - val_loss: 0.6126 - val_acc: 0.7513\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7346 - val_loss: 0.6133 - val_acc: 0.7511\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6541 - acc: 0.7366 - val_loss: 0.6137 - val_acc: 0.7523\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61188 to 0.61051, saving model to best.model\n",
      "1s - loss: 0.6536 - acc: 0.7359 - val_loss: 0.6105 - val_acc: 0.7548\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61051 to 0.60905, saving model to best.model\n",
      "1s - loss: 0.6544 - acc: 0.7363 - val_loss: 0.6090 - val_acc: 0.7533\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6523 - acc: 0.7370 - val_loss: 0.6122 - val_acc: 0.7485\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.60905 to 0.60860, saving model to best.model\n",
      "1s - loss: 0.6566 - acc: 0.7354 - val_loss: 0.6086 - val_acc: 0.7512\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.60860 to 0.60804, saving model to best.model\n",
      "1s - loss: 0.6526 - acc: 0.7374 - val_loss: 0.6080 - val_acc: 0.7529\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.60804 to 0.60737, saving model to best.model\n",
      "1s - loss: 0.6523 - acc: 0.7379 - val_loss: 0.6074 - val_acc: 0.7521\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6516 - acc: 0.7374 - val_loss: 0.6093 - val_acc: 0.7522\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6511 - acc: 0.7378 - val_loss: 0.6092 - val_acc: 0.7533\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6532 - acc: 0.7351 - val_loss: 0.6077 - val_acc: 0.7521\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6517 - acc: 0.7372 - val_loss: 0.6075 - val_acc: 0.7528\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.60737 to 0.60441, saving model to best.model\n",
      "1s - loss: 0.6471 - acc: 0.7410 - val_loss: 0.6044 - val_acc: 0.7542\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6511 - acc: 0.7374 - val_loss: 0.6053 - val_acc: 0.7543\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6496 - acc: 0.7388 - val_loss: 0.6060 - val_acc: 0.7535\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60441 to 0.60437, saving model to best.model\n",
      "1s - loss: 0.6510 - acc: 0.7369 - val_loss: 0.6044 - val_acc: 0.7542\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6522 - acc: 0.7364 - val_loss: 0.6050 - val_acc: 0.7516\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60437 to 0.60372, saving model to best.model\n",
      "1s - loss: 0.6473 - acc: 0.7393 - val_loss: 0.6037 - val_acc: 0.7539\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.60372 to 0.60256, saving model to best.model\n",
      "1s - loss: 0.6499 - acc: 0.7382 - val_loss: 0.6026 - val_acc: 0.7571\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6462 - acc: 0.7395 - val_loss: 0.6041 - val_acc: 0.7520\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60256 to 0.60120, saving model to best.model\n",
      "1s - loss: 0.6475 - acc: 0.7390 - val_loss: 0.6012 - val_acc: 0.7544\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6456 - acc: 0.7420 - val_loss: 0.6017 - val_acc: 0.7556\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6489 - acc: 0.7396 - val_loss: 0.6016 - val_acc: 0.7594\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60120 to 0.60008, saving model to best.model\n",
      "1s - loss: 0.6495 - acc: 0.7384 - val_loss: 0.6001 - val_acc: 0.7591\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6451 - acc: 0.7424 - val_loss: 0.6031 - val_acc: 0.7519\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6448 - acc: 0.7401 - val_loss: 0.6021 - val_acc: 0.7532\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60008 to 0.59952, saving model to best.model\n",
      "1s - loss: 0.6435 - acc: 0.7419 - val_loss: 0.5995 - val_acc: 0.7577\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6443 - acc: 0.7408 - val_loss: 0.6001 - val_acc: 0.7577\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.59952 to 0.59702, saving model to best.model\n",
      "1s - loss: 0.6431 - acc: 0.7417 - val_loss: 0.5970 - val_acc: 0.7590\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6411 - acc: 0.7422 - val_loss: 0.5999 - val_acc: 0.7556\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6428 - acc: 0.7417 - val_loss: 0.5982 - val_acc: 0.7595\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6446 - acc: 0.7401 - val_loss: 0.5987 - val_acc: 0.7571\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6433 - acc: 0.7405 - val_loss: 0.5988 - val_acc: 0.7575\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83432, saving model to best.model\n",
      "1s - loss: 0.9005 - acc: 0.6361 - val_loss: 0.8343 - val_acc: 0.6623\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8570 - acc: 0.6590 - val_loss: 0.8364 - val_acc: 0.6623\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.83432 to 0.83204, saving model to best.model\n",
      "1s - loss: 0.8497 - acc: 0.6592 - val_loss: 0.8320 - val_acc: 0.6623\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83204 to 0.82785, saving model to best.model\n",
      "1s - loss: 0.8459 - acc: 0.6592 - val_loss: 0.8278 - val_acc: 0.6623\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.82785 to 0.82403, saving model to best.model\n",
      "1s - loss: 0.8415 - acc: 0.6592 - val_loss: 0.8240 - val_acc: 0.6623\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82403 to 0.81746, saving model to best.model\n",
      "1s - loss: 0.8361 - acc: 0.6592 - val_loss: 0.8175 - val_acc: 0.6623\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.81746 to 0.81642, saving model to best.model\n",
      "1s - loss: 0.8327 - acc: 0.6592 - val_loss: 0.8164 - val_acc: 0.6623\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81642 to 0.81543, saving model to best.model\n",
      "1s - loss: 0.8317 - acc: 0.6592 - val_loss: 0.8154 - val_acc: 0.6623\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "1s - loss: 0.8298 - acc: 0.6592 - val_loss: 0.8186 - val_acc: 0.6623\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81543 to 0.81376, saving model to best.model\n",
      "1s - loss: 0.8291 - acc: 0.6592 - val_loss: 0.8138 - val_acc: 0.6623\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8286 - acc: 0.6592 - val_loss: 0.8139 - val_acc: 0.6623\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81376 to 0.81312, saving model to best.model\n",
      "1s - loss: 0.8272 - acc: 0.6592 - val_loss: 0.8131 - val_acc: 0.6623\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8265 - acc: 0.6591 - val_loss: 0.8132 - val_acc: 0.6623\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81312 to 0.81272, saving model to best.model\n",
      "1s - loss: 0.8263 - acc: 0.6592 - val_loss: 0.8127 - val_acc: 0.6623\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81272 to 0.81242, saving model to best.model\n",
      "1s - loss: 0.8242 - acc: 0.6592 - val_loss: 0.8124 - val_acc: 0.6623\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81242 to 0.81232, saving model to best.model\n",
      "1s - loss: 0.8254 - acc: 0.6592 - val_loss: 0.8123 - val_acc: 0.6623\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81232 to 0.81195, saving model to best.model\n",
      "1s - loss: 0.8244 - acc: 0.6592 - val_loss: 0.8120 - val_acc: 0.6623\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81195 to 0.81018, saving model to best.model\n",
      "1s - loss: 0.8228 - acc: 0.6591 - val_loss: 0.8102 - val_acc: 0.6623\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "1s - loss: 0.8217 - acc: 0.6593 - val_loss: 0.8103 - val_acc: 0.6623\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81018 to 0.80890, saving model to best.model\n",
      "1s - loss: 0.8217 - acc: 0.6590 - val_loss: 0.8089 - val_acc: 0.6623\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.80890 to 0.80783, saving model to best.model\n",
      "1s - loss: 0.8202 - acc: 0.6592 - val_loss: 0.8078 - val_acc: 0.6623\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.80783 to 0.80752, saving model to best.model\n",
      "1s - loss: 0.8202 - acc: 0.6592 - val_loss: 0.8075 - val_acc: 0.6623\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.80752 to 0.80694, saving model to best.model\n",
      "1s - loss: 0.8203 - acc: 0.6590 - val_loss: 0.8069 - val_acc: 0.6623\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80694 to 0.80638, saving model to best.model\n",
      "1s - loss: 0.8184 - acc: 0.6597 - val_loss: 0.8064 - val_acc: 0.6623\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80638 to 0.80485, saving model to best.model\n",
      "1s - loss: 0.8173 - acc: 0.6593 - val_loss: 0.8048 - val_acc: 0.6621\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80485 to 0.80344, saving model to best.model\n",
      "1s - loss: 0.8171 - acc: 0.6605 - val_loss: 0.8034 - val_acc: 0.6624\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80344 to 0.80226, saving model to best.model\n",
      "1s - loss: 0.8151 - acc: 0.6613 - val_loss: 0.8023 - val_acc: 0.6635\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80226 to 0.80175, saving model to best.model\n",
      "1s - loss: 0.8136 - acc: 0.6626 - val_loss: 0.8017 - val_acc: 0.6637\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.8129 - acc: 0.6623 - val_loss: 0.8019 - val_acc: 0.6632\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80175 to 0.80041, saving model to best.model\n",
      "1s - loss: 0.8120 - acc: 0.6623 - val_loss: 0.8004 - val_acc: 0.6681\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80041 to 0.79768, saving model to best.model\n",
      "1s - loss: 0.8107 - acc: 0.6635 - val_loss: 0.7977 - val_acc: 0.6662\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79768 to 0.79706, saving model to best.model\n",
      "1s - loss: 0.8097 - acc: 0.6634 - val_loss: 0.7971 - val_acc: 0.6691\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79706 to 0.79470, saving model to best.model\n",
      "0s - loss: 0.8083 - acc: 0.6630 - val_loss: 0.7947 - val_acc: 0.6694\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79470 to 0.79381, saving model to best.model\n",
      "0s - loss: 0.8077 - acc: 0.6642 - val_loss: 0.7938 - val_acc: 0.6666\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79381 to 0.79305, saving model to best.model\n",
      "0s - loss: 0.8061 - acc: 0.6655 - val_loss: 0.7931 - val_acc: 0.6666\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79305 to 0.79066, saving model to best.model\n",
      "0s - loss: 0.8058 - acc: 0.6656 - val_loss: 0.7907 - val_acc: 0.6685\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79066 to 0.78901, saving model to best.model\n",
      "0s - loss: 0.8035 - acc: 0.6643 - val_loss: 0.7890 - val_acc: 0.6683\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78901 to 0.78648, saving model to best.model\n",
      "0s - loss: 0.8020 - acc: 0.6651 - val_loss: 0.7865 - val_acc: 0.6698\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78648 to 0.78327, saving model to best.model\n",
      "0s - loss: 0.8004 - acc: 0.6668 - val_loss: 0.7833 - val_acc: 0.6753\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78327 to 0.78252, saving model to best.model\n",
      "0s - loss: 0.7995 - acc: 0.6665 - val_loss: 0.7825 - val_acc: 0.6738\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78252 to 0.78115, saving model to best.model\n",
      "0s - loss: 0.7977 - acc: 0.6686 - val_loss: 0.7811 - val_acc: 0.6707\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78115 to 0.77763, saving model to best.model\n",
      "1s - loss: 0.7958 - acc: 0.6686 - val_loss: 0.7776 - val_acc: 0.6751\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77763 to 0.77503, saving model to best.model\n",
      "1s - loss: 0.7917 - acc: 0.6705 - val_loss: 0.7750 - val_acc: 0.6790\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77503 to 0.77213, saving model to best.model\n",
      "1s - loss: 0.7919 - acc: 0.6704 - val_loss: 0.7721 - val_acc: 0.6783\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77213 to 0.77021, saving model to best.model\n",
      "1s - loss: 0.7898 - acc: 0.6706 - val_loss: 0.7702 - val_acc: 0.6807\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77021 to 0.77011, saving model to best.model\n",
      "1s - loss: 0.7895 - acc: 0.6695 - val_loss: 0.7701 - val_acc: 0.6793\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77011 to 0.76509, saving model to best.model\n",
      "1s - loss: 0.7873 - acc: 0.6725 - val_loss: 0.7651 - val_acc: 0.6807\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76509 to 0.76269, saving model to best.model\n",
      "0s - loss: 0.7845 - acc: 0.6739 - val_loss: 0.7627 - val_acc: 0.6828\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76269 to 0.76103, saving model to best.model\n",
      "1s - loss: 0.7821 - acc: 0.6745 - val_loss: 0.7610 - val_acc: 0.6808\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76103 to 0.75801, saving model to best.model\n",
      "1s - loss: 0.7796 - acc: 0.6748 - val_loss: 0.7580 - val_acc: 0.6817\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75801 to 0.75434, saving model to best.model\n",
      "1s - loss: 0.7760 - acc: 0.6759 - val_loss: 0.7543 - val_acc: 0.6843\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75434 to 0.75380, saving model to best.model\n",
      "1s - loss: 0.7771 - acc: 0.6757 - val_loss: 0.7538 - val_acc: 0.6828\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75380 to 0.74727, saving model to best.model\n",
      "1s - loss: 0.7736 - acc: 0.6777 - val_loss: 0.7473 - val_acc: 0.6900\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.74727 to 0.74399, saving model to best.model\n",
      "1s - loss: 0.7725 - acc: 0.6793 - val_loss: 0.7440 - val_acc: 0.6918\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74399 to 0.74271, saving model to best.model\n",
      "1s - loss: 0.7700 - acc: 0.6813 - val_loss: 0.7427 - val_acc: 0.6879\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74271 to 0.74268, saving model to best.model\n",
      "1s - loss: 0.7719 - acc: 0.6814 - val_loss: 0.7427 - val_acc: 0.6875\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74268 to 0.74091, saving model to best.model\n",
      "1s - loss: 0.7668 - acc: 0.6821 - val_loss: 0.7409 - val_acc: 0.6855\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74091 to 0.73407, saving model to best.model\n",
      "1s - loss: 0.7629 - acc: 0.6832 - val_loss: 0.7341 - val_acc: 0.6947\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "1s - loss: 0.7631 - acc: 0.6836 - val_loss: 0.7346 - val_acc: 0.6915\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73407 to 0.73260, saving model to best.model\n",
      "1s - loss: 0.7620 - acc: 0.6856 - val_loss: 0.7326 - val_acc: 0.6938\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73260 to 0.72950, saving model to best.model\n",
      "1s - loss: 0.7583 - acc: 0.6853 - val_loss: 0.7295 - val_acc: 0.6965\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72950 to 0.72517, saving model to best.model\n",
      "1s - loss: 0.7547 - acc: 0.6845 - val_loss: 0.7252 - val_acc: 0.6970\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "1s - loss: 0.7581 - acc: 0.6867 - val_loss: 0.7255 - val_acc: 0.7022\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72517 to 0.72115, saving model to best.model\n",
      "1s - loss: 0.7526 - acc: 0.6895 - val_loss: 0.7211 - val_acc: 0.6993\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72115 to 0.71717, saving model to best.model\n",
      "1s - loss: 0.7491 - acc: 0.6891 - val_loss: 0.7172 - val_acc: 0.7026\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71717 to 0.71534, saving model to best.model\n",
      "1s - loss: 0.7516 - acc: 0.6866 - val_loss: 0.7153 - val_acc: 0.7006\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.71534 to 0.71172, saving model to best.model\n",
      "1s - loss: 0.7478 - acc: 0.6905 - val_loss: 0.7117 - val_acc: 0.7055\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71172 to 0.70818, saving model to best.model\n",
      "1s - loss: 0.7439 - acc: 0.6926 - val_loss: 0.7082 - val_acc: 0.7085\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "1s - loss: 0.7433 - acc: 0.6920 - val_loss: 0.7098 - val_acc: 0.7061\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70818 to 0.70385, saving model to best.model\n",
      "1s - loss: 0.7415 - acc: 0.6933 - val_loss: 0.7038 - val_acc: 0.7109\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss did not improve\n",
      "1s - loss: 0.7397 - acc: 0.6959 - val_loss: 0.7054 - val_acc: 0.7041\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70385 to 0.70045, saving model to best.model\n",
      "1s - loss: 0.7380 - acc: 0.6951 - val_loss: 0.7005 - val_acc: 0.7132\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70045 to 0.69916, saving model to best.model\n",
      "1s - loss: 0.7353 - acc: 0.6967 - val_loss: 0.6992 - val_acc: 0.7129\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.69916 to 0.69751, saving model to best.model\n",
      "1s - loss: 0.7348 - acc: 0.6956 - val_loss: 0.6975 - val_acc: 0.7104\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69751 to 0.69610, saving model to best.model\n",
      "1s - loss: 0.7350 - acc: 0.6947 - val_loss: 0.6961 - val_acc: 0.7109\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69610 to 0.69349, saving model to best.model\n",
      "1s - loss: 0.7308 - acc: 0.6989 - val_loss: 0.6935 - val_acc: 0.7126\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69349 to 0.69174, saving model to best.model\n",
      "1s - loss: 0.7298 - acc: 0.6991 - val_loss: 0.6917 - val_acc: 0.7170\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69174 to 0.69148, saving model to best.model\n",
      "1s - loss: 0.7298 - acc: 0.6999 - val_loss: 0.6915 - val_acc: 0.7131\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69148 to 0.68929, saving model to best.model\n",
      "1s - loss: 0.7295 - acc: 0.6993 - val_loss: 0.6893 - val_acc: 0.7130\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.68929 to 0.68809, saving model to best.model\n",
      "1s - loss: 0.7269 - acc: 0.6999 - val_loss: 0.6881 - val_acc: 0.7158\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68809 to 0.68654, saving model to best.model\n",
      "1s - loss: 0.7228 - acc: 0.7015 - val_loss: 0.6865 - val_acc: 0.7187\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.68654 to 0.67966, saving model to best.model\n",
      "1s - loss: 0.7196 - acc: 0.7034 - val_loss: 0.6797 - val_acc: 0.7221\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "1s - loss: 0.7247 - acc: 0.6990 - val_loss: 0.6825 - val_acc: 0.7198\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7212 - acc: 0.7002 - val_loss: 0.6835 - val_acc: 0.7206\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7212 - acc: 0.7033 - val_loss: 0.6838 - val_acc: 0.7157\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7233 - acc: 0.7003 - val_loss: 0.6816 - val_acc: 0.7162\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67966 to 0.67961, saving model to best.model\n",
      "1s - loss: 0.7150 - acc: 0.7058 - val_loss: 0.6796 - val_acc: 0.7177\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67961 to 0.67620, saving model to best.model\n",
      "1s - loss: 0.7173 - acc: 0.7010 - val_loss: 0.6762 - val_acc: 0.7201\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.7168 - acc: 0.7041 - val_loss: 0.6785 - val_acc: 0.7194\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67620 to 0.67320, saving model to best.model\n",
      "1s - loss: 0.7150 - acc: 0.7055 - val_loss: 0.6732 - val_acc: 0.7221\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67320 to 0.67139, saving model to best.model\n",
      "1s - loss: 0.7139 - acc: 0.7061 - val_loss: 0.6714 - val_acc: 0.7235\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67139 to 0.67102, saving model to best.model\n",
      "1s - loss: 0.7108 - acc: 0.7076 - val_loss: 0.6710 - val_acc: 0.7240\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7113 - acc: 0.7080 - val_loss: 0.6717 - val_acc: 0.7221\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7102 - acc: 0.7085 - val_loss: 0.6728 - val_acc: 0.7215\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67102 to 0.66644, saving model to best.model\n",
      "1s - loss: 0.7112 - acc: 0.7091 - val_loss: 0.6664 - val_acc: 0.7248\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7083 - acc: 0.7088 - val_loss: 0.6675 - val_acc: 0.7240\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66644 to 0.66550, saving model to best.model\n",
      "1s - loss: 0.7037 - acc: 0.7135 - val_loss: 0.6655 - val_acc: 0.7240\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.66550 to 0.66268, saving model to best.model\n",
      "1s - loss: 0.7049 - acc: 0.7082 - val_loss: 0.6627 - val_acc: 0.7270\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7019 - acc: 0.7112 - val_loss: 0.6665 - val_acc: 0.7234\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66268 to 0.65966, saving model to best.model\n",
      "1s - loss: 0.7014 - acc: 0.7121 - val_loss: 0.6597 - val_acc: 0.7294\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7064 - acc: 0.7102 - val_loss: 0.6631 - val_acc: 0.7269\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.6997 - acc: 0.7147 - val_loss: 0.6598 - val_acc: 0.7280\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.6998 - acc: 0.7127 - val_loss: 0.6607 - val_acc: 0.7268\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65966 to 0.65590, saving model to best.model\n",
      "1s - loss: 0.7005 - acc: 0.7154 - val_loss: 0.6559 - val_acc: 0.7294\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.6991 - acc: 0.7131 - val_loss: 0.6577 - val_acc: 0.7302\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7002 - acc: 0.7121 - val_loss: 0.6585 - val_acc: 0.7285\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.6951 - acc: 0.7143 - val_loss: 0.6584 - val_acc: 0.7262\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6949 - acc: 0.7161 - val_loss: 0.6579 - val_acc: 0.7274\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65590 to 0.65327, saving model to best.model\n",
      "1s - loss: 0.6924 - acc: 0.7150 - val_loss: 0.6533 - val_acc: 0.7337\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6935 - acc: 0.7156 - val_loss: 0.6543 - val_acc: 0.7279\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6970 - acc: 0.7140 - val_loss: 0.6559 - val_acc: 0.7285\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65327 to 0.65302, saving model to best.model\n",
      "1s - loss: 0.6936 - acc: 0.7166 - val_loss: 0.6530 - val_acc: 0.7311\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6923 - acc: 0.7170 - val_loss: 0.6538 - val_acc: 0.7278\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65302 to 0.65238, saving model to best.model\n",
      "1s - loss: 0.6902 - acc: 0.7170 - val_loss: 0.6524 - val_acc: 0.7280\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.65238 to 0.65202, saving model to best.model\n",
      "1s - loss: 0.6918 - acc: 0.7138 - val_loss: 0.6520 - val_acc: 0.7289\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65202 to 0.64761, saving model to best.model\n",
      "1s - loss: 0.6912 - acc: 0.7171 - val_loss: 0.6476 - val_acc: 0.7335\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6893 - acc: 0.7192 - val_loss: 0.6492 - val_acc: 0.7308\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6868 - acc: 0.7193 - val_loss: 0.6479 - val_acc: 0.7306\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6885 - acc: 0.7181 - val_loss: 0.6503 - val_acc: 0.7314\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64761 to 0.64370, saving model to best.model\n",
      "1s - loss: 0.6837 - acc: 0.7196 - val_loss: 0.6437 - val_acc: 0.7329\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6837 - acc: 0.7211 - val_loss: 0.6453 - val_acc: 0.7329\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6856 - acc: 0.7202 - val_loss: 0.6439 - val_acc: 0.7334\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6846 - acc: 0.7205 - val_loss: 0.6465 - val_acc: 0.7317\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64370 to 0.64340, saving model to best.model\n",
      "1s - loss: 0.6834 - acc: 0.7192 - val_loss: 0.6434 - val_acc: 0.7323\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64340 to 0.64103, saving model to best.model\n",
      "1s - loss: 0.6819 - acc: 0.7227 - val_loss: 0.6410 - val_acc: 0.7361\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6816 - acc: 0.7211 - val_loss: 0.6427 - val_acc: 0.7342\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.64103 to 0.63740, saving model to best.model\n",
      "1s - loss: 0.6810 - acc: 0.7222 - val_loss: 0.6374 - val_acc: 0.7365\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6763 - acc: 0.7237 - val_loss: 0.6422 - val_acc: 0.7335\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6832 - acc: 0.7211 - val_loss: 0.6392 - val_acc: 0.7365\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6801 - acc: 0.7231 - val_loss: 0.6392 - val_acc: 0.7361\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6816 - acc: 0.7212 - val_loss: 0.6390 - val_acc: 0.7358\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6779 - acc: 0.7232 - val_loss: 0.6376 - val_acc: 0.7376\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6765 - acc: 0.7244 - val_loss: 0.6390 - val_acc: 0.7372\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63740 to 0.63343, saving model to best.model\n",
      "0s - loss: 0.6769 - acc: 0.7238 - val_loss: 0.6334 - val_acc: 0.7403\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6769 - acc: 0.7235 - val_loss: 0.6370 - val_acc: 0.7365\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.6759 - acc: 0.7254 - val_loss: 0.6341 - val_acc: 0.7384\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6780 - acc: 0.7219 - val_loss: 0.6351 - val_acc: 0.7400\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6788 - acc: 0.7225 - val_loss: 0.6367 - val_acc: 0.7340\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.63343 to 0.63121, saving model to best.model\n",
      "0s - loss: 0.6764 - acc: 0.7230 - val_loss: 0.6312 - val_acc: 0.7411\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63121 to 0.63074, saving model to best.model\n",
      "0s - loss: 0.6757 - acc: 0.7232 - val_loss: 0.6307 - val_acc: 0.7413\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63074 to 0.63041, saving model to best.model\n",
      "0s - loss: 0.6732 - acc: 0.7241 - val_loss: 0.6304 - val_acc: 0.7405\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6725 - acc: 0.7266 - val_loss: 0.6306 - val_acc: 0.7379\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6760 - acc: 0.7228 - val_loss: 0.6340 - val_acc: 0.7349\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6686 - acc: 0.7287 - val_loss: 0.6322 - val_acc: 0.7383\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63041 to 0.62766, saving model to best.model\n",
      "0s - loss: 0.6722 - acc: 0.7239 - val_loss: 0.6277 - val_acc: 0.7482\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62766 to 0.62748, saving model to best.model\n",
      "0s - loss: 0.6690 - acc: 0.7261 - val_loss: 0.6275 - val_acc: 0.7404\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6697 - acc: 0.7280 - val_loss: 0.6304 - val_acc: 0.7357\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6745 - acc: 0.7256 - val_loss: 0.6291 - val_acc: 0.7385\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6688 - acc: 0.7271 - val_loss: 0.6277 - val_acc: 0.7402\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7269 - val_loss: 0.6275 - val_acc: 0.7440\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62748 to 0.62530, saving model to best.model\n",
      "0s - loss: 0.6660 - acc: 0.7286 - val_loss: 0.6253 - val_acc: 0.7415\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6651 - acc: 0.7284 - val_loss: 0.6276 - val_acc: 0.7405\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6698 - acc: 0.7266 - val_loss: 0.6276 - val_acc: 0.7427\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6668 - acc: 0.7289 - val_loss: 0.6291 - val_acc: 0.7376\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62530 to 0.62274, saving model to best.model\n",
      "1s - loss: 0.6639 - acc: 0.7289 - val_loss: 0.6227 - val_acc: 0.7445\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6669 - acc: 0.7279 - val_loss: 0.6251 - val_acc: 0.7397\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6663 - acc: 0.7275 - val_loss: 0.6228 - val_acc: 0.7468\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.62274 to 0.62117, saving model to best.model\n",
      "1s - loss: 0.6649 - acc: 0.7296 - val_loss: 0.6212 - val_acc: 0.7467\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6631 - acc: 0.7279 - val_loss: 0.6216 - val_acc: 0.7489\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.62117 to 0.62092, saving model to best.model\n",
      "1s - loss: 0.6659 - acc: 0.7280 - val_loss: 0.6209 - val_acc: 0.7440\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6624 - acc: 0.7287 - val_loss: 0.6217 - val_acc: 0.7445\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6617 - acc: 0.7282 - val_loss: 0.6222 - val_acc: 0.7437\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62092 to 0.61899, saving model to best.model\n",
      "0s - loss: 0.6613 - acc: 0.7296 - val_loss: 0.6190 - val_acc: 0.7477\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61899 to 0.61859, saving model to best.model\n",
      "0s - loss: 0.6596 - acc: 0.7311 - val_loss: 0.6186 - val_acc: 0.7463\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.61859 to 0.61847, saving model to best.model\n",
      "0s - loss: 0.6607 - acc: 0.7294 - val_loss: 0.6185 - val_acc: 0.7481\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6623 - acc: 0.7286 - val_loss: 0.6211 - val_acc: 0.7405\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6595 - acc: 0.7305 - val_loss: 0.6210 - val_acc: 0.7412\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6621 - acc: 0.7323 - val_loss: 0.6197 - val_acc: 0.7460\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.61847 to 0.61651, saving model to best.model\n",
      "0s - loss: 0.6567 - acc: 0.7330 - val_loss: 0.6165 - val_acc: 0.7465\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61651 to 0.61638, saving model to best.model\n",
      "0s - loss: 0.6586 - acc: 0.7327 - val_loss: 0.6164 - val_acc: 0.7472\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61638 to 0.61615, saving model to best.model\n",
      "0s - loss: 0.6589 - acc: 0.7318 - val_loss: 0.6162 - val_acc: 0.7467\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6560 - acc: 0.7322 - val_loss: 0.6167 - val_acc: 0.7445\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6556 - acc: 0.7328 - val_loss: 0.6168 - val_acc: 0.7481\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.61615 to 0.61573, saving model to best.model\n",
      "1s - loss: 0.6577 - acc: 0.7328 - val_loss: 0.6157 - val_acc: 0.7486\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61573 to 0.61143, saving model to best.model\n",
      "1s - loss: 0.6542 - acc: 0.7329 - val_loss: 0.6114 - val_acc: 0.7522\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6570 - acc: 0.7316 - val_loss: 0.6130 - val_acc: 0.7484\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6550 - acc: 0.7336 - val_loss: 0.6157 - val_acc: 0.7431\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6565 - acc: 0.7320 - val_loss: 0.6141 - val_acc: 0.7466\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6517 - acc: 0.7361 - val_loss: 0.6115 - val_acc: 0.7479\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61143 to 0.60885, saving model to best.model\n",
      "1s - loss: 0.6511 - acc: 0.7340 - val_loss: 0.6088 - val_acc: 0.7506\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6553 - acc: 0.7333 - val_loss: 0.6150 - val_acc: 0.7450\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60885 to 0.60846, saving model to best.model\n",
      "1s - loss: 0.6537 - acc: 0.7316 - val_loss: 0.6085 - val_acc: 0.7527\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7332 - val_loss: 0.6091 - val_acc: 0.7502\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60846 to 0.60779, saving model to best.model\n",
      "1s - loss: 0.6491 - acc: 0.7353 - val_loss: 0.6078 - val_acc: 0.7525\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6481 - acc: 0.7336 - val_loss: 0.6088 - val_acc: 0.7504\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7327 - val_loss: 0.6099 - val_acc: 0.7480\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60779 to 0.60694, saving model to best.model\n",
      "1s - loss: 0.6529 - acc: 0.7333 - val_loss: 0.6069 - val_acc: 0.7539\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6479 - acc: 0.7366 - val_loss: 0.6078 - val_acc: 0.7499\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6477 - acc: 0.7356 - val_loss: 0.6079 - val_acc: 0.7494\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6502 - acc: 0.7339 - val_loss: 0.6076 - val_acc: 0.7550\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60694 to 0.60521, saving model to best.model\n",
      "1s - loss: 0.6516 - acc: 0.7330 - val_loss: 0.6052 - val_acc: 0.7539\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6479 - acc: 0.7341 - val_loss: 0.6061 - val_acc: 0.7518\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6476 - acc: 0.7378 - val_loss: 0.6084 - val_acc: 0.7470\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60521 to 0.60362, saving model to best.model\n",
      "0s - loss: 0.6465 - acc: 0.7363 - val_loss: 0.6036 - val_acc: 0.7534\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6440 - acc: 0.7365 - val_loss: 0.6039 - val_acc: 0.7568\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6505 - acc: 0.7355 - val_loss: 0.6053 - val_acc: 0.7528\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6451 - acc: 0.7369 - val_loss: 0.6070 - val_acc: 0.7479\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6452 - acc: 0.7372 - val_loss: 0.6049 - val_acc: 0.7496\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60362 to 0.60355, saving model to best.model\n",
      "1s - loss: 0.6446 - acc: 0.7355 - val_loss: 0.6035 - val_acc: 0.7554\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.60355 to 0.60060, saving model to best.model\n",
      "1s - loss: 0.6441 - acc: 0.7381 - val_loss: 0.6006 - val_acc: 0.7571\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.85129, saving model to best.model\n",
      "1s - loss: 0.9474 - acc: 0.6128 - val_loss: 0.8513 - val_acc: 0.6536\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.85129 to 0.84880, saving model to best.model\n",
      "1s - loss: 0.8643 - acc: 0.6578 - val_loss: 0.8488 - val_acc: 0.6536\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84880 to 0.84760, saving model to best.model\n",
      "1s - loss: 0.8538 - acc: 0.6596 - val_loss: 0.8476 - val_acc: 0.6536\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "1s - loss: 0.8513 - acc: 0.6595 - val_loss: 0.8478 - val_acc: 0.6536\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84760 to 0.84197, saving model to best.model\n",
      "1s - loss: 0.8480 - acc: 0.6595 - val_loss: 0.8420 - val_acc: 0.6536\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.84197 to 0.83626, saving model to best.model\n",
      "1s - loss: 0.8418 - acc: 0.6595 - val_loss: 0.8363 - val_acc: 0.6536\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83626 to 0.83217, saving model to best.model\n",
      "1s - loss: 0.8368 - acc: 0.6595 - val_loss: 0.8322 - val_acc: 0.6536\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.83217 to 0.83023, saving model to best.model\n",
      "1s - loss: 0.8332 - acc: 0.6596 - val_loss: 0.8302 - val_acc: 0.6536\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.83023 to 0.82892, saving model to best.model\n",
      "1s - loss: 0.8317 - acc: 0.6595 - val_loss: 0.8289 - val_acc: 0.6536\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82892 to 0.82830, saving model to best.model\n",
      "1s - loss: 0.8284 - acc: 0.6595 - val_loss: 0.8283 - val_acc: 0.6536\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82830 to 0.82822, saving model to best.model\n",
      "1s - loss: 0.8281 - acc: 0.6595 - val_loss: 0.8282 - val_acc: 0.6536\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82822 to 0.82784, saving model to best.model\n",
      "1s - loss: 0.8269 - acc: 0.6594 - val_loss: 0.8278 - val_acc: 0.6536\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82784 to 0.82704, saving model to best.model\n",
      "1s - loss: 0.8264 - acc: 0.6597 - val_loss: 0.8270 - val_acc: 0.6536\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8259 - acc: 0.6594 - val_loss: 0.8271 - val_acc: 0.6536\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82704 to 0.82704, saving model to best.model\n",
      "1s - loss: 0.8257 - acc: 0.6595 - val_loss: 0.8270 - val_acc: 0.6536\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8240 - acc: 0.6595 - val_loss: 0.8283 - val_acc: 0.6536\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82704 to 0.82595, saving model to best.model\n",
      "1s - loss: 0.8233 - acc: 0.6594 - val_loss: 0.8259 - val_acc: 0.6536\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82595 to 0.82568, saving model to best.model\n",
      "1s - loss: 0.8236 - acc: 0.6594 - val_loss: 0.8257 - val_acc: 0.6536\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.82568 to 0.82524, saving model to best.model\n",
      "0s - loss: 0.8239 - acc: 0.6594 - val_loss: 0.8252 - val_acc: 0.6536\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82524 to 0.82493, saving model to best.model\n",
      "0s - loss: 0.8220 - acc: 0.6595 - val_loss: 0.8249 - val_acc: 0.6536\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82493 to 0.82405, saving model to best.model\n",
      "0s - loss: 0.8199 - acc: 0.6595 - val_loss: 0.8240 - val_acc: 0.6536\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82405 to 0.82335, saving model to best.model\n",
      "1s - loss: 0.8207 - acc: 0.6596 - val_loss: 0.8234 - val_acc: 0.6536\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.82335 to 0.82225, saving model to best.model\n",
      "1s - loss: 0.8200 - acc: 0.6596 - val_loss: 0.8222 - val_acc: 0.6536\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.82225 to 0.82177, saving model to best.model\n",
      "1s - loss: 0.8187 - acc: 0.6593 - val_loss: 0.8218 - val_acc: 0.6536\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.82177 to 0.82128, saving model to best.model\n",
      "1s - loss: 0.8179 - acc: 0.6595 - val_loss: 0.8213 - val_acc: 0.6536\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.82128 to 0.81974, saving model to best.model\n",
      "1s - loss: 0.8170 - acc: 0.6598 - val_loss: 0.8197 - val_acc: 0.6536\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81974 to 0.81790, saving model to best.model\n",
      "1s - loss: 0.8163 - acc: 0.6600 - val_loss: 0.8179 - val_acc: 0.6536\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81790 to 0.81711, saving model to best.model\n",
      "1s - loss: 0.8162 - acc: 0.6603 - val_loss: 0.8171 - val_acc: 0.6554\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81711 to 0.81697, saving model to best.model\n",
      "1s - loss: 0.8151 - acc: 0.6609 - val_loss: 0.8170 - val_acc: 0.6536\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81697 to 0.81447, saving model to best.model\n",
      "1s - loss: 0.8141 - acc: 0.6614 - val_loss: 0.8145 - val_acc: 0.6567\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.81447 to 0.81349, saving model to best.model\n",
      "1s - loss: 0.8115 - acc: 0.6615 - val_loss: 0.8135 - val_acc: 0.6576\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.81349 to 0.81204, saving model to best.model\n",
      "1s - loss: 0.8100 - acc: 0.6618 - val_loss: 0.8120 - val_acc: 0.6568\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.81204 to 0.80926, saving model to best.model\n",
      "1s - loss: 0.8093 - acc: 0.6642 - val_loss: 0.8093 - val_acc: 0.6607\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80926 to 0.80716, saving model to best.model\n",
      "1s - loss: 0.8072 - acc: 0.6633 - val_loss: 0.8072 - val_acc: 0.6616\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80716 to 0.80583, saving model to best.model\n",
      "1s - loss: 0.8078 - acc: 0.6631 - val_loss: 0.8058 - val_acc: 0.6624\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80583 to 0.80522, saving model to best.model\n",
      "1s - loss: 0.8042 - acc: 0.6654 - val_loss: 0.8052 - val_acc: 0.6619\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.80522 to 0.80203, saving model to best.model\n",
      "1s - loss: 0.8023 - acc: 0.6665 - val_loss: 0.8020 - val_acc: 0.6641\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.80203 to 0.80095, saving model to best.model\n",
      "1s - loss: 0.8038 - acc: 0.6656 - val_loss: 0.8010 - val_acc: 0.6619\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.80095 to 0.79723, saving model to best.model\n",
      "0s - loss: 0.7999 - acc: 0.6671 - val_loss: 0.7972 - val_acc: 0.6657\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79723 to 0.79567, saving model to best.model\n",
      "0s - loss: 0.7993 - acc: 0.6664 - val_loss: 0.7957 - val_acc: 0.6665\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79567 to 0.79477, saving model to best.model\n",
      "0s - loss: 0.7983 - acc: 0.6683 - val_loss: 0.7948 - val_acc: 0.6653\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.79477 to 0.79455, saving model to best.model\n",
      "0s - loss: 0.7963 - acc: 0.6686 - val_loss: 0.7946 - val_acc: 0.6638\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.79455 to 0.79179, saving model to best.model\n",
      "0s - loss: 0.7945 - acc: 0.6703 - val_loss: 0.7918 - val_acc: 0.6684\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.79179 to 0.79083, saving model to best.model\n",
      "0s - loss: 0.7937 - acc: 0.6695 - val_loss: 0.7908 - val_acc: 0.6658\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.79083 to 0.78798, saving model to best.model\n",
      "0s - loss: 0.7937 - acc: 0.6698 - val_loss: 0.7880 - val_acc: 0.6700\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78798 to 0.78711, saving model to best.model\n",
      "1s - loss: 0.7927 - acc: 0.6716 - val_loss: 0.7871 - val_acc: 0.6694\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.78711 to 0.78359, saving model to best.model\n",
      "1s - loss: 0.7884 - acc: 0.6718 - val_loss: 0.7836 - val_acc: 0.6707\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.78359 to 0.78212, saving model to best.model\n",
      "1s - loss: 0.7887 - acc: 0.6725 - val_loss: 0.7821 - val_acc: 0.6726\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.78212 to 0.78109, saving model to best.model\n",
      "1s - loss: 0.7865 - acc: 0.6734 - val_loss: 0.7811 - val_acc: 0.6734\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.78109 to 0.77898, saving model to best.model\n",
      "1s - loss: 0.7858 - acc: 0.6737 - val_loss: 0.7790 - val_acc: 0.6772\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77898 to 0.77872, saving model to best.model\n",
      "1s - loss: 0.7856 - acc: 0.6725 - val_loss: 0.7787 - val_acc: 0.6754\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.77872 to 0.77513, saving model to best.model\n",
      "1s - loss: 0.7832 - acc: 0.6761 - val_loss: 0.7751 - val_acc: 0.6801\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.77513 to 0.77511, saving model to best.model\n",
      "1s - loss: 0.7802 - acc: 0.6738 - val_loss: 0.7751 - val_acc: 0.6797\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.77511 to 0.76957, saving model to best.model\n",
      "1s - loss: 0.7801 - acc: 0.6770 - val_loss: 0.7696 - val_acc: 0.6859\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76957 to 0.76844, saving model to best.model\n",
      "1s - loss: 0.7787 - acc: 0.6777 - val_loss: 0.7684 - val_acc: 0.6826\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76844 to 0.76522, saving model to best.model\n",
      "1s - loss: 0.7764 - acc: 0.6787 - val_loss: 0.7652 - val_acc: 0.6847\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.76522 to 0.76195, saving model to best.model\n",
      "1s - loss: 0.7749 - acc: 0.6788 - val_loss: 0.7620 - val_acc: 0.6876\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.76195 to 0.76186, saving model to best.model\n",
      "1s - loss: 0.7749 - acc: 0.6799 - val_loss: 0.7619 - val_acc: 0.6868\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.76186 to 0.75682, saving model to best.model\n",
      "1s - loss: 0.7716 - acc: 0.6804 - val_loss: 0.7568 - val_acc: 0.6907\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.75682 to 0.75447, saving model to best.model\n",
      "1s - loss: 0.7710 - acc: 0.6803 - val_loss: 0.7545 - val_acc: 0.6959\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.75447 to 0.75379, saving model to best.model\n",
      "1s - loss: 0.7668 - acc: 0.6838 - val_loss: 0.7538 - val_acc: 0.6913\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.75379 to 0.75269, saving model to best.model\n",
      "1s - loss: 0.7672 - acc: 0.6827 - val_loss: 0.7527 - val_acc: 0.6903\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.75269 to 0.74816, saving model to best.model\n",
      "1s - loss: 0.7647 - acc: 0.6838 - val_loss: 0.7482 - val_acc: 0.6956\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.74816 to 0.74568, saving model to best.model\n",
      "1s - loss: 0.7629 - acc: 0.6855 - val_loss: 0.7457 - val_acc: 0.6954\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.74568 to 0.74261, saving model to best.model\n",
      "1s - loss: 0.7609 - acc: 0.6865 - val_loss: 0.7426 - val_acc: 0.6970\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.74261 to 0.74140, saving model to best.model\n",
      "1s - loss: 0.7616 - acc: 0.6878 - val_loss: 0.7414 - val_acc: 0.6967\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.74140 to 0.74111, saving model to best.model\n",
      "1s - loss: 0.7588 - acc: 0.6870 - val_loss: 0.7411 - val_acc: 0.6913\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.74111 to 0.73532, saving model to best.model\n",
      "1s - loss: 0.7549 - acc: 0.6883 - val_loss: 0.7353 - val_acc: 0.6980\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.73532 to 0.73136, saving model to best.model\n",
      "1s - loss: 0.7538 - acc: 0.6899 - val_loss: 0.7314 - val_acc: 0.7046\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.73136 to 0.73073, saving model to best.model\n",
      "1s - loss: 0.7504 - acc: 0.6904 - val_loss: 0.7307 - val_acc: 0.6975\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.73073 to 0.72819, saving model to best.model\n",
      "1s - loss: 0.7486 - acc: 0.6910 - val_loss: 0.7282 - val_acc: 0.7057\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72819 to 0.72766, saving model to best.model\n",
      "1s - loss: 0.7478 - acc: 0.6902 - val_loss: 0.7277 - val_acc: 0.6988\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.72766 to 0.72117, saving model to best.model\n",
      "1s - loss: 0.7476 - acc: 0.6904 - val_loss: 0.7212 - val_acc: 0.7082\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.72117 to 0.72114, saving model to best.model\n",
      "1s - loss: 0.7428 - acc: 0.6931 - val_loss: 0.7211 - val_acc: 0.7056\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.72114 to 0.71698, saving model to best.model\n",
      "1s - loss: 0.7427 - acc: 0.6933 - val_loss: 0.7170 - val_acc: 0.7070\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "1s - loss: 0.7424 - acc: 0.6974 - val_loss: 0.7178 - val_acc: 0.7094\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.71698 to 0.71469, saving model to best.model\n",
      "1s - loss: 0.7412 - acc: 0.6965 - val_loss: 0.7147 - val_acc: 0.7068\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.71469 to 0.71234, saving model to best.model\n",
      "1s - loss: 0.7386 - acc: 0.6982 - val_loss: 0.7123 - val_acc: 0.7136\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.71234 to 0.71093, saving model to best.model\n",
      "1s - loss: 0.7350 - acc: 0.6976 - val_loss: 0.7109 - val_acc: 0.7102\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.71093 to 0.70674, saving model to best.model\n",
      "1s - loss: 0.7366 - acc: 0.6983 - val_loss: 0.7067 - val_acc: 0.7123\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70674 to 0.70498, saving model to best.model\n",
      "1s - loss: 0.7329 - acc: 0.7012 - val_loss: 0.7050 - val_acc: 0.7126\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.70498 to 0.70421, saving model to best.model\n",
      "1s - loss: 0.7326 - acc: 0.6999 - val_loss: 0.7042 - val_acc: 0.7124\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.70421 to 0.69900, saving model to best.model\n",
      "1s - loss: 0.7285 - acc: 0.7004 - val_loss: 0.6990 - val_acc: 0.7142\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7317 - acc: 0.6990 - val_loss: 0.7006 - val_acc: 0.7152\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7308 - acc: 0.6992 - val_loss: 0.7005 - val_acc: 0.7141\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69900 to 0.69876, saving model to best.model\n",
      "1s - loss: 0.7293 - acc: 0.7007 - val_loss: 0.6988 - val_acc: 0.7148\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.69876 to 0.69224, saving model to best.model\n",
      "1s - loss: 0.7253 - acc: 0.7036 - val_loss: 0.6922 - val_acc: 0.7173\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7263 - acc: 0.7025 - val_loss: 0.6983 - val_acc: 0.7141\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.7264 - acc: 0.7056 - val_loss: 0.6930 - val_acc: 0.7169\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "1s - loss: 0.7234 - acc: 0.7049 - val_loss: 0.6925 - val_acc: 0.7179\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.69224 to 0.68886, saving model to best.model\n",
      "1s - loss: 0.7203 - acc: 0.7057 - val_loss: 0.6889 - val_acc: 0.7191\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68886 to 0.68872, saving model to best.model\n",
      "1s - loss: 0.7200 - acc: 0.7046 - val_loss: 0.6887 - val_acc: 0.7205\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68872 to 0.68726, saving model to best.model\n",
      "1s - loss: 0.7173 - acc: 0.7060 - val_loss: 0.6873 - val_acc: 0.7185\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68726 to 0.68500, saving model to best.model\n",
      "1s - loss: 0.7190 - acc: 0.7065 - val_loss: 0.6850 - val_acc: 0.7211\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.68500 to 0.68348, saving model to best.model\n",
      "1s - loss: 0.7163 - acc: 0.7055 - val_loss: 0.6835 - val_acc: 0.7244\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7171 - acc: 0.7059 - val_loss: 0.6849 - val_acc: 0.7183\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7152 - acc: 0.7074 - val_loss: 0.6839 - val_acc: 0.7230\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.68348 to 0.67863, saving model to best.model\n",
      "1s - loss: 0.7127 - acc: 0.7096 - val_loss: 0.6786 - val_acc: 0.7235\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67863 to 0.67847, saving model to best.model\n",
      "1s - loss: 0.7114 - acc: 0.7099 - val_loss: 0.6785 - val_acc: 0.7213\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7144 - acc: 0.7087 - val_loss: 0.6792 - val_acc: 0.7247\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.67847 to 0.67563, saving model to best.model\n",
      "1s - loss: 0.7124 - acc: 0.7099 - val_loss: 0.6756 - val_acc: 0.7262\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67563 to 0.67371, saving model to best.model\n",
      "0s - loss: 0.7096 - acc: 0.7104 - val_loss: 0.6737 - val_acc: 0.7275\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7085 - acc: 0.7115 - val_loss: 0.6750 - val_acc: 0.7265\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.67371 to 0.67002, saving model to best.model\n",
      "1s - loss: 0.7070 - acc: 0.7118 - val_loss: 0.6700 - val_acc: 0.7290\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7056 - acc: 0.7126 - val_loss: 0.6727 - val_acc: 0.7260\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7095 - acc: 0.7115 - val_loss: 0.6740 - val_acc: 0.7252\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.7053 - acc: 0.7094 - val_loss: 0.6733 - val_acc: 0.7259\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7031 - acc: 0.7135 - val_loss: 0.6713 - val_acc: 0.7268\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.7056 - acc: 0.7121 - val_loss: 0.6717 - val_acc: 0.7269\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.7026 - acc: 0.7126 - val_loss: 0.6713 - val_acc: 0.7255\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.67002 to 0.66315, saving model to best.model\n",
      "1s - loss: 0.7025 - acc: 0.7138 - val_loss: 0.6631 - val_acc: 0.7315\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6985 - acc: 0.7157 - val_loss: 0.6633 - val_acc: 0.7308\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6998 - acc: 0.7135 - val_loss: 0.6654 - val_acc: 0.7275\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6995 - acc: 0.7154 - val_loss: 0.6658 - val_acc: 0.7288\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.66315 to 0.66256, saving model to best.model\n",
      "1s - loss: 0.6980 - acc: 0.7172 - val_loss: 0.6626 - val_acc: 0.7310\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.66256 to 0.66083, saving model to best.model\n",
      "1s - loss: 0.6931 - acc: 0.7186 - val_loss: 0.6608 - val_acc: 0.7327\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6966 - acc: 0.7175 - val_loss: 0.6631 - val_acc: 0.7314\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6971 - acc: 0.7168 - val_loss: 0.6630 - val_acc: 0.7285\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.66083 to 0.65862, saving model to best.model\n",
      "1s - loss: 0.6941 - acc: 0.7177 - val_loss: 0.6586 - val_acc: 0.7304\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65862 to 0.65773, saving model to best.model\n",
      "1s - loss: 0.6899 - acc: 0.7180 - val_loss: 0.6577 - val_acc: 0.7317\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65773 to 0.65771, saving model to best.model\n",
      "1s - loss: 0.6913 - acc: 0.7216 - val_loss: 0.6577 - val_acc: 0.7310\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.65771 to 0.65601, saving model to best.model\n",
      "1s - loss: 0.6934 - acc: 0.7174 - val_loss: 0.6560 - val_acc: 0.7324\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6926 - acc: 0.7205 - val_loss: 0.6576 - val_acc: 0.7327\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6925 - acc: 0.7182 - val_loss: 0.6560 - val_acc: 0.7334\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.65601 to 0.65434, saving model to best.model\n",
      "0s - loss: 0.6912 - acc: 0.7183 - val_loss: 0.6543 - val_acc: 0.7333\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.65434 to 0.64974, saving model to best.model\n",
      "0s - loss: 0.6893 - acc: 0.7198 - val_loss: 0.6497 - val_acc: 0.7334\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.64974 to 0.64958, saving model to best.model\n",
      "0s - loss: 0.6882 - acc: 0.7182 - val_loss: 0.6496 - val_acc: 0.7344\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6897 - acc: 0.7188 - val_loss: 0.6533 - val_acc: 0.7341\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.64958 to 0.64869, saving model to best.model\n",
      "0s - loss: 0.6845 - acc: 0.7227 - val_loss: 0.6487 - val_acc: 0.7350\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6876 - acc: 0.7180 - val_loss: 0.6510 - val_acc: 0.7362\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64869 to 0.64851, saving model to best.model\n",
      "0s - loss: 0.6874 - acc: 0.7212 - val_loss: 0.6485 - val_acc: 0.7347\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6871 - acc: 0.7208 - val_loss: 0.6489 - val_acc: 0.7348\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.64851 to 0.64680, saving model to best.model\n",
      "1s - loss: 0.6829 - acc: 0.7206 - val_loss: 0.6468 - val_acc: 0.7357\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6822 - acc: 0.7234 - val_loss: 0.6489 - val_acc: 0.7337\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.64680 to 0.64492, saving model to best.model\n",
      "1s - loss: 0.6844 - acc: 0.7222 - val_loss: 0.6449 - val_acc: 0.7359\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64492 to 0.64400, saving model to best.model\n",
      "1s - loss: 0.6791 - acc: 0.7251 - val_loss: 0.6440 - val_acc: 0.7374\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6814 - acc: 0.7229 - val_loss: 0.6453 - val_acc: 0.7355\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6847 - acc: 0.7193 - val_loss: 0.6442 - val_acc: 0.7362\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.64400 to 0.64296, saving model to best.model\n",
      "1s - loss: 0.6843 - acc: 0.7216 - val_loss: 0.6430 - val_acc: 0.7370\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.64296 to 0.64151, saving model to best.model\n",
      "1s - loss: 0.6787 - acc: 0.7230 - val_loss: 0.6415 - val_acc: 0.7355\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6805 - acc: 0.7234 - val_loss: 0.6444 - val_acc: 0.7358\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6765 - acc: 0.7247 - val_loss: 0.6444 - val_acc: 0.7385\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.64151 to 0.63913, saving model to best.model\n",
      "1s - loss: 0.6787 - acc: 0.7244 - val_loss: 0.6391 - val_acc: 0.7392\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63913 to 0.63760, saving model to best.model\n",
      "1s - loss: 0.6735 - acc: 0.7263 - val_loss: 0.6376 - val_acc: 0.7397\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6784 - acc: 0.7233 - val_loss: 0.6404 - val_acc: 0.7396\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6768 - acc: 0.7239 - val_loss: 0.6389 - val_acc: 0.7377\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63760 to 0.63716, saving model to best.model\n",
      "1s - loss: 0.6764 - acc: 0.7249 - val_loss: 0.6372 - val_acc: 0.7395\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6759 - acc: 0.7258 - val_loss: 0.6382 - val_acc: 0.7381\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6752 - acc: 0.7263 - val_loss: 0.6402 - val_acc: 0.7371\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.63716 to 0.63630, saving model to best.model\n",
      "1s - loss: 0.6747 - acc: 0.7258 - val_loss: 0.6363 - val_acc: 0.7388\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.63630 to 0.63577, saving model to best.model\n",
      "1s - loss: 0.6740 - acc: 0.7241 - val_loss: 0.6358 - val_acc: 0.7399\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.63577 to 0.63406, saving model to best.model\n",
      "1s - loss: 0.6710 - acc: 0.7271 - val_loss: 0.6341 - val_acc: 0.7399\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6716 - acc: 0.7259 - val_loss: 0.6344 - val_acc: 0.7395\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6690 - acc: 0.7254 - val_loss: 0.6343 - val_acc: 0.7374\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.63406 to 0.63256, saving model to best.model\n",
      "1s - loss: 0.6715 - acc: 0.7278 - val_loss: 0.6326 - val_acc: 0.7405\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6737 - acc: 0.7254 - val_loss: 0.6357 - val_acc: 0.7391\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6726 - acc: 0.7259 - val_loss: 0.6334 - val_acc: 0.7402\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.63256 to 0.63174, saving model to best.model\n",
      "1s - loss: 0.6692 - acc: 0.7284 - val_loss: 0.6317 - val_acc: 0.7412\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6728 - acc: 0.7253 - val_loss: 0.6329 - val_acc: 0.7397\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.63174 to 0.63002, saving model to best.model\n",
      "1s - loss: 0.6655 - acc: 0.7300 - val_loss: 0.6300 - val_acc: 0.7399\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.63002 to 0.62995, saving model to best.model\n",
      "0s - loss: 0.6692 - acc: 0.7289 - val_loss: 0.6300 - val_acc: 0.7413\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62995 to 0.62930, saving model to best.model\n",
      "1s - loss: 0.6649 - acc: 0.7305 - val_loss: 0.6293 - val_acc: 0.7395\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62930 to 0.62769, saving model to best.model\n",
      "1s - loss: 0.6674 - acc: 0.7276 - val_loss: 0.6277 - val_acc: 0.7419\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6649 - acc: 0.7287 - val_loss: 0.6301 - val_acc: 0.7395\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6668 - acc: 0.7276 - val_loss: 0.6318 - val_acc: 0.7375\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.62769 to 0.62673, saving model to best.model\n",
      "1s - loss: 0.6666 - acc: 0.7303 - val_loss: 0.6267 - val_acc: 0.7426\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6655 - acc: 0.7283 - val_loss: 0.6270 - val_acc: 0.7420\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6645 - acc: 0.7303 - val_loss: 0.6274 - val_acc: 0.7406\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6635 - acc: 0.7294 - val_loss: 0.6283 - val_acc: 0.7425\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.62673 to 0.62642, saving model to best.model\n",
      "1s - loss: 0.6664 - acc: 0.7295 - val_loss: 0.6264 - val_acc: 0.7422\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6647 - acc: 0.7296 - val_loss: 0.6289 - val_acc: 0.7410\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.62642 to 0.62473, saving model to best.model\n",
      "1s - loss: 0.6624 - acc: 0.7299 - val_loss: 0.6247 - val_acc: 0.7433\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.62473 to 0.62378, saving model to best.model\n",
      "1s - loss: 0.6618 - acc: 0.7301 - val_loss: 0.6238 - val_acc: 0.7425\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.62378 to 0.62288, saving model to best.model\n",
      "1s - loss: 0.6641 - acc: 0.7309 - val_loss: 0.6229 - val_acc: 0.7473\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6620 - acc: 0.7300 - val_loss: 0.6258 - val_acc: 0.7419\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.62288 to 0.62282, saving model to best.model\n",
      "1s - loss: 0.6648 - acc: 0.7297 - val_loss: 0.6228 - val_acc: 0.7444\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6625 - acc: 0.7304 - val_loss: 0.6228 - val_acc: 0.7434\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6603 - acc: 0.7313 - val_loss: 0.6240 - val_acc: 0.7419\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6593 - acc: 0.7323 - val_loss: 0.6228 - val_acc: 0.7447\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.62282 to 0.62137, saving model to best.model\n",
      "1s - loss: 0.6595 - acc: 0.7310 - val_loss: 0.6214 - val_acc: 0.7456\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.62137 to 0.62077, saving model to best.model\n",
      "1s - loss: 0.6598 - acc: 0.7315 - val_loss: 0.6208 - val_acc: 0.7451\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6606 - acc: 0.7319 - val_loss: 0.6226 - val_acc: 0.7427\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6554 - acc: 0.7321 - val_loss: 0.6224 - val_acc: 0.7434\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.62077 to 0.62011, saving model to best.model\n",
      "1s - loss: 0.6599 - acc: 0.7302 - val_loss: 0.6201 - val_acc: 0.7454\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.62011 to 0.61817, saving model to best.model\n",
      "1s - loss: 0.6552 - acc: 0.7339 - val_loss: 0.6182 - val_acc: 0.7467\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6596 - acc: 0.7307 - val_loss: 0.6208 - val_acc: 0.7425\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.61817 to 0.61722, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7344 - val_loss: 0.6172 - val_acc: 0.7470\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7345 - val_loss: 0.6181 - val_acc: 0.7450\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6535 - acc: 0.7353 - val_loss: 0.6176 - val_acc: 0.7451\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.61722 to 0.61562, saving model to best.model\n",
      "1s - loss: 0.6530 - acc: 0.7360 - val_loss: 0.6156 - val_acc: 0.7477\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.61562 to 0.61528, saving model to best.model\n",
      "1s - loss: 0.6562 - acc: 0.7346 - val_loss: 0.6153 - val_acc: 0.7492\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.61528 to 0.61438, saving model to best.model\n",
      "1s - loss: 0.6560 - acc: 0.7336 - val_loss: 0.6144 - val_acc: 0.7461\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6554 - acc: 0.7316 - val_loss: 0.6180 - val_acc: 0.7440\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6527 - acc: 0.7343 - val_loss: 0.6157 - val_acc: 0.7468\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6546 - acc: 0.7347 - val_loss: 0.6155 - val_acc: 0.7471\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6538 - acc: 0.7332 - val_loss: 0.6170 - val_acc: 0.7450\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.61438 to 0.61193, saving model to best.model\n",
      "1s - loss: 0.6522 - acc: 0.7321 - val_loss: 0.6119 - val_acc: 0.7473\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.61193 to 0.61166, saving model to best.model\n",
      "1s - loss: 0.6527 - acc: 0.7352 - val_loss: 0.6117 - val_acc: 0.7482\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6538 - acc: 0.7330 - val_loss: 0.6134 - val_acc: 0.7479\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.61166 to 0.61047, saving model to best.model\n",
      "1s - loss: 0.6505 - acc: 0.7349 - val_loss: 0.6105 - val_acc: 0.7506\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84568, saving model to best.model\n",
      "1s - loss: 0.9268 - acc: 0.6191 - val_loss: 0.8457 - val_acc: 0.6553\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84568 to 0.84458, saving model to best.model\n",
      "1s - loss: 0.8576 - acc: 0.6572 - val_loss: 0.8446 - val_acc: 0.6553\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84458 to 0.84426, saving model to best.model\n",
      "1s - loss: 0.8504 - acc: 0.6595 - val_loss: 0.8443 - val_acc: 0.6553\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84426 to 0.84188, saving model to best.model\n",
      "1s - loss: 0.8468 - acc: 0.6595 - val_loss: 0.8419 - val_acc: 0.6553\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84188 to 0.83624, saving model to best.model\n",
      "1s - loss: 0.8437 - acc: 0.6595 - val_loss: 0.8362 - val_acc: 0.6553\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83624 to 0.83311, saving model to best.model\n",
      "1s - loss: 0.8374 - acc: 0.6595 - val_loss: 0.8331 - val_acc: 0.6553\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83311 to 0.82754, saving model to best.model\n",
      "1s - loss: 0.8328 - acc: 0.6595 - val_loss: 0.8275 - val_acc: 0.6553\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82754 to 0.82574, saving model to best.model\n",
      "1s - loss: 0.8299 - acc: 0.6595 - val_loss: 0.8257 - val_acc: 0.6553\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82574 to 0.82566, saving model to best.model\n",
      "1s - loss: 0.8285 - acc: 0.6595 - val_loss: 0.8257 - val_acc: 0.6553\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82566 to 0.82502, saving model to best.model\n",
      "0s - loss: 0.8276 - acc: 0.6595 - val_loss: 0.8250 - val_acc: 0.6553\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82502 to 0.82468, saving model to best.model\n",
      "0s - loss: 0.8264 - acc: 0.6595 - val_loss: 0.8247 - val_acc: 0.6553\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82468 to 0.82308, saving model to best.model\n",
      "0s - loss: 0.8257 - acc: 0.6593 - val_loss: 0.8231 - val_acc: 0.6553\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82308 to 0.82242, saving model to best.model\n",
      "0s - loss: 0.8251 - acc: 0.6595 - val_loss: 0.8224 - val_acc: 0.6553\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.8235 - acc: 0.6594 - val_loss: 0.8235 - val_acc: 0.6553\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82242 to 0.82227, saving model to best.model\n",
      "0s - loss: 0.8241 - acc: 0.6594 - val_loss: 0.8223 - val_acc: 0.6553\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82227 to 0.82207, saving model to best.model\n",
      "0s - loss: 0.8217 - acc: 0.6595 - val_loss: 0.8221 - val_acc: 0.6553\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82207 to 0.82155, saving model to best.model\n",
      "0s - loss: 0.8226 - acc: 0.6595 - val_loss: 0.8215 - val_acc: 0.6553\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82155 to 0.82109, saving model to best.model\n",
      "0s - loss: 0.8209 - acc: 0.6595 - val_loss: 0.8211 - val_acc: 0.6553\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.82109 to 0.81986, saving model to best.model\n",
      "0s - loss: 0.8212 - acc: 0.6593 - val_loss: 0.8199 - val_acc: 0.6553\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81986 to 0.81896, saving model to best.model\n",
      "0s - loss: 0.8207 - acc: 0.6594 - val_loss: 0.8190 - val_acc: 0.6553\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81896 to 0.81881, saving model to best.model\n",
      "0s - loss: 0.8198 - acc: 0.6595 - val_loss: 0.8188 - val_acc: 0.6553\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81881 to 0.81809, saving model to best.model\n",
      "0s - loss: 0.8195 - acc: 0.6596 - val_loss: 0.8181 - val_acc: 0.6553\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81809 to 0.81656, saving model to best.model\n",
      "0s - loss: 0.8170 - acc: 0.6595 - val_loss: 0.8166 - val_acc: 0.6553\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81656 to 0.81570, saving model to best.model\n",
      "1s - loss: 0.8171 - acc: 0.6597 - val_loss: 0.8157 - val_acc: 0.6553\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81570 to 0.81560, saving model to best.model\n",
      "1s - loss: 0.8168 - acc: 0.6597 - val_loss: 0.8156 - val_acc: 0.6553\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81560 to 0.81333, saving model to best.model\n",
      "1s - loss: 0.8140 - acc: 0.6602 - val_loss: 0.8133 - val_acc: 0.6554\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81333 to 0.81202, saving model to best.model\n",
      "1s - loss: 0.8156 - acc: 0.6595 - val_loss: 0.8120 - val_acc: 0.6553\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81202 to 0.81130, saving model to best.model\n",
      "1s - loss: 0.8141 - acc: 0.6601 - val_loss: 0.8113 - val_acc: 0.6553\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81130 to 0.80929, saving model to best.model\n",
      "1s - loss: 0.8125 - acc: 0.6612 - val_loss: 0.8093 - val_acc: 0.6588\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80929 to 0.80909, saving model to best.model\n",
      "1s - loss: 0.8102 - acc: 0.6618 - val_loss: 0.8091 - val_acc: 0.6554\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80909 to 0.80628, saving model to best.model\n",
      "1s - loss: 0.8096 - acc: 0.6623 - val_loss: 0.8063 - val_acc: 0.6591\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "1s - loss: 0.8095 - acc: 0.6621 - val_loss: 0.8071 - val_acc: 0.6602\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80628 to 0.80265, saving model to best.model\n",
      "1s - loss: 0.8081 - acc: 0.6625 - val_loss: 0.8027 - val_acc: 0.6670\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80265 to 0.80159, saving model to best.model\n",
      "1s - loss: 0.8057 - acc: 0.6638 - val_loss: 0.8016 - val_acc: 0.6624\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80159 to 0.79976, saving model to best.model\n",
      "1s - loss: 0.8046 - acc: 0.6638 - val_loss: 0.7998 - val_acc: 0.6659\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79976 to 0.79752, saving model to best.model\n",
      "1s - loss: 0.8032 - acc: 0.6650 - val_loss: 0.7975 - val_acc: 0.6642\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79752 to 0.79592, saving model to best.model\n",
      "1s - loss: 0.8016 - acc: 0.6652 - val_loss: 0.7959 - val_acc: 0.6646\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "1s - loss: 0.8015 - acc: 0.6642 - val_loss: 0.7962 - val_acc: 0.6584\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79592 to 0.79144, saving model to best.model\n",
      "1s - loss: 0.7994 - acc: 0.6652 - val_loss: 0.7914 - val_acc: 0.6676\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "1s - loss: 0.7992 - acc: 0.6660 - val_loss: 0.7922 - val_acc: 0.6674\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79144 to 0.78769, saving model to best.model\n",
      "1s - loss: 0.7968 - acc: 0.6659 - val_loss: 0.7877 - val_acc: 0.6697\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78769 to 0.78481, saving model to best.model\n",
      "1s - loss: 0.7953 - acc: 0.6664 - val_loss: 0.7848 - val_acc: 0.6735\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78481 to 0.78448, saving model to best.model\n",
      "1s - loss: 0.7934 - acc: 0.6686 - val_loss: 0.7845 - val_acc: 0.6686\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78448 to 0.78190, saving model to best.model\n",
      "1s - loss: 0.7910 - acc: 0.6690 - val_loss: 0.7819 - val_acc: 0.6689\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78190 to 0.77812, saving model to best.model\n",
      "1s - loss: 0.7904 - acc: 0.6690 - val_loss: 0.7781 - val_acc: 0.6772\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77812 to 0.77740, saving model to best.model\n",
      "0s - loss: 0.7892 - acc: 0.6704 - val_loss: 0.7774 - val_acc: 0.6759\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77740 to 0.77556, saving model to best.model\n",
      "0s - loss: 0.7890 - acc: 0.6702 - val_loss: 0.7756 - val_acc: 0.6766\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77556 to 0.77406, saving model to best.model\n",
      "0s - loss: 0.7871 - acc: 0.6716 - val_loss: 0.7741 - val_acc: 0.6767\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77406 to 0.77334, saving model to best.model\n",
      "0s - loss: 0.7848 - acc: 0.6705 - val_loss: 0.7733 - val_acc: 0.6721\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77334 to 0.76988, saving model to best.model\n",
      "0s - loss: 0.7827 - acc: 0.6730 - val_loss: 0.7699 - val_acc: 0.6768\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76988 to 0.76644, saving model to best.model\n",
      "0s - loss: 0.7818 - acc: 0.6729 - val_loss: 0.7664 - val_acc: 0.6833\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.7810 - acc: 0.6731 - val_loss: 0.7666 - val_acc: 0.6768\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76644 to 0.76293, saving model to best.model\n",
      "0s - loss: 0.7796 - acc: 0.6734 - val_loss: 0.7629 - val_acc: 0.6806\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76293 to 0.75922, saving model to best.model\n",
      "0s - loss: 0.7776 - acc: 0.6753 - val_loss: 0.7592 - val_acc: 0.6842\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75922 to 0.75679, saving model to best.model\n",
      "1s - loss: 0.7751 - acc: 0.6751 - val_loss: 0.7568 - val_acc: 0.6884\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75679 to 0.75414, saving model to best.model\n",
      "1s - loss: 0.7761 - acc: 0.6752 - val_loss: 0.7541 - val_acc: 0.6859\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75414 to 0.75109, saving model to best.model\n",
      "1s - loss: 0.7728 - acc: 0.6780 - val_loss: 0.7511 - val_acc: 0.6865\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75109 to 0.74646, saving model to best.model\n",
      "1s - loss: 0.7695 - acc: 0.6797 - val_loss: 0.7465 - val_acc: 0.6904\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74646 to 0.74417, saving model to best.model\n",
      "1s - loss: 0.7684 - acc: 0.6804 - val_loss: 0.7442 - val_acc: 0.6903\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74417 to 0.74207, saving model to best.model\n",
      "1s - loss: 0.7658 - acc: 0.6799 - val_loss: 0.7421 - val_acc: 0.6903\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74207 to 0.74054, saving model to best.model\n",
      "1s - loss: 0.7641 - acc: 0.6802 - val_loss: 0.7405 - val_acc: 0.6931\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74054 to 0.73438, saving model to best.model\n",
      "1s - loss: 0.7611 - acc: 0.6827 - val_loss: 0.7344 - val_acc: 0.6946\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73438 to 0.73303, saving model to best.model\n",
      "1s - loss: 0.7576 - acc: 0.6850 - val_loss: 0.7330 - val_acc: 0.6975\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73303 to 0.72892, saving model to best.model\n",
      "1s - loss: 0.7570 - acc: 0.6850 - val_loss: 0.7289 - val_acc: 0.7001\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72892 to 0.72500, saving model to best.model\n",
      "1s - loss: 0.7551 - acc: 0.6842 - val_loss: 0.7250 - val_acc: 0.7014\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "1s - loss: 0.7534 - acc: 0.6866 - val_loss: 0.7264 - val_acc: 0.6984\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.7522 - acc: 0.6867 - val_loss: 0.7261 - val_acc: 0.6982\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72500 to 0.71954, saving model to best.model\n",
      "1s - loss: 0.7482 - acc: 0.6892 - val_loss: 0.7195 - val_acc: 0.6999\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71954 to 0.71752, saving model to best.model\n",
      "1s - loss: 0.7468 - acc: 0.6899 - val_loss: 0.7175 - val_acc: 0.7018\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71752 to 0.70956, saving model to best.model\n",
      "1s - loss: 0.7456 - acc: 0.6892 - val_loss: 0.7096 - val_acc: 0.7103\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70956 to 0.70871, saving model to best.model\n",
      "1s - loss: 0.7414 - acc: 0.6923 - val_loss: 0.7087 - val_acc: 0.7091\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70871 to 0.70515, saving model to best.model\n",
      "1s - loss: 0.7427 - acc: 0.6903 - val_loss: 0.7051 - val_acc: 0.7132\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7391 - acc: 0.6949 - val_loss: 0.7072 - val_acc: 0.7094\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70515 to 0.70266, saving model to best.model\n",
      "0s - loss: 0.7375 - acc: 0.6937 - val_loss: 0.7027 - val_acc: 0.7133\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.7376 - acc: 0.6939 - val_loss: 0.7027 - val_acc: 0.7101\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70266 to 0.70088, saving model to best.model\n",
      "1s - loss: 0.7362 - acc: 0.6951 - val_loss: 0.7009 - val_acc: 0.7095\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70088 to 0.69576, saving model to best.model\n",
      "1s - loss: 0.7316 - acc: 0.6973 - val_loss: 0.6958 - val_acc: 0.7142\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.7333 - acc: 0.6950 - val_loss: 0.6966 - val_acc: 0.7115\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69576 to 0.69146, saving model to best.model\n",
      "1s - loss: 0.7310 - acc: 0.6977 - val_loss: 0.6915 - val_acc: 0.7187\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7298 - acc: 0.7018 - val_loss: 0.6931 - val_acc: 0.7181\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69146 to 0.68706, saving model to best.model\n",
      "1s - loss: 0.7276 - acc: 0.6998 - val_loss: 0.6871 - val_acc: 0.7219\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7266 - acc: 0.6995 - val_loss: 0.6925 - val_acc: 0.7208\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68706 to 0.68655, saving model to best.model\n",
      "1s - loss: 0.7270 - acc: 0.6999 - val_loss: 0.6865 - val_acc: 0.7237\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68655 to 0.68067, saving model to best.model\n",
      "1s - loss: 0.7228 - acc: 0.7030 - val_loss: 0.6807 - val_acc: 0.7237\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7238 - acc: 0.7018 - val_loss: 0.6807 - val_acc: 0.7227\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7228 - acc: 0.7033 - val_loss: 0.6839 - val_acc: 0.7180\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68067 to 0.67807, saving model to best.model\n",
      "1s - loss: 0.7178 - acc: 0.7037 - val_loss: 0.6781 - val_acc: 0.7231\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7202 - acc: 0.7023 - val_loss: 0.6796 - val_acc: 0.7246\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67807 to 0.67218, saving model to best.model\n",
      "1s - loss: 0.7157 - acc: 0.7049 - val_loss: 0.6722 - val_acc: 0.7295\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67218 to 0.67184, saving model to best.model\n",
      "1s - loss: 0.7160 - acc: 0.7046 - val_loss: 0.6718 - val_acc: 0.7260\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7161 - acc: 0.7060 - val_loss: 0.6732 - val_acc: 0.7259\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67184 to 0.66918, saving model to best.model\n",
      "1s - loss: 0.7138 - acc: 0.7064 - val_loss: 0.6692 - val_acc: 0.7267\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7130 - acc: 0.7077 - val_loss: 0.6712 - val_acc: 0.7275\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66918 to 0.66913, saving model to best.model\n",
      "1s - loss: 0.7103 - acc: 0.7081 - val_loss: 0.6691 - val_acc: 0.7275\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66913 to 0.66640, saving model to best.model\n",
      "1s - loss: 0.7076 - acc: 0.7107 - val_loss: 0.6664 - val_acc: 0.7286\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7070 - acc: 0.7102 - val_loss: 0.6668 - val_acc: 0.7297\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66640 to 0.66313, saving model to best.model\n",
      "1s - loss: 0.7058 - acc: 0.7087 - val_loss: 0.6631 - val_acc: 0.7297\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.66313 to 0.66273, saving model to best.model\n",
      "1s - loss: 0.7071 - acc: 0.7090 - val_loss: 0.6627 - val_acc: 0.7315\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66273 to 0.66089, saving model to best.model\n",
      "1s - loss: 0.7040 - acc: 0.7101 - val_loss: 0.6609 - val_acc: 0.7311\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66089 to 0.65997, saving model to best.model\n",
      "1s - loss: 0.7050 - acc: 0.7096 - val_loss: 0.6600 - val_acc: 0.7313\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.65997 to 0.65752, saving model to best.model\n",
      "1s - loss: 0.7018 - acc: 0.7125 - val_loss: 0.6575 - val_acc: 0.7340\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.65752 to 0.65613, saving model to best.model\n",
      "1s - loss: 0.7024 - acc: 0.7109 - val_loss: 0.6561 - val_acc: 0.7338\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7003 - acc: 0.7103 - val_loss: 0.6573 - val_acc: 0.7317\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65613 to 0.65332, saving model to best.model\n",
      "1s - loss: 0.6969 - acc: 0.7157 - val_loss: 0.6533 - val_acc: 0.7352\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7009 - acc: 0.7131 - val_loss: 0.6535 - val_acc: 0.7323\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65332 to 0.65107, saving model to best.model\n",
      "1s - loss: 0.6984 - acc: 0.7142 - val_loss: 0.6511 - val_acc: 0.7341\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.6990 - acc: 0.7158 - val_loss: 0.6554 - val_acc: 0.7335\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65107 to 0.64893, saving model to best.model\n",
      "1s - loss: 0.6965 - acc: 0.7126 - val_loss: 0.6489 - val_acc: 0.7351\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6950 - acc: 0.7137 - val_loss: 0.6501 - val_acc: 0.7335\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.64893 to 0.64819, saving model to best.model\n",
      "1s - loss: 0.6957 - acc: 0.7151 - val_loss: 0.6482 - val_acc: 0.7352\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.64819 to 0.64652, saving model to best.model\n",
      "1s - loss: 0.6928 - acc: 0.7191 - val_loss: 0.6465 - val_acc: 0.7337\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.64652 to 0.64543, saving model to best.model\n",
      "1s - loss: 0.6934 - acc: 0.7179 - val_loss: 0.6454 - val_acc: 0.7365\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.64543 to 0.64445, saving model to best.model\n",
      "1s - loss: 0.6932 - acc: 0.7156 - val_loss: 0.6445 - val_acc: 0.7372\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6920 - acc: 0.7150 - val_loss: 0.6451 - val_acc: 0.7378\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6922 - acc: 0.7169 - val_loss: 0.6473 - val_acc: 0.7358\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.64445 to 0.64169, saving model to best.model\n",
      "1s - loss: 0.6879 - acc: 0.7182 - val_loss: 0.6417 - val_acc: 0.7374\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64169 to 0.64073, saving model to best.model\n",
      "1s - loss: 0.6892 - acc: 0.7202 - val_loss: 0.6407 - val_acc: 0.7383\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6888 - acc: 0.7170 - val_loss: 0.6421 - val_acc: 0.7370\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64073 to 0.63992, saving model to best.model\n",
      "1s - loss: 0.6848 - acc: 0.7210 - val_loss: 0.6399 - val_acc: 0.7400\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.63992 to 0.63883, saving model to best.model\n",
      "1s - loss: 0.6853 - acc: 0.7200 - val_loss: 0.6388 - val_acc: 0.7423\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6832 - acc: 0.7209 - val_loss: 0.6394 - val_acc: 0.7348\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.63883 to 0.63872, saving model to best.model\n",
      "0s - loss: 0.6853 - acc: 0.7217 - val_loss: 0.6387 - val_acc: 0.7361\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6853 - acc: 0.7199 - val_loss: 0.6393 - val_acc: 0.7369\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.63872 to 0.63760, saving model to best.model\n",
      "0s - loss: 0.6827 - acc: 0.7197 - val_loss: 0.6376 - val_acc: 0.7371\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6850 - acc: 0.7192 - val_loss: 0.6391 - val_acc: 0.7389\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63760 to 0.63627, saving model to best.model\n",
      "0s - loss: 0.6849 - acc: 0.7205 - val_loss: 0.6363 - val_acc: 0.7381\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6816 - acc: 0.7202 - val_loss: 0.6372 - val_acc: 0.7436\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6826 - acc: 0.7224 - val_loss: 0.6377 - val_acc: 0.7354\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.63627 to 0.63576, saving model to best.model\n",
      "1s - loss: 0.6778 - acc: 0.7232 - val_loss: 0.6358 - val_acc: 0.7368\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63576 to 0.63486, saving model to best.model\n",
      "1s - loss: 0.6784 - acc: 0.7235 - val_loss: 0.6349 - val_acc: 0.7382\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.63486 to 0.63275, saving model to best.model\n",
      "1s - loss: 0.6759 - acc: 0.7242 - val_loss: 0.6327 - val_acc: 0.7400\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.63275 to 0.63131, saving model to best.model\n",
      "1s - loss: 0.6784 - acc: 0.7219 - val_loss: 0.6313 - val_acc: 0.7400\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63131 to 0.62847, saving model to best.model\n",
      "1s - loss: 0.6784 - acc: 0.7235 - val_loss: 0.6285 - val_acc: 0.7448\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.62847 to 0.62840, saving model to best.model\n",
      "1s - loss: 0.6779 - acc: 0.7225 - val_loss: 0.6284 - val_acc: 0.7429\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.62840 to 0.62692, saving model to best.model\n",
      "1s - loss: 0.6756 - acc: 0.7253 - val_loss: 0.6269 - val_acc: 0.7451\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6751 - acc: 0.7248 - val_loss: 0.6287 - val_acc: 0.7410\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.62692 to 0.62679, saving model to best.model\n",
      "1s - loss: 0.6749 - acc: 0.7236 - val_loss: 0.6268 - val_acc: 0.7425\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6754 - acc: 0.7263 - val_loss: 0.6273 - val_acc: 0.7436\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62679 to 0.62593, saving model to best.model\n",
      "1s - loss: 0.6711 - acc: 0.7262 - val_loss: 0.6259 - val_acc: 0.7423\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6763 - acc: 0.7227 - val_loss: 0.6279 - val_acc: 0.7412\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62593 to 0.62592, saving model to best.model\n",
      "1s - loss: 0.6724 - acc: 0.7286 - val_loss: 0.6259 - val_acc: 0.7429\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.62592 to 0.62292, saving model to best.model\n",
      "1s - loss: 0.6723 - acc: 0.7237 - val_loss: 0.6229 - val_acc: 0.7496\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.62292 to 0.62174, saving model to best.model\n",
      "1s - loss: 0.6700 - acc: 0.7261 - val_loss: 0.6217 - val_acc: 0.7463\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6738 - acc: 0.7247 - val_loss: 0.6219 - val_acc: 0.7446\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6682 - acc: 0.7257 - val_loss: 0.6240 - val_acc: 0.7448\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6662 - acc: 0.7273 - val_loss: 0.6248 - val_acc: 0.7400\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6694 - acc: 0.7266 - val_loss: 0.6227 - val_acc: 0.7479\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6691 - acc: 0.7268 - val_loss: 0.6221 - val_acc: 0.7438\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6693 - acc: 0.7254 - val_loss: 0.6246 - val_acc: 0.7429\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6664 - acc: 0.7298 - val_loss: 0.6238 - val_acc: 0.7438\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6672 - acc: 0.7272 - val_loss: 0.6245 - val_acc: 0.7407\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62174 to 0.61736, saving model to best.model\n",
      "1s - loss: 0.6681 - acc: 0.7273 - val_loss: 0.6174 - val_acc: 0.7475\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6640 - acc: 0.7294 - val_loss: 0.6181 - val_acc: 0.7450\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6704 - acc: 0.7267 - val_loss: 0.6184 - val_acc: 0.7478\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6650 - acc: 0.7298 - val_loss: 0.6180 - val_acc: 0.7474\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.61736 to 0.61301, saving model to best.model\n",
      "1s - loss: 0.6645 - acc: 0.7286 - val_loss: 0.6130 - val_acc: 0.7512\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61301 to 0.61253, saving model to best.model\n",
      "1s - loss: 0.6609 - acc: 0.7319 - val_loss: 0.6125 - val_acc: 0.7478\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6663 - acc: 0.7277 - val_loss: 0.6137 - val_acc: 0.7479\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61253 to 0.61246, saving model to best.model\n",
      "1s - loss: 0.6599 - acc: 0.7297 - val_loss: 0.6125 - val_acc: 0.7527\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6622 - acc: 0.7295 - val_loss: 0.6162 - val_acc: 0.7453\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6615 - acc: 0.7288 - val_loss: 0.6159 - val_acc: 0.7471\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61246 to 0.61050, saving model to best.model\n",
      "0s - loss: 0.6605 - acc: 0.7304 - val_loss: 0.6105 - val_acc: 0.7507\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6609 - acc: 0.7309 - val_loss: 0.6129 - val_acc: 0.7474\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61050 to 0.60880, saving model to best.model\n",
      "0s - loss: 0.6552 - acc: 0.7316 - val_loss: 0.6088 - val_acc: 0.7511\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6582 - acc: 0.7315 - val_loss: 0.6104 - val_acc: 0.7484\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6587 - acc: 0.7296 - val_loss: 0.6103 - val_acc: 0.7502\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6578 - acc: 0.7334 - val_loss: 0.6099 - val_acc: 0.7493\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6569 - acc: 0.7319 - val_loss: 0.6104 - val_acc: 0.7480\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.60880 to 0.60824, saving model to best.model\n",
      "0s - loss: 0.6581 - acc: 0.7321 - val_loss: 0.6082 - val_acc: 0.7502\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6568 - acc: 0.7317 - val_loss: 0.6095 - val_acc: 0.7525\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6586 - acc: 0.7290 - val_loss: 0.6091 - val_acc: 0.7492\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.60824 to 0.60630, saving model to best.model\n",
      "0s - loss: 0.6572 - acc: 0.7304 - val_loss: 0.6063 - val_acc: 0.7508\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.60630 to 0.60468, saving model to best.model\n",
      "0s - loss: 0.6560 - acc: 0.7328 - val_loss: 0.6047 - val_acc: 0.7522\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6551 - acc: 0.7326 - val_loss: 0.6062 - val_acc: 0.7512\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6567 - acc: 0.7321 - val_loss: 0.6087 - val_acc: 0.7509\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6539 - acc: 0.7327 - val_loss: 0.6049 - val_acc: 0.7536\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6563 - acc: 0.7336 - val_loss: 0.6060 - val_acc: 0.7508\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.60468 to 0.60395, saving model to best.model\n",
      "1s - loss: 0.6544 - acc: 0.7338 - val_loss: 0.6039 - val_acc: 0.7532\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6531 - acc: 0.7330 - val_loss: 0.6042 - val_acc: 0.7515\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6525 - acc: 0.7325 - val_loss: 0.6049 - val_acc: 0.7499\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60395 to 0.60387, saving model to best.model\n",
      "1s - loss: 0.6521 - acc: 0.7342 - val_loss: 0.6039 - val_acc: 0.7519\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60387 to 0.59987, saving model to best.model\n",
      "1s - loss: 0.6497 - acc: 0.7352 - val_loss: 0.5999 - val_acc: 0.7569\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6513 - acc: 0.7343 - val_loss: 0.6020 - val_acc: 0.7521\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.59987 to 0.59680, saving model to best.model\n",
      "1s - loss: 0.6507 - acc: 0.7342 - val_loss: 0.5968 - val_acc: 0.7576\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6513 - acc: 0.7361 - val_loss: 0.5999 - val_acc: 0.7576\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6486 - acc: 0.7343 - val_loss: 0.5990 - val_acc: 0.7574\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6501 - acc: 0.7354 - val_loss: 0.6017 - val_acc: 0.7518\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6481 - acc: 0.7353 - val_loss: 0.5994 - val_acc: 0.7552\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6491 - acc: 0.7335 - val_loss: 0.5975 - val_acc: 0.7564\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6507 - acc: 0.7343 - val_loss: 0.5981 - val_acc: 0.7564\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6491 - acc: 0.7345 - val_loss: 0.5983 - val_acc: 0.7548\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6478 - acc: 0.7373 - val_loss: 0.6023 - val_acc: 0.7514\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.59680 to 0.59649, saving model to best.model\n",
      "1s - loss: 0.6463 - acc: 0.7350 - val_loss: 0.5965 - val_acc: 0.7560\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6448 - acc: 0.7377 - val_loss: 0.6003 - val_acc: 0.7550\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.59649 to 0.59622, saving model to best.model\n",
      "1s - loss: 0.6469 - acc: 0.7360 - val_loss: 0.5962 - val_acc: 0.7560\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.59622 to 0.59545, saving model to best.model\n",
      "1s - loss: 0.6444 - acc: 0.7381 - val_loss: 0.5955 - val_acc: 0.7559\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.59545 to 0.59542, saving model to best.model\n",
      "1s - loss: 0.6447 - acc: 0.7365 - val_loss: 0.5954 - val_acc: 0.7578\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6434 - acc: 0.7382 - val_loss: 0.5957 - val_acc: 0.7569\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.59542 to 0.59385, saving model to best.model\n",
      "1s - loss: 0.6458 - acc: 0.7373 - val_loss: 0.5939 - val_acc: 0.7594\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.59385 to 0.59318, saving model to best.model\n",
      "1s - loss: 0.6411 - acc: 0.7398 - val_loss: 0.5932 - val_acc: 0.7569\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83910, saving model to best.model\n",
      "1s - loss: 0.9215 - acc: 0.6236 - val_loss: 0.8391 - val_acc: 0.6624\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8561 - acc: 0.6609 - val_loss: 0.8416 - val_acc: 0.6624\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.83910 to 0.83791, saving model to best.model\n",
      "1s - loss: 0.8475 - acc: 0.6613 - val_loss: 0.8379 - val_acc: 0.6624\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83791 to 0.83702, saving model to best.model\n",
      "1s - loss: 0.8443 - acc: 0.6613 - val_loss: 0.8370 - val_acc: 0.6624\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83702 to 0.83022, saving model to best.model\n",
      "1s - loss: 0.8404 - acc: 0.6613 - val_loss: 0.8302 - val_acc: 0.6624\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83022 to 0.82556, saving model to best.model\n",
      "1s - loss: 0.8354 - acc: 0.6613 - val_loss: 0.8256 - val_acc: 0.6624\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82556 to 0.82185, saving model to best.model\n",
      "1s - loss: 0.8306 - acc: 0.6613 - val_loss: 0.8219 - val_acc: 0.6624\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82185 to 0.82073, saving model to best.model\n",
      "1s - loss: 0.8287 - acc: 0.6613 - val_loss: 0.8207 - val_acc: 0.6624\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82073 to 0.81958, saving model to best.model\n",
      "1s - loss: 0.8263 - acc: 0.6612 - val_loss: 0.8196 - val_acc: 0.6624\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81958 to 0.81838, saving model to best.model\n",
      "1s - loss: 0.8259 - acc: 0.6612 - val_loss: 0.8184 - val_acc: 0.6624\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81838 to 0.81807, saving model to best.model\n",
      "1s - loss: 0.8246 - acc: 0.6613 - val_loss: 0.8181 - val_acc: 0.6624\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8240 - acc: 0.6613 - val_loss: 0.8203 - val_acc: 0.6624\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81807 to 0.81788, saving model to best.model\n",
      "1s - loss: 0.8233 - acc: 0.6614 - val_loss: 0.8179 - val_acc: 0.6624\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81788 to 0.81567, saving model to best.model\n",
      "0s - loss: 0.8211 - acc: 0.6613 - val_loss: 0.8157 - val_acc: 0.6624\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8214 - acc: 0.6616 - val_loss: 0.8157 - val_acc: 0.6624\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81567 to 0.81385, saving model to best.model\n",
      "0s - loss: 0.8203 - acc: 0.6616 - val_loss: 0.8138 - val_acc: 0.6624\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8203 - acc: 0.6614 - val_loss: 0.8147 - val_acc: 0.6624\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81385 to 0.81259, saving model to best.model\n",
      "0s - loss: 0.8176 - acc: 0.6618 - val_loss: 0.8126 - val_acc: 0.6624\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81259 to 0.81181, saving model to best.model\n",
      "0s - loss: 0.8179 - acc: 0.6616 - val_loss: 0.8118 - val_acc: 0.6624\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81181 to 0.81035, saving model to best.model\n",
      "0s - loss: 0.8164 - acc: 0.6611 - val_loss: 0.8104 - val_acc: 0.6624\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.8155 - acc: 0.6622 - val_loss: 0.8111 - val_acc: 0.6624\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81035 to 0.81019, saving model to best.model\n",
      "0s - loss: 0.8158 - acc: 0.6629 - val_loss: 0.8102 - val_acc: 0.6623\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81019 to 0.80852, saving model to best.model\n",
      "0s - loss: 0.8150 - acc: 0.6632 - val_loss: 0.8085 - val_acc: 0.6629\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80852 to 0.80772, saving model to best.model\n",
      "0s - loss: 0.8134 - acc: 0.6624 - val_loss: 0.8077 - val_acc: 0.6633\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80772 to 0.80653, saving model to best.model\n",
      "0s - loss: 0.8123 - acc: 0.6642 - val_loss: 0.8065 - val_acc: 0.6673\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80653 to 0.80542, saving model to best.model\n",
      "0s - loss: 0.8118 - acc: 0.6649 - val_loss: 0.8054 - val_acc: 0.6692\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80542 to 0.80502, saving model to best.model\n",
      "1s - loss: 0.8124 - acc: 0.6638 - val_loss: 0.8050 - val_acc: 0.6699\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80502 to 0.80420, saving model to best.model\n",
      "1s - loss: 0.8105 - acc: 0.6657 - val_loss: 0.8042 - val_acc: 0.6657\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.8081 - acc: 0.6659 - val_loss: 0.8043 - val_acc: 0.6652\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80420 to 0.80109, saving model to best.model\n",
      "1s - loss: 0.8088 - acc: 0.6656 - val_loss: 0.8011 - val_acc: 0.6705\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80109 to 0.80048, saving model to best.model\n",
      "1s - loss: 0.8069 - acc: 0.6666 - val_loss: 0.8005 - val_acc: 0.6687\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80048 to 0.79847, saving model to best.model\n",
      "1s - loss: 0.8060 - acc: 0.6657 - val_loss: 0.7985 - val_acc: 0.6717\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79847 to 0.79664, saving model to best.model\n",
      "1s - loss: 0.8047 - acc: 0.6666 - val_loss: 0.7966 - val_acc: 0.6719\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79664 to 0.79488, saving model to best.model\n",
      "1s - loss: 0.8031 - acc: 0.6682 - val_loss: 0.7949 - val_acc: 0.6711\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79488 to 0.79242, saving model to best.model\n",
      "1s - loss: 0.8018 - acc: 0.6668 - val_loss: 0.7924 - val_acc: 0.6731\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "1s - loss: 0.8006 - acc: 0.6683 - val_loss: 0.7943 - val_acc: 0.6697\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79242 to 0.78914, saving model to best.model\n",
      "1s - loss: 0.7971 - acc: 0.6690 - val_loss: 0.7891 - val_acc: 0.6705\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78914 to 0.78698, saving model to best.model\n",
      "1s - loss: 0.7961 - acc: 0.6702 - val_loss: 0.7870 - val_acc: 0.6739\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78698 to 0.78443, saving model to best.model\n",
      "1s - loss: 0.7947 - acc: 0.6690 - val_loss: 0.7844 - val_acc: 0.6751\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78443 to 0.78139, saving model to best.model\n",
      "1s - loss: 0.7934 - acc: 0.6704 - val_loss: 0.7814 - val_acc: 0.6751\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78139 to 0.78112, saving model to best.model\n",
      "1s - loss: 0.7911 - acc: 0.6726 - val_loss: 0.7811 - val_acc: 0.6740\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78112 to 0.77675, saving model to best.model\n",
      "1s - loss: 0.7883 - acc: 0.6746 - val_loss: 0.7768 - val_acc: 0.6778\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77675 to 0.77460, saving model to best.model\n",
      "1s - loss: 0.7887 - acc: 0.6739 - val_loss: 0.7746 - val_acc: 0.6800\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77460 to 0.77356, saving model to best.model\n",
      "1s - loss: 0.7863 - acc: 0.6726 - val_loss: 0.7736 - val_acc: 0.6767\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77356 to 0.77140, saving model to best.model\n",
      "1s - loss: 0.7854 - acc: 0.6746 - val_loss: 0.7714 - val_acc: 0.6793\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77140 to 0.76908, saving model to best.model\n",
      "1s - loss: 0.7843 - acc: 0.6759 - val_loss: 0.7691 - val_acc: 0.6817\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76908 to 0.76483, saving model to best.model\n",
      "1s - loss: 0.7815 - acc: 0.6758 - val_loss: 0.7648 - val_acc: 0.6826\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76483 to 0.76317, saving model to best.model\n",
      "1s - loss: 0.7805 - acc: 0.6765 - val_loss: 0.7632 - val_acc: 0.6831\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76317 to 0.76104, saving model to best.model\n",
      "1s - loss: 0.7788 - acc: 0.6782 - val_loss: 0.7610 - val_acc: 0.6834\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76104 to 0.75907, saving model to best.model\n",
      "1s - loss: 0.7775 - acc: 0.6795 - val_loss: 0.7591 - val_acc: 0.6848\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75907 to 0.75740, saving model to best.model\n",
      "1s - loss: 0.7746 - acc: 0.6799 - val_loss: 0.7574 - val_acc: 0.6842\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75740 to 0.75353, saving model to best.model\n",
      "1s - loss: 0.7734 - acc: 0.6800 - val_loss: 0.7535 - val_acc: 0.6898\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75353 to 0.75003, saving model to best.model\n",
      "1s - loss: 0.7709 - acc: 0.6800 - val_loss: 0.7500 - val_acc: 0.6867\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75003 to 0.74747, saving model to best.model\n",
      "1s - loss: 0.7677 - acc: 0.6817 - val_loss: 0.7475 - val_acc: 0.6936\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74747 to 0.74440, saving model to best.model\n",
      "1s - loss: 0.7675 - acc: 0.6819 - val_loss: 0.7444 - val_acc: 0.6933\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74440 to 0.74297, saving model to best.model\n",
      "1s - loss: 0.7667 - acc: 0.6814 - val_loss: 0.7430 - val_acc: 0.6937\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74297 to 0.73779, saving model to best.model\n",
      "1s - loss: 0.7627 - acc: 0.6847 - val_loss: 0.7378 - val_acc: 0.6944\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "1s - loss: 0.7634 - acc: 0.6849 - val_loss: 0.7393 - val_acc: 0.6946\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73779 to 0.73730, saving model to best.model\n",
      "1s - loss: 0.7612 - acc: 0.6866 - val_loss: 0.7373 - val_acc: 0.6950\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73730 to 0.73512, saving model to best.model\n",
      "1s - loss: 0.7577 - acc: 0.6853 - val_loss: 0.7351 - val_acc: 0.6984\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73512 to 0.72914, saving model to best.model\n",
      "1s - loss: 0.7554 - acc: 0.6885 - val_loss: 0.7291 - val_acc: 0.7009\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72914 to 0.72587, saving model to best.model\n",
      "1s - loss: 0.7526 - acc: 0.6903 - val_loss: 0.7259 - val_acc: 0.7020\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.72587 to 0.72442, saving model to best.model\n",
      "1s - loss: 0.7521 - acc: 0.6893 - val_loss: 0.7244 - val_acc: 0.7073\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72442 to 0.71896, saving model to best.model\n",
      "1s - loss: 0.7492 - acc: 0.6917 - val_loss: 0.7190 - val_acc: 0.7096\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.71896 to 0.71884, saving model to best.model\n",
      "1s - loss: 0.7485 - acc: 0.6924 - val_loss: 0.7188 - val_acc: 0.7036\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71884 to 0.71283, saving model to best.model\n",
      "1s - loss: 0.7464 - acc: 0.6914 - val_loss: 0.7128 - val_acc: 0.7064\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.7423 - acc: 0.6942 - val_loss: 0.7130 - val_acc: 0.7115\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71283 to 0.70937, saving model to best.model\n",
      "1s - loss: 0.7424 - acc: 0.6952 - val_loss: 0.7094 - val_acc: 0.7091\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "1s - loss: 0.7436 - acc: 0.6955 - val_loss: 0.7106 - val_acc: 0.7156\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70937 to 0.70695, saving model to best.model\n",
      "1s - loss: 0.7416 - acc: 0.6953 - val_loss: 0.7069 - val_acc: 0.7085\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70695 to 0.70385, saving model to best.model\n",
      "1s - loss: 0.7374 - acc: 0.6979 - val_loss: 0.7039 - val_acc: 0.7133\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70385 to 0.70132, saving model to best.model\n",
      "1s - loss: 0.7348 - acc: 0.6965 - val_loss: 0.7013 - val_acc: 0.7183\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70132 to 0.69777, saving model to best.model\n",
      "0s - loss: 0.7324 - acc: 0.6988 - val_loss: 0.6978 - val_acc: 0.7205\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.69777 to 0.69736, saving model to best.model\n",
      "0s - loss: 0.7338 - acc: 0.6957 - val_loss: 0.6974 - val_acc: 0.7218\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69736 to 0.69611, saving model to best.model\n",
      "0s - loss: 0.7320 - acc: 0.7008 - val_loss: 0.6961 - val_acc: 0.7200\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69611 to 0.68937, saving model to best.model\n",
      "0s - loss: 0.7274 - acc: 0.7020 - val_loss: 0.6894 - val_acc: 0.7226\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.68937 to 0.68904, saving model to best.model\n",
      "0s - loss: 0.7280 - acc: 0.7021 - val_loss: 0.6890 - val_acc: 0.7244\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.7285 - acc: 0.7026 - val_loss: 0.6902 - val_acc: 0.7255\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.68904 to 0.68704, saving model to best.model\n",
      "1s - loss: 0.7255 - acc: 0.7030 - val_loss: 0.6870 - val_acc: 0.7256\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7244 - acc: 0.7046 - val_loss: 0.6893 - val_acc: 0.7299\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68704 to 0.68577, saving model to best.model\n",
      "1s - loss: 0.7220 - acc: 0.7043 - val_loss: 0.6858 - val_acc: 0.7233\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.68577 to 0.67825, saving model to best.model\n",
      "1s - loss: 0.7203 - acc: 0.7047 - val_loss: 0.6783 - val_acc: 0.7295\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "1s - loss: 0.7202 - acc: 0.7058 - val_loss: 0.6821 - val_acc: 0.7260\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7206 - acc: 0.7065 - val_loss: 0.6787 - val_acc: 0.7244\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.67825 to 0.67239, saving model to best.model\n",
      "1s - loss: 0.7155 - acc: 0.7107 - val_loss: 0.6724 - val_acc: 0.7326\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7149 - acc: 0.7086 - val_loss: 0.6739 - val_acc: 0.7261\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "1s - loss: 0.7141 - acc: 0.7072 - val_loss: 0.6754 - val_acc: 0.7314\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7164 - acc: 0.7078 - val_loss: 0.6744 - val_acc: 0.7281\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67239 to 0.66885, saving model to best.model\n",
      "1s - loss: 0.7116 - acc: 0.7103 - val_loss: 0.6688 - val_acc: 0.7334\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.66885 to 0.66773, saving model to best.model\n",
      "1s - loss: 0.7092 - acc: 0.7117 - val_loss: 0.6677 - val_acc: 0.7317\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7097 - acc: 0.7119 - val_loss: 0.6679 - val_acc: 0.7328\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "1s - loss: 0.7078 - acc: 0.7117 - val_loss: 0.6681 - val_acc: 0.7357\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.7067 - acc: 0.7119 - val_loss: 0.6685 - val_acc: 0.7259\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66773 to 0.66411, saving model to best.model\n",
      "1s - loss: 0.7057 - acc: 0.7111 - val_loss: 0.6641 - val_acc: 0.7316\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66411 to 0.66027, saving model to best.model\n",
      "1s - loss: 0.7057 - acc: 0.7101 - val_loss: 0.6603 - val_acc: 0.7356\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7053 - acc: 0.7122 - val_loss: 0.6614 - val_acc: 0.7352\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66027 to 0.65961, saving model to best.model\n",
      "1s - loss: 0.7021 - acc: 0.7141 - val_loss: 0.6596 - val_acc: 0.7364\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7050 - acc: 0.7103 - val_loss: 0.6642 - val_acc: 0.7344\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.65961 to 0.65802, saving model to best.model\n",
      "1s - loss: 0.7000 - acc: 0.7162 - val_loss: 0.6580 - val_acc: 0.7328\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.65802 to 0.65732, saving model to best.model\n",
      "1s - loss: 0.6999 - acc: 0.7137 - val_loss: 0.6573 - val_acc: 0.7351\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.65732 to 0.65618, saving model to best.model\n",
      "1s - loss: 0.6976 - acc: 0.7167 - val_loss: 0.6562 - val_acc: 0.7375\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.6987 - acc: 0.7144 - val_loss: 0.6580 - val_acc: 0.7351\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.65618 to 0.65425, saving model to best.model\n",
      "0s - loss: 0.6971 - acc: 0.7135 - val_loss: 0.6542 - val_acc: 0.7357\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.65425 to 0.65234, saving model to best.model\n",
      "0s - loss: 0.6944 - acc: 0.7179 - val_loss: 0.6523 - val_acc: 0.7362\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.6957 - acc: 0.7171 - val_loss: 0.6529 - val_acc: 0.7350\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65234 to 0.65052, saving model to best.model\n",
      "1s - loss: 0.6925 - acc: 0.7195 - val_loss: 0.6505 - val_acc: 0.7391\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65052 to 0.64949, saving model to best.model\n",
      "1s - loss: 0.6901 - acc: 0.7190 - val_loss: 0.6495 - val_acc: 0.7367\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6944 - acc: 0.7170 - val_loss: 0.6525 - val_acc: 0.7372\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64949 to 0.64707, saving model to best.model\n",
      "1s - loss: 0.6888 - acc: 0.7203 - val_loss: 0.6471 - val_acc: 0.7392\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6912 - acc: 0.7169 - val_loss: 0.6486 - val_acc: 0.7399\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6898 - acc: 0.7205 - val_loss: 0.6477 - val_acc: 0.7379\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6893 - acc: 0.7192 - val_loss: 0.6497 - val_acc: 0.7382\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.64707 to 0.64642, saving model to best.model\n",
      "1s - loss: 0.6879 - acc: 0.7205 - val_loss: 0.6464 - val_acc: 0.7388\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.64642 to 0.64567, saving model to best.model\n",
      "1s - loss: 0.6890 - acc: 0.7179 - val_loss: 0.6457 - val_acc: 0.7384\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.64567 to 0.64352, saving model to best.model\n",
      "1s - loss: 0.6859 - acc: 0.7191 - val_loss: 0.6435 - val_acc: 0.7379\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.64352 to 0.64047, saving model to best.model\n",
      "1s - loss: 0.6860 - acc: 0.7210 - val_loss: 0.6405 - val_acc: 0.7402\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6846 - acc: 0.7233 - val_loss: 0.6417 - val_acc: 0.7397\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6842 - acc: 0.7224 - val_loss: 0.6431 - val_acc: 0.7409\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64047 to 0.64036, saving model to best.model\n",
      "1s - loss: 0.6816 - acc: 0.7223 - val_loss: 0.6404 - val_acc: 0.7407\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6828 - acc: 0.7228 - val_loss: 0.6423 - val_acc: 0.7404\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64036 to 0.63865, saving model to best.model\n",
      "1s - loss: 0.6820 - acc: 0.7230 - val_loss: 0.6386 - val_acc: 0.7398\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6859 - acc: 0.7222 - val_loss: 0.6394 - val_acc: 0.7412\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6797 - acc: 0.7217 - val_loss: 0.6403 - val_acc: 0.7411\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.63865 to 0.63644, saving model to best.model\n",
      "0s - loss: 0.6795 - acc: 0.7237 - val_loss: 0.6364 - val_acc: 0.7404\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "0s - loss: 0.6821 - acc: 0.7235 - val_loss: 0.6385 - val_acc: 0.7409\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63644 to 0.63623, saving model to best.model\n",
      "0s - loss: 0.6763 - acc: 0.7254 - val_loss: 0.6362 - val_acc: 0.7425\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.6794 - acc: 0.7238 - val_loss: 0.6367 - val_acc: 0.7429\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.63623 to 0.63602, saving model to best.model\n",
      "0s - loss: 0.6775 - acc: 0.7254 - val_loss: 0.6360 - val_acc: 0.7417\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6763 - acc: 0.7243 - val_loss: 0.6368 - val_acc: 0.7406\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63602 to 0.63300, saving model to best.model\n",
      "1s - loss: 0.6775 - acc: 0.7251 - val_loss: 0.6330 - val_acc: 0.7422\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.63300 to 0.63120, saving model to best.model\n",
      "1s - loss: 0.6754 - acc: 0.7262 - val_loss: 0.6312 - val_acc: 0.7441\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.63120 to 0.63109, saving model to best.model\n",
      "0s - loss: 0.6752 - acc: 0.7253 - val_loss: 0.6311 - val_acc: 0.7422\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6766 - acc: 0.7241 - val_loss: 0.6352 - val_acc: 0.7409\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6739 - acc: 0.7256 - val_loss: 0.6341 - val_acc: 0.7412\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6746 - acc: 0.7237 - val_loss: 0.6341 - val_acc: 0.7405\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6731 - acc: 0.7269 - val_loss: 0.6312 - val_acc: 0.7452\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63109 to 0.62916, saving model to best.model\n",
      "1s - loss: 0.6722 - acc: 0.7282 - val_loss: 0.6292 - val_acc: 0.7431\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62916 to 0.62870, saving model to best.model\n",
      "1s - loss: 0.6722 - acc: 0.7261 - val_loss: 0.6287 - val_acc: 0.7443\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62870 to 0.62849, saving model to best.model\n",
      "1s - loss: 0.6698 - acc: 0.7261 - val_loss: 0.6285 - val_acc: 0.7425\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.62849 to 0.62694, saving model to best.model\n",
      "1s - loss: 0.6679 - acc: 0.7284 - val_loss: 0.6269 - val_acc: 0.7444\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6718 - acc: 0.7266 - val_loss: 0.6322 - val_acc: 0.7405\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.62694 to 0.62601, saving model to best.model\n",
      "1s - loss: 0.6673 - acc: 0.7297 - val_loss: 0.6260 - val_acc: 0.7436\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.62601 to 0.62451, saving model to best.model\n",
      "1s - loss: 0.6665 - acc: 0.7285 - val_loss: 0.6245 - val_acc: 0.7458\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62451 to 0.62365, saving model to best.model\n",
      "1s - loss: 0.6650 - acc: 0.7295 - val_loss: 0.6237 - val_acc: 0.7463\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.62365 to 0.62282, saving model to best.model\n",
      "0s - loss: 0.6645 - acc: 0.7292 - val_loss: 0.6228 - val_acc: 0.7456\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6625 - acc: 0.7288 - val_loss: 0.6250 - val_acc: 0.7459\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.62282 to 0.62143, saving model to best.model\n",
      "1s - loss: 0.6609 - acc: 0.7312 - val_loss: 0.6214 - val_acc: 0.7459\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6625 - acc: 0.7317 - val_loss: 0.6232 - val_acc: 0.7453\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6647 - acc: 0.7294 - val_loss: 0.6236 - val_acc: 0.7456\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7296 - val_loss: 0.6218 - val_acc: 0.7456\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62143 to 0.62074, saving model to best.model\n",
      "1s - loss: 0.6641 - acc: 0.7310 - val_loss: 0.6207 - val_acc: 0.7454\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6613 - acc: 0.7316 - val_loss: 0.6220 - val_acc: 0.7459\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62074 to 0.62010, saving model to best.model\n",
      "1s - loss: 0.6611 - acc: 0.7314 - val_loss: 0.6201 - val_acc: 0.7475\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62010 to 0.61992, saving model to best.model\n",
      "1s - loss: 0.6633 - acc: 0.7300 - val_loss: 0.6199 - val_acc: 0.7474\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.61992 to 0.61801, saving model to best.model\n",
      "1s - loss: 0.6593 - acc: 0.7328 - val_loss: 0.6180 - val_acc: 0.7482\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.61801 to 0.61568, saving model to best.model\n",
      "1s - loss: 0.6565 - acc: 0.7320 - val_loss: 0.6157 - val_acc: 0.7498\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6625 - acc: 0.7311 - val_loss: 0.6186 - val_acc: 0.7480\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6580 - acc: 0.7307 - val_loss: 0.6167 - val_acc: 0.7496\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6602 - acc: 0.7298 - val_loss: 0.6182 - val_acc: 0.7492\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6589 - acc: 0.7311 - val_loss: 0.6160 - val_acc: 0.7499\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6593 - acc: 0.7328 - val_loss: 0.6163 - val_acc: 0.7498\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61568 to 0.61333, saving model to best.model\n",
      "1s - loss: 0.6558 - acc: 0.7338 - val_loss: 0.6133 - val_acc: 0.7499\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6605 - acc: 0.7297 - val_loss: 0.6166 - val_acc: 0.7488\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6562 - acc: 0.7338 - val_loss: 0.6134 - val_acc: 0.7496\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.61333 to 0.61038, saving model to best.model\n",
      "1s - loss: 0.6529 - acc: 0.7354 - val_loss: 0.6104 - val_acc: 0.7515\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7317 - val_loss: 0.6135 - val_acc: 0.7486\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61038 to 0.61017, saving model to best.model\n",
      "1s - loss: 0.6546 - acc: 0.7338 - val_loss: 0.6102 - val_acc: 0.7526\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6517 - acc: 0.7340 - val_loss: 0.6105 - val_acc: 0.7488\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6514 - acc: 0.7346 - val_loss: 0.6111 - val_acc: 0.7486\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6524 - acc: 0.7372 - val_loss: 0.6115 - val_acc: 0.7533\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6507 - acc: 0.7377 - val_loss: 0.6104 - val_acc: 0.7508\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61017 to 0.61005, saving model to best.model\n",
      "0s - loss: 0.6507 - acc: 0.7364 - val_loss: 0.6101 - val_acc: 0.7529\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6504 - acc: 0.7378 - val_loss: 0.6108 - val_acc: 0.7473\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6512 - acc: 0.7341 - val_loss: 0.6102 - val_acc: 0.7536\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61005 to 0.60921, saving model to best.model\n",
      "0s - loss: 0.6490 - acc: 0.7369 - val_loss: 0.6092 - val_acc: 0.7533\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.60921 to 0.60738, saving model to best.model\n",
      "0s - loss: 0.6485 - acc: 0.7358 - val_loss: 0.6074 - val_acc: 0.7528\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.60738 to 0.60474, saving model to best.model\n",
      "0s - loss: 0.6513 - acc: 0.7376 - val_loss: 0.6047 - val_acc: 0.7570\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6473 - acc: 0.7367 - val_loss: 0.6048 - val_acc: 0.7557\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6508 - acc: 0.7359 - val_loss: 0.6102 - val_acc: 0.7507\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6495 - acc: 0.7367 - val_loss: 0.6082 - val_acc: 0.7501\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6483 - acc: 0.7368 - val_loss: 0.6054 - val_acc: 0.7553\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6475 - acc: 0.7366 - val_loss: 0.6067 - val_acc: 0.7530\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60474 to 0.60251, saving model to best.model\n",
      "1s - loss: 0.6462 - acc: 0.7369 - val_loss: 0.6025 - val_acc: 0.7560\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6462 - acc: 0.7350 - val_loss: 0.6061 - val_acc: 0.7514\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6453 - acc: 0.7378 - val_loss: 0.6029 - val_acc: 0.7554\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6457 - acc: 0.7358 - val_loss: 0.6052 - val_acc: 0.7549\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6475 - acc: 0.7367 - val_loss: 0.6063 - val_acc: 0.7540\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6429 - acc: 0.7387 - val_loss: 0.6025 - val_acc: 0.7564\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60251 to 0.60144, saving model to best.model\n",
      "0s - loss: 0.6443 - acc: 0.7357 - val_loss: 0.6014 - val_acc: 0.7552\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6446 - acc: 0.7373 - val_loss: 0.6034 - val_acc: 0.7580\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60144 to 0.59760, saving model to best.model\n",
      "0s - loss: 0.6407 - acc: 0.7403 - val_loss: 0.5976 - val_acc: 0.7594\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6434 - acc: 0.7383 - val_loss: 0.6000 - val_acc: 0.7578\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6412 - acc: 0.7385 - val_loss: 0.5996 - val_acc: 0.7584\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6404 - acc: 0.7395 - val_loss: 0.6012 - val_acc: 0.7605\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6429 - acc: 0.7386 - val_loss: 0.5984 - val_acc: 0.7590\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6399 - acc: 0.7379 - val_loss: 0.6022 - val_acc: 0.7585\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6397 - acc: 0.7407 - val_loss: 0.5998 - val_acc: 0.7594\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6420 - acc: 0.7398 - val_loss: 0.5981 - val_acc: 0.7571\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6421 - acc: 0.7405 - val_loss: 0.5976 - val_acc: 0.7585\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6379 - acc: 0.7404 - val_loss: 0.5981 - val_acc: 0.7582\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.86184, saving model to best.model\n",
      "1s - loss: 0.9337 - acc: 0.6179 - val_loss: 0.8618 - val_acc: 0.6502\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.86184 to 0.85625, saving model to best.model\n",
      "1s - loss: 0.8673 - acc: 0.6550 - val_loss: 0.8563 - val_acc: 0.6502\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.85625 to 0.85618, saving model to best.model\n",
      "1s - loss: 0.8583 - acc: 0.6570 - val_loss: 0.8562 - val_acc: 0.6502\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.85618 to 0.85294, saving model to best.model\n",
      "1s - loss: 0.8547 - acc: 0.6570 - val_loss: 0.8529 - val_acc: 0.6502\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.85294 to 0.84869, saving model to best.model\n",
      "0s - loss: 0.8489 - acc: 0.6570 - val_loss: 0.8487 - val_acc: 0.6502\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.84869 to 0.84275, saving model to best.model\n",
      "1s - loss: 0.8441 - acc: 0.6570 - val_loss: 0.8428 - val_acc: 0.6502\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.84275 to 0.83929, saving model to best.model\n",
      "1s - loss: 0.8398 - acc: 0.6570 - val_loss: 0.8393 - val_acc: 0.6502\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.83929 to 0.83802, saving model to best.model\n",
      "1s - loss: 0.8377 - acc: 0.6570 - val_loss: 0.8380 - val_acc: 0.6502\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.83802 to 0.83674, saving model to best.model\n",
      "1s - loss: 0.8358 - acc: 0.6570 - val_loss: 0.8367 - val_acc: 0.6502\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.83674 to 0.83576, saving model to best.model\n",
      "1s - loss: 0.8341 - acc: 0.6570 - val_loss: 0.8358 - val_acc: 0.6502\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8323 - acc: 0.6570 - val_loss: 0.8359 - val_acc: 0.6502\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.83576 to 0.83486, saving model to best.model\n",
      "1s - loss: 0.8312 - acc: 0.6569 - val_loss: 0.8349 - val_acc: 0.6502\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.83486 to 0.83439, saving model to best.model\n",
      "1s - loss: 0.8322 - acc: 0.6571 - val_loss: 0.8344 - val_acc: 0.6502\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8301 - acc: 0.6573 - val_loss: 0.8347 - val_acc: 0.6502\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.83439 to 0.83354, saving model to best.model\n",
      "1s - loss: 0.8309 - acc: 0.6569 - val_loss: 0.8335 - val_acc: 0.6502\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.83354 to 0.83328, saving model to best.model\n",
      "1s - loss: 0.8297 - acc: 0.6572 - val_loss: 0.8333 - val_acc: 0.6502\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.83328 to 0.83228, saving model to best.model\n",
      "1s - loss: 0.8283 - acc: 0.6568 - val_loss: 0.8323 - val_acc: 0.6502\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "1s - loss: 0.8285 - acc: 0.6574 - val_loss: 0.8336 - val_acc: 0.6502\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.83228 to 0.83184, saving model to best.model\n",
      "1s - loss: 0.8272 - acc: 0.6575 - val_loss: 0.8318 - val_acc: 0.6502\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.83184 to 0.83172, saving model to best.model\n",
      "1s - loss: 0.8263 - acc: 0.6578 - val_loss: 0.8317 - val_acc: 0.6502\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.83172 to 0.82980, saving model to best.model\n",
      "1s - loss: 0.8264 - acc: 0.6575 - val_loss: 0.8298 - val_acc: 0.6502\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82980 to 0.82867, saving model to best.model\n",
      "0s - loss: 0.8237 - acc: 0.6576 - val_loss: 0.8287 - val_acc: 0.6504\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "1s - loss: 0.8242 - acc: 0.6579 - val_loss: 0.8288 - val_acc: 0.6504\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.82867 to 0.82828, saving model to best.model\n",
      "1s - loss: 0.8227 - acc: 0.6589 - val_loss: 0.8283 - val_acc: 0.6505\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.82828 to 0.82697, saving model to best.model\n",
      "0s - loss: 0.8229 - acc: 0.6587 - val_loss: 0.8270 - val_acc: 0.6512\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.82697 to 0.82644, saving model to best.model\n",
      "1s - loss: 0.8217 - acc: 0.6593 - val_loss: 0.8264 - val_acc: 0.6525\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.82644 to 0.82419, saving model to best.model\n",
      "1s - loss: 0.8210 - acc: 0.6613 - val_loss: 0.8242 - val_acc: 0.6546\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.82419 to 0.82387, saving model to best.model\n",
      "1s - loss: 0.8204 - acc: 0.6603 - val_loss: 0.8239 - val_acc: 0.6525\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.82387 to 0.82224, saving model to best.model\n",
      "1s - loss: 0.8181 - acc: 0.6609 - val_loss: 0.8222 - val_acc: 0.6580\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.82224 to 0.82163, saving model to best.model\n",
      "1s - loss: 0.8187 - acc: 0.6605 - val_loss: 0.8216 - val_acc: 0.6547\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.82163 to 0.82013, saving model to best.model\n",
      "1s - loss: 0.8167 - acc: 0.6621 - val_loss: 0.8201 - val_acc: 0.6552\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.82013 to 0.81921, saving model to best.model\n",
      "1s - loss: 0.8161 - acc: 0.6616 - val_loss: 0.8192 - val_acc: 0.6544\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.81921 to 0.81739, saving model to best.model\n",
      "1s - loss: 0.8146 - acc: 0.6619 - val_loss: 0.8174 - val_acc: 0.6583\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.81739 to 0.81637, saving model to best.model\n",
      "1s - loss: 0.8145 - acc: 0.6620 - val_loss: 0.8164 - val_acc: 0.6563\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.81637 to 0.81422, saving model to best.model\n",
      "1s - loss: 0.8117 - acc: 0.6628 - val_loss: 0.8142 - val_acc: 0.6583\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.81422 to 0.81294, saving model to best.model\n",
      "1s - loss: 0.8113 - acc: 0.6631 - val_loss: 0.8129 - val_acc: 0.6570\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.81294 to 0.81128, saving model to best.model\n",
      "1s - loss: 0.8086 - acc: 0.6639 - val_loss: 0.8113 - val_acc: 0.6560\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.81128 to 0.80802, saving model to best.model\n",
      "1s - loss: 0.8074 - acc: 0.6647 - val_loss: 0.8080 - val_acc: 0.6598\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.80802 to 0.80564, saving model to best.model\n",
      "1s - loss: 0.8060 - acc: 0.6638 - val_loss: 0.8056 - val_acc: 0.6614\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.80564 to 0.80341, saving model to best.model\n",
      "0s - loss: 0.8036 - acc: 0.6655 - val_loss: 0.8034 - val_acc: 0.6602\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.80341 to 0.80248, saving model to best.model\n",
      "1s - loss: 0.8026 - acc: 0.6651 - val_loss: 0.8025 - val_acc: 0.6585\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.80248 to 0.79907, saving model to best.model\n",
      "1s - loss: 0.8005 - acc: 0.6645 - val_loss: 0.7991 - val_acc: 0.6610\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.79907 to 0.79615, saving model to best.model\n",
      "1s - loss: 0.7985 - acc: 0.6671 - val_loss: 0.7961 - val_acc: 0.6660\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.79615 to 0.79476, saving model to best.model\n",
      "1s - loss: 0.7970 - acc: 0.6653 - val_loss: 0.7948 - val_acc: 0.6655\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.79476 to 0.79180, saving model to best.model\n",
      "1s - loss: 0.7958 - acc: 0.6683 - val_loss: 0.7918 - val_acc: 0.6666\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.79180 to 0.78931, saving model to best.model\n",
      "0s - loss: 0.7940 - acc: 0.6700 - val_loss: 0.7893 - val_acc: 0.6681\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.78931 to 0.78691, saving model to best.model\n",
      "0s - loss: 0.7915 - acc: 0.6708 - val_loss: 0.7869 - val_acc: 0.6703\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.78691 to 0.78435, saving model to best.model\n",
      "0s - loss: 0.7912 - acc: 0.6714 - val_loss: 0.7843 - val_acc: 0.6704\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.78435 to 0.77991, saving model to best.model\n",
      "0s - loss: 0.7873 - acc: 0.6725 - val_loss: 0.7799 - val_acc: 0.6710\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77991 to 0.77883, saving model to best.model\n",
      "0s - loss: 0.7867 - acc: 0.6701 - val_loss: 0.7788 - val_acc: 0.6719\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77883 to 0.77730, saving model to best.model\n",
      "0s - loss: 0.7850 - acc: 0.6716 - val_loss: 0.7773 - val_acc: 0.6698\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.77730 to 0.77461, saving model to best.model\n",
      "0s - loss: 0.7825 - acc: 0.6748 - val_loss: 0.7746 - val_acc: 0.6724\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.77461 to 0.77282, saving model to best.model\n",
      "0s - loss: 0.7813 - acc: 0.6744 - val_loss: 0.7728 - val_acc: 0.6735\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.77282 to 0.76739, saving model to best.model\n",
      "0s - loss: 0.7783 - acc: 0.6765 - val_loss: 0.7674 - val_acc: 0.6822\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76739 to 0.76556, saving model to best.model\n",
      "0s - loss: 0.7760 - acc: 0.6787 - val_loss: 0.7656 - val_acc: 0.6817\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76556 to 0.76298, saving model to best.model\n",
      "0s - loss: 0.7745 - acc: 0.6792 - val_loss: 0.7630 - val_acc: 0.6810\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.76298 to 0.76283, saving model to best.model\n",
      "0s - loss: 0.7743 - acc: 0.6772 - val_loss: 0.7628 - val_acc: 0.6779\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.76283 to 0.76157, saving model to best.model\n",
      "0s - loss: 0.7709 - acc: 0.6798 - val_loss: 0.7616 - val_acc: 0.6762\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.76157 to 0.75829, saving model to best.model\n",
      "0s - loss: 0.7700 - acc: 0.6813 - val_loss: 0.7583 - val_acc: 0.6794\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.75829 to 0.75457, saving model to best.model\n",
      "0s - loss: 0.7654 - acc: 0.6841 - val_loss: 0.7546 - val_acc: 0.6841\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.75457 to 0.75386, saving model to best.model\n",
      "1s - loss: 0.7680 - acc: 0.6801 - val_loss: 0.7539 - val_acc: 0.6824\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.75386 to 0.74992, saving model to best.model\n",
      "1s - loss: 0.7649 - acc: 0.6825 - val_loss: 0.7499 - val_acc: 0.6856\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74992 to 0.74672, saving model to best.model\n",
      "1s - loss: 0.7599 - acc: 0.6873 - val_loss: 0.7467 - val_acc: 0.6890\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.74672 to 0.74365, saving model to best.model\n",
      "1s - loss: 0.7599 - acc: 0.6864 - val_loss: 0.7436 - val_acc: 0.6934\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.74365 to 0.74334, saving model to best.model\n",
      "1s - loss: 0.7576 - acc: 0.6863 - val_loss: 0.7433 - val_acc: 0.6907\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.74334 to 0.73866, saving model to best.model\n",
      "1s - loss: 0.7568 - acc: 0.6871 - val_loss: 0.7387 - val_acc: 0.6948\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73866 to 0.73687, saving model to best.model\n",
      "1s - loss: 0.7550 - acc: 0.6866 - val_loss: 0.7369 - val_acc: 0.6951\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73687 to 0.73408, saving model to best.model\n",
      "1s - loss: 0.7535 - acc: 0.6884 - val_loss: 0.7341 - val_acc: 0.6965\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.73408 to 0.73199, saving model to best.model\n",
      "1s - loss: 0.7517 - acc: 0.6892 - val_loss: 0.7320 - val_acc: 0.6966\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "1s - loss: 0.7497 - acc: 0.6904 - val_loss: 0.7326 - val_acc: 0.6953\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.73199 to 0.73005, saving model to best.model\n",
      "1s - loss: 0.7501 - acc: 0.6916 - val_loss: 0.7300 - val_acc: 0.6970\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.73005 to 0.72665, saving model to best.model\n",
      "1s - loss: 0.7454 - acc: 0.6932 - val_loss: 0.7266 - val_acc: 0.7011\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.72665 to 0.72213, saving model to best.model\n",
      "1s - loss: 0.7455 - acc: 0.6916 - val_loss: 0.7221 - val_acc: 0.6992\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.7445 - acc: 0.6932 - val_loss: 0.7234 - val_acc: 0.7015\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.7421 - acc: 0.6937 - val_loss: 0.7245 - val_acc: 0.6991\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.72213 to 0.71810, saving model to best.model\n",
      "0s - loss: 0.7402 - acc: 0.6945 - val_loss: 0.7181 - val_acc: 0.7019\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.7378 - acc: 0.6970 - val_loss: 0.7186 - val_acc: 0.7006\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.7378 - acc: 0.6970 - val_loss: 0.7186 - val_acc: 0.7001\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.71810 to 0.71377, saving model to best.model\n",
      "0s - loss: 0.7384 - acc: 0.6967 - val_loss: 0.7138 - val_acc: 0.7037\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.71377 to 0.70933, saving model to best.model\n",
      "0s - loss: 0.7351 - acc: 0.7000 - val_loss: 0.7093 - val_acc: 0.7070\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.7323 - acc: 0.6981 - val_loss: 0.7105 - val_acc: 0.7054\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.70933 to 0.70836, saving model to best.model\n",
      "0s - loss: 0.7327 - acc: 0.6986 - val_loss: 0.7084 - val_acc: 0.7084\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.70836 to 0.70445, saving model to best.model\n",
      "0s - loss: 0.7309 - acc: 0.7013 - val_loss: 0.7045 - val_acc: 0.7083\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.7298 - acc: 0.7001 - val_loss: 0.7069 - val_acc: 0.7077\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7267 - acc: 0.7039 - val_loss: 0.7057 - val_acc: 0.7060\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.7279 - acc: 0.7020 - val_loss: 0.7057 - val_acc: 0.7050\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.70445 to 0.70393, saving model to best.model\n",
      "0s - loss: 0.7270 - acc: 0.7030 - val_loss: 0.7039 - val_acc: 0.7081\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.70393 to 0.70282, saving model to best.model\n",
      "0s - loss: 0.7249 - acc: 0.7019 - val_loss: 0.7028 - val_acc: 0.7073\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.70282 to 0.69780, saving model to best.model\n",
      "0s - loss: 0.7246 - acc: 0.7036 - val_loss: 0.6978 - val_acc: 0.7104\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69780 to 0.69677, saving model to best.model\n",
      "0s - loss: 0.7228 - acc: 0.7053 - val_loss: 0.6968 - val_acc: 0.7118\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.69677 to 0.69669, saving model to best.model\n",
      "1s - loss: 0.7212 - acc: 0.7035 - val_loss: 0.6967 - val_acc: 0.7103\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.69669 to 0.69418, saving model to best.model\n",
      "1s - loss: 0.7213 - acc: 0.7030 - val_loss: 0.6942 - val_acc: 0.7153\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.69418 to 0.69266, saving model to best.model\n",
      "1s - loss: 0.7201 - acc: 0.7053 - val_loss: 0.6927 - val_acc: 0.7125\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7226 - acc: 0.7045 - val_loss: 0.6948 - val_acc: 0.7116\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.69266 to 0.69043, saving model to best.model\n",
      "1s - loss: 0.7166 - acc: 0.7069 - val_loss: 0.6904 - val_acc: 0.7145\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.69043 to 0.69004, saving model to best.model\n",
      "1s - loss: 0.7193 - acc: 0.7046 - val_loss: 0.6900 - val_acc: 0.7133\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7175 - acc: 0.7064 - val_loss: 0.6918 - val_acc: 0.7111\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.69004 to 0.68971, saving model to best.model\n",
      "1s - loss: 0.7182 - acc: 0.7041 - val_loss: 0.6897 - val_acc: 0.7136\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.68971 to 0.68706, saving model to best.model\n",
      "1s - loss: 0.7173 - acc: 0.7091 - val_loss: 0.6871 - val_acc: 0.7143\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7151 - acc: 0.7075 - val_loss: 0.6878 - val_acc: 0.7196\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.68706 to 0.68592, saving model to best.model\n",
      "1s - loss: 0.7138 - acc: 0.7074 - val_loss: 0.6859 - val_acc: 0.7150\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.68592 to 0.68494, saving model to best.model\n",
      "0s - loss: 0.7118 - acc: 0.7076 - val_loss: 0.6849 - val_acc: 0.7177\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.68494 to 0.68395, saving model to best.model\n",
      "0s - loss: 0.7124 - acc: 0.7095 - val_loss: 0.6840 - val_acc: 0.7181\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.7121 - acc: 0.7095 - val_loss: 0.6844 - val_acc: 0.7180\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.68395 to 0.68358, saving model to best.model\n",
      "1s - loss: 0.7088 - acc: 0.7091 - val_loss: 0.6836 - val_acc: 0.7177\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.68358 to 0.67798, saving model to best.model\n",
      "1s - loss: 0.7067 - acc: 0.7095 - val_loss: 0.6780 - val_acc: 0.7220\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.67798 to 0.67615, saving model to best.model\n",
      "1s - loss: 0.7045 - acc: 0.7097 - val_loss: 0.6761 - val_acc: 0.7193\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.7074 - acc: 0.7110 - val_loss: 0.6808 - val_acc: 0.7146\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.67615 to 0.67517, saving model to best.model\n",
      "1s - loss: 0.7050 - acc: 0.7116 - val_loss: 0.6752 - val_acc: 0.7212\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.7055 - acc: 0.7110 - val_loss: 0.6763 - val_acc: 0.7196\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.7052 - acc: 0.7110 - val_loss: 0.6768 - val_acc: 0.7206\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.7033 - acc: 0.7131 - val_loss: 0.6752 - val_acc: 0.7213\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.67517 to 0.67325, saving model to best.model\n",
      "1s - loss: 0.7011 - acc: 0.7142 - val_loss: 0.6732 - val_acc: 0.7205\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.7006 - acc: 0.7114 - val_loss: 0.6734 - val_acc: 0.7201\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.67325 to 0.67315, saving model to best.model\n",
      "1s - loss: 0.7022 - acc: 0.7123 - val_loss: 0.6732 - val_acc: 0.7226\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.67315 to 0.67179, saving model to best.model\n",
      "1s - loss: 0.6984 - acc: 0.7116 - val_loss: 0.6718 - val_acc: 0.7204\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.7000 - acc: 0.7149 - val_loss: 0.6719 - val_acc: 0.7201\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.67179 to 0.67147, saving model to best.model\n",
      "1s - loss: 0.6995 - acc: 0.7126 - val_loss: 0.6715 - val_acc: 0.7222\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.67147 to 0.66859, saving model to best.model\n",
      "1s - loss: 0.6958 - acc: 0.7153 - val_loss: 0.6686 - val_acc: 0.7218\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.66859 to 0.66736, saving model to best.model\n",
      "1s - loss: 0.6964 - acc: 0.7138 - val_loss: 0.6674 - val_acc: 0.7242\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6987 - acc: 0.7137 - val_loss: 0.6689 - val_acc: 0.7242\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6945 - acc: 0.7169 - val_loss: 0.6686 - val_acc: 0.7232\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.66736 to 0.66537, saving model to best.model\n",
      "1s - loss: 0.6947 - acc: 0.7155 - val_loss: 0.6654 - val_acc: 0.7235\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6960 - acc: 0.7180 - val_loss: 0.6675 - val_acc: 0.7231\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.66537 to 0.66531, saving model to best.model\n",
      "1s - loss: 0.6937 - acc: 0.7160 - val_loss: 0.6653 - val_acc: 0.7228\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6936 - acc: 0.7172 - val_loss: 0.6665 - val_acc: 0.7244\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.66531 to 0.66257, saving model to best.model\n",
      "1s - loss: 0.6918 - acc: 0.7159 - val_loss: 0.6626 - val_acc: 0.7280\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.66257 to 0.66231, saving model to best.model\n",
      "1s - loss: 0.6916 - acc: 0.7178 - val_loss: 0.6623 - val_acc: 0.7260\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.66231 to 0.66091, saving model to best.model\n",
      "1s - loss: 0.6911 - acc: 0.7189 - val_loss: 0.6609 - val_acc: 0.7258\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.66091 to 0.66013, saving model to best.model\n",
      "1s - loss: 0.6929 - acc: 0.7162 - val_loss: 0.6601 - val_acc: 0.7273\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.66013 to 0.65945, saving model to best.model\n",
      "1s - loss: 0.6867 - acc: 0.7203 - val_loss: 0.6595 - val_acc: 0.7260\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.65945 to 0.65741, saving model to best.model\n",
      "1s - loss: 0.6862 - acc: 0.7190 - val_loss: 0.6574 - val_acc: 0.7282\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6862 - acc: 0.7186 - val_loss: 0.6577 - val_acc: 0.7278\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6841 - acc: 0.7196 - val_loss: 0.6595 - val_acc: 0.7239\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.65741 to 0.65520, saving model to best.model\n",
      "0s - loss: 0.6856 - acc: 0.7203 - val_loss: 0.6552 - val_acc: 0.7266\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.65520 to 0.65376, saving model to best.model\n",
      "0s - loss: 0.6853 - acc: 0.7199 - val_loss: 0.6538 - val_acc: 0.7299\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.65376 to 0.65368, saving model to best.model\n",
      "0s - loss: 0.6843 - acc: 0.7200 - val_loss: 0.6537 - val_acc: 0.7276\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.65368 to 0.65320, saving model to best.model\n",
      "0s - loss: 0.6860 - acc: 0.7191 - val_loss: 0.6532 - val_acc: 0.7286\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6863 - acc: 0.7191 - val_loss: 0.6566 - val_acc: 0.7262\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.65320 to 0.65100, saving model to best.model\n",
      "1s - loss: 0.6824 - acc: 0.7211 - val_loss: 0.6510 - val_acc: 0.7303\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6839 - acc: 0.7187 - val_loss: 0.6542 - val_acc: 0.7265\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6826 - acc: 0.7212 - val_loss: 0.6523 - val_acc: 0.7296\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6810 - acc: 0.7193 - val_loss: 0.6514 - val_acc: 0.7310\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6846 - acc: 0.7217 - val_loss: 0.6518 - val_acc: 0.7293\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.65100 to 0.65071, saving model to best.model\n",
      "1s - loss: 0.6814 - acc: 0.7220 - val_loss: 0.6507 - val_acc: 0.7278\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.65071 to 0.65013, saving model to best.model\n",
      "1s - loss: 0.6817 - acc: 0.7214 - val_loss: 0.6501 - val_acc: 0.7294\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6785 - acc: 0.7222 - val_loss: 0.6514 - val_acc: 0.7267\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6802 - acc: 0.7201 - val_loss: 0.6506 - val_acc: 0.7275\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.65013 to 0.64945, saving model to best.model\n",
      "1s - loss: 0.6782 - acc: 0.7226 - val_loss: 0.6495 - val_acc: 0.7278\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.64945 to 0.64801, saving model to best.model\n",
      "1s - loss: 0.6779 - acc: 0.7226 - val_loss: 0.6480 - val_acc: 0.7283\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.64801 to 0.64799, saving model to best.model\n",
      "1s - loss: 0.6774 - acc: 0.7229 - val_loss: 0.6480 - val_acc: 0.7309\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.64799 to 0.64692, saving model to best.model\n",
      "1s - loss: 0.6785 - acc: 0.7230 - val_loss: 0.6469 - val_acc: 0.7289\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.64692 to 0.64559, saving model to best.model\n",
      "1s - loss: 0.6780 - acc: 0.7241 - val_loss: 0.6456 - val_acc: 0.7308\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.64559 to 0.64559, saving model to best.model\n",
      "1s - loss: 0.6741 - acc: 0.7258 - val_loss: 0.6456 - val_acc: 0.7331\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.64559 to 0.64484, saving model to best.model\n",
      "0s - loss: 0.6771 - acc: 0.7216 - val_loss: 0.6448 - val_acc: 0.7342\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6735 - acc: 0.7245 - val_loss: 0.6451 - val_acc: 0.7282\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.64484 to 0.64470, saving model to best.model\n",
      "1s - loss: 0.6726 - acc: 0.7246 - val_loss: 0.6447 - val_acc: 0.7329\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6746 - acc: 0.7236 - val_loss: 0.6461 - val_acc: 0.7272\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.64470 to 0.64305, saving model to best.model\n",
      "1s - loss: 0.6745 - acc: 0.7261 - val_loss: 0.6431 - val_acc: 0.7330\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6718 - acc: 0.7246 - val_loss: 0.6438 - val_acc: 0.7289\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.64305 to 0.64241, saving model to best.model\n",
      "1s - loss: 0.6728 - acc: 0.7256 - val_loss: 0.6424 - val_acc: 0.7331\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.64241 to 0.64122, saving model to best.model\n",
      "1s - loss: 0.6708 - acc: 0.7263 - val_loss: 0.6412 - val_acc: 0.7331\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.64122 to 0.64058, saving model to best.model\n",
      "1s - loss: 0.6685 - acc: 0.7277 - val_loss: 0.6406 - val_acc: 0.7317\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.64058 to 0.63886, saving model to best.model\n",
      "0s - loss: 0.6693 - acc: 0.7263 - val_loss: 0.6389 - val_acc: 0.7328\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6705 - acc: 0.7249 - val_loss: 0.6408 - val_acc: 0.7336\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6723 - acc: 0.7243 - val_loss: 0.6396 - val_acc: 0.7329\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7265 - val_loss: 0.6408 - val_acc: 0.7301\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.63886 to 0.63881, saving model to best.model\n",
      "1s - loss: 0.6679 - acc: 0.7268 - val_loss: 0.6388 - val_acc: 0.7352\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6680 - acc: 0.7250 - val_loss: 0.6394 - val_acc: 0.7322\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.63881 to 0.63617, saving model to best.model\n",
      "1s - loss: 0.6688 - acc: 0.7271 - val_loss: 0.6362 - val_acc: 0.7344\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6643 - acc: 0.7268 - val_loss: 0.6399 - val_acc: 0.7333\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6667 - acc: 0.7274 - val_loss: 0.6395 - val_acc: 0.7362\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.63617 to 0.63600, saving model to best.model\n",
      "1s - loss: 0.6673 - acc: 0.7273 - val_loss: 0.6360 - val_acc: 0.7337\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6646 - acc: 0.7290 - val_loss: 0.6362 - val_acc: 0.7354\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.63600 to 0.63400, saving model to best.model\n",
      "1s - loss: 0.6627 - acc: 0.7278 - val_loss: 0.6340 - val_acc: 0.7358\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.63400 to 0.63358, saving model to best.model\n",
      "1s - loss: 0.6642 - acc: 0.7273 - val_loss: 0.6336 - val_acc: 0.7376\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6683 - acc: 0.7265 - val_loss: 0.6353 - val_acc: 0.7365\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6637 - acc: 0.7291 - val_loss: 0.6344 - val_acc: 0.7371\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6638 - acc: 0.7291 - val_loss: 0.6345 - val_acc: 0.7340\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.6628 - acc: 0.7275 - val_loss: 0.6336 - val_acc: 0.7344\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.63358 to 0.63331, saving model to best.model\n",
      "0s - loss: 0.6620 - acc: 0.7291 - val_loss: 0.6333 - val_acc: 0.7368\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6637 - acc: 0.7278 - val_loss: 0.6339 - val_acc: 0.7330\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6653 - acc: 0.7264 - val_loss: 0.6349 - val_acc: 0.7327\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.63331 to 0.63230, saving model to best.model\n",
      "0s - loss: 0.6614 - acc: 0.7280 - val_loss: 0.6323 - val_acc: 0.7358\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6632 - acc: 0.7297 - val_loss: 0.6348 - val_acc: 0.7328\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.63230 to 0.63219, saving model to best.model\n",
      "0s - loss: 0.6639 - acc: 0.7300 - val_loss: 0.6322 - val_acc: 0.7388\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.63219 to 0.63094, saving model to best.model\n",
      "0s - loss: 0.6588 - acc: 0.7304 - val_loss: 0.6309 - val_acc: 0.7357\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.63094 to 0.62898, saving model to best.model\n",
      "0s - loss: 0.6601 - acc: 0.7298 - val_loss: 0.6290 - val_acc: 0.7405\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.62898 to 0.62734, saving model to best.model\n",
      "0s - loss: 0.6613 - acc: 0.7302 - val_loss: 0.6273 - val_acc: 0.7404\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6588 - acc: 0.7321 - val_loss: 0.6303 - val_acc: 0.7370\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6584 - acc: 0.7316 - val_loss: 0.6322 - val_acc: 0.7356\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6611 - acc: 0.7299 - val_loss: 0.6298 - val_acc: 0.7393\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6603 - acc: 0.7297 - val_loss: 0.6284 - val_acc: 0.7381\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6582 - acc: 0.7297 - val_loss: 0.6286 - val_acc: 0.7395\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.62734 to 0.62658, saving model to best.model\n",
      "1s - loss: 0.6584 - acc: 0.7307 - val_loss: 0.6266 - val_acc: 0.7395\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6577 - acc: 0.7311 - val_loss: 0.6280 - val_acc: 0.7382\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6591 - acc: 0.7298 - val_loss: 0.6272 - val_acc: 0.7369\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.62658 to 0.62550, saving model to best.model\n",
      "1s - loss: 0.6560 - acc: 0.7306 - val_loss: 0.6255 - val_acc: 0.7405\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6561 - acc: 0.7327 - val_loss: 0.6271 - val_acc: 0.7383\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.62550 to 0.62411, saving model to best.model\n",
      "1s - loss: 0.6553 - acc: 0.7321 - val_loss: 0.6241 - val_acc: 0.7417\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84465, saving model to best.model\n",
      "1s - loss: 0.9269 - acc: 0.6192 - val_loss: 0.8446 - val_acc: 0.6588\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8634 - acc: 0.6582 - val_loss: 0.8451 - val_acc: 0.6588\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84465 to 0.84456, saving model to best.model\n",
      "1s - loss: 0.8540 - acc: 0.6589 - val_loss: 0.8446 - val_acc: 0.6588\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84456 to 0.84307, saving model to best.model\n",
      "1s - loss: 0.8514 - acc: 0.6589 - val_loss: 0.8431 - val_acc: 0.6588\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84307 to 0.83774, saving model to best.model\n",
      "1s - loss: 0.8488 - acc: 0.6589 - val_loss: 0.8377 - val_acc: 0.6588\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83774 to 0.83226, saving model to best.model\n",
      "1s - loss: 0.8432 - acc: 0.6589 - val_loss: 0.8323 - val_acc: 0.6588\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83226 to 0.82998, saving model to best.model\n",
      "1s - loss: 0.8382 - acc: 0.6589 - val_loss: 0.8300 - val_acc: 0.6588\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82998 to 0.82764, saving model to best.model\n",
      "1s - loss: 0.8345 - acc: 0.6589 - val_loss: 0.8276 - val_acc: 0.6588\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82764 to 0.82686, saving model to best.model\n",
      "1s - loss: 0.8318 - acc: 0.6589 - val_loss: 0.8269 - val_acc: 0.6588\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8323 - acc: 0.6588 - val_loss: 0.8271 - val_acc: 0.6588\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82686 to 0.82583, saving model to best.model\n",
      "1s - loss: 0.8319 - acc: 0.6588 - val_loss: 0.8258 - val_acc: 0.6588\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82583 to 0.82565, saving model to best.model\n",
      "1s - loss: 0.8300 - acc: 0.6588 - val_loss: 0.8256 - val_acc: 0.6588\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82565 to 0.82511, saving model to best.model\n",
      "1s - loss: 0.8300 - acc: 0.6588 - val_loss: 0.8251 - val_acc: 0.6588\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82511 to 0.82484, saving model to best.model\n",
      "1s - loss: 0.8289 - acc: 0.6587 - val_loss: 0.8248 - val_acc: 0.6588\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82484 to 0.82482, saving model to best.model\n",
      "1s - loss: 0.8286 - acc: 0.6589 - val_loss: 0.8248 - val_acc: 0.6588\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82482 to 0.82396, saving model to best.model\n",
      "1s - loss: 0.8281 - acc: 0.6588 - val_loss: 0.8240 - val_acc: 0.6588\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82396 to 0.82365, saving model to best.model\n",
      "1s - loss: 0.8278 - acc: 0.6589 - val_loss: 0.8237 - val_acc: 0.6588\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82365 to 0.82311, saving model to best.model\n",
      "1s - loss: 0.8272 - acc: 0.6589 - val_loss: 0.8231 - val_acc: 0.6588\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.82311 to 0.82235, saving model to best.model\n",
      "1s - loss: 0.8266 - acc: 0.6589 - val_loss: 0.8224 - val_acc: 0.6588\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82235 to 0.82183, saving model to best.model\n",
      "1s - loss: 0.8240 - acc: 0.6588 - val_loss: 0.8218 - val_acc: 0.6588\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82183 to 0.82115, saving model to best.model\n",
      "1s - loss: 0.8235 - acc: 0.6592 - val_loss: 0.8212 - val_acc: 0.6588\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82115 to 0.82041, saving model to best.model\n",
      "1s - loss: 0.8239 - acc: 0.6594 - val_loss: 0.8204 - val_acc: 0.6588\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.82041 to 0.82033, saving model to best.model\n",
      "1s - loss: 0.8220 - acc: 0.6589 - val_loss: 0.8203 - val_acc: 0.6588\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.82033 to 0.81769, saving model to best.model\n",
      "1s - loss: 0.8204 - acc: 0.6598 - val_loss: 0.8177 - val_acc: 0.6589\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81769 to 0.81727, saving model to best.model\n",
      "1s - loss: 0.8208 - acc: 0.6596 - val_loss: 0.8173 - val_acc: 0.6598\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81727 to 0.81514, saving model to best.model\n",
      "1s - loss: 0.8183 - acc: 0.6601 - val_loss: 0.8151 - val_acc: 0.6616\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81514 to 0.81369, saving model to best.model\n",
      "1s - loss: 0.8176 - acc: 0.6609 - val_loss: 0.8137 - val_acc: 0.6615\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81369 to 0.81210, saving model to best.model\n",
      "1s - loss: 0.8163 - acc: 0.6612 - val_loss: 0.8121 - val_acc: 0.6621\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81210 to 0.81174, saving model to best.model\n",
      "1s - loss: 0.8155 - acc: 0.6621 - val_loss: 0.8117 - val_acc: 0.6601\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81174 to 0.80964, saving model to best.model\n",
      "1s - loss: 0.8143 - acc: 0.6625 - val_loss: 0.8096 - val_acc: 0.6629\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80964 to 0.80746, saving model to best.model\n",
      "1s - loss: 0.8111 - acc: 0.6634 - val_loss: 0.8075 - val_acc: 0.6632\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80746 to 0.80532, saving model to best.model\n",
      "1s - loss: 0.8100 - acc: 0.6647 - val_loss: 0.8053 - val_acc: 0.6636\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80532 to 0.80413, saving model to best.model\n",
      "1s - loss: 0.8096 - acc: 0.6629 - val_loss: 0.8041 - val_acc: 0.6638\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80413 to 0.80308, saving model to best.model\n",
      "1s - loss: 0.8065 - acc: 0.6655 - val_loss: 0.8031 - val_acc: 0.6641\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80308 to 0.79897, saving model to best.model\n",
      "1s - loss: 0.8051 - acc: 0.6640 - val_loss: 0.7990 - val_acc: 0.6658\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79897 to 0.79677, saving model to best.model\n",
      "1s - loss: 0.8027 - acc: 0.6666 - val_loss: 0.7968 - val_acc: 0.6674\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79677 to 0.79517, saving model to best.model\n",
      "1s - loss: 0.8023 - acc: 0.6648 - val_loss: 0.7952 - val_acc: 0.6676\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79517 to 0.79254, saving model to best.model\n",
      "1s - loss: 0.7995 - acc: 0.6676 - val_loss: 0.7925 - val_acc: 0.6681\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "1s - loss: 0.7982 - acc: 0.6674 - val_loss: 0.7934 - val_acc: 0.6680\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79254 to 0.78919, saving model to best.model\n",
      "1s - loss: 0.7977 - acc: 0.6678 - val_loss: 0.7892 - val_acc: 0.6681\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78919 to 0.78749, saving model to best.model\n",
      "1s - loss: 0.7956 - acc: 0.6691 - val_loss: 0.7875 - val_acc: 0.6697\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78749 to 0.78592, saving model to best.model\n",
      "1s - loss: 0.7933 - acc: 0.6694 - val_loss: 0.7859 - val_acc: 0.6704\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78592 to 0.78512, saving model to best.model\n",
      "1s - loss: 0.7929 - acc: 0.6719 - val_loss: 0.7851 - val_acc: 0.6686\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78512 to 0.77984, saving model to best.model\n",
      "1s - loss: 0.7893 - acc: 0.6729 - val_loss: 0.7798 - val_acc: 0.6748\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "1s - loss: 0.7878 - acc: 0.6718 - val_loss: 0.7808 - val_acc: 0.6712\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77984 to 0.77568, saving model to best.model\n",
      "1s - loss: 0.7864 - acc: 0.6729 - val_loss: 0.7757 - val_acc: 0.6754\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77568 to 0.77271, saving model to best.model\n",
      "1s - loss: 0.7847 - acc: 0.6732 - val_loss: 0.7727 - val_acc: 0.6794\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "1s - loss: 0.7850 - acc: 0.6731 - val_loss: 0.7738 - val_acc: 0.6748\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77271 to 0.77014, saving model to best.model\n",
      "1s - loss: 0.7821 - acc: 0.6747 - val_loss: 0.7701 - val_acc: 0.6780\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77014 to 0.76482, saving model to best.model\n",
      "1s - loss: 0.7773 - acc: 0.6770 - val_loss: 0.7648 - val_acc: 0.6813\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76482 to 0.76414, saving model to best.model\n",
      "1s - loss: 0.7771 - acc: 0.6775 - val_loss: 0.7641 - val_acc: 0.6794\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76414 to 0.76186, saving model to best.model\n",
      "1s - loss: 0.7768 - acc: 0.6778 - val_loss: 0.7619 - val_acc: 0.6806\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76186 to 0.75634, saving model to best.model\n",
      "1s - loss: 0.7733 - acc: 0.6782 - val_loss: 0.7563 - val_acc: 0.6874\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75634 to 0.75463, saving model to best.model\n",
      "1s - loss: 0.7738 - acc: 0.6796 - val_loss: 0.7546 - val_acc: 0.6838\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75463 to 0.75317, saving model to best.model\n",
      "1s - loss: 0.7707 - acc: 0.6796 - val_loss: 0.7532 - val_acc: 0.6877\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75317 to 0.74997, saving model to best.model\n",
      "1s - loss: 0.7698 - acc: 0.6814 - val_loss: 0.7500 - val_acc: 0.6888\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74997 to 0.74710, saving model to best.model\n",
      "1s - loss: 0.7647 - acc: 0.6844 - val_loss: 0.7471 - val_acc: 0.6855\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74710 to 0.74652, saving model to best.model\n",
      "1s - loss: 0.7642 - acc: 0.6832 - val_loss: 0.7465 - val_acc: 0.6848\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74652 to 0.74488, saving model to best.model\n",
      "1s - loss: 0.7622 - acc: 0.6854 - val_loss: 0.7449 - val_acc: 0.6865\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74488 to 0.74120, saving model to best.model\n",
      "1s - loss: 0.7606 - acc: 0.6863 - val_loss: 0.7412 - val_acc: 0.6882\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74120 to 0.73640, saving model to best.model\n",
      "1s - loss: 0.7601 - acc: 0.6888 - val_loss: 0.7364 - val_acc: 0.6957\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73640 to 0.73588, saving model to best.model\n",
      "1s - loss: 0.7554 - acc: 0.6885 - val_loss: 0.7359 - val_acc: 0.6910\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73588 to 0.73270, saving model to best.model\n",
      "1s - loss: 0.7545 - acc: 0.6885 - val_loss: 0.7327 - val_acc: 0.6953\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73270 to 0.72707, saving model to best.model\n",
      "1s - loss: 0.7510 - acc: 0.6905 - val_loss: 0.7271 - val_acc: 0.7004\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "1s - loss: 0.7503 - acc: 0.6933 - val_loss: 0.7273 - val_acc: 0.6987\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72707 to 0.72519, saving model to best.model\n",
      "1s - loss: 0.7508 - acc: 0.6923 - val_loss: 0.7252 - val_acc: 0.6994\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72519 to 0.72370, saving model to best.model\n",
      "1s - loss: 0.7457 - acc: 0.6950 - val_loss: 0.7237 - val_acc: 0.6998\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72370 to 0.71942, saving model to best.model\n",
      "1s - loss: 0.7467 - acc: 0.6909 - val_loss: 0.7194 - val_acc: 0.7053\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71942 to 0.71679, saving model to best.model\n",
      "1s - loss: 0.7434 - acc: 0.6953 - val_loss: 0.7168 - val_acc: 0.7049\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "1s - loss: 0.7439 - acc: 0.6943 - val_loss: 0.7180 - val_acc: 0.7047\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71679 to 0.71457, saving model to best.model\n",
      "1s - loss: 0.7415 - acc: 0.6953 - val_loss: 0.7146 - val_acc: 0.7060\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71457 to 0.70995, saving model to best.model\n",
      "1s - loss: 0.7378 - acc: 0.6977 - val_loss: 0.7099 - val_acc: 0.7096\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70995 to 0.70586, saving model to best.model\n",
      "1s - loss: 0.7355 - acc: 0.6971 - val_loss: 0.7059 - val_acc: 0.7098\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.7366 - acc: 0.6981 - val_loss: 0.7077 - val_acc: 0.7108\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.7337 - acc: 0.6984 - val_loss: 0.7065 - val_acc: 0.7097\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.7363 - acc: 0.6974 - val_loss: 0.7059 - val_acc: 0.7091\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70586 to 0.70305, saving model to best.model\n",
      "1s - loss: 0.7312 - acc: 0.6989 - val_loss: 0.7031 - val_acc: 0.7118\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70305 to 0.70250, saving model to best.model\n",
      "1s - loss: 0.7319 - acc: 0.7008 - val_loss: 0.7025 - val_acc: 0.7110\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70250 to 0.69918, saving model to best.model\n",
      "0s - loss: 0.7279 - acc: 0.7026 - val_loss: 0.6992 - val_acc: 0.7151\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69918 to 0.69400, saving model to best.model\n",
      "0s - loss: 0.7248 - acc: 0.7038 - val_loss: 0.6940 - val_acc: 0.7170\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.7259 - acc: 0.7039 - val_loss: 0.6947 - val_acc: 0.7156\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69400 to 0.69347, saving model to best.model\n",
      "0s - loss: 0.7260 - acc: 0.7051 - val_loss: 0.6935 - val_acc: 0.7169\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.7221 - acc: 0.7053 - val_loss: 0.6950 - val_acc: 0.7158\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.7239 - acc: 0.7036 - val_loss: 0.6955 - val_acc: 0.7139\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7200 - acc: 0.7053 - val_loss: 0.6939 - val_acc: 0.7151\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69347 to 0.68841, saving model to best.model\n",
      "0s - loss: 0.7222 - acc: 0.7053 - val_loss: 0.6884 - val_acc: 0.7172\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68841 to 0.68836, saving model to best.model\n",
      "0s - loss: 0.7158 - acc: 0.7089 - val_loss: 0.6884 - val_acc: 0.7157\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68836 to 0.68582, saving model to best.model\n",
      "0s - loss: 0.7161 - acc: 0.7077 - val_loss: 0.6858 - val_acc: 0.7187\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68582 to 0.68029, saving model to best.model\n",
      "0s - loss: 0.7149 - acc: 0.7073 - val_loss: 0.6803 - val_acc: 0.7218\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.7165 - acc: 0.7078 - val_loss: 0.6822 - val_acc: 0.7200\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.7139 - acc: 0.7094 - val_loss: 0.6807 - val_acc: 0.7199\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68029 to 0.68022, saving model to best.model\n",
      "0s - loss: 0.7103 - acc: 0.7106 - val_loss: 0.6802 - val_acc: 0.7198\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68022 to 0.67774, saving model to best.model\n",
      "0s - loss: 0.7102 - acc: 0.7102 - val_loss: 0.6777 - val_acc: 0.7222\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67774 to 0.67530, saving model to best.model\n",
      "0s - loss: 0.7118 - acc: 0.7092 - val_loss: 0.6753 - val_acc: 0.7219\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.7081 - acc: 0.7131 - val_loss: 0.6786 - val_acc: 0.7189\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7085 - acc: 0.7119 - val_loss: 0.6765 - val_acc: 0.7203\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.67530 to 0.67358, saving model to best.model\n",
      "1s - loss: 0.7048 - acc: 0.7140 - val_loss: 0.6736 - val_acc: 0.7212\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67358 to 0.67253, saving model to best.model\n",
      "1s - loss: 0.7053 - acc: 0.7142 - val_loss: 0.6725 - val_acc: 0.7221\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7044 - acc: 0.7137 - val_loss: 0.6741 - val_acc: 0.7211\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.67253 to 0.67156, saving model to best.model\n",
      "1s - loss: 0.7057 - acc: 0.7122 - val_loss: 0.6716 - val_acc: 0.7218\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.67156 to 0.66950, saving model to best.model\n",
      "1s - loss: 0.7032 - acc: 0.7127 - val_loss: 0.6695 - val_acc: 0.7248\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7034 - acc: 0.7137 - val_loss: 0.6698 - val_acc: 0.7228\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7008 - acc: 0.7146 - val_loss: 0.6715 - val_acc: 0.7211\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66950 to 0.66672, saving model to best.model\n",
      "1s - loss: 0.7005 - acc: 0.7141 - val_loss: 0.6667 - val_acc: 0.7237\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66672 to 0.66307, saving model to best.model\n",
      "1s - loss: 0.6976 - acc: 0.7164 - val_loss: 0.6631 - val_acc: 0.7300\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.6964 - acc: 0.7152 - val_loss: 0.6637 - val_acc: 0.7289\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66307 to 0.66128, saving model to best.model\n",
      "1s - loss: 0.6981 - acc: 0.7143 - val_loss: 0.6613 - val_acc: 0.7304\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6944 - acc: 0.7160 - val_loss: 0.6654 - val_acc: 0.7258\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66128 to 0.66094, saving model to best.model\n",
      "0s - loss: 0.6935 - acc: 0.7178 - val_loss: 0.6609 - val_acc: 0.7281\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66094 to 0.66082, saving model to best.model\n",
      "0s - loss: 0.6971 - acc: 0.7158 - val_loss: 0.6608 - val_acc: 0.7299\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.6942 - acc: 0.7164 - val_loss: 0.6619 - val_acc: 0.7282\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.66082 to 0.65951, saving model to best.model\n",
      "0s - loss: 0.6937 - acc: 0.7196 - val_loss: 0.6595 - val_acc: 0.7290\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65951 to 0.65651, saving model to best.model\n",
      "0s - loss: 0.6897 - acc: 0.7198 - val_loss: 0.6565 - val_acc: 0.7317\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.6908 - acc: 0.7205 - val_loss: 0.6573 - val_acc: 0.7295\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6896 - acc: 0.7181 - val_loss: 0.6578 - val_acc: 0.7321\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65651 to 0.65349, saving model to best.model\n",
      "0s - loss: 0.6877 - acc: 0.7206 - val_loss: 0.6535 - val_acc: 0.7321\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.6892 - acc: 0.7196 - val_loss: 0.6545 - val_acc: 0.7306\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "0s - loss: 0.6888 - acc: 0.7203 - val_loss: 0.6536 - val_acc: 0.7326\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.65349 to 0.65280, saving model to best.model\n",
      "0s - loss: 0.6865 - acc: 0.7209 - val_loss: 0.6528 - val_acc: 0.7338\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65280 to 0.65055, saving model to best.model\n",
      "0s - loss: 0.6822 - acc: 0.7224 - val_loss: 0.6505 - val_acc: 0.7348\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65055 to 0.64967, saving model to best.model\n",
      "0s - loss: 0.6839 - acc: 0.7202 - val_loss: 0.6497 - val_acc: 0.7331\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6870 - acc: 0.7206 - val_loss: 0.6513 - val_acc: 0.7323\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64967 to 0.64954, saving model to best.model\n",
      "0s - loss: 0.6842 - acc: 0.7212 - val_loss: 0.6495 - val_acc: 0.7349\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64954 to 0.64653, saving model to best.model\n",
      "0s - loss: 0.6797 - acc: 0.7228 - val_loss: 0.6465 - val_acc: 0.7364\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6859 - acc: 0.7206 - val_loss: 0.6478 - val_acc: 0.7345\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6838 - acc: 0.7208 - val_loss: 0.6474 - val_acc: 0.7359\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.64653 to 0.64558, saving model to best.model\n",
      "1s - loss: 0.6806 - acc: 0.7233 - val_loss: 0.6456 - val_acc: 0.7349\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64558 to 0.64308, saving model to best.model\n",
      "1s - loss: 0.6800 - acc: 0.7233 - val_loss: 0.6431 - val_acc: 0.7362\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6784 - acc: 0.7250 - val_loss: 0.6438 - val_acc: 0.7365\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64308 to 0.64155, saving model to best.model\n",
      "1s - loss: 0.6790 - acc: 0.7244 - val_loss: 0.6416 - val_acc: 0.7356\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6795 - acc: 0.7224 - val_loss: 0.6437 - val_acc: 0.7369\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.64155 to 0.64069, saving model to best.model\n",
      "1s - loss: 0.6759 - acc: 0.7245 - val_loss: 0.6407 - val_acc: 0.7377\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.64069 to 0.64032, saving model to best.model\n",
      "1s - loss: 0.6769 - acc: 0.7252 - val_loss: 0.6403 - val_acc: 0.7355\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.64032 to 0.63936, saving model to best.model\n",
      "1s - loss: 0.6778 - acc: 0.7247 - val_loss: 0.6394 - val_acc: 0.7356\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.63936 to 0.63904, saving model to best.model\n",
      "1s - loss: 0.6767 - acc: 0.7243 - val_loss: 0.6390 - val_acc: 0.7398\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63904 to 0.63681, saving model to best.model\n",
      "1s - loss: 0.6777 - acc: 0.7223 - val_loss: 0.6368 - val_acc: 0.7382\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6737 - acc: 0.7246 - val_loss: 0.6396 - val_acc: 0.7376\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6746 - acc: 0.7261 - val_loss: 0.6385 - val_acc: 0.7381\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6746 - acc: 0.7236 - val_loss: 0.6368 - val_acc: 0.7385\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6739 - acc: 0.7264 - val_loss: 0.6375 - val_acc: 0.7378\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63681 to 0.63412, saving model to best.model\n",
      "1s - loss: 0.6705 - acc: 0.7270 - val_loss: 0.6341 - val_acc: 0.7386\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6729 - acc: 0.7252 - val_loss: 0.6359 - val_acc: 0.7384\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6734 - acc: 0.7275 - val_loss: 0.6357 - val_acc: 0.7397\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63412 to 0.63189, saving model to best.model\n",
      "1s - loss: 0.6707 - acc: 0.7274 - val_loss: 0.6319 - val_acc: 0.7395\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6701 - acc: 0.7258 - val_loss: 0.6353 - val_acc: 0.7390\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6696 - acc: 0.7236 - val_loss: 0.6321 - val_acc: 0.7395\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63189 to 0.63019, saving model to best.model\n",
      "0s - loss: 0.6665 - acc: 0.7278 - val_loss: 0.6302 - val_acc: 0.7395\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6636 - acc: 0.7314 - val_loss: 0.6312 - val_acc: 0.7409\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63019 to 0.62867, saving model to best.model\n",
      "0s - loss: 0.6659 - acc: 0.7288 - val_loss: 0.6287 - val_acc: 0.7399\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7258 - val_loss: 0.6297 - val_acc: 0.7411\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62867 to 0.62755, saving model to best.model\n",
      "0s - loss: 0.6652 - acc: 0.7290 - val_loss: 0.6275 - val_acc: 0.7416\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62755 to 0.62737, saving model to best.model\n",
      "0s - loss: 0.6655 - acc: 0.7299 - val_loss: 0.6274 - val_acc: 0.7416\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6652 - acc: 0.7276 - val_loss: 0.6308 - val_acc: 0.7400\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62737 to 0.62623, saving model to best.model\n",
      "1s - loss: 0.6669 - acc: 0.7271 - val_loss: 0.6262 - val_acc: 0.7413\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62623 to 0.62561, saving model to best.model\n",
      "1s - loss: 0.6648 - acc: 0.7310 - val_loss: 0.6256 - val_acc: 0.7422\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6655 - acc: 0.7274 - val_loss: 0.6283 - val_acc: 0.7426\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6655 - acc: 0.7299 - val_loss: 0.6276 - val_acc: 0.7419\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6614 - acc: 0.7311 - val_loss: 0.6256 - val_acc: 0.7429\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6613 - acc: 0.7304 - val_loss: 0.6264 - val_acc: 0.7437\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.62561 to 0.62393, saving model to best.model\n",
      "1s - loss: 0.6621 - acc: 0.7309 - val_loss: 0.6239 - val_acc: 0.7437\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62393 to 0.62277, saving model to best.model\n",
      "1s - loss: 0.6618 - acc: 0.7291 - val_loss: 0.6228 - val_acc: 0.7450\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62277 to 0.62010, saving model to best.model\n",
      "1s - loss: 0.6610 - acc: 0.7306 - val_loss: 0.6201 - val_acc: 0.7447\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6616 - acc: 0.7294 - val_loss: 0.6216 - val_acc: 0.7447\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6580 - acc: 0.7318 - val_loss: 0.6204 - val_acc: 0.7433\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.62010 to 0.61912, saving model to best.model\n",
      "1s - loss: 0.6600 - acc: 0.7312 - val_loss: 0.6191 - val_acc: 0.7452\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61912 to 0.61901, saving model to best.model\n",
      "1s - loss: 0.6573 - acc: 0.7312 - val_loss: 0.6190 - val_acc: 0.7453\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61901 to 0.61846, saving model to best.model\n",
      "1s - loss: 0.6585 - acc: 0.7299 - val_loss: 0.6185 - val_acc: 0.7458\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61846 to 0.61808, saving model to best.model\n",
      "0s - loss: 0.6534 - acc: 0.7323 - val_loss: 0.6181 - val_acc: 0.7454\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6584 - acc: 0.7320 - val_loss: 0.6201 - val_acc: 0.7447\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61808 to 0.61715, saving model to best.model\n",
      "0s - loss: 0.6555 - acc: 0.7314 - val_loss: 0.6172 - val_acc: 0.7458\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6580 - acc: 0.7318 - val_loss: 0.6188 - val_acc: 0.7434\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61715 to 0.61692, saving model to best.model\n",
      "0s - loss: 0.6551 - acc: 0.7349 - val_loss: 0.6169 - val_acc: 0.7453\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6572 - acc: 0.7319 - val_loss: 0.6173 - val_acc: 0.7457\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6582 - acc: 0.7295 - val_loss: 0.6177 - val_acc: 0.7467\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61692 to 0.61559, saving model to best.model\n",
      "0s - loss: 0.6557 - acc: 0.7326 - val_loss: 0.6156 - val_acc: 0.7456\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.61559 to 0.61426, saving model to best.model\n",
      "0s - loss: 0.6514 - acc: 0.7346 - val_loss: 0.6143 - val_acc: 0.7471\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61426 to 0.61363, saving model to best.model\n",
      "0s - loss: 0.6523 - acc: 0.7340 - val_loss: 0.6136 - val_acc: 0.7470\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61363 to 0.61324, saving model to best.model\n",
      "1s - loss: 0.6524 - acc: 0.7338 - val_loss: 0.6132 - val_acc: 0.7473\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61324 to 0.61158, saving model to best.model\n",
      "1s - loss: 0.6516 - acc: 0.7335 - val_loss: 0.6116 - val_acc: 0.7480\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7337 - val_loss: 0.6128 - val_acc: 0.7472\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6504 - acc: 0.7335 - val_loss: 0.6123 - val_acc: 0.7465\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7318 - val_loss: 0.6126 - val_acc: 0.7472\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.61158 to 0.61124, saving model to best.model\n",
      "1s - loss: 0.6530 - acc: 0.7339 - val_loss: 0.6112 - val_acc: 0.7471\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6503 - acc: 0.7361 - val_loss: 0.6121 - val_acc: 0.7474\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6507 - acc: 0.7341 - val_loss: 0.6132 - val_acc: 0.7477\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.61124 to 0.61039, saving model to best.model\n",
      "1s - loss: 0.6524 - acc: 0.7342 - val_loss: 0.6104 - val_acc: 0.7481\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6478 - acc: 0.7363 - val_loss: 0.6120 - val_acc: 0.7480\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6486 - acc: 0.7331 - val_loss: 0.6121 - val_acc: 0.7465\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.61039 to 0.60949, saving model to best.model\n",
      "1s - loss: 0.6491 - acc: 0.7358 - val_loss: 0.6095 - val_acc: 0.7478\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60949 to 0.60808, saving model to best.model\n",
      "1s - loss: 0.6467 - acc: 0.7363 - val_loss: 0.6081 - val_acc: 0.7494\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6501 - acc: 0.7348 - val_loss: 0.6106 - val_acc: 0.7472\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6519 - acc: 0.7350 - val_loss: 0.6099 - val_acc: 0.7459\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6528 - acc: 0.7338 - val_loss: 0.6089 - val_acc: 0.7474\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60808 to 0.60687, saving model to best.model\n",
      "1s - loss: 0.6448 - acc: 0.7362 - val_loss: 0.6069 - val_acc: 0.7515\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6514 - acc: 0.7340 - val_loss: 0.6091 - val_acc: 0.7475\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.60687 to 0.60574, saving model to best.model\n",
      "1s - loss: 0.6461 - acc: 0.7362 - val_loss: 0.6057 - val_acc: 0.7493\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60574 to 0.60475, saving model to best.model\n",
      "1s - loss: 0.6458 - acc: 0.7391 - val_loss: 0.6048 - val_acc: 0.7505\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.6454 - acc: 0.7347 - val_loss: 0.6059 - val_acc: 0.7498\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.6402 - acc: 0.7396 - val_loss: 0.6057 - val_acc: 0.7499\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.60475 to 0.60345, saving model to best.model\n",
      "0s - loss: 0.6454 - acc: 0.7353 - val_loss: 0.6034 - val_acc: 0.7502\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84020, saving model to best.model\n",
      "1s - loss: 0.9170 - acc: 0.6261 - val_loss: 0.8402 - val_acc: 0.6635\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84020 to 0.83854, saving model to best.model\n",
      "1s - loss: 0.8582 - acc: 0.6586 - val_loss: 0.8385 - val_acc: 0.6635\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.8512 - acc: 0.6587 - val_loss: 0.8403 - val_acc: 0.6635\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83854 to 0.83781, saving model to best.model\n",
      "0s - loss: 0.8477 - acc: 0.6587 - val_loss: 0.8378 - val_acc: 0.6635\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83781 to 0.83091, saving model to best.model\n",
      "0s - loss: 0.8428 - acc: 0.6587 - val_loss: 0.8309 - val_acc: 0.6635\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83091 to 0.82396, saving model to best.model\n",
      "1s - loss: 0.8381 - acc: 0.6587 - val_loss: 0.8240 - val_acc: 0.6635\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82396 to 0.82148, saving model to best.model\n",
      "1s - loss: 0.8342 - acc: 0.6587 - val_loss: 0.8215 - val_acc: 0.6635\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82148 to 0.82118, saving model to best.model\n",
      "1s - loss: 0.8316 - acc: 0.6587 - val_loss: 0.8212 - val_acc: 0.6635\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82118 to 0.81904, saving model to best.model\n",
      "1s - loss: 0.8282 - acc: 0.6587 - val_loss: 0.8190 - val_acc: 0.6635\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8283 - acc: 0.6586 - val_loss: 0.8191 - val_acc: 0.6635\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81904 to 0.81777, saving model to best.model\n",
      "0s - loss: 0.8270 - acc: 0.6587 - val_loss: 0.8178 - val_acc: 0.6635\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81777 to 0.81709, saving model to best.model\n",
      "1s - loss: 0.8263 - acc: 0.6587 - val_loss: 0.8171 - val_acc: 0.6635\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81709 to 0.81663, saving model to best.model\n",
      "1s - loss: 0.8251 - acc: 0.6587 - val_loss: 0.8166 - val_acc: 0.6635\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81663 to 0.81631, saving model to best.model\n",
      "1s - loss: 0.8241 - acc: 0.6584 - val_loss: 0.8163 - val_acc: 0.6635\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8237 - acc: 0.6584 - val_loss: 0.8165 - val_acc: 0.6635\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81631 to 0.81566, saving model to best.model\n",
      "0s - loss: 0.8225 - acc: 0.6584 - val_loss: 0.8157 - val_acc: 0.6635\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8223 - acc: 0.6590 - val_loss: 0.8160 - val_acc: 0.6635\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81566 to 0.81421, saving model to best.model\n",
      "0s - loss: 0.8215 - acc: 0.6591 - val_loss: 0.8142 - val_acc: 0.6635\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81421 to 0.81343, saving model to best.model\n",
      "0s - loss: 0.8204 - acc: 0.6587 - val_loss: 0.8134 - val_acc: 0.6635\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81343 to 0.81259, saving model to best.model\n",
      "0s - loss: 0.8200 - acc: 0.6593 - val_loss: 0.8126 - val_acc: 0.6635\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81259 to 0.81129, saving model to best.model\n",
      "0s - loss: 0.8193 - acc: 0.6592 - val_loss: 0.8113 - val_acc: 0.6643\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81129 to 0.81065, saving model to best.model\n",
      "0s - loss: 0.8172 - acc: 0.6609 - val_loss: 0.8106 - val_acc: 0.6641\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81065 to 0.80986, saving model to best.model\n",
      "0s - loss: 0.8180 - acc: 0.6608 - val_loss: 0.8099 - val_acc: 0.6646\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80986 to 0.80873, saving model to best.model\n",
      "0s - loss: 0.8153 - acc: 0.6631 - val_loss: 0.8087 - val_acc: 0.6656\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80873 to 0.80749, saving model to best.model\n",
      "0s - loss: 0.8136 - acc: 0.6628 - val_loss: 0.8075 - val_acc: 0.6703\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80749 to 0.80517, saving model to best.model\n",
      "1s - loss: 0.8132 - acc: 0.6635 - val_loss: 0.8052 - val_acc: 0.6700\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80517 to 0.80471, saving model to best.model\n",
      "1s - loss: 0.8145 - acc: 0.6622 - val_loss: 0.8047 - val_acc: 0.6701\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80471 to 0.80271, saving model to best.model\n",
      "1s - loss: 0.8105 - acc: 0.6657 - val_loss: 0.8027 - val_acc: 0.6713\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.8106 - acc: 0.6646 - val_loss: 0.8032 - val_acc: 0.6713\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80271 to 0.80018, saving model to best.model\n",
      "1s - loss: 0.8089 - acc: 0.6656 - val_loss: 0.8002 - val_acc: 0.6726\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80018 to 0.79946, saving model to best.model\n",
      "0s - loss: 0.8083 - acc: 0.6654 - val_loss: 0.7995 - val_acc: 0.6730\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "1s - loss: 0.8073 - acc: 0.6656 - val_loss: 0.7997 - val_acc: 0.6730\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79946 to 0.79696, saving model to best.model\n",
      "1s - loss: 0.8079 - acc: 0.6662 - val_loss: 0.7970 - val_acc: 0.6738\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79696 to 0.79530, saving model to best.model\n",
      "1s - loss: 0.8047 - acc: 0.6674 - val_loss: 0.7953 - val_acc: 0.6740\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79530 to 0.79454, saving model to best.model\n",
      "1s - loss: 0.8032 - acc: 0.6676 - val_loss: 0.7945 - val_acc: 0.6744\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79454 to 0.79189, saving model to best.model\n",
      "1s - loss: 0.8028 - acc: 0.6681 - val_loss: 0.7919 - val_acc: 0.6747\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79189 to 0.79023, saving model to best.model\n",
      "1s - loss: 0.8014 - acc: 0.6682 - val_loss: 0.7902 - val_acc: 0.6741\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79023 to 0.78741, saving model to best.model\n",
      "1s - loss: 0.7984 - acc: 0.6694 - val_loss: 0.7874 - val_acc: 0.6776\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78741 to 0.78715, saving model to best.model\n",
      "1s - loss: 0.8004 - acc: 0.6686 - val_loss: 0.7871 - val_acc: 0.6755\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78715 to 0.78374, saving model to best.model\n",
      "1s - loss: 0.7969 - acc: 0.6684 - val_loss: 0.7837 - val_acc: 0.6801\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78374 to 0.78369, saving model to best.model\n",
      "1s - loss: 0.7957 - acc: 0.6706 - val_loss: 0.7837 - val_acc: 0.6758\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78369 to 0.78309, saving model to best.model\n",
      "1s - loss: 0.7944 - acc: 0.6698 - val_loss: 0.7831 - val_acc: 0.6807\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78309 to 0.77815, saving model to best.model\n",
      "1s - loss: 0.7925 - acc: 0.6722 - val_loss: 0.7782 - val_acc: 0.6813\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "1s - loss: 0.7925 - acc: 0.6713 - val_loss: 0.7787 - val_acc: 0.6779\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77815 to 0.77407, saving model to best.model\n",
      "1s - loss: 0.7905 - acc: 0.6719 - val_loss: 0.7741 - val_acc: 0.6858\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "1s - loss: 0.7883 - acc: 0.6727 - val_loss: 0.7749 - val_acc: 0.6793\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77407 to 0.77178, saving model to best.model\n",
      "1s - loss: 0.7879 - acc: 0.6737 - val_loss: 0.7718 - val_acc: 0.6864\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77178 to 0.76992, saving model to best.model\n",
      "1s - loss: 0.7870 - acc: 0.6736 - val_loss: 0.7699 - val_acc: 0.6861\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76992 to 0.76971, saving model to best.model\n",
      "1s - loss: 0.7860 - acc: 0.6729 - val_loss: 0.7697 - val_acc: 0.6869\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76971 to 0.76577, saving model to best.model\n",
      "1s - loss: 0.7847 - acc: 0.6741 - val_loss: 0.7658 - val_acc: 0.6899\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76577 to 0.76352, saving model to best.model\n",
      "1s - loss: 0.7822 - acc: 0.6753 - val_loss: 0.7635 - val_acc: 0.6917\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76352 to 0.76313, saving model to best.model\n",
      "0s - loss: 0.7798 - acc: 0.6766 - val_loss: 0.7631 - val_acc: 0.6855\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76313 to 0.76166, saving model to best.model\n",
      "1s - loss: 0.7800 - acc: 0.6758 - val_loss: 0.7617 - val_acc: 0.6867\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76166 to 0.75740, saving model to best.model\n",
      "1s - loss: 0.7756 - acc: 0.6765 - val_loss: 0.7574 - val_acc: 0.6925\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.7768 - acc: 0.6770 - val_loss: 0.7580 - val_acc: 0.6917\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75740 to 0.75183, saving model to best.model\n",
      "1s - loss: 0.7724 - acc: 0.6798 - val_loss: 0.7518 - val_acc: 0.6948\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75183 to 0.75153, saving model to best.model\n",
      "1s - loss: 0.7694 - acc: 0.6800 - val_loss: 0.7515 - val_acc: 0.6934\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75153 to 0.74741, saving model to best.model\n",
      "1s - loss: 0.7698 - acc: 0.6799 - val_loss: 0.7474 - val_acc: 0.6946\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74741 to 0.74648, saving model to best.model\n",
      "1s - loss: 0.7688 - acc: 0.6797 - val_loss: 0.7465 - val_acc: 0.6963\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74648 to 0.74198, saving model to best.model\n",
      "1s - loss: 0.7663 - acc: 0.6813 - val_loss: 0.7420 - val_acc: 0.6982\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74198 to 0.74010, saving model to best.model\n",
      "1s - loss: 0.7657 - acc: 0.6816 - val_loss: 0.7401 - val_acc: 0.6992\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74010 to 0.73769, saving model to best.model\n",
      "1s - loss: 0.7614 - acc: 0.6844 - val_loss: 0.7377 - val_acc: 0.6987\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73769 to 0.73378, saving model to best.model\n",
      "1s - loss: 0.7605 - acc: 0.6859 - val_loss: 0.7338 - val_acc: 0.7054\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73378 to 0.73369, saving model to best.model\n",
      "1s - loss: 0.7607 - acc: 0.6825 - val_loss: 0.7337 - val_acc: 0.7042\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73369 to 0.73115, saving model to best.model\n",
      "1s - loss: 0.7579 - acc: 0.6852 - val_loss: 0.7311 - val_acc: 0.7035\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.7586 - acc: 0.6843 - val_loss: 0.7332 - val_acc: 0.7053\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73115 to 0.72578, saving model to best.model\n",
      "0s - loss: 0.7559 - acc: 0.6852 - val_loss: 0.7258 - val_acc: 0.7033\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72578 to 0.71946, saving model to best.model\n",
      "0s - loss: 0.7507 - acc: 0.6885 - val_loss: 0.7195 - val_acc: 0.7109\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.7511 - acc: 0.6887 - val_loss: 0.7214 - val_acc: 0.7087\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71946 to 0.71639, saving model to best.model\n",
      "0s - loss: 0.7492 - acc: 0.6900 - val_loss: 0.7164 - val_acc: 0.7097\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71639 to 0.71402, saving model to best.model\n",
      "0s - loss: 0.7491 - acc: 0.6915 - val_loss: 0.7140 - val_acc: 0.7114\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.7463 - acc: 0.6917 - val_loss: 0.7141 - val_acc: 0.7149\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71402 to 0.71132, saving model to best.model\n",
      "0s - loss: 0.7437 - acc: 0.6907 - val_loss: 0.7113 - val_acc: 0.7163\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71132 to 0.70900, saving model to best.model\n",
      "0s - loss: 0.7412 - acc: 0.6917 - val_loss: 0.7090 - val_acc: 0.7143\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70900 to 0.70638, saving model to best.model\n",
      "0s - loss: 0.7393 - acc: 0.6929 - val_loss: 0.7064 - val_acc: 0.7148\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70638 to 0.70380, saving model to best.model\n",
      "1s - loss: 0.7408 - acc: 0.6945 - val_loss: 0.7038 - val_acc: 0.7162\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70380 to 0.70337, saving model to best.model\n",
      "1s - loss: 0.7393 - acc: 0.6915 - val_loss: 0.7034 - val_acc: 0.7122\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70337 to 0.69702, saving model to best.model\n",
      "1s - loss: 0.7341 - acc: 0.6986 - val_loss: 0.6970 - val_acc: 0.7208\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7360 - acc: 0.6968 - val_loss: 0.6971 - val_acc: 0.7225\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69702 to 0.69619, saving model to best.model\n",
      "1s - loss: 0.7362 - acc: 0.6985 - val_loss: 0.6962 - val_acc: 0.7226\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69619 to 0.69540, saving model to best.model\n",
      "0s - loss: 0.7329 - acc: 0.6968 - val_loss: 0.6954 - val_acc: 0.7208\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69540 to 0.68955, saving model to best.model\n",
      "1s - loss: 0.7261 - acc: 0.7017 - val_loss: 0.6895 - val_acc: 0.7251\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68955 to 0.68764, saving model to best.model\n",
      "1s - loss: 0.7278 - acc: 0.7013 - val_loss: 0.6876 - val_acc: 0.7238\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68764 to 0.68574, saving model to best.model\n",
      "0s - loss: 0.7267 - acc: 0.7010 - val_loss: 0.6857 - val_acc: 0.7259\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7250 - acc: 0.7028 - val_loss: 0.6858 - val_acc: 0.7230\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68574 to 0.68291, saving model to best.model\n",
      "0s - loss: 0.7224 - acc: 0.7041 - val_loss: 0.6829 - val_acc: 0.7255\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68291 to 0.68266, saving model to best.model\n",
      "0s - loss: 0.7243 - acc: 0.7030 - val_loss: 0.6827 - val_acc: 0.7246\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68266 to 0.67914, saving model to best.model\n",
      "0s - loss: 0.7214 - acc: 0.7039 - val_loss: 0.6791 - val_acc: 0.7280\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67914 to 0.67640, saving model to best.model\n",
      "0s - loss: 0.7195 - acc: 0.7051 - val_loss: 0.6764 - val_acc: 0.7307\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.7201 - acc: 0.7017 - val_loss: 0.6788 - val_acc: 0.7280\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67640 to 0.67385, saving model to best.model\n",
      "0s - loss: 0.7182 - acc: 0.7030 - val_loss: 0.6738 - val_acc: 0.7308\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.7174 - acc: 0.7074 - val_loss: 0.6761 - val_acc: 0.7317\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67385 to 0.67208, saving model to best.model\n",
      "0s - loss: 0.7150 - acc: 0.7059 - val_loss: 0.6721 - val_acc: 0.7304\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67208 to 0.66993, saving model to best.model\n",
      "0s - loss: 0.7160 - acc: 0.7039 - val_loss: 0.6699 - val_acc: 0.7347\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.7103 - acc: 0.7091 - val_loss: 0.6704 - val_acc: 0.7293\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.66993 to 0.66858, saving model to best.model\n",
      "0s - loss: 0.7101 - acc: 0.7066 - val_loss: 0.6686 - val_acc: 0.7321\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66858 to 0.66763, saving model to best.model\n",
      "0s - loss: 0.7100 - acc: 0.7096 - val_loss: 0.6676 - val_acc: 0.7319\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.66763 to 0.66540, saving model to best.model\n",
      "1s - loss: 0.7103 - acc: 0.7083 - val_loss: 0.6654 - val_acc: 0.7372\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66540 to 0.66510, saving model to best.model\n",
      "0s - loss: 0.7073 - acc: 0.7102 - val_loss: 0.6651 - val_acc: 0.7372\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66510 to 0.66381, saving model to best.model\n",
      "1s - loss: 0.7067 - acc: 0.7128 - val_loss: 0.6638 - val_acc: 0.7327\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7037 - acc: 0.7120 - val_loss: 0.6653 - val_acc: 0.7323\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66381 to 0.66066, saving model to best.model\n",
      "1s - loss: 0.7040 - acc: 0.7124 - val_loss: 0.6607 - val_acc: 0.7369\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66066 to 0.65811, saving model to best.model\n",
      "1s - loss: 0.7052 - acc: 0.7129 - val_loss: 0.6581 - val_acc: 0.7391\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.7003 - acc: 0.7128 - val_loss: 0.6606 - val_acc: 0.7340\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7046 - acc: 0.7128 - val_loss: 0.6600 - val_acc: 0.7356\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7006 - acc: 0.7136 - val_loss: 0.6598 - val_acc: 0.7371\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65811 to 0.65564, saving model to best.model\n",
      "1s - loss: 0.7011 - acc: 0.7127 - val_loss: 0.6556 - val_acc: 0.7374\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65564 to 0.65482, saving model to best.model\n",
      "1s - loss: 0.6996 - acc: 0.7123 - val_loss: 0.6548 - val_acc: 0.7383\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65482 to 0.65399, saving model to best.model\n",
      "1s - loss: 0.6990 - acc: 0.7152 - val_loss: 0.6540 - val_acc: 0.7389\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6986 - acc: 0.7132 - val_loss: 0.6572 - val_acc: 0.7379\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6967 - acc: 0.7146 - val_loss: 0.6541 - val_acc: 0.7388\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6993 - acc: 0.7161 - val_loss: 0.6548 - val_acc: 0.7363\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65399 to 0.65398, saving model to best.model\n",
      "1s - loss: 0.6963 - acc: 0.7148 - val_loss: 0.6540 - val_acc: 0.7371\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65398 to 0.64926, saving model to best.model\n",
      "1s - loss: 0.6917 - acc: 0.7164 - val_loss: 0.6493 - val_acc: 0.7389\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6952 - acc: 0.7159 - val_loss: 0.6502 - val_acc: 0.7384\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.64926 to 0.64865, saving model to best.model\n",
      "0s - loss: 0.6934 - acc: 0.7190 - val_loss: 0.6487 - val_acc: 0.7409\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64865 to 0.64772, saving model to best.model\n",
      "1s - loss: 0.6912 - acc: 0.7166 - val_loss: 0.6477 - val_acc: 0.7392\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6921 - acc: 0.7148 - val_loss: 0.6485 - val_acc: 0.7382\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6897 - acc: 0.7172 - val_loss: 0.6512 - val_acc: 0.7362\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64772 to 0.64489, saving model to best.model\n",
      "1s - loss: 0.6896 - acc: 0.7187 - val_loss: 0.6449 - val_acc: 0.7407\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64489 to 0.64106, saving model to best.model\n",
      "1s - loss: 0.6898 - acc: 0.7171 - val_loss: 0.6411 - val_acc: 0.7415\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6851 - acc: 0.7203 - val_loss: 0.6457 - val_acc: 0.7397\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6854 - acc: 0.7210 - val_loss: 0.6416 - val_acc: 0.7404\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "1s - loss: 0.6843 - acc: 0.7218 - val_loss: 0.6413 - val_acc: 0.7398\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6871 - acc: 0.7193 - val_loss: 0.6426 - val_acc: 0.7422\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.64106 to 0.64051, saving model to best.model\n",
      "1s - loss: 0.6854 - acc: 0.7198 - val_loss: 0.6405 - val_acc: 0.7413\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6826 - acc: 0.7214 - val_loss: 0.6415 - val_acc: 0.7392\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64051 to 0.63916, saving model to best.model\n",
      "1s - loss: 0.6820 - acc: 0.7217 - val_loss: 0.6392 - val_acc: 0.7409\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6849 - acc: 0.7218 - val_loss: 0.6411 - val_acc: 0.7409\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6851 - acc: 0.7191 - val_loss: 0.6415 - val_acc: 0.7407\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.63916 to 0.63717, saving model to best.model\n",
      "1s - loss: 0.6818 - acc: 0.7198 - val_loss: 0.6372 - val_acc: 0.7411\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.63717 to 0.63611, saving model to best.model\n",
      "1s - loss: 0.6844 - acc: 0.7196 - val_loss: 0.6361 - val_acc: 0.7415\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6812 - acc: 0.7206 - val_loss: 0.6375 - val_acc: 0.7411\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63611 to 0.63491, saving model to best.model\n",
      "0s - loss: 0.6839 - acc: 0.7212 - val_loss: 0.6349 - val_acc: 0.7412\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.63491 to 0.63239, saving model to best.model\n",
      "1s - loss: 0.6786 - acc: 0.7239 - val_loss: 0.6324 - val_acc: 0.7448\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6809 - acc: 0.7223 - val_loss: 0.6347 - val_acc: 0.7419\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6800 - acc: 0.7229 - val_loss: 0.6338 - val_acc: 0.7427\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6791 - acc: 0.7232 - val_loss: 0.6329 - val_acc: 0.7437\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6770 - acc: 0.7250 - val_loss: 0.6329 - val_acc: 0.7456\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6764 - acc: 0.7226 - val_loss: 0.6331 - val_acc: 0.7445\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63239 to 0.62950, saving model to best.model\n",
      "1s - loss: 0.6740 - acc: 0.7276 - val_loss: 0.6295 - val_acc: 0.7448\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6758 - acc: 0.7246 - val_loss: 0.6305 - val_acc: 0.7439\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6789 - acc: 0.7184 - val_loss: 0.6306 - val_acc: 0.7457\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62950 to 0.62759, saving model to best.model\n",
      "0s - loss: 0.6704 - acc: 0.7256 - val_loss: 0.6276 - val_acc: 0.7439\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6736 - acc: 0.7252 - val_loss: 0.6298 - val_acc: 0.7430\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6729 - acc: 0.7243 - val_loss: 0.6280 - val_acc: 0.7436\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6702 - acc: 0.7271 - val_loss: 0.6282 - val_acc: 0.7438\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62759 to 0.62542, saving model to best.model\n",
      "0s - loss: 0.6722 - acc: 0.7260 - val_loss: 0.6254 - val_acc: 0.7486\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.62542 to 0.62125, saving model to best.model\n",
      "0s - loss: 0.6677 - acc: 0.7277 - val_loss: 0.6212 - val_acc: 0.7498\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6705 - acc: 0.7283 - val_loss: 0.6267 - val_acc: 0.7456\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62125 to 0.62122, saving model to best.model\n",
      "0s - loss: 0.6684 - acc: 0.7268 - val_loss: 0.6212 - val_acc: 0.7500\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6711 - acc: 0.7256 - val_loss: 0.6234 - val_acc: 0.7479\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6691 - acc: 0.7271 - val_loss: 0.6218 - val_acc: 0.7494\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62122 to 0.61958, saving model to best.model\n",
      "0s - loss: 0.6675 - acc: 0.7253 - val_loss: 0.6196 - val_acc: 0.7496\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6664 - acc: 0.7285 - val_loss: 0.6212 - val_acc: 0.7501\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6697 - acc: 0.7268 - val_loss: 0.6208 - val_acc: 0.7486\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61958 to 0.61755, saving model to best.model\n",
      "0s - loss: 0.6663 - acc: 0.7282 - val_loss: 0.6176 - val_acc: 0.7496\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6656 - acc: 0.7282 - val_loss: 0.6181 - val_acc: 0.7487\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6659 - acc: 0.7267 - val_loss: 0.6214 - val_acc: 0.7475\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6651 - acc: 0.7268 - val_loss: 0.6203 - val_acc: 0.7472\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6650 - acc: 0.7274 - val_loss: 0.6199 - val_acc: 0.7471\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61755 to 0.61676, saving model to best.model\n",
      "1s - loss: 0.6670 - acc: 0.7268 - val_loss: 0.6168 - val_acc: 0.7498\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6613 - acc: 0.7309 - val_loss: 0.6178 - val_acc: 0.7491\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61676 to 0.61665, saving model to best.model\n",
      "1s - loss: 0.6651 - acc: 0.7291 - val_loss: 0.6166 - val_acc: 0.7485\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.61665 to 0.61552, saving model to best.model\n",
      "1s - loss: 0.6638 - acc: 0.7287 - val_loss: 0.6155 - val_acc: 0.7494\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61552 to 0.61538, saving model to best.model\n",
      "1s - loss: 0.6646 - acc: 0.7278 - val_loss: 0.6154 - val_acc: 0.7487\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61538 to 0.61537, saving model to best.model\n",
      "1s - loss: 0.6624 - acc: 0.7303 - val_loss: 0.6154 - val_acc: 0.7485\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61537 to 0.61211, saving model to best.model\n",
      "1s - loss: 0.6625 - acc: 0.7288 - val_loss: 0.6121 - val_acc: 0.7516\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6588 - acc: 0.7312 - val_loss: 0.6137 - val_acc: 0.7508\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6596 - acc: 0.7311 - val_loss: 0.6141 - val_acc: 0.7502\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61211 to 0.61069, saving model to best.model\n",
      "1s - loss: 0.6577 - acc: 0.7306 - val_loss: 0.6107 - val_acc: 0.7494\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6624 - acc: 0.7293 - val_loss: 0.6128 - val_acc: 0.7500\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6605 - acc: 0.7300 - val_loss: 0.6124 - val_acc: 0.7504\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6617 - acc: 0.7294 - val_loss: 0.6119 - val_acc: 0.7523\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6566 - acc: 0.7281 - val_loss: 0.6108 - val_acc: 0.7509\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.61069 to 0.60886, saving model to best.model\n",
      "1s - loss: 0.6554 - acc: 0.7334 - val_loss: 0.6089 - val_acc: 0.7511\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.60886 to 0.60834, saving model to best.model\n",
      "0s - loss: 0.6549 - acc: 0.7299 - val_loss: 0.6083 - val_acc: 0.7523\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6578 - acc: 0.7292 - val_loss: 0.6127 - val_acc: 0.7521\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.60834 to 0.60613, saving model to best.model\n",
      "1s - loss: 0.6559 - acc: 0.7333 - val_loss: 0.6061 - val_acc: 0.7526\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6561 - acc: 0.7337 - val_loss: 0.6093 - val_acc: 0.7508\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6580 - acc: 0.7345 - val_loss: 0.6116 - val_acc: 0.7518\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6560 - acc: 0.7328 - val_loss: 0.6084 - val_acc: 0.7518\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60613 to 0.60579, saving model to best.model\n",
      "1s - loss: 0.6554 - acc: 0.7313 - val_loss: 0.6058 - val_acc: 0.7540\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60579 to 0.60471, saving model to best.model\n",
      "1s - loss: 0.6494 - acc: 0.7351 - val_loss: 0.6047 - val_acc: 0.7534\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6543 - acc: 0.7315 - val_loss: 0.6054 - val_acc: 0.7539\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.60471 to 0.60406, saving model to best.model\n",
      "1s - loss: 0.6564 - acc: 0.7321 - val_loss: 0.6041 - val_acc: 0.7528\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6560 - acc: 0.7326 - val_loss: 0.6051 - val_acc: 0.7525\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60406 to 0.60228, saving model to best.model\n",
      "1s - loss: 0.6513 - acc: 0.7352 - val_loss: 0.6023 - val_acc: 0.7552\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6520 - acc: 0.7357 - val_loss: 0.6053 - val_acc: 0.7530\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6512 - acc: 0.7323 - val_loss: 0.6051 - val_acc: 0.7552\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6507 - acc: 0.7343 - val_loss: 0.6028 - val_acc: 0.7543\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6511 - acc: 0.7334 - val_loss: 0.6025 - val_acc: 0.7544\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "16s - loss: 0.6524 - acc: 0.7320 - val_loss: 0.6028 - val_acc: 0.7549\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6497 - acc: 0.7362 - val_loss: 0.6047 - val_acc: 0.7549\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6484 - acc: 0.7339 - val_loss: 0.6028 - val_acc: 0.7547\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.60228 to 0.60141, saving model to best.model\n",
      "1s - loss: 0.6507 - acc: 0.7341 - val_loss: 0.6014 - val_acc: 0.7550\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60141 to 0.59887, saving model to best.model\n",
      "1s - loss: 0.6445 - acc: 0.7369 - val_loss: 0.5989 - val_acc: 0.7562\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6461 - acc: 0.7354 - val_loss: 0.5989 - val_acc: 0.7560\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6477 - acc: 0.7355 - val_loss: 0.5994 - val_acc: 0.7561\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.59887 to 0.59809, saving model to best.model\n",
      "1s - loss: 0.6494 - acc: 0.7353 - val_loss: 0.5981 - val_acc: 0.7574\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.82983, saving model to best.model\n",
      "0s - loss: 0.9524 - acc: 0.6071 - val_loss: 0.8298 - val_acc: 0.6655\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.8684 - acc: 0.6488 - val_loss: 0.8317 - val_acc: 0.6655\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.8589 - acc: 0.6537 - val_loss: 0.8318 - val_acc: 0.6655\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.82983 to 0.82694, saving model to best.model\n",
      "1s - loss: 0.8538 - acc: 0.6540 - val_loss: 0.8269 - val_acc: 0.6655\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.82694 to 0.82463, saving model to best.model\n",
      "1s - loss: 0.8525 - acc: 0.6540 - val_loss: 0.8246 - val_acc: 0.6655\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82463 to 0.81617, saving model to best.model\n",
      "1s - loss: 0.8455 - acc: 0.6540 - val_loss: 0.8162 - val_acc: 0.6655\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.81617 to 0.81292, saving model to best.model\n",
      "1s - loss: 0.8400 - acc: 0.6540 - val_loss: 0.8129 - val_acc: 0.6655\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.81292 to 0.80953, saving model to best.model\n",
      "1s - loss: 0.8365 - acc: 0.6540 - val_loss: 0.8095 - val_acc: 0.6655\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.80953 to 0.80945, saving model to best.model\n",
      "1s - loss: 0.8334 - acc: 0.6539 - val_loss: 0.8095 - val_acc: 0.6655\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.80945 to 0.80746, saving model to best.model\n",
      "1s - loss: 0.8321 - acc: 0.6539 - val_loss: 0.8075 - val_acc: 0.6655\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.80746 to 0.80544, saving model to best.model\n",
      "1s - loss: 0.8292 - acc: 0.6540 - val_loss: 0.8054 - val_acc: 0.6655\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.80544 to 0.80294, saving model to best.model\n",
      "1s - loss: 0.8291 - acc: 0.6540 - val_loss: 0.8029 - val_acc: 0.6655\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8282 - acc: 0.6541 - val_loss: 0.8051 - val_acc: 0.6655\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.80294 to 0.80198, saving model to best.model\n",
      "1s - loss: 0.8278 - acc: 0.6547 - val_loss: 0.8020 - val_acc: 0.6655\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.80198 to 0.80111, saving model to best.model\n",
      "1s - loss: 0.8269 - acc: 0.6541 - val_loss: 0.8011 - val_acc: 0.6655\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.80111 to 0.80095, saving model to best.model\n",
      "1s - loss: 0.8261 - acc: 0.6539 - val_loss: 0.8009 - val_acc: 0.6655\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.80095 to 0.79946, saving model to best.model\n",
      "1s - loss: 0.8255 - acc: 0.6550 - val_loss: 0.7995 - val_acc: 0.6655\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.79946 to 0.79941, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6551 - val_loss: 0.7994 - val_acc: 0.6663\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.79941 to 0.79767, saving model to best.model\n",
      "1s - loss: 0.8232 - acc: 0.6546 - val_loss: 0.7977 - val_acc: 0.6665\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "1s - loss: 0.8216 - acc: 0.6575 - val_loss: 0.8013 - val_acc: 0.6708\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "1s - loss: 0.8214 - acc: 0.6572 - val_loss: 0.7979 - val_acc: 0.6696\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.79767 to 0.79443, saving model to best.model\n",
      "1s - loss: 0.8203 - acc: 0.6568 - val_loss: 0.7944 - val_acc: 0.6720\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "1s - loss: 0.8181 - acc: 0.6591 - val_loss: 0.7947 - val_acc: 0.6741\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.79443 to 0.79222, saving model to best.model\n",
      "1s - loss: 0.8175 - acc: 0.6597 - val_loss: 0.7922 - val_acc: 0.6748\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "1s - loss: 0.8170 - acc: 0.6578 - val_loss: 0.7938 - val_acc: 0.6746\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.79222 to 0.79118, saving model to best.model\n",
      "1s - loss: 0.8159 - acc: 0.6618 - val_loss: 0.7912 - val_acc: 0.6758\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.79118 to 0.79023, saving model to best.model\n",
      "1s - loss: 0.8148 - acc: 0.6602 - val_loss: 0.7902 - val_acc: 0.6763\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.79023 to 0.78807, saving model to best.model\n",
      "1s - loss: 0.8140 - acc: 0.6628 - val_loss: 0.7881 - val_acc: 0.6768\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.78807 to 0.78750, saving model to best.model\n",
      "1s - loss: 0.8123 - acc: 0.6637 - val_loss: 0.7875 - val_acc: 0.6773\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.78750 to 0.78687, saving model to best.model\n",
      "1s - loss: 0.8118 - acc: 0.6617 - val_loss: 0.7869 - val_acc: 0.6775\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.78687 to 0.78568, saving model to best.model\n",
      "1s - loss: 0.8107 - acc: 0.6625 - val_loss: 0.7857 - val_acc: 0.6778\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.78568 to 0.78192, saving model to best.model\n",
      "1s - loss: 0.8091 - acc: 0.6631 - val_loss: 0.7819 - val_acc: 0.6781\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.78192 to 0.77969, saving model to best.model\n",
      "1s - loss: 0.8064 - acc: 0.6629 - val_loss: 0.7797 - val_acc: 0.6794\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "1s - loss: 0.8061 - acc: 0.6653 - val_loss: 0.7841 - val_acc: 0.6819\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.77969 to 0.77494, saving model to best.model\n",
      "1s - loss: 0.8040 - acc: 0.6662 - val_loss: 0.7749 - val_acc: 0.6821\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "1s - loss: 0.8040 - acc: 0.6661 - val_loss: 0.7762 - val_acc: 0.6821\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.77494 to 0.77448, saving model to best.model\n",
      "1s - loss: 0.8019 - acc: 0.6663 - val_loss: 0.7745 - val_acc: 0.6796\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.77448 to 0.77050, saving model to best.model\n",
      "1s - loss: 0.7986 - acc: 0.6680 - val_loss: 0.7705 - val_acc: 0.6850\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.77050 to 0.76942, saving model to best.model\n",
      "1s - loss: 0.7975 - acc: 0.6686 - val_loss: 0.7694 - val_acc: 0.6836\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "1s - loss: 0.7957 - acc: 0.6696 - val_loss: 0.7701 - val_acc: 0.6840\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.76942 to 0.76649, saving model to best.model\n",
      "1s - loss: 0.7965 - acc: 0.6691 - val_loss: 0.7665 - val_acc: 0.6847\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.76649 to 0.76312, saving model to best.model\n",
      "1s - loss: 0.7943 - acc: 0.6708 - val_loss: 0.7631 - val_acc: 0.6874\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.76312 to 0.76122, saving model to best.model\n",
      "1s - loss: 0.7921 - acc: 0.6700 - val_loss: 0.7612 - val_acc: 0.6883\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.76122 to 0.76090, saving model to best.model\n",
      "1s - loss: 0.7910 - acc: 0.6699 - val_loss: 0.7609 - val_acc: 0.6869\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.76090 to 0.75610, saving model to best.model\n",
      "1s - loss: 0.7891 - acc: 0.6719 - val_loss: 0.7561 - val_acc: 0.6920\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "1s - loss: 0.7862 - acc: 0.6749 - val_loss: 0.7571 - val_acc: 0.6924\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.75610 to 0.75341, saving model to best.model\n",
      "1s - loss: 0.7851 - acc: 0.6745 - val_loss: 0.7534 - val_acc: 0.6930\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.75341 to 0.75086, saving model to best.model\n",
      "1s - loss: 0.7829 - acc: 0.6756 - val_loss: 0.7509 - val_acc: 0.6904\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.75086 to 0.74873, saving model to best.model\n",
      "1s - loss: 0.7839 - acc: 0.6759 - val_loss: 0.7487 - val_acc: 0.6951\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "1s - loss: 0.7807 - acc: 0.6755 - val_loss: 0.7489 - val_acc: 0.6926\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.74873 to 0.74732, saving model to best.model\n",
      "1s - loss: 0.7812 - acc: 0.6744 - val_loss: 0.7473 - val_acc: 0.6984\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.74732 to 0.73958, saving model to best.model\n",
      "1s - loss: 0.7765 - acc: 0.6772 - val_loss: 0.7396 - val_acc: 0.6986\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.7750 - acc: 0.6784 - val_loss: 0.7415 - val_acc: 0.6988\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.7741 - acc: 0.6770 - val_loss: 0.7397 - val_acc: 0.6995\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.73958 to 0.73303, saving model to best.model\n",
      "0s - loss: 0.7717 - acc: 0.6783 - val_loss: 0.7330 - val_acc: 0.7018\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.73303 to 0.73291, saving model to best.model\n",
      "0s - loss: 0.7716 - acc: 0.6795 - val_loss: 0.7329 - val_acc: 0.7042\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.73291 to 0.73130, saving model to best.model\n",
      "0s - loss: 0.7687 - acc: 0.6824 - val_loss: 0.7313 - val_acc: 0.7028\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.73130 to 0.73108, saving model to best.model\n",
      "0s - loss: 0.7668 - acc: 0.6809 - val_loss: 0.7311 - val_acc: 0.7015\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73108 to 0.72466, saving model to best.model\n",
      "0s - loss: 0.7641 - acc: 0.6841 - val_loss: 0.7247 - val_acc: 0.7042\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.72466 to 0.72307, saving model to best.model\n",
      "0s - loss: 0.7621 - acc: 0.6841 - val_loss: 0.7231 - val_acc: 0.7061\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "0s - loss: 0.7618 - acc: 0.6840 - val_loss: 0.7247 - val_acc: 0.7097\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72307 to 0.71689, saving model to best.model\n",
      "0s - loss: 0.7583 - acc: 0.6847 - val_loss: 0.7169 - val_acc: 0.7089\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.7592 - acc: 0.6856 - val_loss: 0.7190 - val_acc: 0.7125\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.71689 to 0.71411, saving model to best.model\n",
      "0s - loss: 0.7577 - acc: 0.6859 - val_loss: 0.7141 - val_acc: 0.7108\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.71411 to 0.70897, saving model to best.model\n",
      "0s - loss: 0.7519 - acc: 0.6878 - val_loss: 0.7090 - val_acc: 0.7133\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.70897 to 0.70645, saving model to best.model\n",
      "0s - loss: 0.7525 - acc: 0.6889 - val_loss: 0.7064 - val_acc: 0.7146\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.70645 to 0.70454, saving model to best.model\n",
      "0s - loss: 0.7499 - acc: 0.6907 - val_loss: 0.7045 - val_acc: 0.7165\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.70454 to 0.70298, saving model to best.model\n",
      "0s - loss: 0.7479 - acc: 0.6902 - val_loss: 0.7030 - val_acc: 0.7138\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.70298 to 0.69943, saving model to best.model\n",
      "0s - loss: 0.7453 - acc: 0.6919 - val_loss: 0.6994 - val_acc: 0.7178\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "1s - loss: 0.7445 - acc: 0.6923 - val_loss: 0.6995 - val_acc: 0.7158\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.69943 to 0.69357, saving model to best.model\n",
      "1s - loss: 0.7452 - acc: 0.6950 - val_loss: 0.6936 - val_acc: 0.7192\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.7422 - acc: 0.6926 - val_loss: 0.6972 - val_acc: 0.7189\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.69357 to 0.68923, saving model to best.model\n",
      "1s - loss: 0.7419 - acc: 0.6928 - val_loss: 0.6892 - val_acc: 0.7221\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.7416 - acc: 0.6949 - val_loss: 0.6932 - val_acc: 0.7191\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.68923 to 0.68870, saving model to best.model\n",
      "1s - loss: 0.7385 - acc: 0.6973 - val_loss: 0.6887 - val_acc: 0.7238\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.68870 to 0.68376, saving model to best.model\n",
      "1s - loss: 0.7354 - acc: 0.6960 - val_loss: 0.6838 - val_acc: 0.7254\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7367 - acc: 0.6940 - val_loss: 0.6853 - val_acc: 0.7249\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.68376 to 0.67998, saving model to best.model\n",
      "1s - loss: 0.7331 - acc: 0.6979 - val_loss: 0.6800 - val_acc: 0.7259\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.67998 to 0.67893, saving model to best.model\n",
      "1s - loss: 0.7264 - acc: 0.7021 - val_loss: 0.6789 - val_acc: 0.7266\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.67893 to 0.67766, saving model to best.model\n",
      "1s - loss: 0.7296 - acc: 0.7008 - val_loss: 0.6777 - val_acc: 0.7290\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7269 - acc: 0.6983 - val_loss: 0.6783 - val_acc: 0.7301\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.67766 to 0.67471, saving model to best.model\n",
      "0s - loss: 0.7240 - acc: 0.7034 - val_loss: 0.6747 - val_acc: 0.7301\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.7281 - acc: 0.7014 - val_loss: 0.6784 - val_acc: 0.7335\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.67471 to 0.67266, saving model to best.model\n",
      "0s - loss: 0.7254 - acc: 0.7021 - val_loss: 0.6727 - val_acc: 0.7311\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.67266 to 0.66810, saving model to best.model\n",
      "0s - loss: 0.7189 - acc: 0.7070 - val_loss: 0.6681 - val_acc: 0.7330\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.66810 to 0.66765, saving model to best.model\n",
      "0s - loss: 0.7210 - acc: 0.7046 - val_loss: 0.6677 - val_acc: 0.7324\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.66765 to 0.66692, saving model to best.model\n",
      "0s - loss: 0.7184 - acc: 0.7064 - val_loss: 0.6669 - val_acc: 0.7309\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.66692 to 0.66256, saving model to best.model\n",
      "0s - loss: 0.7171 - acc: 0.7043 - val_loss: 0.6626 - val_acc: 0.7345\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.7170 - acc: 0.7049 - val_loss: 0.6633 - val_acc: 0.7311\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.7165 - acc: 0.7043 - val_loss: 0.6644 - val_acc: 0.7308\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.7145 - acc: 0.7044 - val_loss: 0.6628 - val_acc: 0.7365\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.66256 to 0.65744, saving model to best.model\n",
      "1s - loss: 0.7139 - acc: 0.7074 - val_loss: 0.6574 - val_acc: 0.7369\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.65744 to 0.65565, saving model to best.model\n",
      "1s - loss: 0.7115 - acc: 0.7075 - val_loss: 0.6557 - val_acc: 0.7345\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7119 - acc: 0.7081 - val_loss: 0.6562 - val_acc: 0.7379\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.65565 to 0.65400, saving model to best.model\n",
      "1s - loss: 0.7115 - acc: 0.7097 - val_loss: 0.6540 - val_acc: 0.7355\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.65400 to 0.65366, saving model to best.model\n",
      "1s - loss: 0.7109 - acc: 0.7084 - val_loss: 0.6537 - val_acc: 0.7381\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.65366 to 0.64821, saving model to best.model\n",
      "1s - loss: 0.7059 - acc: 0.7110 - val_loss: 0.6482 - val_acc: 0.7413\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7074 - acc: 0.7097 - val_loss: 0.6490 - val_acc: 0.7396\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7078 - acc: 0.7098 - val_loss: 0.6512 - val_acc: 0.7389\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.64821 to 0.64704, saving model to best.model\n",
      "1s - loss: 0.7038 - acc: 0.7090 - val_loss: 0.6470 - val_acc: 0.7431\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.64704 to 0.64596, saving model to best.model\n",
      "1s - loss: 0.7043 - acc: 0.7119 - val_loss: 0.6460 - val_acc: 0.7427\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7034 - acc: 0.7123 - val_loss: 0.6504 - val_acc: 0.7412\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.64596 to 0.64161, saving model to best.model\n",
      "1s - loss: 0.7029 - acc: 0.7122 - val_loss: 0.6416 - val_acc: 0.7436\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.6995 - acc: 0.7148 - val_loss: 0.6443 - val_acc: 0.7363\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7012 - acc: 0.7126 - val_loss: 0.6456 - val_acc: 0.7413\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.64161 to 0.64158, saving model to best.model\n",
      "1s - loss: 0.7035 - acc: 0.7117 - val_loss: 0.6416 - val_acc: 0.7432\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.6965 - acc: 0.7154 - val_loss: 0.6433 - val_acc: 0.7456\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6952 - acc: 0.7150 - val_loss: 0.6416 - val_acc: 0.7420\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64158 to 0.63894, saving model to best.model\n",
      "1s - loss: 0.6964 - acc: 0.7134 - val_loss: 0.6389 - val_acc: 0.7445\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6954 - acc: 0.7150 - val_loss: 0.6397 - val_acc: 0.7416\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.63894 to 0.63570, saving model to best.model\n",
      "0s - loss: 0.6958 - acc: 0.7157 - val_loss: 0.6357 - val_acc: 0.7445\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6936 - acc: 0.7154 - val_loss: 0.6374 - val_acc: 0.7423\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6932 - acc: 0.7180 - val_loss: 0.6371 - val_acc: 0.7477\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.63570 to 0.63429, saving model to best.model\n",
      "1s - loss: 0.6916 - acc: 0.7152 - val_loss: 0.6343 - val_acc: 0.7433\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.63429 to 0.63214, saving model to best.model\n",
      "1s - loss: 0.6905 - acc: 0.7178 - val_loss: 0.6321 - val_acc: 0.7478\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6928 - acc: 0.7158 - val_loss: 0.6349 - val_acc: 0.7451\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.6870 - acc: 0.7171 - val_loss: 0.6338 - val_acc: 0.7457\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.63214 to 0.63137, saving model to best.model\n",
      "1s - loss: 0.6869 - acc: 0.7177 - val_loss: 0.6314 - val_acc: 0.7441\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.63137 to 0.62853, saving model to best.model\n",
      "1s - loss: 0.6858 - acc: 0.7192 - val_loss: 0.6285 - val_acc: 0.7504\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.62853 to 0.62725, saving model to best.model\n",
      "1s - loss: 0.6855 - acc: 0.7186 - val_loss: 0.6272 - val_acc: 0.7457\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.62725 to 0.62554, saving model to best.model\n",
      "1s - loss: 0.6840 - acc: 0.7217 - val_loss: 0.6255 - val_acc: 0.7482\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.62554 to 0.62467, saving model to best.model\n",
      "0s - loss: 0.6814 - acc: 0.7188 - val_loss: 0.6247 - val_acc: 0.7485\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6853 - acc: 0.7207 - val_loss: 0.6249 - val_acc: 0.7534\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "1s - loss: 0.6867 - acc: 0.7201 - val_loss: 0.6281 - val_acc: 0.7499\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.62467 to 0.62415, saving model to best.model\n",
      "1s - loss: 0.6823 - acc: 0.7205 - val_loss: 0.6241 - val_acc: 0.7523\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.62415 to 0.62411, saving model to best.model\n",
      "1s - loss: 0.6822 - acc: 0.7212 - val_loss: 0.6241 - val_acc: 0.7479\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.62411 to 0.62207, saving model to best.model\n",
      "1s - loss: 0.6825 - acc: 0.7198 - val_loss: 0.6221 - val_acc: 0.7482\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6794 - acc: 0.7216 - val_loss: 0.6271 - val_acc: 0.7508\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.62207 to 0.61881, saving model to best.model\n",
      "1s - loss: 0.6792 - acc: 0.7227 - val_loss: 0.6188 - val_acc: 0.7500\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6804 - acc: 0.7205 - val_loss: 0.6201 - val_acc: 0.7506\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6786 - acc: 0.7225 - val_loss: 0.6239 - val_acc: 0.7444\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.61881 to 0.61687, saving model to best.model\n",
      "0s - loss: 0.6773 - acc: 0.7209 - val_loss: 0.6169 - val_acc: 0.7553\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6757 - acc: 0.7220 - val_loss: 0.6176 - val_acc: 0.7512\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.61687 to 0.61590, saving model to best.model\n",
      "0s - loss: 0.6733 - acc: 0.7224 - val_loss: 0.6159 - val_acc: 0.7537\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.61590 to 0.61476, saving model to best.model\n",
      "0s - loss: 0.6735 - acc: 0.7250 - val_loss: 0.6148 - val_acc: 0.7529\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.6759 - acc: 0.7234 - val_loss: 0.6175 - val_acc: 0.7523\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.61476 to 0.61424, saving model to best.model\n",
      "0s - loss: 0.6711 - acc: 0.7264 - val_loss: 0.6142 - val_acc: 0.7552\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6700 - acc: 0.7259 - val_loss: 0.6158 - val_acc: 0.7513\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.61424 to 0.61382, saving model to best.model\n",
      "0s - loss: 0.6718 - acc: 0.7256 - val_loss: 0.6138 - val_acc: 0.7515\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.61382 to 0.61243, saving model to best.model\n",
      "0s - loss: 0.6701 - acc: 0.7274 - val_loss: 0.6124 - val_acc: 0.7563\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.61243 to 0.61085, saving model to best.model\n",
      "0s - loss: 0.6715 - acc: 0.7264 - val_loss: 0.6108 - val_acc: 0.7549\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6705 - acc: 0.7255 - val_loss: 0.6117 - val_acc: 0.7533\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6726 - acc: 0.7242 - val_loss: 0.6133 - val_acc: 0.7553\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6742 - acc: 0.7234 - val_loss: 0.6116 - val_acc: 0.7559\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.61085 to 0.61030, saving model to best.model\n",
      "1s - loss: 0.6696 - acc: 0.7249 - val_loss: 0.6103 - val_acc: 0.7568\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6701 - acc: 0.7249 - val_loss: 0.6115 - val_acc: 0.7527\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.61030 to 0.60721, saving model to best.model\n",
      "1s - loss: 0.6653 - acc: 0.7253 - val_loss: 0.6072 - val_acc: 0.7567\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6669 - acc: 0.7249 - val_loss: 0.6086 - val_acc: 0.7593\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6685 - acc: 0.7268 - val_loss: 0.6094 - val_acc: 0.7535\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6630 - acc: 0.7287 - val_loss: 0.6088 - val_acc: 0.7571\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6662 - acc: 0.7279 - val_loss: 0.6089 - val_acc: 0.7542\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6661 - acc: 0.7300 - val_loss: 0.6083 - val_acc: 0.7600\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6679 - acc: 0.7261 - val_loss: 0.6105 - val_acc: 0.7576\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.60721 to 0.60566, saving model to best.model\n",
      "1s - loss: 0.6629 - acc: 0.7300 - val_loss: 0.6057 - val_acc: 0.7577\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6615 - acc: 0.7320 - val_loss: 0.6057 - val_acc: 0.7589\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.60566 to 0.60323, saving model to best.model\n",
      "1s - loss: 0.6597 - acc: 0.7297 - val_loss: 0.6032 - val_acc: 0.7593\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6615 - acc: 0.7291 - val_loss: 0.6038 - val_acc: 0.7573\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6654 - acc: 0.7265 - val_loss: 0.6044 - val_acc: 0.7563\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.60323 to 0.60177, saving model to best.model\n",
      "1s - loss: 0.6615 - acc: 0.7293 - val_loss: 0.6018 - val_acc: 0.7622\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.60177 to 0.60040, saving model to best.model\n",
      "1s - loss: 0.6586 - acc: 0.7307 - val_loss: 0.6004 - val_acc: 0.7616\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6629 - acc: 0.7273 - val_loss: 0.6040 - val_acc: 0.7587\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6583 - acc: 0.7291 - val_loss: 0.6011 - val_acc: 0.7603\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6573 - acc: 0.7318 - val_loss: 0.6005 - val_acc: 0.7609\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.60040 to 0.60001, saving model to best.model\n",
      "1s - loss: 0.6596 - acc: 0.7284 - val_loss: 0.6000 - val_acc: 0.7604\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6574 - acc: 0.7306 - val_loss: 0.6035 - val_acc: 0.7569\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6576 - acc: 0.7282 - val_loss: 0.6003 - val_acc: 0.7624\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.60001 to 0.59751, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7310 - val_loss: 0.5975 - val_acc: 0.7638\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6566 - acc: 0.7323 - val_loss: 0.5984 - val_acc: 0.7601\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6563 - acc: 0.7299 - val_loss: 0.5979 - val_acc: 0.7624\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6603 - acc: 0.7304 - val_loss: 0.5982 - val_acc: 0.7607\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6541 - acc: 0.7320 - val_loss: 0.5980 - val_acc: 0.7594\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.59751 to 0.59587, saving model to best.model\n",
      "1s - loss: 0.6568 - acc: 0.7318 - val_loss: 0.5959 - val_acc: 0.7644\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6567 - acc: 0.7317 - val_loss: 0.5965 - val_acc: 0.7623\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.59587 to 0.59574, saving model to best.model\n",
      "1s - loss: 0.6518 - acc: 0.7343 - val_loss: 0.5957 - val_acc: 0.7636\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6573 - acc: 0.7320 - val_loss: 0.5962 - val_acc: 0.7610\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.59574 to 0.59470, saving model to best.model\n",
      "1s - loss: 0.6527 - acc: 0.7339 - val_loss: 0.5947 - val_acc: 0.7630\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6522 - acc: 0.7341 - val_loss: 0.5948 - val_acc: 0.7648\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6506 - acc: 0.7331 - val_loss: 0.5952 - val_acc: 0.7645\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.59470 to 0.59422, saving model to best.model\n",
      "1s - loss: 0.6500 - acc: 0.7351 - val_loss: 0.5942 - val_acc: 0.7629\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.59422 to 0.59207, saving model to best.model\n",
      "1s - loss: 0.6516 - acc: 0.7321 - val_loss: 0.5921 - val_acc: 0.7635\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6482 - acc: 0.7350 - val_loss: 0.5954 - val_acc: 0.7650\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6512 - acc: 0.7333 - val_loss: 0.5952 - val_acc: 0.7643\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6487 - acc: 0.7360 - val_loss: 0.5930 - val_acc: 0.7637\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.59207 to 0.59139, saving model to best.model\n",
      "0s - loss: 0.6503 - acc: 0.7335 - val_loss: 0.5914 - val_acc: 0.7653\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6524 - acc: 0.7326 - val_loss: 0.5962 - val_acc: 0.7584\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.59139 to 0.58986, saving model to best.model\n",
      "0s - loss: 0.6486 - acc: 0.7338 - val_loss: 0.5899 - val_acc: 0.7662\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.58986 to 0.58976, saving model to best.model\n",
      "0s - loss: 0.6485 - acc: 0.7352 - val_loss: 0.5898 - val_acc: 0.7671\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6478 - acc: 0.7355 - val_loss: 0.5905 - val_acc: 0.7655\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.58976 to 0.58915, saving model to best.model\n",
      "0s - loss: 0.6453 - acc: 0.7344 - val_loss: 0.5891 - val_acc: 0.7679\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6454 - acc: 0.7352 - val_loss: 0.5904 - val_acc: 0.7662\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.58915 to 0.58818, saving model to best.model\n",
      "0s - loss: 0.6462 - acc: 0.7367 - val_loss: 0.5882 - val_acc: 0.7658\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.58818 to 0.58719, saving model to best.model\n",
      "0s - loss: 0.6445 - acc: 0.7350 - val_loss: 0.5872 - val_acc: 0.7681\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.58719 to 0.58642, saving model to best.model\n",
      "0s - loss: 0.6476 - acc: 0.7352 - val_loss: 0.5864 - val_acc: 0.7670\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6489 - acc: 0.7328 - val_loss: 0.5884 - val_acc: 0.7670\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.58642 to 0.58532, saving model to best.model\n",
      "1s - loss: 0.6469 - acc: 0.7359 - val_loss: 0.5853 - val_acc: 0.7689\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6426 - acc: 0.7378 - val_loss: 0.5871 - val_acc: 0.7667\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6463 - acc: 0.7371 - val_loss: 0.5880 - val_acc: 0.7687\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.58532 to 0.58518, saving model to best.model\n",
      "1s - loss: 0.6415 - acc: 0.7379 - val_loss: 0.5852 - val_acc: 0.7671\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6437 - acc: 0.7383 - val_loss: 0.5871 - val_acc: 0.7704\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.58518 to 0.58505, saving model to best.model\n",
      "1s - loss: 0.6424 - acc: 0.7382 - val_loss: 0.5850 - val_acc: 0.7683\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.85701, saving model to best.model\n",
      "0s - loss: 0.9546 - acc: 0.6143 - val_loss: 0.8570 - val_acc: 0.6571\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.85701 to 0.84953, saving model to best.model\n",
      "1s - loss: 0.8661 - acc: 0.6585 - val_loss: 0.8495 - val_acc: 0.6571\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84953 to 0.84752, saving model to best.model\n",
      "1s - loss: 0.8550 - acc: 0.6600 - val_loss: 0.8475 - val_acc: 0.6571\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84752 to 0.84647, saving model to best.model\n",
      "1s - loss: 0.8514 - acc: 0.6600 - val_loss: 0.8465 - val_acc: 0.6571\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84647 to 0.84338, saving model to best.model\n",
      "1s - loss: 0.8469 - acc: 0.6600 - val_loss: 0.8434 - val_acc: 0.6571\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.84338 to 0.83653, saving model to best.model\n",
      "1s - loss: 0.8430 - acc: 0.6600 - val_loss: 0.8365 - val_acc: 0.6571\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83653 to 0.83133, saving model to best.model\n",
      "1s - loss: 0.8377 - acc: 0.6599 - val_loss: 0.8313 - val_acc: 0.6571\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.83133 to 0.82829, saving model to best.model\n",
      "1s - loss: 0.8330 - acc: 0.6600 - val_loss: 0.8283 - val_acc: 0.6571\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82829 to 0.82646, saving model to best.model\n",
      "0s - loss: 0.8300 - acc: 0.6596 - val_loss: 0.8265 - val_acc: 0.6571\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82646 to 0.82565, saving model to best.model\n",
      "0s - loss: 0.8270 - acc: 0.6599 - val_loss: 0.8257 - val_acc: 0.6571\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82565 to 0.82408, saving model to best.model\n",
      "0s - loss: 0.8274 - acc: 0.6600 - val_loss: 0.8241 - val_acc: 0.6571\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.8250 - acc: 0.6601 - val_loss: 0.8277 - val_acc: 0.6571\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82408 to 0.82349, saving model to best.model\n",
      "0s - loss: 0.8251 - acc: 0.6603 - val_loss: 0.8235 - val_acc: 0.6571\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.8241 - acc: 0.6596 - val_loss: 0.8239 - val_acc: 0.6571\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82349 to 0.82306, saving model to best.model\n",
      "0s - loss: 0.8239 - acc: 0.6598 - val_loss: 0.8231 - val_acc: 0.6571\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82306 to 0.82230, saving model to best.model\n",
      "0s - loss: 0.8221 - acc: 0.6596 - val_loss: 0.8223 - val_acc: 0.6571\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "1s - loss: 0.8215 - acc: 0.6593 - val_loss: 0.8226 - val_acc: 0.6571\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82230 to 0.82177, saving model to best.model\n",
      "1s - loss: 0.8219 - acc: 0.6595 - val_loss: 0.8218 - val_acc: 0.6571\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.8200 - acc: 0.6598 - val_loss: 0.8218 - val_acc: 0.6571\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82177 to 0.82069, saving model to best.model\n",
      "1s - loss: 0.8209 - acc: 0.6596 - val_loss: 0.8207 - val_acc: 0.6571\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "1s - loss: 0.8198 - acc: 0.6599 - val_loss: 0.8209 - val_acc: 0.6571\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "1s - loss: 0.8186 - acc: 0.6596 - val_loss: 0.8215 - val_acc: 0.6571\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.82069 to 0.81972, saving model to best.model\n",
      "1s - loss: 0.8173 - acc: 0.6595 - val_loss: 0.8197 - val_acc: 0.6571\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81972 to 0.81949, saving model to best.model\n",
      "1s - loss: 0.8176 - acc: 0.6596 - val_loss: 0.8195 - val_acc: 0.6571\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81949 to 0.81812, saving model to best.model\n",
      "1s - loss: 0.8175 - acc: 0.6600 - val_loss: 0.8181 - val_acc: 0.6571\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81812 to 0.81757, saving model to best.model\n",
      "1s - loss: 0.8153 - acc: 0.6606 - val_loss: 0.8176 - val_acc: 0.6571\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81757 to 0.81645, saving model to best.model\n",
      "0s - loss: 0.8151 - acc: 0.6592 - val_loss: 0.8165 - val_acc: 0.6571\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81645 to 0.81550, saving model to best.model\n",
      "0s - loss: 0.8143 - acc: 0.6603 - val_loss: 0.8155 - val_acc: 0.6577\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.8132 - acc: 0.6615 - val_loss: 0.8170 - val_acc: 0.6590\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81550 to 0.81340, saving model to best.model\n",
      "0s - loss: 0.8127 - acc: 0.6618 - val_loss: 0.8134 - val_acc: 0.6591\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.81340 to 0.81312, saving model to best.model\n",
      "0s - loss: 0.8106 - acc: 0.6627 - val_loss: 0.8131 - val_acc: 0.6614\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.81312 to 0.81147, saving model to best.model\n",
      "0s - loss: 0.8113 - acc: 0.6619 - val_loss: 0.8115 - val_acc: 0.6603\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.81147 to 0.81032, saving model to best.model\n",
      "0s - loss: 0.8089 - acc: 0.6644 - val_loss: 0.8103 - val_acc: 0.6621\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.81032 to 0.80850, saving model to best.model\n",
      "0s - loss: 0.8075 - acc: 0.6644 - val_loss: 0.8085 - val_acc: 0.6624\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80850 to 0.80785, saving model to best.model\n",
      "0s - loss: 0.8063 - acc: 0.6660 - val_loss: 0.8079 - val_acc: 0.6687\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80785 to 0.80540, saving model to best.model\n",
      "0s - loss: 0.8068 - acc: 0.6629 - val_loss: 0.8054 - val_acc: 0.6680\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.80540 to 0.80324, saving model to best.model\n",
      "0s - loss: 0.8045 - acc: 0.6641 - val_loss: 0.8032 - val_acc: 0.6651\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.80324 to 0.80122, saving model to best.model\n",
      "0s - loss: 0.8029 - acc: 0.6663 - val_loss: 0.8012 - val_acc: 0.6694\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.80122 to 0.80087, saving model to best.model\n",
      "0s - loss: 0.8023 - acc: 0.6667 - val_loss: 0.8009 - val_acc: 0.6641\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.80087 to 0.79860, saving model to best.model\n",
      "0s - loss: 0.7998 - acc: 0.6652 - val_loss: 0.7986 - val_acc: 0.6703\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79860 to 0.79702, saving model to best.model\n",
      "0s - loss: 0.8002 - acc: 0.6680 - val_loss: 0.7970 - val_acc: 0.6663\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.79702 to 0.79544, saving model to best.model\n",
      "0s - loss: 0.7986 - acc: 0.6693 - val_loss: 0.7954 - val_acc: 0.6696\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.79544 to 0.79373, saving model to best.model\n",
      "0s - loss: 0.7977 - acc: 0.6682 - val_loss: 0.7937 - val_acc: 0.6694\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.79373 to 0.79065, saving model to best.model\n",
      "1s - loss: 0.7957 - acc: 0.6680 - val_loss: 0.7906 - val_acc: 0.6737\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.79065 to 0.78857, saving model to best.model\n",
      "1s - loss: 0.7933 - acc: 0.6686 - val_loss: 0.7886 - val_acc: 0.6749\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78857 to 0.78667, saving model to best.model\n",
      "1s - loss: 0.7928 - acc: 0.6699 - val_loss: 0.7867 - val_acc: 0.6749\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.78667 to 0.78353, saving model to best.model\n",
      "1s - loss: 0.7883 - acc: 0.6719 - val_loss: 0.7835 - val_acc: 0.6787\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.78353 to 0.78243, saving model to best.model\n",
      "1s - loss: 0.7903 - acc: 0.6718 - val_loss: 0.7824 - val_acc: 0.6819\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "1s - loss: 0.7890 - acc: 0.6700 - val_loss: 0.7825 - val_acc: 0.6789\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.78243 to 0.77695, saving model to best.model\n",
      "1s - loss: 0.7875 - acc: 0.6718 - val_loss: 0.7769 - val_acc: 0.6808\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77695 to 0.77567, saving model to best.model\n",
      "1s - loss: 0.7844 - acc: 0.6738 - val_loss: 0.7757 - val_acc: 0.6799\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.77567 to 0.77445, saving model to best.model\n",
      "1s - loss: 0.7820 - acc: 0.6743 - val_loss: 0.7744 - val_acc: 0.6806\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.77445 to 0.77106, saving model to best.model\n",
      "1s - loss: 0.7816 - acc: 0.6756 - val_loss: 0.7711 - val_acc: 0.6809\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.77106 to 0.76551, saving model to best.model\n",
      "1s - loss: 0.7785 - acc: 0.6756 - val_loss: 0.7655 - val_acc: 0.6864\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76551 to 0.76342, saving model to best.model\n",
      "1s - loss: 0.7775 - acc: 0.6742 - val_loss: 0.7634 - val_acc: 0.6868\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76342 to 0.76062, saving model to best.model\n",
      "1s - loss: 0.7748 - acc: 0.6761 - val_loss: 0.7606 - val_acc: 0.6895\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.76062 to 0.75951, saving model to best.model\n",
      "1s - loss: 0.7734 - acc: 0.6781 - val_loss: 0.7595 - val_acc: 0.6882\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75951 to 0.75213, saving model to best.model\n",
      "1s - loss: 0.7723 - acc: 0.6776 - val_loss: 0.7521 - val_acc: 0.6910\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75213 to 0.74964, saving model to best.model\n",
      "0s - loss: 0.7688 - acc: 0.6791 - val_loss: 0.7496 - val_acc: 0.6895\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74964 to 0.74607, saving model to best.model\n",
      "0s - loss: 0.7661 - acc: 0.6837 - val_loss: 0.7461 - val_acc: 0.6933\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74607 to 0.74259, saving model to best.model\n",
      "0s - loss: 0.7631 - acc: 0.6814 - val_loss: 0.7426 - val_acc: 0.6923\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74259 to 0.73858, saving model to best.model\n",
      "0s - loss: 0.7614 - acc: 0.6830 - val_loss: 0.7386 - val_acc: 0.6971\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73858 to 0.73640, saving model to best.model\n",
      "0s - loss: 0.7598 - acc: 0.6863 - val_loss: 0.7364 - val_acc: 0.7022\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73640 to 0.73202, saving model to best.model\n",
      "0s - loss: 0.7594 - acc: 0.6846 - val_loss: 0.7320 - val_acc: 0.7000\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.7548 - acc: 0.6862 - val_loss: 0.7326 - val_acc: 0.7042\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73202 to 0.72357, saving model to best.model\n",
      "0s - loss: 0.7518 - acc: 0.6900 - val_loss: 0.7236 - val_acc: 0.7062\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72357 to 0.72099, saving model to best.model\n",
      "0s - loss: 0.7504 - acc: 0.6894 - val_loss: 0.7210 - val_acc: 0.7083\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72099 to 0.72011, saving model to best.model\n",
      "0s - loss: 0.7488 - acc: 0.6904 - val_loss: 0.7201 - val_acc: 0.7087\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72011 to 0.71889, saving model to best.model\n",
      "1s - loss: 0.7476 - acc: 0.6910 - val_loss: 0.7189 - val_acc: 0.7060\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71889 to 0.71312, saving model to best.model\n",
      "1s - loss: 0.7472 - acc: 0.6898 - val_loss: 0.7131 - val_acc: 0.7128\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71312 to 0.70953, saving model to best.model\n",
      "1s - loss: 0.7423 - acc: 0.6927 - val_loss: 0.7095 - val_acc: 0.7177\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.7411 - acc: 0.6938 - val_loss: 0.7107 - val_acc: 0.7126\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70953 to 0.70908, saving model to best.model\n",
      "0s - loss: 0.7387 - acc: 0.6991 - val_loss: 0.7091 - val_acc: 0.7116\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70908 to 0.70318, saving model to best.model\n",
      "0s - loss: 0.7377 - acc: 0.6947 - val_loss: 0.7032 - val_acc: 0.7184\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70318 to 0.70198, saving model to best.model\n",
      "0s - loss: 0.7369 - acc: 0.6967 - val_loss: 0.7020 - val_acc: 0.7218\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.7380 - acc: 0.6970 - val_loss: 0.7039 - val_acc: 0.7180\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70198 to 0.70019, saving model to best.model\n",
      "0s - loss: 0.7317 - acc: 0.6991 - val_loss: 0.7002 - val_acc: 0.7177\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70019 to 0.69776, saving model to best.model\n",
      "0s - loss: 0.7320 - acc: 0.6985 - val_loss: 0.6978 - val_acc: 0.7233\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69776 to 0.69766, saving model to best.model\n",
      "0s - loss: 0.7320 - acc: 0.6999 - val_loss: 0.6977 - val_acc: 0.7192\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69766 to 0.69025, saving model to best.model\n",
      "0s - loss: 0.7301 - acc: 0.7006 - val_loss: 0.6902 - val_acc: 0.7237\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.7288 - acc: 0.7021 - val_loss: 0.6929 - val_acc: 0.7279\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.7261 - acc: 0.7030 - val_loss: 0.6930 - val_acc: 0.7297\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69025 to 0.68529, saving model to best.model\n",
      "0s - loss: 0.7228 - acc: 0.7049 - val_loss: 0.6853 - val_acc: 0.7304\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68529 to 0.68500, saving model to best.model\n",
      "0s - loss: 0.7216 - acc: 0.7046 - val_loss: 0.6850 - val_acc: 0.7287\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7231 - acc: 0.7057 - val_loss: 0.6869 - val_acc: 0.7246\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.7241 - acc: 0.7030 - val_loss: 0.6859 - val_acc: 0.7287\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68500 to 0.68256, saving model to best.model\n",
      "0s - loss: 0.7194 - acc: 0.7030 - val_loss: 0.6826 - val_acc: 0.7299\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68256 to 0.68009, saving model to best.model\n",
      "0s - loss: 0.7190 - acc: 0.7063 - val_loss: 0.6801 - val_acc: 0.7330\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68009 to 0.67576, saving model to best.model\n",
      "0s - loss: 0.7177 - acc: 0.7072 - val_loss: 0.6758 - val_acc: 0.7324\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.7189 - acc: 0.7038 - val_loss: 0.6777 - val_acc: 0.7324\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67576 to 0.67519, saving model to best.model\n",
      "0s - loss: 0.7163 - acc: 0.7086 - val_loss: 0.6752 - val_acc: 0.7328\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "1s - loss: 0.7177 - acc: 0.7060 - val_loss: 0.6774 - val_acc: 0.7343\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67519 to 0.67481, saving model to best.model\n",
      "1s - loss: 0.7119 - acc: 0.7079 - val_loss: 0.6748 - val_acc: 0.7347\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7149 - acc: 0.7082 - val_loss: 0.6767 - val_acc: 0.7331\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67481 to 0.67053, saving model to best.model\n",
      "1s - loss: 0.7104 - acc: 0.7100 - val_loss: 0.6705 - val_acc: 0.7357\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67053 to 0.66852, saving model to best.model\n",
      "1s - loss: 0.7107 - acc: 0.7119 - val_loss: 0.6685 - val_acc: 0.7365\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66852 to 0.66555, saving model to best.model\n",
      "1s - loss: 0.7066 - acc: 0.7111 - val_loss: 0.6656 - val_acc: 0.7361\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.7071 - acc: 0.7101 - val_loss: 0.6670 - val_acc: 0.7359\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66555 to 0.66457, saving model to best.model\n",
      "0s - loss: 0.7087 - acc: 0.7098 - val_loss: 0.6646 - val_acc: 0.7372\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7068 - acc: 0.7107 - val_loss: 0.6655 - val_acc: 0.7375\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7073 - acc: 0.7131 - val_loss: 0.6670 - val_acc: 0.7364\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66457 to 0.66414, saving model to best.model\n",
      "0s - loss: 0.7049 - acc: 0.7147 - val_loss: 0.6641 - val_acc: 0.7374\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66414 to 0.66191, saving model to best.model\n",
      "0s - loss: 0.7035 - acc: 0.7138 - val_loss: 0.6619 - val_acc: 0.7368\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66191 to 0.65940, saving model to best.model\n",
      "0s - loss: 0.7005 - acc: 0.7129 - val_loss: 0.6594 - val_acc: 0.7371\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.7010 - acc: 0.7139 - val_loss: 0.6611 - val_acc: 0.7375\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7027 - acc: 0.7131 - val_loss: 0.6599 - val_acc: 0.7382\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65940 to 0.65534, saving model to best.model\n",
      "0s - loss: 0.6987 - acc: 0.7147 - val_loss: 0.6553 - val_acc: 0.7393\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.6960 - acc: 0.7150 - val_loss: 0.6571 - val_acc: 0.7397\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65534 to 0.65494, saving model to best.model\n",
      "0s - loss: 0.6980 - acc: 0.7134 - val_loss: 0.6549 - val_acc: 0.7397\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65494 to 0.65358, saving model to best.model\n",
      "0s - loss: 0.6966 - acc: 0.7148 - val_loss: 0.6536 - val_acc: 0.7398\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.6935 - acc: 0.7169 - val_loss: 0.6547 - val_acc: 0.7400\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65358 to 0.65226, saving model to best.model\n",
      "0s - loss: 0.6943 - acc: 0.7180 - val_loss: 0.6523 - val_acc: 0.7386\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65226 to 0.65201, saving model to best.model\n",
      "0s - loss: 0.6917 - acc: 0.7186 - val_loss: 0.6520 - val_acc: 0.7396\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65201 to 0.64721, saving model to best.model\n",
      "0s - loss: 0.6918 - acc: 0.7176 - val_loss: 0.6472 - val_acc: 0.7417\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6886 - acc: 0.7201 - val_loss: 0.6479 - val_acc: 0.7402\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.6934 - acc: 0.7189 - val_loss: 0.6491 - val_acc: 0.7409\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64721 to 0.64705, saving model to best.model\n",
      "1s - loss: 0.6890 - acc: 0.7204 - val_loss: 0.6471 - val_acc: 0.7405\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.64705 to 0.64425, saving model to best.model\n",
      "0s - loss: 0.6898 - acc: 0.7192 - val_loss: 0.6443 - val_acc: 0.7424\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6913 - acc: 0.7174 - val_loss: 0.6466 - val_acc: 0.7402\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.6886 - acc: 0.7195 - val_loss: 0.6448 - val_acc: 0.7409\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6864 - acc: 0.7209 - val_loss: 0.6453 - val_acc: 0.7431\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.64425 to 0.64270, saving model to best.model\n",
      "0s - loss: 0.6868 - acc: 0.7201 - val_loss: 0.6427 - val_acc: 0.7436\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64270 to 0.64061, saving model to best.model\n",
      "0s - loss: 0.6872 - acc: 0.7199 - val_loss: 0.6406 - val_acc: 0.7447\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6854 - acc: 0.7218 - val_loss: 0.6426 - val_acc: 0.7439\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64061 to 0.63764, saving model to best.model\n",
      "0s - loss: 0.6812 - acc: 0.7217 - val_loss: 0.6376 - val_acc: 0.7441\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.6836 - acc: 0.7210 - val_loss: 0.6408 - val_acc: 0.7423\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.63764 to 0.63762, saving model to best.model\n",
      "0s - loss: 0.6837 - acc: 0.7215 - val_loss: 0.6376 - val_acc: 0.7430\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6821 - acc: 0.7225 - val_loss: 0.6383 - val_acc: 0.7422\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.63762 to 0.63372, saving model to best.model\n",
      "0s - loss: 0.6808 - acc: 0.7217 - val_loss: 0.6337 - val_acc: 0.7448\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6838 - acc: 0.7212 - val_loss: 0.6382 - val_acc: 0.7431\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.63372 to 0.63146, saving model to best.model\n",
      "0s - loss: 0.6820 - acc: 0.7216 - val_loss: 0.6315 - val_acc: 0.7465\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6788 - acc: 0.7229 - val_loss: 0.6350 - val_acc: 0.7416\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.6803 - acc: 0.7221 - val_loss: 0.6370 - val_acc: 0.7423\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63146 to 0.63096, saving model to best.model\n",
      "0s - loss: 0.6766 - acc: 0.7252 - val_loss: 0.6310 - val_acc: 0.7457\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6805 - acc: 0.7221 - val_loss: 0.6342 - val_acc: 0.7461\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.6776 - acc: 0.7221 - val_loss: 0.6329 - val_acc: 0.7436\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63096 to 0.62898, saving model to best.model\n",
      "0s - loss: 0.6736 - acc: 0.7252 - val_loss: 0.6290 - val_acc: 0.7464\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62898 to 0.62823, saving model to best.model\n",
      "0s - loss: 0.6783 - acc: 0.7227 - val_loss: 0.6282 - val_acc: 0.7448\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6751 - acc: 0.7240 - val_loss: 0.6302 - val_acc: 0.7454\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.62823 to 0.62799, saving model to best.model\n",
      "0s - loss: 0.6733 - acc: 0.7258 - val_loss: 0.6280 - val_acc: 0.7474\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "0s - loss: 0.6750 - acc: 0.7262 - val_loss: 0.6287 - val_acc: 0.7459\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6733 - acc: 0.7277 - val_loss: 0.6304 - val_acc: 0.7467\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.62799 to 0.62474, saving model to best.model\n",
      "1s - loss: 0.6722 - acc: 0.7260 - val_loss: 0.6247 - val_acc: 0.7488\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6714 - acc: 0.7274 - val_loss: 0.6261 - val_acc: 0.7484\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6695 - acc: 0.7242 - val_loss: 0.6281 - val_acc: 0.7461\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6719 - acc: 0.7255 - val_loss: 0.6251 - val_acc: 0.7473\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.62474 to 0.62414, saving model to best.model\n",
      "1s - loss: 0.6701 - acc: 0.7260 - val_loss: 0.6241 - val_acc: 0.7506\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62414 to 0.62162, saving model to best.model\n",
      "1s - loss: 0.6676 - acc: 0.7258 - val_loss: 0.6216 - val_acc: 0.7486\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6658 - acc: 0.7289 - val_loss: 0.6242 - val_acc: 0.7474\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6687 - acc: 0.7259 - val_loss: 0.6220 - val_acc: 0.7479\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62162 to 0.62008, saving model to best.model\n",
      "0s - loss: 0.6672 - acc: 0.7278 - val_loss: 0.6201 - val_acc: 0.7486\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6685 - acc: 0.7249 - val_loss: 0.6206 - val_acc: 0.7509\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6664 - acc: 0.7286 - val_loss: 0.6219 - val_acc: 0.7492\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62008 to 0.61919, saving model to best.model\n",
      "0s - loss: 0.6631 - acc: 0.7297 - val_loss: 0.6192 - val_acc: 0.7495\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6669 - acc: 0.7277 - val_loss: 0.6194 - val_acc: 0.7474\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.61919 to 0.61816, saving model to best.model\n",
      "0s - loss: 0.6649 - acc: 0.7280 - val_loss: 0.6182 - val_acc: 0.7493\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61816 to 0.61668, saving model to best.model\n",
      "0s - loss: 0.6628 - acc: 0.7287 - val_loss: 0.6167 - val_acc: 0.7498\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6637 - acc: 0.7311 - val_loss: 0.6173 - val_acc: 0.7507\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61668 to 0.61528, saving model to best.model\n",
      "0s - loss: 0.6646 - acc: 0.7286 - val_loss: 0.6153 - val_acc: 0.7514\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6643 - acc: 0.7273 - val_loss: 0.6162 - val_acc: 0.7493\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6647 - acc: 0.7309 - val_loss: 0.6175 - val_acc: 0.7488\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61528 to 0.61506, saving model to best.model\n",
      "1s - loss: 0.6618 - acc: 0.7305 - val_loss: 0.6151 - val_acc: 0.7501\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6616 - acc: 0.7296 - val_loss: 0.6170 - val_acc: 0.7493\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61506 to 0.61392, saving model to best.model\n",
      "1s - loss: 0.6621 - acc: 0.7301 - val_loss: 0.6139 - val_acc: 0.7484\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6591 - acc: 0.7300 - val_loss: 0.6146 - val_acc: 0.7482\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61392 to 0.61287, saving model to best.model\n",
      "1s - loss: 0.6594 - acc: 0.7300 - val_loss: 0.6129 - val_acc: 0.7499\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6604 - acc: 0.7306 - val_loss: 0.6131 - val_acc: 0.7482\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61287 to 0.60872, saving model to best.model\n",
      "1s - loss: 0.6613 - acc: 0.7308 - val_loss: 0.6087 - val_acc: 0.7536\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6610 - acc: 0.7311 - val_loss: 0.6117 - val_acc: 0.7504\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6598 - acc: 0.7294 - val_loss: 0.6104 - val_acc: 0.7511\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6596 - acc: 0.7301 - val_loss: 0.6124 - val_acc: 0.7500\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.60872 to 0.60850, saving model to best.model\n",
      "1s - loss: 0.6581 - acc: 0.7303 - val_loss: 0.6085 - val_acc: 0.7518\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6588 - acc: 0.7315 - val_loss: 0.6107 - val_acc: 0.7506\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.60850 to 0.60688, saving model to best.model\n",
      "1s - loss: 0.6563 - acc: 0.7326 - val_loss: 0.6069 - val_acc: 0.7530\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6534 - acc: 0.7331 - val_loss: 0.6099 - val_acc: 0.7567\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6573 - acc: 0.7313 - val_loss: 0.6102 - val_acc: 0.7533\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.60688 to 0.60591, saving model to best.model\n",
      "1s - loss: 0.6545 - acc: 0.7319 - val_loss: 0.6059 - val_acc: 0.7536\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.60591 to 0.60590, saving model to best.model\n",
      "1s - loss: 0.6539 - acc: 0.7325 - val_loss: 0.6059 - val_acc: 0.7537\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6534 - acc: 0.7313 - val_loss: 0.6087 - val_acc: 0.7508\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6558 - acc: 0.7328 - val_loss: 0.6063 - val_acc: 0.7548\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6521 - acc: 0.7354 - val_loss: 0.6060 - val_acc: 0.7526\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60590 to 0.60450, saving model to best.model\n",
      "1s - loss: 0.6517 - acc: 0.7333 - val_loss: 0.6045 - val_acc: 0.7559\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60450 to 0.60409, saving model to best.model\n",
      "0s - loss: 0.6523 - acc: 0.7333 - val_loss: 0.6041 - val_acc: 0.7542\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60409 to 0.60255, saving model to best.model\n",
      "1s - loss: 0.6506 - acc: 0.7338 - val_loss: 0.6025 - val_acc: 0.7567\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60255 to 0.60200, saving model to best.model\n",
      "1s - loss: 0.6509 - acc: 0.7365 - val_loss: 0.6020 - val_acc: 0.7576\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6512 - acc: 0.7364 - val_loss: 0.6022 - val_acc: 0.7550\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.6509 - acc: 0.7350 - val_loss: 0.6022 - val_acc: 0.7573\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60200 to 0.60144, saving model to best.model\n",
      "1s - loss: 0.6514 - acc: 0.7331 - val_loss: 0.6014 - val_acc: 0.7566\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60144 to 0.60039, saving model to best.model\n",
      "1s - loss: 0.6490 - acc: 0.7338 - val_loss: 0.6004 - val_acc: 0.7563\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6521 - acc: 0.7339 - val_loss: 0.6007 - val_acc: 0.7560\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60039 to 0.60028, saving model to best.model\n",
      "1s - loss: 0.6501 - acc: 0.7336 - val_loss: 0.6003 - val_acc: 0.7557\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.60028 to 0.60028, saving model to best.model\n",
      "1s - loss: 0.6511 - acc: 0.7333 - val_loss: 0.6003 - val_acc: 0.7574\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60028 to 0.60010, saving model to best.model\n",
      "1s - loss: 0.6507 - acc: 0.7333 - val_loss: 0.6001 - val_acc: 0.7542\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60010 to 0.59821, saving model to best.model\n",
      "1s - loss: 0.6492 - acc: 0.7358 - val_loss: 0.5982 - val_acc: 0.7573\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.59821 to 0.59721, saving model to best.model\n",
      "1s - loss: 0.6484 - acc: 0.7351 - val_loss: 0.5972 - val_acc: 0.7556\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6497 - acc: 0.7347 - val_loss: 0.6012 - val_acc: 0.7529\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6470 - acc: 0.7340 - val_loss: 0.5976 - val_acc: 0.7570\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.6482 - acc: 0.7371 - val_loss: 0.5989 - val_acc: 0.7563\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.59721 to 0.59530, saving model to best.model\n",
      "0s - loss: 0.6479 - acc: 0.7340 - val_loss: 0.5953 - val_acc: 0.7583\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6457 - acc: 0.7352 - val_loss: 0.6003 - val_acc: 0.7541\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84473, saving model to best.model\n",
      "1s - loss: 0.9150 - acc: 0.6307 - val_loss: 0.8447 - val_acc: 0.6594\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8620 - acc: 0.6576 - val_loss: 0.8450 - val_acc: 0.6594\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.8544 - acc: 0.6577 - val_loss: 0.8459 - val_acc: 0.6594\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84473 to 0.84010, saving model to best.model\n",
      "1s - loss: 0.8526 - acc: 0.6577 - val_loss: 0.8401 - val_acc: 0.6594\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84010 to 0.83476, saving model to best.model\n",
      "1s - loss: 0.8477 - acc: 0.6577 - val_loss: 0.8348 - val_acc: 0.6594\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83476 to 0.82922, saving model to best.model\n",
      "1s - loss: 0.8404 - acc: 0.6577 - val_loss: 0.8292 - val_acc: 0.6594\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.8378 - acc: 0.6577 - val_loss: 0.8295 - val_acc: 0.6594\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82922 to 0.82585, saving model to best.model\n",
      "0s - loss: 0.8354 - acc: 0.6577 - val_loss: 0.8259 - val_acc: 0.6594\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82585 to 0.82527, saving model to best.model\n",
      "0s - loss: 0.8339 - acc: 0.6577 - val_loss: 0.8253 - val_acc: 0.6594\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82527 to 0.82441, saving model to best.model\n",
      "0s - loss: 0.8327 - acc: 0.6577 - val_loss: 0.8244 - val_acc: 0.6594\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.8308 - acc: 0.6578 - val_loss: 0.8244 - val_acc: 0.6594\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.8298 - acc: 0.6577 - val_loss: 0.8244 - val_acc: 0.6594\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.8310 - acc: 0.6577 - val_loss: 0.8259 - val_acc: 0.6594\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82441 to 0.82349, saving model to best.model\n",
      "0s - loss: 0.8288 - acc: 0.6577 - val_loss: 0.8235 - val_acc: 0.6594\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82349 to 0.82233, saving model to best.model\n",
      "0s - loss: 0.8289 - acc: 0.6577 - val_loss: 0.8223 - val_acc: 0.6594\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82233 to 0.82181, saving model to best.model\n",
      "0s - loss: 0.8278 - acc: 0.6577 - val_loss: 0.8218 - val_acc: 0.6594\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8254 - acc: 0.6577 - val_loss: 0.8221 - val_acc: 0.6594\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82181 to 0.82044, saving model to best.model\n",
      "0s - loss: 0.8272 - acc: 0.6577 - val_loss: 0.8204 - val_acc: 0.6594\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.8255 - acc: 0.6578 - val_loss: 0.8230 - val_acc: 0.6594\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82044 to 0.81917, saving model to best.model\n",
      "0s - loss: 0.8244 - acc: 0.6576 - val_loss: 0.8192 - val_acc: 0.6594\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81917 to 0.81792, saving model to best.model\n",
      "0s - loss: 0.8229 - acc: 0.6582 - val_loss: 0.8179 - val_acc: 0.6594\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81792 to 0.81752, saving model to best.model\n",
      "0s - loss: 0.8232 - acc: 0.6584 - val_loss: 0.8175 - val_acc: 0.6594\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81752 to 0.81582, saving model to best.model\n",
      "0s - loss: 0.8228 - acc: 0.6587 - val_loss: 0.8158 - val_acc: 0.6593\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.8215 - acc: 0.6593 - val_loss: 0.8173 - val_acc: 0.6590\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81582 to 0.81419, saving model to best.model\n",
      "0s - loss: 0.8197 - acc: 0.6580 - val_loss: 0.8142 - val_acc: 0.6612\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81419 to 0.81386, saving model to best.model\n",
      "1s - loss: 0.8185 - acc: 0.6594 - val_loss: 0.8139 - val_acc: 0.6602\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81386 to 0.81315, saving model to best.model\n",
      "1s - loss: 0.8173 - acc: 0.6592 - val_loss: 0.8131 - val_acc: 0.6632\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81315 to 0.81186, saving model to best.model\n",
      "1s - loss: 0.8172 - acc: 0.6598 - val_loss: 0.8119 - val_acc: 0.6635\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81186 to 0.80826, saving model to best.model\n",
      "1s - loss: 0.8155 - acc: 0.6599 - val_loss: 0.8083 - val_acc: 0.6646\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80826 to 0.80668, saving model to best.model\n",
      "1s - loss: 0.8143 - acc: 0.6611 - val_loss: 0.8067 - val_acc: 0.6655\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80668 to 0.80511, saving model to best.model\n",
      "0s - loss: 0.8132 - acc: 0.6615 - val_loss: 0.8051 - val_acc: 0.6639\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80511 to 0.80310, saving model to best.model\n",
      "1s - loss: 0.8115 - acc: 0.6609 - val_loss: 0.8031 - val_acc: 0.6658\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80310 to 0.80119, saving model to best.model\n",
      "1s - loss: 0.8097 - acc: 0.6618 - val_loss: 0.8012 - val_acc: 0.6644\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80119 to 0.79902, saving model to best.model\n",
      "0s - loss: 0.8060 - acc: 0.6632 - val_loss: 0.7990 - val_acc: 0.6663\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79902 to 0.79752, saving model to best.model\n",
      "1s - loss: 0.8062 - acc: 0.6642 - val_loss: 0.7975 - val_acc: 0.6657\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79752 to 0.79606, saving model to best.model\n",
      "1s - loss: 0.8055 - acc: 0.6628 - val_loss: 0.7961 - val_acc: 0.6718\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79606 to 0.79326, saving model to best.model\n",
      "1s - loss: 0.8018 - acc: 0.6662 - val_loss: 0.7933 - val_acc: 0.6692\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79326 to 0.79168, saving model to best.model\n",
      "0s - loss: 0.8019 - acc: 0.6655 - val_loss: 0.7917 - val_acc: 0.6706\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79168 to 0.78989, saving model to best.model\n",
      "0s - loss: 0.7989 - acc: 0.6670 - val_loss: 0.7899 - val_acc: 0.6738\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78989 to 0.78840, saving model to best.model\n",
      "0s - loss: 0.8008 - acc: 0.6664 - val_loss: 0.7884 - val_acc: 0.6758\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.7973 - acc: 0.6678 - val_loss: 0.7896 - val_acc: 0.6766\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78840 to 0.78373, saving model to best.model\n",
      "0s - loss: 0.7950 - acc: 0.6678 - val_loss: 0.7837 - val_acc: 0.6776\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78373 to 0.78338, saving model to best.model\n",
      "0s - loss: 0.7953 - acc: 0.6690 - val_loss: 0.7834 - val_acc: 0.6781\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78338 to 0.78063, saving model to best.model\n",
      "0s - loss: 0.7925 - acc: 0.6701 - val_loss: 0.7806 - val_acc: 0.6788\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.7911 - acc: 0.6703 - val_loss: 0.7815 - val_acc: 0.6735\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78063 to 0.77696, saving model to best.model\n",
      "0s - loss: 0.7869 - acc: 0.6736 - val_loss: 0.7770 - val_acc: 0.6806\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.7883 - acc: 0.6701 - val_loss: 0.7776 - val_acc: 0.6782\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77696 to 0.77378, saving model to best.model\n",
      "0s - loss: 0.7879 - acc: 0.6743 - val_loss: 0.7738 - val_acc: 0.6821\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.7846 - acc: 0.6739 - val_loss: 0.7751 - val_acc: 0.6786\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77378 to 0.77250, saving model to best.model\n",
      "1s - loss: 0.7846 - acc: 0.6748 - val_loss: 0.7725 - val_acc: 0.6823\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77250 to 0.76917, saving model to best.model\n",
      "1s - loss: 0.7821 - acc: 0.6745 - val_loss: 0.7692 - val_acc: 0.6851\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76917 to 0.76854, saving model to best.model\n",
      "0s - loss: 0.7806 - acc: 0.6750 - val_loss: 0.7685 - val_acc: 0.6823\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76854 to 0.76426, saving model to best.model\n",
      "1s - loss: 0.7791 - acc: 0.6759 - val_loss: 0.7643 - val_acc: 0.6871\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76426 to 0.76368, saving model to best.model\n",
      "0s - loss: 0.7788 - acc: 0.6777 - val_loss: 0.7637 - val_acc: 0.6851\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76368 to 0.76319, saving model to best.model\n",
      "1s - loss: 0.7782 - acc: 0.6792 - val_loss: 0.7632 - val_acc: 0.6840\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76319 to 0.75944, saving model to best.model\n",
      "1s - loss: 0.7753 - acc: 0.6779 - val_loss: 0.7594 - val_acc: 0.6862\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75944 to 0.75536, saving model to best.model\n",
      "1s - loss: 0.7732 - acc: 0.6793 - val_loss: 0.7554 - val_acc: 0.6929\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.7721 - acc: 0.6808 - val_loss: 0.7563 - val_acc: 0.6879\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75536 to 0.75137, saving model to best.model\n",
      "0s - loss: 0.7711 - acc: 0.6807 - val_loss: 0.7514 - val_acc: 0.6933\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.75137 to 0.74967, saving model to best.model\n",
      "1s - loss: 0.7664 - acc: 0.6852 - val_loss: 0.7497 - val_acc: 0.6950\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74967 to 0.74748, saving model to best.model\n",
      "0s - loss: 0.7668 - acc: 0.6821 - val_loss: 0.7475 - val_acc: 0.6941\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74748 to 0.74443, saving model to best.model\n",
      "1s - loss: 0.7654 - acc: 0.6835 - val_loss: 0.7444 - val_acc: 0.6975\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74443 to 0.74319, saving model to best.model\n",
      "0s - loss: 0.7631 - acc: 0.6859 - val_loss: 0.7432 - val_acc: 0.6974\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.74319 to 0.73890, saving model to best.model\n",
      "1s - loss: 0.7630 - acc: 0.6835 - val_loss: 0.7389 - val_acc: 0.6987\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73890 to 0.73825, saving model to best.model\n",
      "0s - loss: 0.7605 - acc: 0.6850 - val_loss: 0.7382 - val_acc: 0.6975\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73825 to 0.73428, saving model to best.model\n",
      "0s - loss: 0.7588 - acc: 0.6875 - val_loss: 0.7343 - val_acc: 0.7007\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73428 to 0.73044, saving model to best.model\n",
      "0s - loss: 0.7552 - acc: 0.6900 - val_loss: 0.7304 - val_acc: 0.7021\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73044 to 0.73012, saving model to best.model\n",
      "0s - loss: 0.7536 - acc: 0.6876 - val_loss: 0.7301 - val_acc: 0.7027\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.73012 to 0.72631, saving model to best.model\n",
      "0s - loss: 0.7526 - acc: 0.6891 - val_loss: 0.7263 - val_acc: 0.7041\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72631 to 0.72502, saving model to best.model\n",
      "0s - loss: 0.7512 - acc: 0.6914 - val_loss: 0.7250 - val_acc: 0.7034\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72502 to 0.72336, saving model to best.model\n",
      "0s - loss: 0.7483 - acc: 0.6919 - val_loss: 0.7234 - val_acc: 0.7046\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72336 to 0.72022, saving model to best.model\n",
      "1s - loss: 0.7489 - acc: 0.6911 - val_loss: 0.7202 - val_acc: 0.7054\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.72022 to 0.71716, saving model to best.model\n",
      "1s - loss: 0.7460 - acc: 0.6936 - val_loss: 0.7172 - val_acc: 0.7061\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71716 to 0.71693, saving model to best.model\n",
      "1s - loss: 0.7469 - acc: 0.6923 - val_loss: 0.7169 - val_acc: 0.7071\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71693 to 0.71464, saving model to best.model\n",
      "1s - loss: 0.7409 - acc: 0.6953 - val_loss: 0.7146 - val_acc: 0.7082\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71464 to 0.71227, saving model to best.model\n",
      "1s - loss: 0.7405 - acc: 0.6947 - val_loss: 0.7123 - val_acc: 0.7095\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.71227 to 0.70793, saving model to best.model\n",
      "1s - loss: 0.7380 - acc: 0.6946 - val_loss: 0.7079 - val_acc: 0.7121\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70793 to 0.70485, saving model to best.model\n",
      "1s - loss: 0.7395 - acc: 0.6948 - val_loss: 0.7048 - val_acc: 0.7108\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70485 to 0.70439, saving model to best.model\n",
      "1s - loss: 0.7356 - acc: 0.6952 - val_loss: 0.7044 - val_acc: 0.7144\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7369 - acc: 0.6955 - val_loss: 0.7060 - val_acc: 0.7080\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70439 to 0.70112, saving model to best.model\n",
      "1s - loss: 0.7352 - acc: 0.6976 - val_loss: 0.7011 - val_acc: 0.7157\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.70112 to 0.69851, saving model to best.model\n",
      "1s - loss: 0.7312 - acc: 0.6984 - val_loss: 0.6985 - val_acc: 0.7173\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69851 to 0.69688, saving model to best.model\n",
      "1s - loss: 0.7300 - acc: 0.7017 - val_loss: 0.6969 - val_acc: 0.7176\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "1s - loss: 0.7267 - acc: 0.7024 - val_loss: 0.6978 - val_acc: 0.7178\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69688 to 0.69484, saving model to best.model\n",
      "1s - loss: 0.7301 - acc: 0.7000 - val_loss: 0.6948 - val_acc: 0.7186\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69484 to 0.69133, saving model to best.model\n",
      "1s - loss: 0.7242 - acc: 0.7014 - val_loss: 0.6913 - val_acc: 0.7193\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.69133 to 0.68955, saving model to best.model\n",
      "1s - loss: 0.7241 - acc: 0.7033 - val_loss: 0.6895 - val_acc: 0.7179\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7242 - acc: 0.7047 - val_loss: 0.6911 - val_acc: 0.7201\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.7235 - acc: 0.7036 - val_loss: 0.6902 - val_acc: 0.7194\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68955 to 0.68675, saving model to best.model\n",
      "0s - loss: 0.7213 - acc: 0.7037 - val_loss: 0.6867 - val_acc: 0.7201\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68675 to 0.68426, saving model to best.model\n",
      "0s - loss: 0.7193 - acc: 0.7044 - val_loss: 0.6843 - val_acc: 0.7230\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.7195 - acc: 0.7037 - val_loss: 0.6861 - val_acc: 0.7193\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68426 to 0.68096, saving model to best.model\n",
      "0s - loss: 0.7176 - acc: 0.7060 - val_loss: 0.6810 - val_acc: 0.7246\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68096 to 0.68025, saving model to best.model\n",
      "1s - loss: 0.7192 - acc: 0.7059 - val_loss: 0.6803 - val_acc: 0.7249\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.68025 to 0.67929, saving model to best.model\n",
      "0s - loss: 0.7169 - acc: 0.7071 - val_loss: 0.6793 - val_acc: 0.7235\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67929 to 0.67824, saving model to best.model\n",
      "1s - loss: 0.7161 - acc: 0.7061 - val_loss: 0.6782 - val_acc: 0.7232\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7128 - acc: 0.7070 - val_loss: 0.6787 - val_acc: 0.7218\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67824 to 0.67534, saving model to best.model\n",
      "1s - loss: 0.7144 - acc: 0.7078 - val_loss: 0.6753 - val_acc: 0.7260\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67534 to 0.67346, saving model to best.model\n",
      "0s - loss: 0.7125 - acc: 0.7098 - val_loss: 0.6735 - val_acc: 0.7270\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.67346 to 0.67052, saving model to best.model\n",
      "1s - loss: 0.7101 - acc: 0.7086 - val_loss: 0.6705 - val_acc: 0.7293\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7094 - acc: 0.7099 - val_loss: 0.6727 - val_acc: 0.7263\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67052 to 0.67030, saving model to best.model\n",
      "1s - loss: 0.7069 - acc: 0.7087 - val_loss: 0.6703 - val_acc: 0.7286\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7086 - acc: 0.7103 - val_loss: 0.6708 - val_acc: 0.7278\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.7074 - acc: 0.7102 - val_loss: 0.6738 - val_acc: 0.7215\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.67030 to 0.66770, saving model to best.model\n",
      "1s - loss: 0.7036 - acc: 0.7122 - val_loss: 0.6677 - val_acc: 0.7327\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.66770 to 0.66698, saving model to best.model\n",
      "0s - loss: 0.7069 - acc: 0.7094 - val_loss: 0.6670 - val_acc: 0.7286\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66698 to 0.66393, saving model to best.model\n",
      "1s - loss: 0.7053 - acc: 0.7140 - val_loss: 0.6639 - val_acc: 0.7323\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7009 - acc: 0.7134 - val_loss: 0.6650 - val_acc: 0.7324\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66393 to 0.66392, saving model to best.model\n",
      "1s - loss: 0.7034 - acc: 0.7131 - val_loss: 0.6639 - val_acc: 0.7300\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66392 to 0.66269, saving model to best.model\n",
      "1s - loss: 0.7013 - acc: 0.7135 - val_loss: 0.6627 - val_acc: 0.7324\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.66269 to 0.66214, saving model to best.model\n",
      "1s - loss: 0.6989 - acc: 0.7159 - val_loss: 0.6621 - val_acc: 0.7329\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.66214 to 0.66162, saving model to best.model\n",
      "1s - loss: 0.6990 - acc: 0.7160 - val_loss: 0.6616 - val_acc: 0.7294\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.66162 to 0.66038, saving model to best.model\n",
      "1s - loss: 0.6978 - acc: 0.7161 - val_loss: 0.6604 - val_acc: 0.7321\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6969 - acc: 0.7152 - val_loss: 0.6608 - val_acc: 0.7335\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.66038 to 0.65520, saving model to best.model\n",
      "1s - loss: 0.6944 - acc: 0.7155 - val_loss: 0.6552 - val_acc: 0.7342\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6971 - acc: 0.7150 - val_loss: 0.6578 - val_acc: 0.7316\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6985 - acc: 0.7136 - val_loss: 0.6601 - val_acc: 0.7329\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65520 to 0.65461, saving model to best.model\n",
      "1s - loss: 0.6924 - acc: 0.7175 - val_loss: 0.6546 - val_acc: 0.7352\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.65461 to 0.65253, saving model to best.model\n",
      "1s - loss: 0.6926 - acc: 0.7163 - val_loss: 0.6525 - val_acc: 0.7364\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6937 - acc: 0.7168 - val_loss: 0.6541 - val_acc: 0.7334\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6937 - acc: 0.7161 - val_loss: 0.6537 - val_acc: 0.7362\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.65253 to 0.65174, saving model to best.model\n",
      "1s - loss: 0.6926 - acc: 0.7165 - val_loss: 0.6517 - val_acc: 0.7352\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6923 - acc: 0.7172 - val_loss: 0.6524 - val_acc: 0.7354\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "1s - loss: 0.6893 - acc: 0.7198 - val_loss: 0.6530 - val_acc: 0.7386\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.65174 to 0.65009, saving model to best.model\n",
      "1s - loss: 0.6879 - acc: 0.7198 - val_loss: 0.6501 - val_acc: 0.7349\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.65009 to 0.64947, saving model to best.model\n",
      "1s - loss: 0.6875 - acc: 0.7217 - val_loss: 0.6495 - val_acc: 0.7384\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6878 - acc: 0.7202 - val_loss: 0.6514 - val_acc: 0.7347\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64947 to 0.64814, saving model to best.model\n",
      "1s - loss: 0.6863 - acc: 0.7199 - val_loss: 0.6481 - val_acc: 0.7365\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6839 - acc: 0.7192 - val_loss: 0.6509 - val_acc: 0.7349\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64814 to 0.64708, saving model to best.model\n",
      "0s - loss: 0.6862 - acc: 0.7208 - val_loss: 0.6471 - val_acc: 0.7361\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64708 to 0.64498, saving model to best.model\n",
      "1s - loss: 0.6842 - acc: 0.7192 - val_loss: 0.6450 - val_acc: 0.7396\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.64498 to 0.64372, saving model to best.model\n",
      "1s - loss: 0.6821 - acc: 0.7227 - val_loss: 0.6437 - val_acc: 0.7402\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.64372 to 0.64275, saving model to best.model\n",
      "1s - loss: 0.6823 - acc: 0.7192 - val_loss: 0.6428 - val_acc: 0.7385\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6831 - acc: 0.7202 - val_loss: 0.6443 - val_acc: 0.7405\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6834 - acc: 0.7231 - val_loss: 0.6437 - val_acc: 0.7381\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64275 to 0.64209, saving model to best.model\n",
      "1s - loss: 0.6781 - acc: 0.7212 - val_loss: 0.6421 - val_acc: 0.7377\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.64209 to 0.64204, saving model to best.model\n",
      "1s - loss: 0.6807 - acc: 0.7215 - val_loss: 0.6420 - val_acc: 0.7411\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.64204 to 0.63995, saving model to best.model\n",
      "1s - loss: 0.6778 - acc: 0.7244 - val_loss: 0.6400 - val_acc: 0.7411\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.63995 to 0.63762, saving model to best.model\n",
      "1s - loss: 0.6792 - acc: 0.7200 - val_loss: 0.6376 - val_acc: 0.7415\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63762 to 0.63745, saving model to best.model\n",
      "1s - loss: 0.6785 - acc: 0.7240 - val_loss: 0.6374 - val_acc: 0.7419\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6796 - acc: 0.7234 - val_loss: 0.6387 - val_acc: 0.7399\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6761 - acc: 0.7251 - val_loss: 0.6382 - val_acc: 0.7386\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63745 to 0.63466, saving model to best.model\n",
      "1s - loss: 0.6774 - acc: 0.7245 - val_loss: 0.6347 - val_acc: 0.7443\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63466 to 0.63390, saving model to best.model\n",
      "1s - loss: 0.6766 - acc: 0.7255 - val_loss: 0.6339 - val_acc: 0.7441\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.6746 - acc: 0.7252 - val_loss: 0.6352 - val_acc: 0.7432\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6740 - acc: 0.7245 - val_loss: 0.6341 - val_acc: 0.7443\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6771 - acc: 0.7246 - val_loss: 0.6341 - val_acc: 0.7419\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.63390 to 0.63236, saving model to best.model\n",
      "0s - loss: 0.6742 - acc: 0.7260 - val_loss: 0.6324 - val_acc: 0.7420\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63236 to 0.63207, saving model to best.model\n",
      "0s - loss: 0.6727 - acc: 0.7246 - val_loss: 0.6321 - val_acc: 0.7451\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6731 - acc: 0.7254 - val_loss: 0.6352 - val_acc: 0.7384\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.63207 to 0.63117, saving model to best.model\n",
      "1s - loss: 0.6733 - acc: 0.7236 - val_loss: 0.6312 - val_acc: 0.7429\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.63117 to 0.62944, saving model to best.model\n",
      "1s - loss: 0.6684 - acc: 0.7283 - val_loss: 0.6294 - val_acc: 0.7448\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6696 - acc: 0.7263 - val_loss: 0.6321 - val_acc: 0.7436\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6715 - acc: 0.7264 - val_loss: 0.6296 - val_acc: 0.7459\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62944 to 0.62912, saving model to best.model\n",
      "1s - loss: 0.6690 - acc: 0.7271 - val_loss: 0.6291 - val_acc: 0.7451\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6713 - acc: 0.7254 - val_loss: 0.6308 - val_acc: 0.7444\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.62912 to 0.62728, saving model to best.model\n",
      "0s - loss: 0.6702 - acc: 0.7254 - val_loss: 0.6273 - val_acc: 0.7468\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6681 - acc: 0.7253 - val_loss: 0.6292 - val_acc: 0.7440\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62728 to 0.62591, saving model to best.model\n",
      "0s - loss: 0.6681 - acc: 0.7294 - val_loss: 0.6259 - val_acc: 0.7460\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6681 - acc: 0.7252 - val_loss: 0.6268 - val_acc: 0.7468\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62591 to 0.62556, saving model to best.model\n",
      "0s - loss: 0.6660 - acc: 0.7274 - val_loss: 0.6256 - val_acc: 0.7463\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6658 - acc: 0.7276 - val_loss: 0.6273 - val_acc: 0.7446\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6674 - acc: 0.7283 - val_loss: 0.6273 - val_acc: 0.7443\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.62556 to 0.62320, saving model to best.model\n",
      "0s - loss: 0.6632 - acc: 0.7285 - val_loss: 0.6232 - val_acc: 0.7481\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6671 - acc: 0.7271 - val_loss: 0.6253 - val_acc: 0.7453\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6626 - acc: 0.7311 - val_loss: 0.6249 - val_acc: 0.7458\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6629 - acc: 0.7309 - val_loss: 0.6249 - val_acc: 0.7468\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6660 - acc: 0.7275 - val_loss: 0.6261 - val_acc: 0.7468\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.62320 to 0.62100, saving model to best.model\n",
      "0s - loss: 0.6620 - acc: 0.7317 - val_loss: 0.6210 - val_acc: 0.7489\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6629 - acc: 0.7290 - val_loss: 0.6212 - val_acc: 0.7481\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.62100 to 0.61961, saving model to best.model\n",
      "0s - loss: 0.6590 - acc: 0.7309 - val_loss: 0.6196 - val_acc: 0.7504\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6641 - acc: 0.7302 - val_loss: 0.6218 - val_acc: 0.7474\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61961 to 0.61890, saving model to best.model\n",
      "0s - loss: 0.6606 - acc: 0.7326 - val_loss: 0.6189 - val_acc: 0.7489\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6598 - acc: 0.7307 - val_loss: 0.6194 - val_acc: 0.7488\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6591 - acc: 0.7326 - val_loss: 0.6193 - val_acc: 0.7504\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.61890 to 0.61788, saving model to best.model\n",
      "1s - loss: 0.6605 - acc: 0.7308 - val_loss: 0.6179 - val_acc: 0.7501\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6603 - acc: 0.7317 - val_loss: 0.6210 - val_acc: 0.7495\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6597 - acc: 0.7323 - val_loss: 0.6185 - val_acc: 0.7504\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61788 to 0.61629, saving model to best.model\n",
      "1s - loss: 0.6566 - acc: 0.7339 - val_loss: 0.6163 - val_acc: 0.7506\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6616 - acc: 0.7319 - val_loss: 0.6186 - val_acc: 0.7481\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.61629 to 0.61499, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7339 - val_loss: 0.6150 - val_acc: 0.7509\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.61499 to 0.61488, saving model to best.model\n",
      "1s - loss: 0.6540 - acc: 0.7332 - val_loss: 0.6149 - val_acc: 0.7508\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6548 - acc: 0.7349 - val_loss: 0.6156 - val_acc: 0.7505\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.61488 to 0.61358, saving model to best.model\n",
      "1s - loss: 0.6546 - acc: 0.7330 - val_loss: 0.6136 - val_acc: 0.7508\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61358 to 0.61326, saving model to best.model\n",
      "1s - loss: 0.6539 - acc: 0.7339 - val_loss: 0.6133 - val_acc: 0.7519\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6563 - acc: 0.7322 - val_loss: 0.6145 - val_acc: 0.7513\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.6542 - acc: 0.7334 - val_loss: 0.6144 - val_acc: 0.7509\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.61326 to 0.61195, saving model to best.model\n",
      "0s - loss: 0.6547 - acc: 0.7340 - val_loss: 0.6119 - val_acc: 0.7511\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.61195 to 0.61130, saving model to best.model\n",
      "0s - loss: 0.6537 - acc: 0.7337 - val_loss: 0.6113 - val_acc: 0.7520\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6542 - acc: 0.7354 - val_loss: 0.6123 - val_acc: 0.7508\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6538 - acc: 0.7339 - val_loss: 0.6131 - val_acc: 0.7536\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6541 - acc: 0.7336 - val_loss: 0.6122 - val_acc: 0.7514\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.61130 to 0.60996, saving model to best.model\n",
      "0s - loss: 0.6521 - acc: 0.7325 - val_loss: 0.6100 - val_acc: 0.7546\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6524 - acc: 0.7337 - val_loss: 0.6110 - val_acc: 0.7518\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6492 - acc: 0.7372 - val_loss: 0.6108 - val_acc: 0.7520\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.60996 to 0.60901, saving model to best.model\n",
      "0s - loss: 0.6531 - acc: 0.7347 - val_loss: 0.6090 - val_acc: 0.7523\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60901 to 0.60875, saving model to best.model\n",
      "0s - loss: 0.6484 - acc: 0.7360 - val_loss: 0.6088 - val_acc: 0.7514\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60875 to 0.60801, saving model to best.model\n",
      "0s - loss: 0.6521 - acc: 0.7350 - val_loss: 0.6080 - val_acc: 0.7527\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60801 to 0.60618, saving model to best.model\n",
      "0s - loss: 0.6492 - acc: 0.7358 - val_loss: 0.6062 - val_acc: 0.7556\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6495 - acc: 0.7363 - val_loss: 0.6075 - val_acc: 0.7532\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84491, saving model to best.model\n",
      "1s - loss: 0.9255 - acc: 0.6249 - val_loss: 0.8449 - val_acc: 0.6562\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84491 to 0.84447, saving model to best.model\n",
      "1s - loss: 0.8637 - acc: 0.6568 - val_loss: 0.8445 - val_acc: 0.6562\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84447 to 0.84421, saving model to best.model\n",
      "1s - loss: 0.8548 - acc: 0.6570 - val_loss: 0.8442 - val_acc: 0.6562\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84421 to 0.84085, saving model to best.model\n",
      "1s - loss: 0.8523 - acc: 0.6570 - val_loss: 0.8408 - val_acc: 0.6562\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "1s - loss: 0.8484 - acc: 0.6570 - val_loss: 0.8429 - val_acc: 0.6562\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.84085 to 0.83011, saving model to best.model\n",
      "1s - loss: 0.8419 - acc: 0.6570 - val_loss: 0.8301 - val_acc: 0.6562\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83011 to 0.82539, saving model to best.model\n",
      "1s - loss: 0.8394 - acc: 0.6570 - val_loss: 0.8254 - val_acc: 0.6562\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "1s - loss: 0.8359 - acc: 0.6570 - val_loss: 0.8285 - val_acc: 0.6562\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82539 to 0.82388, saving model to best.model\n",
      "1s - loss: 0.8333 - acc: 0.6570 - val_loss: 0.8239 - val_acc: 0.6562\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82388 to 0.82222, saving model to best.model\n",
      "1s - loss: 0.8332 - acc: 0.6570 - val_loss: 0.8222 - val_acc: 0.6562\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8317 - acc: 0.6570 - val_loss: 0.8223 - val_acc: 0.6562\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82222 to 0.82109, saving model to best.model\n",
      "1s - loss: 0.8315 - acc: 0.6570 - val_loss: 0.8211 - val_acc: 0.6562\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "1s - loss: 0.8303 - acc: 0.6570 - val_loss: 0.8243 - val_acc: 0.6562\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82109 to 0.82099, saving model to best.model\n",
      "1s - loss: 0.8297 - acc: 0.6570 - val_loss: 0.8210 - val_acc: 0.6562\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82099 to 0.82024, saving model to best.model\n",
      "1s - loss: 0.8281 - acc: 0.6570 - val_loss: 0.8202 - val_acc: 0.6562\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82024 to 0.82004, saving model to best.model\n",
      "0s - loss: 0.8280 - acc: 0.6568 - val_loss: 0.8200 - val_acc: 0.6562\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "1s - loss: 0.8270 - acc: 0.6570 - val_loss: 0.8202 - val_acc: 0.6562\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82004 to 0.81800, saving model to best.model\n",
      "1s - loss: 0.8279 - acc: 0.6570 - val_loss: 0.8180 - val_acc: 0.6562\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81800 to 0.81753, saving model to best.model\n",
      "1s - loss: 0.8266 - acc: 0.6570 - val_loss: 0.8175 - val_acc: 0.6562\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81753 to 0.81692, saving model to best.model\n",
      "1s - loss: 0.8264 - acc: 0.6568 - val_loss: 0.8169 - val_acc: 0.6562\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81692 to 0.81650, saving model to best.model\n",
      "1s - loss: 0.8265 - acc: 0.6570 - val_loss: 0.8165 - val_acc: 0.6562\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81650 to 0.81530, saving model to best.model\n",
      "1s - loss: 0.8243 - acc: 0.6572 - val_loss: 0.8153 - val_acc: 0.6562\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "1s - loss: 0.8237 - acc: 0.6576 - val_loss: 0.8155 - val_acc: 0.6562\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81530 to 0.81417, saving model to best.model\n",
      "1s - loss: 0.8237 - acc: 0.6570 - val_loss: 0.8142 - val_acc: 0.6562\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81417 to 0.81247, saving model to best.model\n",
      "1s - loss: 0.8222 - acc: 0.6572 - val_loss: 0.8125 - val_acc: 0.6562\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81247 to 0.81233, saving model to best.model\n",
      "1s - loss: 0.8208 - acc: 0.6580 - val_loss: 0.8123 - val_acc: 0.6564\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81233 to 0.80924, saving model to best.model\n",
      "1s - loss: 0.8183 - acc: 0.6582 - val_loss: 0.8092 - val_acc: 0.6574\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80924 to 0.80913, saving model to best.model\n",
      "0s - loss: 0.8196 - acc: 0.6575 - val_loss: 0.8091 - val_acc: 0.6570\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80913 to 0.80781, saving model to best.model\n",
      "1s - loss: 0.8186 - acc: 0.6584 - val_loss: 0.8078 - val_acc: 0.6582\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80781 to 0.80483, saving model to best.model\n",
      "1s - loss: 0.8141 - acc: 0.6599 - val_loss: 0.8048 - val_acc: 0.6608\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80483 to 0.80309, saving model to best.model\n",
      "1s - loss: 0.8147 - acc: 0.6608 - val_loss: 0.8031 - val_acc: 0.6595\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80309 to 0.80125, saving model to best.model\n",
      "1s - loss: 0.8137 - acc: 0.6601 - val_loss: 0.8012 - val_acc: 0.6624\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80125 to 0.79977, saving model to best.model\n",
      "1s - loss: 0.8128 - acc: 0.6608 - val_loss: 0.7998 - val_acc: 0.6617\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79977 to 0.79752, saving model to best.model\n",
      "1s - loss: 0.8102 - acc: 0.6628 - val_loss: 0.7975 - val_acc: 0.6638\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79752 to 0.79370, saving model to best.model\n",
      "1s - loss: 0.8072 - acc: 0.6640 - val_loss: 0.7937 - val_acc: 0.6706\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79370 to 0.79300, saving model to best.model\n",
      "1s - loss: 0.8062 - acc: 0.6635 - val_loss: 0.7930 - val_acc: 0.6642\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79300 to 0.78915, saving model to best.model\n",
      "0s - loss: 0.8058 - acc: 0.6639 - val_loss: 0.7891 - val_acc: 0.6706\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "1s - loss: 0.8035 - acc: 0.6652 - val_loss: 0.7893 - val_acc: 0.6683\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78915 to 0.78626, saving model to best.model\n",
      "1s - loss: 0.8013 - acc: 0.6659 - val_loss: 0.7863 - val_acc: 0.6717\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78626 to 0.78489, saving model to best.model\n",
      "1s - loss: 0.7989 - acc: 0.6676 - val_loss: 0.7849 - val_acc: 0.6758\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "1s - loss: 0.7994 - acc: 0.6675 - val_loss: 0.7865 - val_acc: 0.6673\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78489 to 0.78486, saving model to best.model\n",
      "1s - loss: 0.7998 - acc: 0.6674 - val_loss: 0.7849 - val_acc: 0.6719\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78486 to 0.78376, saving model to best.model\n",
      "1s - loss: 0.7954 - acc: 0.6686 - val_loss: 0.7838 - val_acc: 0.6765\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78376 to 0.77840, saving model to best.model\n",
      "0s - loss: 0.7957 - acc: 0.6688 - val_loss: 0.7784 - val_acc: 0.6786\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77840 to 0.77792, saving model to best.model\n",
      "1s - loss: 0.7925 - acc: 0.6712 - val_loss: 0.7779 - val_acc: 0.6741\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77792 to 0.77684, saving model to best.model\n",
      "1s - loss: 0.7918 - acc: 0.6724 - val_loss: 0.7768 - val_acc: 0.6765\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77684 to 0.77227, saving model to best.model\n",
      "1s - loss: 0.7913 - acc: 0.6724 - val_loss: 0.7723 - val_acc: 0.6844\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77227 to 0.77016, saving model to best.model\n",
      "0s - loss: 0.7885 - acc: 0.6739 - val_loss: 0.7702 - val_acc: 0.6835\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.7867 - acc: 0.6726 - val_loss: 0.7720 - val_acc: 0.6814\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77016 to 0.76917, saving model to best.model\n",
      "0s - loss: 0.7853 - acc: 0.6750 - val_loss: 0.7692 - val_acc: 0.6820\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76917 to 0.76768, saving model to best.model\n",
      "0s - loss: 0.7868 - acc: 0.6745 - val_loss: 0.7677 - val_acc: 0.6830\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76768 to 0.76419, saving model to best.model\n",
      "1s - loss: 0.7850 - acc: 0.6748 - val_loss: 0.7642 - val_acc: 0.6899\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "1s - loss: 0.7828 - acc: 0.6755 - val_loss: 0.7650 - val_acc: 0.6806\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76419 to 0.76108, saving model to best.model\n",
      "1s - loss: 0.7820 - acc: 0.6759 - val_loss: 0.7611 - val_acc: 0.6856\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76108 to 0.76026, saving model to best.model\n",
      "1s - loss: 0.7794 - acc: 0.6770 - val_loss: 0.7603 - val_acc: 0.6867\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76026 to 0.75571, saving model to best.model\n",
      "1s - loss: 0.7776 - acc: 0.6775 - val_loss: 0.7557 - val_acc: 0.6912\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "1s - loss: 0.7772 - acc: 0.6788 - val_loss: 0.7565 - val_acc: 0.6917\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75571 to 0.75368, saving model to best.model\n",
      "1s - loss: 0.7748 - acc: 0.6786 - val_loss: 0.7537 - val_acc: 0.6932\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75368 to 0.75204, saving model to best.model\n",
      "1s - loss: 0.7727 - acc: 0.6793 - val_loss: 0.7520 - val_acc: 0.6883\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.75204 to 0.75056, saving model to best.model\n",
      "1s - loss: 0.7731 - acc: 0.6791 - val_loss: 0.7506 - val_acc: 0.6946\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.75056 to 0.74580, saving model to best.model\n",
      "1s - loss: 0.7689 - acc: 0.6829 - val_loss: 0.7458 - val_acc: 0.6954\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74580 to 0.74243, saving model to best.model\n",
      "0s - loss: 0.7696 - acc: 0.6811 - val_loss: 0.7424 - val_acc: 0.6957\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74243 to 0.74164, saving model to best.model\n",
      "1s - loss: 0.7669 - acc: 0.6822 - val_loss: 0.7416 - val_acc: 0.6941\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.74164 to 0.73979, saving model to best.model\n",
      "1s - loss: 0.7670 - acc: 0.6831 - val_loss: 0.7398 - val_acc: 0.6973\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73979 to 0.73378, saving model to best.model\n",
      "1s - loss: 0.7642 - acc: 0.6858 - val_loss: 0.7338 - val_acc: 0.6988\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.7626 - acc: 0.6873 - val_loss: 0.7354 - val_acc: 0.6964\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73378 to 0.73144, saving model to best.model\n",
      "1s - loss: 0.7588 - acc: 0.6856 - val_loss: 0.7314 - val_acc: 0.6989\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73144 to 0.72965, saving model to best.model\n",
      "1s - loss: 0.7596 - acc: 0.6858 - val_loss: 0.7297 - val_acc: 0.6999\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72965 to 0.72938, saving model to best.model\n",
      "0s - loss: 0.7563 - acc: 0.6879 - val_loss: 0.7294 - val_acc: 0.6980\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72938 to 0.72556, saving model to best.model\n",
      "0s - loss: 0.7548 - acc: 0.6878 - val_loss: 0.7256 - val_acc: 0.7069\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72556 to 0.72481, saving model to best.model\n",
      "0s - loss: 0.7535 - acc: 0.6895 - val_loss: 0.7248 - val_acc: 0.6996\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72481 to 0.72052, saving model to best.model\n",
      "0s - loss: 0.7536 - acc: 0.6884 - val_loss: 0.7205 - val_acc: 0.7036\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.72052 to 0.71815, saving model to best.model\n",
      "0s - loss: 0.7498 - acc: 0.6915 - val_loss: 0.7182 - val_acc: 0.7105\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71815 to 0.71248, saving model to best.model\n",
      "0s - loss: 0.7484 - acc: 0.6900 - val_loss: 0.7125 - val_acc: 0.7060\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71248 to 0.71148, saving model to best.model\n",
      "1s - loss: 0.7445 - acc: 0.6935 - val_loss: 0.7115 - val_acc: 0.7096\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71148 to 0.70810, saving model to best.model\n",
      "1s - loss: 0.7484 - acc: 0.6922 - val_loss: 0.7081 - val_acc: 0.7126\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7446 - acc: 0.6921 - val_loss: 0.7088 - val_acc: 0.7158\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70810 to 0.70474, saving model to best.model\n",
      "1s - loss: 0.7412 - acc: 0.6954 - val_loss: 0.7047 - val_acc: 0.7117\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70474 to 0.70297, saving model to best.model\n",
      "1s - loss: 0.7419 - acc: 0.6953 - val_loss: 0.7030 - val_acc: 0.7150\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70297 to 0.70091, saving model to best.model\n",
      "1s - loss: 0.7387 - acc: 0.6973 - val_loss: 0.7009 - val_acc: 0.7125\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7351 - acc: 0.7014 - val_loss: 0.7019 - val_acc: 0.7126\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.70091 to 0.70017, saving model to best.model\n",
      "1s - loss: 0.7381 - acc: 0.6957 - val_loss: 0.7002 - val_acc: 0.7146\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.70017 to 0.69707, saving model to best.model\n",
      "1s - loss: 0.7344 - acc: 0.6969 - val_loss: 0.6971 - val_acc: 0.7201\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69707 to 0.69339, saving model to best.model\n",
      "1s - loss: 0.7350 - acc: 0.6959 - val_loss: 0.6934 - val_acc: 0.7185\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7348 - acc: 0.6995 - val_loss: 0.6975 - val_acc: 0.7146\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69339 to 0.68698, saving model to best.model\n",
      "1s - loss: 0.7296 - acc: 0.7020 - val_loss: 0.6870 - val_acc: 0.7213\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7282 - acc: 0.7006 - val_loss: 0.6878 - val_acc: 0.7217\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7277 - acc: 0.7024 - val_loss: 0.6885 - val_acc: 0.7169\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68698 to 0.68458, saving model to best.model\n",
      "1s - loss: 0.7257 - acc: 0.7003 - val_loss: 0.6846 - val_acc: 0.7215\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "1s - loss: 0.7284 - acc: 0.6989 - val_loss: 0.6848 - val_acc: 0.7212\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68458 to 0.68342, saving model to best.model\n",
      "0s - loss: 0.7265 - acc: 0.7021 - val_loss: 0.6834 - val_acc: 0.7208\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68342 to 0.67731, saving model to best.model\n",
      "1s - loss: 0.7223 - acc: 0.7013 - val_loss: 0.6773 - val_acc: 0.7233\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67731 to 0.67706, saving model to best.model\n",
      "1s - loss: 0.7219 - acc: 0.7027 - val_loss: 0.6771 - val_acc: 0.7270\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7242 - acc: 0.7002 - val_loss: 0.6796 - val_acc: 0.7227\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67706 to 0.67565, saving model to best.model\n",
      "1s - loss: 0.7192 - acc: 0.7027 - val_loss: 0.6756 - val_acc: 0.7301\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7190 - acc: 0.7052 - val_loss: 0.6760 - val_acc: 0.7239\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7171 - acc: 0.7066 - val_loss: 0.6762 - val_acc: 0.7246\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67565 to 0.67506, saving model to best.model\n",
      "1s - loss: 0.7160 - acc: 0.7068 - val_loss: 0.6751 - val_acc: 0.7226\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67506 to 0.67105, saving model to best.model\n",
      "1s - loss: 0.7158 - acc: 0.7069 - val_loss: 0.6710 - val_acc: 0.7263\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.67105 to 0.67099, saving model to best.model\n",
      "1s - loss: 0.7144 - acc: 0.7063 - val_loss: 0.6710 - val_acc: 0.7244\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.67099 to 0.67011, saving model to best.model\n",
      "1s - loss: 0.7148 - acc: 0.7052 - val_loss: 0.6701 - val_acc: 0.7269\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7152 - acc: 0.7072 - val_loss: 0.6704 - val_acc: 0.7261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.67011 to 0.66948, saving model to best.model\n",
      "0s - loss: 0.7125 - acc: 0.7059 - val_loss: 0.6695 - val_acc: 0.7258\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.7095 - acc: 0.7073 - val_loss: 0.6704 - val_acc: 0.7244\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66948 to 0.66791, saving model to best.model\n",
      "1s - loss: 0.7111 - acc: 0.7091 - val_loss: 0.6679 - val_acc: 0.7263\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.66791 to 0.66396, saving model to best.model\n",
      "1s - loss: 0.7081 - acc: 0.7073 - val_loss: 0.6640 - val_acc: 0.7304\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66396 to 0.66208, saving model to best.model\n",
      "1s - loss: 0.7063 - acc: 0.7119 - val_loss: 0.6621 - val_acc: 0.7331\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.7075 - acc: 0.7114 - val_loss: 0.6621 - val_acc: 0.7278\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66208 to 0.66029, saving model to best.model\n",
      "1s - loss: 0.7067 - acc: 0.7110 - val_loss: 0.6603 - val_acc: 0.7350\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66029 to 0.65784, saving model to best.model\n",
      "0s - loss: 0.7044 - acc: 0.7107 - val_loss: 0.6578 - val_acc: 0.7350\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.7051 - acc: 0.7102 - val_loss: 0.6635 - val_acc: 0.7261\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.7045 - acc: 0.7106 - val_loss: 0.6616 - val_acc: 0.7337\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.7049 - acc: 0.7097 - val_loss: 0.6580 - val_acc: 0.7329\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65784 to 0.65736, saving model to best.model\n",
      "1s - loss: 0.7016 - acc: 0.7132 - val_loss: 0.6574 - val_acc: 0.7335\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.65736 to 0.65418, saving model to best.model\n",
      "1s - loss: 0.7033 - acc: 0.7106 - val_loss: 0.6542 - val_acc: 0.7364\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6992 - acc: 0.7127 - val_loss: 0.6591 - val_acc: 0.7331\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.7005 - acc: 0.7115 - val_loss: 0.6543 - val_acc: 0.7361\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65418 to 0.65304, saving model to best.model\n",
      "1s - loss: 0.6991 - acc: 0.7128 - val_loss: 0.6530 - val_acc: 0.7335\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.65304 to 0.65230, saving model to best.model\n",
      "1s - loss: 0.6997 - acc: 0.7141 - val_loss: 0.6523 - val_acc: 0.7343\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65230 to 0.65123, saving model to best.model\n",
      "1s - loss: 0.6958 - acc: 0.7148 - val_loss: 0.6512 - val_acc: 0.7358\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65123 to 0.64972, saving model to best.model\n",
      "1s - loss: 0.6962 - acc: 0.7169 - val_loss: 0.6497 - val_acc: 0.7357\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6964 - acc: 0.7176 - val_loss: 0.6509 - val_acc: 0.7357\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64972 to 0.64759, saving model to best.model\n",
      "1s - loss: 0.6955 - acc: 0.7138 - val_loss: 0.6476 - val_acc: 0.7375\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64759 to 0.64624, saving model to best.model\n",
      "0s - loss: 0.6945 - acc: 0.7150 - val_loss: 0.6462 - val_acc: 0.7389\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64624 to 0.64497, saving model to best.model\n",
      "1s - loss: 0.6948 - acc: 0.7163 - val_loss: 0.6450 - val_acc: 0.7376\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6961 - acc: 0.7126 - val_loss: 0.6494 - val_acc: 0.7336\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6945 - acc: 0.7181 - val_loss: 0.6454 - val_acc: 0.7405\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64497 to 0.64175, saving model to best.model\n",
      "1s - loss: 0.6912 - acc: 0.7167 - val_loss: 0.6418 - val_acc: 0.7430\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.64175 to 0.64109, saving model to best.model\n",
      "1s - loss: 0.6928 - acc: 0.7179 - val_loss: 0.6411 - val_acc: 0.7410\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6904 - acc: 0.7161 - val_loss: 0.6420 - val_acc: 0.7417\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6896 - acc: 0.7193 - val_loss: 0.6430 - val_acc: 0.7405\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6915 - acc: 0.7150 - val_loss: 0.6455 - val_acc: 0.7317\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6889 - acc: 0.7200 - val_loss: 0.6414 - val_acc: 0.7376\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.64109 to 0.64095, saving model to best.model\n",
      "1s - loss: 0.6877 - acc: 0.7192 - val_loss: 0.6409 - val_acc: 0.7369\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.64095 to 0.63912, saving model to best.model\n",
      "1s - loss: 0.6916 - acc: 0.7165 - val_loss: 0.6391 - val_acc: 0.7424\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63912 to 0.63902, saving model to best.model\n",
      "1s - loss: 0.6847 - acc: 0.7208 - val_loss: 0.6390 - val_acc: 0.7379\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63902 to 0.63725, saving model to best.model\n",
      "1s - loss: 0.6854 - acc: 0.7193 - val_loss: 0.6373 - val_acc: 0.7400\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6854 - acc: 0.7206 - val_loss: 0.6398 - val_acc: 0.7411\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6819 - acc: 0.7211 - val_loss: 0.6379 - val_acc: 0.7402\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63725 to 0.63347, saving model to best.model\n",
      "1s - loss: 0.6860 - acc: 0.7205 - val_loss: 0.6335 - val_acc: 0.7443\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63347 to 0.63228, saving model to best.model\n",
      "1s - loss: 0.6805 - acc: 0.7201 - val_loss: 0.6323 - val_acc: 0.7454\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6836 - acc: 0.7208 - val_loss: 0.6357 - val_acc: 0.7422\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6800 - acc: 0.7227 - val_loss: 0.6332 - val_acc: 0.7440\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63228 to 0.63170, saving model to best.model\n",
      "1s - loss: 0.6815 - acc: 0.7218 - val_loss: 0.6317 - val_acc: 0.7458\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63170 to 0.63076, saving model to best.model\n",
      "0s - loss: 0.6860 - acc: 0.7176 - val_loss: 0.6308 - val_acc: 0.7440\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.63076 to 0.63071, saving model to best.model\n",
      "0s - loss: 0.6790 - acc: 0.7210 - val_loss: 0.6307 - val_acc: 0.7470\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6801 - acc: 0.7203 - val_loss: 0.6323 - val_acc: 0.7427\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.63071 to 0.62885, saving model to best.model\n",
      "0s - loss: 0.6752 - acc: 0.7246 - val_loss: 0.6288 - val_acc: 0.7425\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6785 - acc: 0.7211 - val_loss: 0.6306 - val_acc: 0.7499\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62885 to 0.62813, saving model to best.model\n",
      "0s - loss: 0.6766 - acc: 0.7238 - val_loss: 0.6281 - val_acc: 0.7444\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6738 - acc: 0.7235 - val_loss: 0.6284 - val_acc: 0.7457\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62813 to 0.62660, saving model to best.model\n",
      "1s - loss: 0.6774 - acc: 0.7223 - val_loss: 0.6266 - val_acc: 0.7473\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62660 to 0.62470, saving model to best.model\n",
      "1s - loss: 0.6749 - acc: 0.7251 - val_loss: 0.6247 - val_acc: 0.7485\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6754 - acc: 0.7231 - val_loss: 0.6265 - val_acc: 0.7474\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62470 to 0.62459, saving model to best.model\n",
      "1s - loss: 0.6737 - acc: 0.7233 - val_loss: 0.6246 - val_acc: 0.7519\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6765 - acc: 0.7232 - val_loss: 0.6262 - val_acc: 0.7470\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6763 - acc: 0.7243 - val_loss: 0.6263 - val_acc: 0.7478\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.62459 to 0.62347, saving model to best.model\n",
      "1s - loss: 0.6763 - acc: 0.7226 - val_loss: 0.6235 - val_acc: 0.7505\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62347 to 0.62289, saving model to best.model\n",
      "1s - loss: 0.6761 - acc: 0.7230 - val_loss: 0.6229 - val_acc: 0.7492\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6751 - acc: 0.7231 - val_loss: 0.6258 - val_acc: 0.7441\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6708 - acc: 0.7279 - val_loss: 0.6236 - val_acc: 0.7439\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62289 to 0.62099, saving model to best.model\n",
      "0s - loss: 0.6725 - acc: 0.7256 - val_loss: 0.6210 - val_acc: 0.7505\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62099 to 0.62081, saving model to best.model\n",
      "0s - loss: 0.6686 - acc: 0.7260 - val_loss: 0.6208 - val_acc: 0.7527\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.62081 to 0.62078, saving model to best.model\n",
      "0s - loss: 0.6702 - acc: 0.7244 - val_loss: 0.6208 - val_acc: 0.7495\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6684 - acc: 0.7280 - val_loss: 0.6208 - val_acc: 0.7530\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.62078 to 0.61836, saving model to best.model\n",
      "0s - loss: 0.6691 - acc: 0.7293 - val_loss: 0.6184 - val_acc: 0.7520\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6683 - acc: 0.7270 - val_loss: 0.6186 - val_acc: 0.7507\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61836 to 0.61635, saving model to best.model\n",
      "0s - loss: 0.6683 - acc: 0.7275 - val_loss: 0.6164 - val_acc: 0.7529\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.61635 to 0.61355, saving model to best.model\n",
      "0s - loss: 0.6659 - acc: 0.7282 - val_loss: 0.6136 - val_acc: 0.7543\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6670 - acc: 0.7270 - val_loss: 0.6172 - val_acc: 0.7530\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6647 - acc: 0.7299 - val_loss: 0.6184 - val_acc: 0.7512\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61355 to 0.61302, saving model to best.model\n",
      "0s - loss: 0.6647 - acc: 0.7275 - val_loss: 0.6130 - val_acc: 0.7536\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6649 - acc: 0.7280 - val_loss: 0.6142 - val_acc: 0.7535\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6665 - acc: 0.7271 - val_loss: 0.6192 - val_acc: 0.7484\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6640 - acc: 0.7286 - val_loss: 0.6161 - val_acc: 0.7519\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6654 - acc: 0.7302 - val_loss: 0.6150 - val_acc: 0.7511\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.6663 - acc: 0.7297 - val_loss: 0.6168 - val_acc: 0.7489\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6628 - acc: 0.7300 - val_loss: 0.6132 - val_acc: 0.7539\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6631 - acc: 0.7308 - val_loss: 0.6137 - val_acc: 0.7512\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61302 to 0.60912, saving model to best.model\n",
      "1s - loss: 0.6622 - acc: 0.7301 - val_loss: 0.6091 - val_acc: 0.7563\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6626 - acc: 0.7287 - val_loss: 0.6115 - val_acc: 0.7549\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.60912 to 0.60884, saving model to best.model\n",
      "1s - loss: 0.6596 - acc: 0.7328 - val_loss: 0.6088 - val_acc: 0.7587\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6647 - acc: 0.7269 - val_loss: 0.6107 - val_acc: 0.7567\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6608 - acc: 0.7309 - val_loss: 0.6097 - val_acc: 0.7557\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60884 to 0.60767, saving model to best.model\n",
      "1s - loss: 0.6597 - acc: 0.7297 - val_loss: 0.6077 - val_acc: 0.7560\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6619 - acc: 0.7294 - val_loss: 0.6102 - val_acc: 0.7571\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6597 - acc: 0.7321 - val_loss: 0.6121 - val_acc: 0.7519\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6583 - acc: 0.7314 - val_loss: 0.6096 - val_acc: 0.7561\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60767 to 0.60708, saving model to best.model\n",
      "1s - loss: 0.6591 - acc: 0.7305 - val_loss: 0.6071 - val_acc: 0.7561\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6595 - acc: 0.7314 - val_loss: 0.6081 - val_acc: 0.7562\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6590 - acc: 0.7303 - val_loss: 0.6079 - val_acc: 0.7574\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6582 - acc: 0.7306 - val_loss: 0.6077 - val_acc: 0.7546\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60708 to 0.60571, saving model to best.model\n",
      "0s - loss: 0.6558 - acc: 0.7330 - val_loss: 0.6057 - val_acc: 0.7568\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6568 - acc: 0.7330 - val_loss: 0.6058 - val_acc: 0.7561\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60571 to 0.60456, saving model to best.model\n",
      "0s - loss: 0.6560 - acc: 0.7326 - val_loss: 0.6046 - val_acc: 0.7593\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6571 - acc: 0.7318 - val_loss: 0.6059 - val_acc: 0.7594\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60456 to 0.60382, saving model to best.model\n",
      "0s - loss: 0.6572 - acc: 0.7311 - val_loss: 0.6038 - val_acc: 0.7581\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.6553 - acc: 0.7319 - val_loss: 0.6064 - val_acc: 0.7535\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60382 to 0.60338, saving model to best.model\n",
      "0s - loss: 0.6543 - acc: 0.7318 - val_loss: 0.6034 - val_acc: 0.7568\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6562 - acc: 0.7323 - val_loss: 0.6036 - val_acc: 0.7581\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84744, saving model to best.model\n",
      "0s - loss: 0.9238 - acc: 0.6273 - val_loss: 0.8474 - val_acc: 0.6588\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84744 to 0.84633, saving model to best.model\n",
      "0s - loss: 0.8585 - acc: 0.6619 - val_loss: 0.8463 - val_acc: 0.6588\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84633 to 0.84443, saving model to best.model\n",
      "0s - loss: 0.8491 - acc: 0.6629 - val_loss: 0.8444 - val_acc: 0.6588\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84443 to 0.84288, saving model to best.model\n",
      "0s - loss: 0.8465 - acc: 0.6629 - val_loss: 0.8429 - val_acc: 0.6588\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84288 to 0.84050, saving model to best.model\n",
      "0s - loss: 0.8424 - acc: 0.6629 - val_loss: 0.8405 - val_acc: 0.6588\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.84050 to 0.83376, saving model to best.model\n",
      "0s - loss: 0.8362 - acc: 0.6629 - val_loss: 0.8338 - val_acc: 0.6588\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83376 to 0.83221, saving model to best.model\n",
      "0s - loss: 0.8315 - acc: 0.6629 - val_loss: 0.8322 - val_acc: 0.6588\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.83221 to 0.82997, saving model to best.model\n",
      "0s - loss: 0.8291 - acc: 0.6629 - val_loss: 0.8300 - val_acc: 0.6588\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82997 to 0.82720, saving model to best.model\n",
      "1s - loss: 0.8280 - acc: 0.6629 - val_loss: 0.8272 - val_acc: 0.6588\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8254 - acc: 0.6629 - val_loss: 0.8276 - val_acc: 0.6588\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82720 to 0.82607, saving model to best.model\n",
      "1s - loss: 0.8244 - acc: 0.6629 - val_loss: 0.8261 - val_acc: 0.6588\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8239 - acc: 0.6629 - val_loss: 0.8274 - val_acc: 0.6588\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82607 to 0.82529, saving model to best.model\n",
      "1s - loss: 0.8219 - acc: 0.6629 - val_loss: 0.8253 - val_acc: 0.6588\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82529 to 0.82476, saving model to best.model\n",
      "1s - loss: 0.8227 - acc: 0.6629 - val_loss: 0.8248 - val_acc: 0.6588\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82476 to 0.82453, saving model to best.model\n",
      "1s - loss: 0.8213 - acc: 0.6628 - val_loss: 0.8245 - val_acc: 0.6588\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8210 - acc: 0.6629 - val_loss: 0.8246 - val_acc: 0.6588\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "1s - loss: 0.8185 - acc: 0.6629 - val_loss: 0.8251 - val_acc: 0.6588\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82453 to 0.82290, saving model to best.model\n",
      "1s - loss: 0.8187 - acc: 0.6628 - val_loss: 0.8229 - val_acc: 0.6588\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.82290 to 0.82236, saving model to best.model\n",
      "1s - loss: 0.8185 - acc: 0.6628 - val_loss: 0.8224 - val_acc: 0.6588\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82236 to 0.82112, saving model to best.model\n",
      "1s - loss: 0.8161 - acc: 0.6631 - val_loss: 0.8211 - val_acc: 0.6588\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82112 to 0.82076, saving model to best.model\n",
      "0s - loss: 0.8172 - acc: 0.6631 - val_loss: 0.8208 - val_acc: 0.6588\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82076 to 0.81947, saving model to best.model\n",
      "0s - loss: 0.8142 - acc: 0.6629 - val_loss: 0.8195 - val_acc: 0.6588\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81947 to 0.81886, saving model to best.model\n",
      "0s - loss: 0.8140 - acc: 0.6631 - val_loss: 0.8189 - val_acc: 0.6588\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81886 to 0.81820, saving model to best.model\n",
      "0s - loss: 0.8138 - acc: 0.6633 - val_loss: 0.8182 - val_acc: 0.6588\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81820 to 0.81640, saving model to best.model\n",
      "0s - loss: 0.8120 - acc: 0.6639 - val_loss: 0.8164 - val_acc: 0.6601\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81640 to 0.81586, saving model to best.model\n",
      "0s - loss: 0.8111 - acc: 0.6645 - val_loss: 0.8159 - val_acc: 0.6604\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81586 to 0.81458, saving model to best.model\n",
      "0s - loss: 0.8100 - acc: 0.6649 - val_loss: 0.8146 - val_acc: 0.6644\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81458 to 0.81322, saving model to best.model\n",
      "0s - loss: 0.8086 - acc: 0.6635 - val_loss: 0.8132 - val_acc: 0.6608\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81322 to 0.81244, saving model to best.model\n",
      "0s - loss: 0.8087 - acc: 0.6654 - val_loss: 0.8124 - val_acc: 0.6621\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81244 to 0.80989, saving model to best.model\n",
      "1s - loss: 0.8063 - acc: 0.6657 - val_loss: 0.8099 - val_acc: 0.6646\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80989 to 0.80800, saving model to best.model\n",
      "1s - loss: 0.8057 - acc: 0.6665 - val_loss: 0.8080 - val_acc: 0.6641\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80800 to 0.80778, saving model to best.model\n",
      "1s - loss: 0.8049 - acc: 0.6670 - val_loss: 0.8078 - val_acc: 0.6612\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80778 to 0.80479, saving model to best.model\n",
      "1s - loss: 0.8028 - acc: 0.6681 - val_loss: 0.8048 - val_acc: 0.6638\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80479 to 0.80328, saving model to best.model\n",
      "1s - loss: 0.8013 - acc: 0.6673 - val_loss: 0.8033 - val_acc: 0.6645\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80328 to 0.80050, saving model to best.model\n",
      "1s - loss: 0.7996 - acc: 0.6683 - val_loss: 0.8005 - val_acc: 0.6662\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80050 to 0.79956, saving model to best.model\n",
      "1s - loss: 0.7971 - acc: 0.6691 - val_loss: 0.7996 - val_acc: 0.6672\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79956 to 0.79649, saving model to best.model\n",
      "1s - loss: 0.7971 - acc: 0.6684 - val_loss: 0.7965 - val_acc: 0.6670\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79649 to 0.79570, saving model to best.model\n",
      "1s - loss: 0.7979 - acc: 0.6685 - val_loss: 0.7957 - val_acc: 0.6664\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79570 to 0.79327, saving model to best.model\n",
      "1s - loss: 0.7944 - acc: 0.6711 - val_loss: 0.7933 - val_acc: 0.6678\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79327 to 0.79072, saving model to best.model\n",
      "1s - loss: 0.7934 - acc: 0.6707 - val_loss: 0.7907 - val_acc: 0.6684\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79072 to 0.78953, saving model to best.model\n",
      "1s - loss: 0.7926 - acc: 0.6702 - val_loss: 0.7895 - val_acc: 0.6696\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78953 to 0.78749, saving model to best.model\n",
      "1s - loss: 0.7902 - acc: 0.6718 - val_loss: 0.7875 - val_acc: 0.6703\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "1s - loss: 0.7889 - acc: 0.6731 - val_loss: 0.7877 - val_acc: 0.6697\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78749 to 0.78430, saving model to best.model\n",
      "1s - loss: 0.7877 - acc: 0.6740 - val_loss: 0.7843 - val_acc: 0.6719\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78430 to 0.78178, saving model to best.model\n",
      "1s - loss: 0.7867 - acc: 0.6742 - val_loss: 0.7818 - val_acc: 0.6738\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78178 to 0.78080, saving model to best.model\n",
      "1s - loss: 0.7830 - acc: 0.6749 - val_loss: 0.7808 - val_acc: 0.6714\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "1s - loss: 0.7851 - acc: 0.6753 - val_loss: 0.7816 - val_acc: 0.6692\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.78080 to 0.77522, saving model to best.model\n",
      "1s - loss: 0.7813 - acc: 0.6756 - val_loss: 0.7752 - val_acc: 0.6789\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77522 to 0.77358, saving model to best.model\n",
      "1s - loss: 0.7796 - acc: 0.6757 - val_loss: 0.7736 - val_acc: 0.6755\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77358 to 0.76973, saving model to best.model\n",
      "1s - loss: 0.7781 - acc: 0.6764 - val_loss: 0.7697 - val_acc: 0.6787\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76973 to 0.76843, saving model to best.model\n",
      "1s - loss: 0.7765 - acc: 0.6787 - val_loss: 0.7684 - val_acc: 0.6796\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "1s - loss: 0.7769 - acc: 0.6767 - val_loss: 0.7685 - val_acc: 0.6803\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76843 to 0.76445, saving model to best.model\n",
      "1s - loss: 0.7736 - acc: 0.6779 - val_loss: 0.7644 - val_acc: 0.6827\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76445 to 0.76119, saving model to best.model\n",
      "1s - loss: 0.7724 - acc: 0.6809 - val_loss: 0.7612 - val_acc: 0.6837\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76119 to 0.75927, saving model to best.model\n",
      "1s - loss: 0.7697 - acc: 0.6824 - val_loss: 0.7593 - val_acc: 0.6845\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75927 to 0.75786, saving model to best.model\n",
      "1s - loss: 0.7692 - acc: 0.6798 - val_loss: 0.7579 - val_acc: 0.6848\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75786 to 0.75363, saving model to best.model\n",
      "1s - loss: 0.7668 - acc: 0.6824 - val_loss: 0.7536 - val_acc: 0.6862\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "1s - loss: 0.7662 - acc: 0.6826 - val_loss: 0.7537 - val_acc: 0.6840\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75363 to 0.74769, saving model to best.model\n",
      "1s - loss: 0.7628 - acc: 0.6842 - val_loss: 0.7477 - val_acc: 0.6898\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74769 to 0.74761, saving model to best.model\n",
      "1s - loss: 0.7638 - acc: 0.6832 - val_loss: 0.7476 - val_acc: 0.6865\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74761 to 0.74461, saving model to best.model\n",
      "1s - loss: 0.7612 - acc: 0.6837 - val_loss: 0.7446 - val_acc: 0.6906\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74461 to 0.74055, saving model to best.model\n",
      "1s - loss: 0.7586 - acc: 0.6871 - val_loss: 0.7405 - val_acc: 0.6911\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74055 to 0.73659, saving model to best.model\n",
      "1s - loss: 0.7558 - acc: 0.6881 - val_loss: 0.7366 - val_acc: 0.6964\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "1s - loss: 0.7581 - acc: 0.6848 - val_loss: 0.7373 - val_acc: 0.6918\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73659 to 0.73312, saving model to best.model\n",
      "1s - loss: 0.7560 - acc: 0.6866 - val_loss: 0.7331 - val_acc: 0.6925\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73312 to 0.72917, saving model to best.model\n",
      "1s - loss: 0.7481 - acc: 0.6903 - val_loss: 0.7292 - val_acc: 0.6941\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72917 to 0.72660, saving model to best.model\n",
      "1s - loss: 0.7484 - acc: 0.6902 - val_loss: 0.7266 - val_acc: 0.7004\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72660 to 0.72565, saving model to best.model\n",
      "1s - loss: 0.7474 - acc: 0.6883 - val_loss: 0.7256 - val_acc: 0.6982\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72565 to 0.72179, saving model to best.model\n",
      "1s - loss: 0.7479 - acc: 0.6912 - val_loss: 0.7218 - val_acc: 0.7008\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72179 to 0.72113, saving model to best.model\n",
      "1s - loss: 0.7424 - acc: 0.6934 - val_loss: 0.7211 - val_acc: 0.7027\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72113 to 0.72064, saving model to best.model\n",
      "1s - loss: 0.7427 - acc: 0.6917 - val_loss: 0.7206 - val_acc: 0.6972\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72064 to 0.71605, saving model to best.model\n",
      "1s - loss: 0.7401 - acc: 0.6944 - val_loss: 0.7160 - val_acc: 0.7040\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71605 to 0.71427, saving model to best.model\n",
      "1s - loss: 0.7410 - acc: 0.6935 - val_loss: 0.7143 - val_acc: 0.7019\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71427 to 0.71301, saving model to best.model\n",
      "1s - loss: 0.7401 - acc: 0.6948 - val_loss: 0.7130 - val_acc: 0.7063\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71301 to 0.70967, saving model to best.model\n",
      "1s - loss: 0.7367 - acc: 0.6960 - val_loss: 0.7097 - val_acc: 0.7089\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70967 to 0.70729, saving model to best.model\n",
      "1s - loss: 0.7349 - acc: 0.6963 - val_loss: 0.7073 - val_acc: 0.7047\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70729 to 0.70717, saving model to best.model\n",
      "1s - loss: 0.7341 - acc: 0.6964 - val_loss: 0.7072 - val_acc: 0.7096\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70717 to 0.70597, saving model to best.model\n",
      "1s - loss: 0.7357 - acc: 0.6953 - val_loss: 0.7060 - val_acc: 0.7088\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70597 to 0.70066, saving model to best.model\n",
      "1s - loss: 0.7313 - acc: 0.6983 - val_loss: 0.7007 - val_acc: 0.7121\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70066 to 0.69820, saving model to best.model\n",
      "1s - loss: 0.7309 - acc: 0.6987 - val_loss: 0.6982 - val_acc: 0.7129\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7288 - acc: 0.6978 - val_loss: 0.7007 - val_acc: 0.7057\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69820 to 0.69613, saving model to best.model\n",
      "1s - loss: 0.7276 - acc: 0.6997 - val_loss: 0.6961 - val_acc: 0.7104\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69613 to 0.69318, saving model to best.model\n",
      "1s - loss: 0.7244 - acc: 0.7018 - val_loss: 0.6932 - val_acc: 0.7196\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69318 to 0.69288, saving model to best.model\n",
      "1s - loss: 0.7266 - acc: 0.6995 - val_loss: 0.6929 - val_acc: 0.7208\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69288 to 0.68897, saving model to best.model\n",
      "1s - loss: 0.7245 - acc: 0.7038 - val_loss: 0.6890 - val_acc: 0.7189\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7236 - acc: 0.7006 - val_loss: 0.6909 - val_acc: 0.7178\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68897 to 0.68568, saving model to best.model\n",
      "1s - loss: 0.7180 - acc: 0.7032 - val_loss: 0.6857 - val_acc: 0.7211\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68568 to 0.68505, saving model to best.model\n",
      "1s - loss: 0.7200 - acc: 0.7034 - val_loss: 0.6851 - val_acc: 0.7176\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.7204 - acc: 0.7028 - val_loss: 0.6863 - val_acc: 0.7153\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68505 to 0.68266, saving model to best.model\n",
      "1s - loss: 0.7157 - acc: 0.7056 - val_loss: 0.6827 - val_acc: 0.7199\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7168 - acc: 0.7026 - val_loss: 0.6862 - val_acc: 0.7193\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68266 to 0.68135, saving model to best.model\n",
      "1s - loss: 0.7169 - acc: 0.7033 - val_loss: 0.6814 - val_acc: 0.7196\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7199 - acc: 0.7016 - val_loss: 0.6833 - val_acc: 0.7152\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68135 to 0.67745, saving model to best.model\n",
      "1s - loss: 0.7135 - acc: 0.7082 - val_loss: 0.6775 - val_acc: 0.7227\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7117 - acc: 0.7079 - val_loss: 0.6778 - val_acc: 0.7228\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67745 to 0.67381, saving model to best.model\n",
      "1s - loss: 0.7118 - acc: 0.7061 - val_loss: 0.6738 - val_acc: 0.7228\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7101 - acc: 0.7059 - val_loss: 0.6752 - val_acc: 0.7254\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67381 to 0.66718, saving model to best.model\n",
      "1s - loss: 0.7058 - acc: 0.7090 - val_loss: 0.6672 - val_acc: 0.7311\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.7112 - acc: 0.7062 - val_loss: 0.6718 - val_acc: 0.7259\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7079 - acc: 0.7097 - val_loss: 0.6689 - val_acc: 0.7262\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7077 - acc: 0.7106 - val_loss: 0.6673 - val_acc: 0.7276\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.7037 - acc: 0.7117 - val_loss: 0.6680 - val_acc: 0.7290\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7028 - acc: 0.7125 - val_loss: 0.6681 - val_acc: 0.7255\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66718 to 0.66178, saving model to best.model\n",
      "0s - loss: 0.7031 - acc: 0.7114 - val_loss: 0.6618 - val_acc: 0.7333\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.7018 - acc: 0.7120 - val_loss: 0.6637 - val_acc: 0.7299\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7021 - acc: 0.7116 - val_loss: 0.6641 - val_acc: 0.7314\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66178 to 0.66071, saving model to best.model\n",
      "1s - loss: 0.7010 - acc: 0.7119 - val_loss: 0.6607 - val_acc: 0.7337\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6998 - acc: 0.7122 - val_loss: 0.6609 - val_acc: 0.7314\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66071 to 0.65827, saving model to best.model\n",
      "1s - loss: 0.6976 - acc: 0.7155 - val_loss: 0.6583 - val_acc: 0.7326\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6985 - acc: 0.7148 - val_loss: 0.6602 - val_acc: 0.7338\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6967 - acc: 0.7152 - val_loss: 0.6612 - val_acc: 0.7375\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65827 to 0.65572, saving model to best.model\n",
      "1s - loss: 0.6984 - acc: 0.7141 - val_loss: 0.6557 - val_acc: 0.7390\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6949 - acc: 0.7138 - val_loss: 0.6566 - val_acc: 0.7372\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6939 - acc: 0.7160 - val_loss: 0.6567 - val_acc: 0.7337\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.65572 to 0.65496, saving model to best.model\n",
      "1s - loss: 0.6948 - acc: 0.7154 - val_loss: 0.6550 - val_acc: 0.7368\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65496 to 0.65483, saving model to best.model\n",
      "1s - loss: 0.6951 - acc: 0.7150 - val_loss: 0.6548 - val_acc: 0.7396\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.65483 to 0.65126, saving model to best.model\n",
      "1s - loss: 0.6901 - acc: 0.7185 - val_loss: 0.6513 - val_acc: 0.7393\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65126 to 0.65120, saving model to best.model\n",
      "1s - loss: 0.6929 - acc: 0.7140 - val_loss: 0.6512 - val_acc: 0.7367\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.65120 to 0.65081, saving model to best.model\n",
      "1s - loss: 0.6935 - acc: 0.7154 - val_loss: 0.6508 - val_acc: 0.7372\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65081 to 0.64776, saving model to best.model\n",
      "1s - loss: 0.6911 - acc: 0.7161 - val_loss: 0.6478 - val_acc: 0.7420\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6856 - acc: 0.7191 - val_loss: 0.6507 - val_acc: 0.7402\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6903 - acc: 0.7143 - val_loss: 0.6503 - val_acc: 0.7381\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6862 - acc: 0.7206 - val_loss: 0.6487 - val_acc: 0.7378\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64776 to 0.64457, saving model to best.model\n",
      "1s - loss: 0.6879 - acc: 0.7184 - val_loss: 0.6446 - val_acc: 0.7410\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64457 to 0.64457, saving model to best.model\n",
      "1s - loss: 0.6857 - acc: 0.7205 - val_loss: 0.6446 - val_acc: 0.7404\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.64457 to 0.64261, saving model to best.model\n",
      "1s - loss: 0.6848 - acc: 0.7186 - val_loss: 0.6426 - val_acc: 0.7426\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6878 - acc: 0.7164 - val_loss: 0.6467 - val_acc: 0.7434\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64261 to 0.64125, saving model to best.model\n",
      "1s - loss: 0.6835 - acc: 0.7222 - val_loss: 0.6412 - val_acc: 0.7430\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6857 - acc: 0.7193 - val_loss: 0.6458 - val_acc: 0.7444\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64125 to 0.64084, saving model to best.model\n",
      "1s - loss: 0.6847 - acc: 0.7201 - val_loss: 0.6408 - val_acc: 0.7447\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6838 - acc: 0.7198 - val_loss: 0.6414 - val_acc: 0.7433\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6833 - acc: 0.7202 - val_loss: 0.6409 - val_acc: 0.7418\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.64084 to 0.63904, saving model to best.model\n",
      "1s - loss: 0.6793 - acc: 0.7227 - val_loss: 0.6390 - val_acc: 0.7446\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6824 - acc: 0.7200 - val_loss: 0.6397 - val_acc: 0.7413\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.63904 to 0.63713, saving model to best.model\n",
      "1s - loss: 0.6781 - acc: 0.7220 - val_loss: 0.6371 - val_acc: 0.7441\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6781 - acc: 0.7229 - val_loss: 0.6412 - val_acc: 0.7467\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6783 - acc: 0.7242 - val_loss: 0.6381 - val_acc: 0.7419\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63713 to 0.63546, saving model to best.model\n",
      "1s - loss: 0.6757 - acc: 0.7212 - val_loss: 0.6355 - val_acc: 0.7463\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.63546 to 0.63532, saving model to best.model\n",
      "1s - loss: 0.6779 - acc: 0.7237 - val_loss: 0.6353 - val_acc: 0.7458\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63532 to 0.63407, saving model to best.model\n",
      "1s - loss: 0.6782 - acc: 0.7215 - val_loss: 0.6341 - val_acc: 0.7446\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6762 - acc: 0.7246 - val_loss: 0.6350 - val_acc: 0.7444\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.63407 to 0.63267, saving model to best.model\n",
      "1s - loss: 0.6762 - acc: 0.7234 - val_loss: 0.6327 - val_acc: 0.7489\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63267 to 0.63153, saving model to best.model\n",
      "1s - loss: 0.6748 - acc: 0.7229 - val_loss: 0.6315 - val_acc: 0.7456\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63153 to 0.62907, saving model to best.model\n",
      "1s - loss: 0.6744 - acc: 0.7243 - val_loss: 0.6291 - val_acc: 0.7492\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6759 - acc: 0.7238 - val_loss: 0.6299 - val_acc: 0.7492\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62907 to 0.62901, saving model to best.model\n",
      "1s - loss: 0.6709 - acc: 0.7247 - val_loss: 0.6290 - val_acc: 0.7477\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6750 - acc: 0.7218 - val_loss: 0.6313 - val_acc: 0.7468\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62901 to 0.62817, saving model to best.model\n",
      "1s - loss: 0.6730 - acc: 0.7256 - val_loss: 0.6282 - val_acc: 0.7505\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6733 - acc: 0.7229 - val_loss: 0.6299 - val_acc: 0.7474\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6729 - acc: 0.7236 - val_loss: 0.6282 - val_acc: 0.7508\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62817 to 0.62786, saving model to best.model\n",
      "1s - loss: 0.6699 - acc: 0.7270 - val_loss: 0.6279 - val_acc: 0.7482\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62786 to 0.62620, saving model to best.model\n",
      "1s - loss: 0.6712 - acc: 0.7249 - val_loss: 0.6262 - val_acc: 0.7507\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62620 to 0.62618, saving model to best.model\n",
      "1s - loss: 0.6681 - acc: 0.7262 - val_loss: 0.6262 - val_acc: 0.7509\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62618 to 0.62455, saving model to best.model\n",
      "1s - loss: 0.6686 - acc: 0.7261 - val_loss: 0.6245 - val_acc: 0.7519\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6689 - acc: 0.7243 - val_loss: 0.6269 - val_acc: 0.7502\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6728 - acc: 0.7234 - val_loss: 0.6263 - val_acc: 0.7512\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.62455 to 0.62396, saving model to best.model\n",
      "1s - loss: 0.6659 - acc: 0.7297 - val_loss: 0.6240 - val_acc: 0.7527\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.62396 to 0.62289, saving model to best.model\n",
      "1s - loss: 0.6677 - acc: 0.7266 - val_loss: 0.6229 - val_acc: 0.7498\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.6662 - acc: 0.7299 - val_loss: 0.6241 - val_acc: 0.7505\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.62289 to 0.62072, saving model to best.model\n",
      "1s - loss: 0.6643 - acc: 0.7279 - val_loss: 0.6207 - val_acc: 0.7540\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6654 - acc: 0.7274 - val_loss: 0.6233 - val_acc: 0.7512\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62072 to 0.61976, saving model to best.model\n",
      "1s - loss: 0.6639 - acc: 0.7279 - val_loss: 0.6198 - val_acc: 0.7539\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6663 - acc: 0.7303 - val_loss: 0.6206 - val_acc: 0.7549\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6626 - acc: 0.7300 - val_loss: 0.6198 - val_acc: 0.7549\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6601 - acc: 0.7309 - val_loss: 0.6210 - val_acc: 0.7541\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6629 - acc: 0.7289 - val_loss: 0.6218 - val_acc: 0.7507\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61976 to 0.61700, saving model to best.model\n",
      "1s - loss: 0.6614 - acc: 0.7318 - val_loss: 0.6170 - val_acc: 0.7533\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6627 - acc: 0.7305 - val_loss: 0.6204 - val_acc: 0.7549\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.61700 to 0.61551, saving model to best.model\n",
      "1s - loss: 0.6590 - acc: 0.7315 - val_loss: 0.6155 - val_acc: 0.7547\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6605 - acc: 0.7314 - val_loss: 0.6189 - val_acc: 0.7530\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6597 - acc: 0.7309 - val_loss: 0.6164 - val_acc: 0.7540\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6584 - acc: 0.7287 - val_loss: 0.6169 - val_acc: 0.7550\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61551 to 0.61484, saving model to best.model\n",
      "1s - loss: 0.6586 - acc: 0.7315 - val_loss: 0.6148 - val_acc: 0.7556\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.61484 to 0.61312, saving model to best.model\n",
      "1s - loss: 0.6572 - acc: 0.7327 - val_loss: 0.6131 - val_acc: 0.7564\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6613 - acc: 0.7281 - val_loss: 0.6198 - val_acc: 0.7543\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6551 - acc: 0.7345 - val_loss: 0.6164 - val_acc: 0.7529\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6598 - acc: 0.7297 - val_loss: 0.6156 - val_acc: 0.7556\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6535 - acc: 0.7327 - val_loss: 0.6141 - val_acc: 0.7556\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6568 - acc: 0.7314 - val_loss: 0.6154 - val_acc: 0.7559\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61312 to 0.61279, saving model to best.model\n",
      "1s - loss: 0.6558 - acc: 0.7311 - val_loss: 0.6128 - val_acc: 0.7578\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6545 - acc: 0.7322 - val_loss: 0.6134 - val_acc: 0.7556\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6551 - acc: 0.7310 - val_loss: 0.6153 - val_acc: 0.7568\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.61279 to 0.61158, saving model to best.model\n",
      "1s - loss: 0.6536 - acc: 0.7344 - val_loss: 0.6116 - val_acc: 0.7554\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.61158 to 0.61088, saving model to best.model\n",
      "1s - loss: 0.6567 - acc: 0.7320 - val_loss: 0.6109 - val_acc: 0.7566\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6547 - acc: 0.7306 - val_loss: 0.6111 - val_acc: 0.7570\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.61088 to 0.60796, saving model to best.model\n",
      "1s - loss: 0.6531 - acc: 0.7332 - val_loss: 0.6080 - val_acc: 0.7591\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6520 - acc: 0.7343 - val_loss: 0.6092 - val_acc: 0.7575\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60796 to 0.60784, saving model to best.model\n",
      "1s - loss: 0.6519 - acc: 0.7333 - val_loss: 0.6078 - val_acc: 0.7564\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60784 to 0.60611, saving model to best.model\n",
      "1s - loss: 0.6464 - acc: 0.7361 - val_loss: 0.6061 - val_acc: 0.7600\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60611 to 0.60552, saving model to best.model\n",
      "1s - loss: 0.6519 - acc: 0.7346 - val_loss: 0.6055 - val_acc: 0.7578\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6489 - acc: 0.7355 - val_loss: 0.6063 - val_acc: 0.7574\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6506 - acc: 0.7367 - val_loss: 0.6071 - val_acc: 0.7566\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60552 to 0.60477, saving model to best.model\n",
      "1s - loss: 0.6494 - acc: 0.7359 - val_loss: 0.6048 - val_acc: 0.7582\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60477 to 0.60355, saving model to best.model\n",
      "1s - loss: 0.6490 - acc: 0.7352 - val_loss: 0.6035 - val_acc: 0.7595\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6489 - acc: 0.7343 - val_loss: 0.6077 - val_acc: 0.7594\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6482 - acc: 0.7342 - val_loss: 0.6060 - val_acc: 0.7578\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6440 - acc: 0.7376 - val_loss: 0.6054 - val_acc: 0.7583\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6480 - acc: 0.7336 - val_loss: 0.6038 - val_acc: 0.7595\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6515 - acc: 0.7374 - val_loss: 0.6042 - val_acc: 0.7596\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6513 - acc: 0.7354 - val_loss: 0.6049 - val_acc: 0.7589\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84484, saving model to best.model\n",
      "1s - loss: 0.9208 - acc: 0.6291 - val_loss: 0.8448 - val_acc: 0.6611\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84484 to 0.84293, saving model to best.model\n",
      "1s - loss: 0.8556 - acc: 0.6616 - val_loss: 0.8429 - val_acc: 0.6611\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84293 to 0.84289, saving model to best.model\n",
      "1s - loss: 0.8482 - acc: 0.6618 - val_loss: 0.8429 - val_acc: 0.6611\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84289 to 0.83928, saving model to best.model\n",
      "1s - loss: 0.8465 - acc: 0.6618 - val_loss: 0.8393 - val_acc: 0.6611\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83928 to 0.83310, saving model to best.model\n",
      "1s - loss: 0.8407 - acc: 0.6618 - val_loss: 0.8331 - val_acc: 0.6611\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83310 to 0.83114, saving model to best.model\n",
      "1s - loss: 0.8362 - acc: 0.6618 - val_loss: 0.8311 - val_acc: 0.6611\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83114 to 0.82631, saving model to best.model\n",
      "1s - loss: 0.8313 - acc: 0.6618 - val_loss: 0.8263 - val_acc: 0.6611\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82631 to 0.82424, saving model to best.model\n",
      "1s - loss: 0.8295 - acc: 0.6618 - val_loss: 0.8242 - val_acc: 0.6611\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82424 to 0.82288, saving model to best.model\n",
      "0s - loss: 0.8265 - acc: 0.6618 - val_loss: 0.8229 - val_acc: 0.6611\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82288 to 0.82265, saving model to best.model\n",
      "1s - loss: 0.8263 - acc: 0.6618 - val_loss: 0.8227 - val_acc: 0.6611\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82265 to 0.82196, saving model to best.model\n",
      "1s - loss: 0.8239 - acc: 0.6618 - val_loss: 0.8220 - val_acc: 0.6611\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82196 to 0.82147, saving model to best.model\n",
      "1s - loss: 0.8242 - acc: 0.6618 - val_loss: 0.8215 - val_acc: 0.6611\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82147 to 0.82112, saving model to best.model\n",
      "1s - loss: 0.8238 - acc: 0.6618 - val_loss: 0.8211 - val_acc: 0.6611\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8227 - acc: 0.6618 - val_loss: 0.8222 - val_acc: 0.6611\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82112 to 0.82074, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6618 - val_loss: 0.8207 - val_acc: 0.6611\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82074 to 0.82060, saving model to best.model\n",
      "1s - loss: 0.8209 - acc: 0.6617 - val_loss: 0.8206 - val_acc: 0.6611\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82060 to 0.81993, saving model to best.model\n",
      "1s - loss: 0.8217 - acc: 0.6618 - val_loss: 0.8199 - val_acc: 0.6611\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "1s - loss: 0.8206 - acc: 0.6618 - val_loss: 0.8218 - val_acc: 0.6611\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81993 to 0.81946, saving model to best.model\n",
      "0s - loss: 0.8194 - acc: 0.6617 - val_loss: 0.8195 - val_acc: 0.6611\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81946 to 0.81864, saving model to best.model\n",
      "1s - loss: 0.8187 - acc: 0.6617 - val_loss: 0.8186 - val_acc: 0.6611\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81864 to 0.81829, saving model to best.model\n",
      "1s - loss: 0.8185 - acc: 0.6617 - val_loss: 0.8183 - val_acc: 0.6611\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81829 to 0.81826, saving model to best.model\n",
      "1s - loss: 0.8186 - acc: 0.6615 - val_loss: 0.8183 - val_acc: 0.6611\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81826 to 0.81701, saving model to best.model\n",
      "1s - loss: 0.8161 - acc: 0.6616 - val_loss: 0.8170 - val_acc: 0.6611\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "1s - loss: 0.8171 - acc: 0.6624 - val_loss: 0.8188 - val_acc: 0.6611\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81701 to 0.81482, saving model to best.model\n",
      "1s - loss: 0.8153 - acc: 0.6621 - val_loss: 0.8148 - val_acc: 0.6611\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81482 to 0.81380, saving model to best.model\n",
      "1s - loss: 0.8154 - acc: 0.6617 - val_loss: 0.8138 - val_acc: 0.6611\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81380 to 0.81161, saving model to best.model\n",
      "1s - loss: 0.8134 - acc: 0.6631 - val_loss: 0.8116 - val_acc: 0.6622\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81161 to 0.81104, saving model to best.model\n",
      "1s - loss: 0.8119 - acc: 0.6630 - val_loss: 0.8110 - val_acc: 0.6630\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81104 to 0.80925, saving model to best.model\n",
      "1s - loss: 0.8110 - acc: 0.6651 - val_loss: 0.8093 - val_acc: 0.6635\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80925 to 0.80830, saving model to best.model\n",
      "1s - loss: 0.8107 - acc: 0.6642 - val_loss: 0.8083 - val_acc: 0.6633\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80830 to 0.80641, saving model to best.model\n",
      "1s - loss: 0.8101 - acc: 0.6640 - val_loss: 0.8064 - val_acc: 0.6676\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80641 to 0.80551, saving model to best.model\n",
      "1s - loss: 0.8078 - acc: 0.6641 - val_loss: 0.8055 - val_acc: 0.6633\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80551 to 0.80423, saving model to best.model\n",
      "1s - loss: 0.8074 - acc: 0.6658 - val_loss: 0.8042 - val_acc: 0.6680\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80423 to 0.80187, saving model to best.model\n",
      "0s - loss: 0.8044 - acc: 0.6663 - val_loss: 0.8019 - val_acc: 0.6692\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80187 to 0.80037, saving model to best.model\n",
      "1s - loss: 0.8039 - acc: 0.6673 - val_loss: 0.8004 - val_acc: 0.6657\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80037 to 0.79965, saving model to best.model\n",
      "1s - loss: 0.8023 - acc: 0.6679 - val_loss: 0.7997 - val_acc: 0.6699\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79965 to 0.79617, saving model to best.model\n",
      "1s - loss: 0.8008 - acc: 0.6677 - val_loss: 0.7962 - val_acc: 0.6671\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79617 to 0.79430, saving model to best.model\n",
      "1s - loss: 0.8005 - acc: 0.6670 - val_loss: 0.7943 - val_acc: 0.6678\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79430 to 0.79256, saving model to best.model\n",
      "1s - loss: 0.7984 - acc: 0.6676 - val_loss: 0.7926 - val_acc: 0.6686\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79256 to 0.79023, saving model to best.model\n",
      "1s - loss: 0.7963 - acc: 0.6687 - val_loss: 0.7902 - val_acc: 0.6701\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79023 to 0.78711, saving model to best.model\n",
      "1s - loss: 0.7941 - acc: 0.6693 - val_loss: 0.7871 - val_acc: 0.6742\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78711 to 0.78477, saving model to best.model\n",
      "1s - loss: 0.7926 - acc: 0.6695 - val_loss: 0.7848 - val_acc: 0.6731\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78477 to 0.78315, saving model to best.model\n",
      "0s - loss: 0.7912 - acc: 0.6686 - val_loss: 0.7831 - val_acc: 0.6760\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78315 to 0.78160, saving model to best.model\n",
      "1s - loss: 0.7903 - acc: 0.6712 - val_loss: 0.7816 - val_acc: 0.6769\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78160 to 0.77918, saving model to best.model\n",
      "1s - loss: 0.7885 - acc: 0.6716 - val_loss: 0.7792 - val_acc: 0.6787\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77918 to 0.77778, saving model to best.model\n",
      "1s - loss: 0.7866 - acc: 0.6726 - val_loss: 0.7778 - val_acc: 0.6752\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77778 to 0.77544, saving model to best.model\n",
      "1s - loss: 0.7840 - acc: 0.6729 - val_loss: 0.7754 - val_acc: 0.6779\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77544 to 0.77298, saving model to best.model\n",
      "1s - loss: 0.7836 - acc: 0.6719 - val_loss: 0.7730 - val_acc: 0.6811\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77298 to 0.77275, saving model to best.model\n",
      "1s - loss: 0.7806 - acc: 0.6735 - val_loss: 0.7728 - val_acc: 0.6824\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77275 to 0.77017, saving model to best.model\n",
      "1s - loss: 0.7809 - acc: 0.6723 - val_loss: 0.7702 - val_acc: 0.6831\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77017 to 0.76506, saving model to best.model\n",
      "1s - loss: 0.7780 - acc: 0.6741 - val_loss: 0.7651 - val_acc: 0.6827\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76506 to 0.76430, saving model to best.model\n",
      "0s - loss: 0.7765 - acc: 0.6770 - val_loss: 0.7643 - val_acc: 0.6822\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76430 to 0.76148, saving model to best.model\n",
      "1s - loss: 0.7754 - acc: 0.6765 - val_loss: 0.7615 - val_acc: 0.6829\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76148 to 0.75792, saving model to best.model\n",
      "1s - loss: 0.7720 - acc: 0.6792 - val_loss: 0.7579 - val_acc: 0.6875\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75792 to 0.75566, saving model to best.model\n",
      "1s - loss: 0.7722 - acc: 0.6780 - val_loss: 0.7557 - val_acc: 0.6863\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75566 to 0.75235, saving model to best.model\n",
      "1s - loss: 0.7714 - acc: 0.6796 - val_loss: 0.7523 - val_acc: 0.6882\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75235 to 0.75199, saving model to best.model\n",
      "1s - loss: 0.7689 - acc: 0.6784 - val_loss: 0.7520 - val_acc: 0.6871\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75199 to 0.74790, saving model to best.model\n",
      "1s - loss: 0.7660 - acc: 0.6818 - val_loss: 0.7479 - val_acc: 0.6896\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74790 to 0.74498, saving model to best.model\n",
      "1s - loss: 0.7639 - acc: 0.6816 - val_loss: 0.7450 - val_acc: 0.6922\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74498 to 0.74354, saving model to best.model\n",
      "1s - loss: 0.7640 - acc: 0.6810 - val_loss: 0.7435 - val_acc: 0.6944\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74354 to 0.73861, saving model to best.model\n",
      "1s - loss: 0.7604 - acc: 0.6840 - val_loss: 0.7386 - val_acc: 0.6958\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73861 to 0.73762, saving model to best.model\n",
      "1s - loss: 0.7595 - acc: 0.6854 - val_loss: 0.7376 - val_acc: 0.6899\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73762 to 0.73355, saving model to best.model\n",
      "1s - loss: 0.7587 - acc: 0.6848 - val_loss: 0.7336 - val_acc: 0.6986\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73355 to 0.73189, saving model to best.model\n",
      "1s - loss: 0.7562 - acc: 0.6871 - val_loss: 0.7319 - val_acc: 0.6992\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73189 to 0.72944, saving model to best.model\n",
      "1s - loss: 0.7536 - acc: 0.6859 - val_loss: 0.7294 - val_acc: 0.6996\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "1s - loss: 0.7534 - acc: 0.6871 - val_loss: 0.7304 - val_acc: 0.6971\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72944 to 0.72229, saving model to best.model\n",
      "1s - loss: 0.7490 - acc: 0.6874 - val_loss: 0.7223 - val_acc: 0.7083\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72229 to 0.72180, saving model to best.model\n",
      "1s - loss: 0.7472 - acc: 0.6905 - val_loss: 0.7218 - val_acc: 0.7071\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.7479 - acc: 0.6903 - val_loss: 0.7219 - val_acc: 0.7123\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72180 to 0.71839, saving model to best.model\n",
      "1s - loss: 0.7479 - acc: 0.6892 - val_loss: 0.7184 - val_acc: 0.7066\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71839 to 0.71638, saving model to best.model\n",
      "1s - loss: 0.7432 - acc: 0.6913 - val_loss: 0.7164 - val_acc: 0.7034\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71638 to 0.71103, saving model to best.model\n",
      "1s - loss: 0.7424 - acc: 0.6915 - val_loss: 0.7110 - val_acc: 0.7162\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7419 - acc: 0.6946 - val_loss: 0.7133 - val_acc: 0.7074\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71103 to 0.70858, saving model to best.model\n",
      "1s - loss: 0.7397 - acc: 0.6965 - val_loss: 0.7086 - val_acc: 0.7131\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.7383 - acc: 0.6948 - val_loss: 0.7093 - val_acc: 0.7061\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70858 to 0.70422, saving model to best.model\n",
      "0s - loss: 0.7372 - acc: 0.6963 - val_loss: 0.7042 - val_acc: 0.7115\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70422 to 0.70152, saving model to best.model\n",
      "1s - loss: 0.7348 - acc: 0.6949 - val_loss: 0.7015 - val_acc: 0.7152\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.7344 - acc: 0.6974 - val_loss: 0.7022 - val_acc: 0.7112\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70152 to 0.69930, saving model to best.model\n",
      "1s - loss: 0.7325 - acc: 0.6975 - val_loss: 0.6993 - val_acc: 0.7149\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69930 to 0.69339, saving model to best.model\n",
      "1s - loss: 0.7270 - acc: 0.6993 - val_loss: 0.6934 - val_acc: 0.7192\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7268 - acc: 0.7031 - val_loss: 0.6966 - val_acc: 0.7153\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7275 - acc: 0.7023 - val_loss: 0.6942 - val_acc: 0.7180\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69339 to 0.69329, saving model to best.model\n",
      "1s - loss: 0.7266 - acc: 0.7030 - val_loss: 0.6933 - val_acc: 0.7174\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69329 to 0.68810, saving model to best.model\n",
      "1s - loss: 0.7243 - acc: 0.7017 - val_loss: 0.6881 - val_acc: 0.7197\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.68810 to 0.68679, saving model to best.model\n",
      "1s - loss: 0.7231 - acc: 0.7043 - val_loss: 0.6868 - val_acc: 0.7231\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68679 to 0.68474, saving model to best.model\n",
      "1s - loss: 0.7222 - acc: 0.7068 - val_loss: 0.6847 - val_acc: 0.7218\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68474 to 0.68368, saving model to best.model\n",
      "1s - loss: 0.7204 - acc: 0.7052 - val_loss: 0.6837 - val_acc: 0.7238\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7188 - acc: 0.7052 - val_loss: 0.6845 - val_acc: 0.7180\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68368 to 0.67830, saving model to best.model\n",
      "1s - loss: 0.7183 - acc: 0.7063 - val_loss: 0.6783 - val_acc: 0.7248\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "1s - loss: 0.7189 - acc: 0.7061 - val_loss: 0.6788 - val_acc: 0.7266\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67830 to 0.67748, saving model to best.model\n",
      "0s - loss: 0.7154 - acc: 0.7077 - val_loss: 0.6775 - val_acc: 0.7220\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67748 to 0.67735, saving model to best.model\n",
      "1s - loss: 0.7158 - acc: 0.7080 - val_loss: 0.6774 - val_acc: 0.7246\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67735 to 0.67364, saving model to best.model\n",
      "1s - loss: 0.7141 - acc: 0.7096 - val_loss: 0.6736 - val_acc: 0.7262\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7125 - acc: 0.7087 - val_loss: 0.6758 - val_acc: 0.7206\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67364 to 0.67277, saving model to best.model\n",
      "1s - loss: 0.7116 - acc: 0.7095 - val_loss: 0.6728 - val_acc: 0.7219\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67277 to 0.67013, saving model to best.model\n",
      "1s - loss: 0.7097 - acc: 0.7087 - val_loss: 0.6701 - val_acc: 0.7279\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.7078 - acc: 0.7107 - val_loss: 0.6715 - val_acc: 0.7292\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67013 to 0.66538, saving model to best.model\n",
      "1s - loss: 0.7077 - acc: 0.7122 - val_loss: 0.6654 - val_acc: 0.7316\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7066 - acc: 0.7131 - val_loss: 0.6671 - val_acc: 0.7279\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66538 to 0.66454, saving model to best.model\n",
      "1s - loss: 0.7035 - acc: 0.7133 - val_loss: 0.6645 - val_acc: 0.7276\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66454 to 0.66300, saving model to best.model\n",
      "1s - loss: 0.7051 - acc: 0.7120 - val_loss: 0.6630 - val_acc: 0.7337\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66300 to 0.66253, saving model to best.model\n",
      "1s - loss: 0.7044 - acc: 0.7117 - val_loss: 0.6625 - val_acc: 0.7314\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66253 to 0.66216, saving model to best.model\n",
      "1s - loss: 0.7049 - acc: 0.7117 - val_loss: 0.6622 - val_acc: 0.7316\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66216 to 0.65815, saving model to best.model\n",
      "1s - loss: 0.7023 - acc: 0.7147 - val_loss: 0.6581 - val_acc: 0.7358\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7034 - acc: 0.7134 - val_loss: 0.6616 - val_acc: 0.7331\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65815 to 0.65435, saving model to best.model\n",
      "0s - loss: 0.6987 - acc: 0.7127 - val_loss: 0.6544 - val_acc: 0.7367\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.7007 - acc: 0.7156 - val_loss: 0.6586 - val_acc: 0.7364\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.7003 - acc: 0.7149 - val_loss: 0.6597 - val_acc: 0.7337\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65435 to 0.65225, saving model to best.model\n",
      "1s - loss: 0.6959 - acc: 0.7143 - val_loss: 0.6522 - val_acc: 0.7389\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6982 - acc: 0.7154 - val_loss: 0.6558 - val_acc: 0.7300\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.65225 to 0.65178, saving model to best.model\n",
      "1s - loss: 0.6928 - acc: 0.7186 - val_loss: 0.6518 - val_acc: 0.7369\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65178 to 0.65048, saving model to best.model\n",
      "1s - loss: 0.6967 - acc: 0.7157 - val_loss: 0.6505 - val_acc: 0.7329\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6955 - acc: 0.7149 - val_loss: 0.6528 - val_acc: 0.7320\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65048 to 0.64416, saving model to best.model\n",
      "1s - loss: 0.6913 - acc: 0.7181 - val_loss: 0.6442 - val_acc: 0.7411\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6914 - acc: 0.7188 - val_loss: 0.6484 - val_acc: 0.7398\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.6908 - acc: 0.7171 - val_loss: 0.6475 - val_acc: 0.7386\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6902 - acc: 0.7185 - val_loss: 0.6456 - val_acc: 0.7409\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6880 - acc: 0.7184 - val_loss: 0.6461 - val_acc: 0.7371\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64416 to 0.64122, saving model to best.model\n",
      "1s - loss: 0.6892 - acc: 0.7174 - val_loss: 0.6412 - val_acc: 0.7436\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6867 - acc: 0.7215 - val_loss: 0.6432 - val_acc: 0.7372\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6903 - acc: 0.7174 - val_loss: 0.6432 - val_acc: 0.7423\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6870 - acc: 0.7203 - val_loss: 0.6433 - val_acc: 0.7424\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6849 - acc: 0.7213 - val_loss: 0.6419 - val_acc: 0.7399\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64122 to 0.63908, saving model to best.model\n",
      "1s - loss: 0.6865 - acc: 0.7186 - val_loss: 0.6391 - val_acc: 0.7454\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6843 - acc: 0.7232 - val_loss: 0.6426 - val_acc: 0.7440\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.63908 to 0.63773, saving model to best.model\n",
      "1s - loss: 0.6852 - acc: 0.7207 - val_loss: 0.6377 - val_acc: 0.7441\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.63773 to 0.63601, saving model to best.model\n",
      "1s - loss: 0.6861 - acc: 0.7227 - val_loss: 0.6360 - val_acc: 0.7459\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6864 - acc: 0.7194 - val_loss: 0.6398 - val_acc: 0.7433\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.63601 to 0.63448, saving model to best.model\n",
      "1s - loss: 0.6845 - acc: 0.7213 - val_loss: 0.6345 - val_acc: 0.7456\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6804 - acc: 0.7242 - val_loss: 0.6366 - val_acc: 0.7436\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6796 - acc: 0.7246 - val_loss: 0.6388 - val_acc: 0.7391\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6800 - acc: 0.7213 - val_loss: 0.6350 - val_acc: 0.7418\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63448 to 0.63219, saving model to best.model\n",
      "1s - loss: 0.6814 - acc: 0.7229 - val_loss: 0.6322 - val_acc: 0.7477\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6805 - acc: 0.7226 - val_loss: 0.6322 - val_acc: 0.7482\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.63219 to 0.63064, saving model to best.model\n",
      "1s - loss: 0.6781 - acc: 0.7254 - val_loss: 0.6306 - val_acc: 0.7463\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6797 - acc: 0.7222 - val_loss: 0.6308 - val_acc: 0.7486\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6758 - acc: 0.7258 - val_loss: 0.6326 - val_acc: 0.7474\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63064 to 0.62653, saving model to best.model\n",
      "1s - loss: 0.6792 - acc: 0.7224 - val_loss: 0.6265 - val_acc: 0.7491\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6726 - acc: 0.7259 - val_loss: 0.6271 - val_acc: 0.7492\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6738 - acc: 0.7255 - val_loss: 0.6291 - val_acc: 0.7443\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62653 to 0.62574, saving model to best.model\n",
      "1s - loss: 0.6723 - acc: 0.7266 - val_loss: 0.6257 - val_acc: 0.7502\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.62574 to 0.62405, saving model to best.model\n",
      "1s - loss: 0.6753 - acc: 0.7268 - val_loss: 0.6241 - val_acc: 0.7504\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6714 - acc: 0.7275 - val_loss: 0.6272 - val_acc: 0.7433\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62405 to 0.62383, saving model to best.model\n",
      "1s - loss: 0.6729 - acc: 0.7271 - val_loss: 0.6238 - val_acc: 0.7501\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6729 - acc: 0.7280 - val_loss: 0.6254 - val_acc: 0.7471\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62383 to 0.62167, saving model to best.model\n",
      "1s - loss: 0.6723 - acc: 0.7289 - val_loss: 0.6217 - val_acc: 0.7496\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6681 - acc: 0.7278 - val_loss: 0.6226 - val_acc: 0.7478\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62167 to 0.62136, saving model to best.model\n",
      "1s - loss: 0.6707 - acc: 0.7257 - val_loss: 0.6214 - val_acc: 0.7507\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6694 - acc: 0.7282 - val_loss: 0.6231 - val_acc: 0.7457\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62136 to 0.62097, saving model to best.model\n",
      "1s - loss: 0.6686 - acc: 0.7288 - val_loss: 0.6210 - val_acc: 0.7478\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62097 to 0.61707, saving model to best.model\n",
      "1s - loss: 0.6675 - acc: 0.7289 - val_loss: 0.6171 - val_acc: 0.7523\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.61707 to 0.61575, saving model to best.model\n",
      "1s - loss: 0.6688 - acc: 0.7285 - val_loss: 0.6158 - val_acc: 0.7532\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6655 - acc: 0.7315 - val_loss: 0.6185 - val_acc: 0.7520\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6662 - acc: 0.7281 - val_loss: 0.6176 - val_acc: 0.7493\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6633 - acc: 0.7315 - val_loss: 0.6166 - val_acc: 0.7543\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.6667 - acc: 0.7290 - val_loss: 0.6182 - val_acc: 0.7468\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61575 to 0.61517, saving model to best.model\n",
      "1s - loss: 0.6659 - acc: 0.7296 - val_loss: 0.6152 - val_acc: 0.7502\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6657 - acc: 0.7292 - val_loss: 0.6167 - val_acc: 0.7488\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6592 - acc: 0.7300 - val_loss: 0.6157 - val_acc: 0.7513\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6632 - acc: 0.7297 - val_loss: 0.6166 - val_acc: 0.7491\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.61517 to 0.61420, saving model to best.model\n",
      "1s - loss: 0.6613 - acc: 0.7319 - val_loss: 0.6142 - val_acc: 0.7492\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61420 to 0.60883, saving model to best.model\n",
      "1s - loss: 0.6579 - acc: 0.7328 - val_loss: 0.6088 - val_acc: 0.7542\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6623 - acc: 0.7312 - val_loss: 0.6125 - val_acc: 0.7519\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6602 - acc: 0.7322 - val_loss: 0.6132 - val_acc: 0.7493\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6585 - acc: 0.7311 - val_loss: 0.6093 - val_acc: 0.7536\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6595 - acc: 0.7315 - val_loss: 0.6089 - val_acc: 0.7526\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6579 - acc: 0.7335 - val_loss: 0.6105 - val_acc: 0.7528\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6595 - acc: 0.7331 - val_loss: 0.6118 - val_acc: 0.7501\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6573 - acc: 0.7323 - val_loss: 0.6106 - val_acc: 0.7519\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.60883 to 0.60678, saving model to best.model\n",
      "1s - loss: 0.6577 - acc: 0.7333 - val_loss: 0.6068 - val_acc: 0.7540\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6589 - acc: 0.7325 - val_loss: 0.6083 - val_acc: 0.7543\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "1s - loss: 0.6562 - acc: 0.7348 - val_loss: 0.6080 - val_acc: 0.7529\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.60678 to 0.60530, saving model to best.model\n",
      "0s - loss: 0.6542 - acc: 0.7345 - val_loss: 0.6053 - val_acc: 0.7549\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6567 - acc: 0.7334 - val_loss: 0.6053 - val_acc: 0.7544\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.60530 to 0.60372, saving model to best.model\n",
      "1s - loss: 0.6550 - acc: 0.7358 - val_loss: 0.6037 - val_acc: 0.7575\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6564 - acc: 0.7328 - val_loss: 0.6043 - val_acc: 0.7567\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6513 - acc: 0.7351 - val_loss: 0.6049 - val_acc: 0.7523\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6560 - acc: 0.7338 - val_loss: 0.6060 - val_acc: 0.7526\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6520 - acc: 0.7339 - val_loss: 0.6037 - val_acc: 0.7571\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6538 - acc: 0.7343 - val_loss: 0.6042 - val_acc: 0.7559\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60372 to 0.59965, saving model to best.model\n",
      "1s - loss: 0.6521 - acc: 0.7342 - val_loss: 0.5997 - val_acc: 0.7595\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6527 - acc: 0.7357 - val_loss: 0.6013 - val_acc: 0.7552\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6505 - acc: 0.7383 - val_loss: 0.6014 - val_acc: 0.7532\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6497 - acc: 0.7350 - val_loss: 0.6003 - val_acc: 0.7539\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.59965 to 0.59724, saving model to best.model\n",
      "0s - loss: 0.6495 - acc: 0.7353 - val_loss: 0.5972 - val_acc: 0.7580\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6496 - acc: 0.7354 - val_loss: 0.5995 - val_acc: 0.7571\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59724 to 0.59661, saving model to best.model\n",
      "0s - loss: 0.6512 - acc: 0.7369 - val_loss: 0.5966 - val_acc: 0.7595\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6516 - acc: 0.7357 - val_loss: 0.5988 - val_acc: 0.7597\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6505 - acc: 0.7357 - val_loss: 0.5971 - val_acc: 0.7600\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6468 - acc: 0.7373 - val_loss: 0.5987 - val_acc: 0.7582\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6481 - acc: 0.7382 - val_loss: 0.5971 - val_acc: 0.7580\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6476 - acc: 0.7372 - val_loss: 0.5985 - val_acc: 0.7584\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.59661 to 0.59360, saving model to best.model\n",
      "0s - loss: 0.6472 - acc: 0.7357 - val_loss: 0.5936 - val_acc: 0.7610\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6445 - acc: 0.7393 - val_loss: 0.5958 - val_acc: 0.7587\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6443 - acc: 0.7395 - val_loss: 0.5977 - val_acc: 0.7560\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6475 - acc: 0.7376 - val_loss: 0.5944 - val_acc: 0.7596\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6451 - acc: 0.7394 - val_loss: 0.5942 - val_acc: 0.7589\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6437 - acc: 0.7391 - val_loss: 0.5949 - val_acc: 0.7591\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6454 - acc: 0.7384 - val_loss: 0.5941 - val_acc: 0.7576\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6459 - acc: 0.7369 - val_loss: 0.5958 - val_acc: 0.7581\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84491, saving model to best.model\n",
      "0s - loss: 0.9446 - acc: 0.6144 - val_loss: 0.8449 - val_acc: 0.6614\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "1s - loss: 0.8642 - acc: 0.6565 - val_loss: 0.8452 - val_acc: 0.6614\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84491 to 0.84270, saving model to best.model\n",
      "1s - loss: 0.8544 - acc: 0.6572 - val_loss: 0.8427 - val_acc: 0.6614\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84270 to 0.84226, saving model to best.model\n",
      "0s - loss: 0.8507 - acc: 0.6572 - val_loss: 0.8423 - val_acc: 0.6614\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84226 to 0.83830, saving model to best.model\n",
      "0s - loss: 0.8479 - acc: 0.6572 - val_loss: 0.8383 - val_acc: 0.6614\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83830 to 0.83257, saving model to best.model\n",
      "0s - loss: 0.8435 - acc: 0.6572 - val_loss: 0.8326 - val_acc: 0.6614\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83257 to 0.82728, saving model to best.model\n",
      "0s - loss: 0.8357 - acc: 0.6573 - val_loss: 0.8273 - val_acc: 0.6614\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82728 to 0.82121, saving model to best.model\n",
      "0s - loss: 0.8343 - acc: 0.6572 - val_loss: 0.8212 - val_acc: 0.6614\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82121 to 0.82003, saving model to best.model\n",
      "0s - loss: 0.8296 - acc: 0.6571 - val_loss: 0.8200 - val_acc: 0.6614\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.8305 - acc: 0.6572 - val_loss: 0.8206 - val_acc: 0.6614\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.8268 - acc: 0.6570 - val_loss: 0.8201 - val_acc: 0.6614\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82003 to 0.81978, saving model to best.model\n",
      "0s - loss: 0.8270 - acc: 0.6571 - val_loss: 0.8198 - val_acc: 0.6614\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81978 to 0.81893, saving model to best.model\n",
      "1s - loss: 0.8255 - acc: 0.6568 - val_loss: 0.8189 - val_acc: 0.6614\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81893 to 0.81829, saving model to best.model\n",
      "0s - loss: 0.8244 - acc: 0.6570 - val_loss: 0.8183 - val_acc: 0.6614\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.8244 - acc: 0.6573 - val_loss: 0.8184 - val_acc: 0.6614\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.8238 - acc: 0.6572 - val_loss: 0.8190 - val_acc: 0.6614\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81829 to 0.81800, saving model to best.model\n",
      "0s - loss: 0.8232 - acc: 0.6575 - val_loss: 0.8180 - val_acc: 0.6614\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81800 to 0.81748, saving model to best.model\n",
      "1s - loss: 0.8225 - acc: 0.6570 - val_loss: 0.8175 - val_acc: 0.6614\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "1s - loss: 0.8215 - acc: 0.6574 - val_loss: 0.8199 - val_acc: 0.6614\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81748 to 0.81648, saving model to best.model\n",
      "1s - loss: 0.8203 - acc: 0.6572 - val_loss: 0.8165 - val_acc: 0.6614\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81648 to 0.81561, saving model to best.model\n",
      "1s - loss: 0.8208 - acc: 0.6569 - val_loss: 0.8156 - val_acc: 0.6614\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81561 to 0.81502, saving model to best.model\n",
      "1s - loss: 0.8206 - acc: 0.6570 - val_loss: 0.8150 - val_acc: 0.6614\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.8189 - acc: 0.6564 - val_loss: 0.8153 - val_acc: 0.6614\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81502 to 0.81351, saving model to best.model\n",
      "1s - loss: 0.8181 - acc: 0.6586 - val_loss: 0.8135 - val_acc: 0.6614\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.8175 - acc: 0.6579 - val_loss: 0.8144 - val_acc: 0.6614\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81351 to 0.81210, saving model to best.model\n",
      "0s - loss: 0.8163 - acc: 0.6579 - val_loss: 0.8121 - val_acc: 0.6619\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81210 to 0.81146, saving model to best.model\n",
      "0s - loss: 0.8160 - acc: 0.6576 - val_loss: 0.8115 - val_acc: 0.6621\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81146 to 0.80926, saving model to best.model\n",
      "0s - loss: 0.8147 - acc: 0.6590 - val_loss: 0.8093 - val_acc: 0.6628\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80926 to 0.80803, saving model to best.model\n",
      "0s - loss: 0.8126 - acc: 0.6581 - val_loss: 0.8080 - val_acc: 0.6635\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80803 to 0.80707, saving model to best.model\n",
      "0s - loss: 0.8125 - acc: 0.6599 - val_loss: 0.8071 - val_acc: 0.6636\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80707 to 0.80584, saving model to best.model\n",
      "0s - loss: 0.8116 - acc: 0.6601 - val_loss: 0.8058 - val_acc: 0.6650\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80584 to 0.80481, saving model to best.model\n",
      "0s - loss: 0.8095 - acc: 0.6598 - val_loss: 0.8048 - val_acc: 0.6719\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80481 to 0.80304, saving model to best.model\n",
      "0s - loss: 0.8087 - acc: 0.6606 - val_loss: 0.8030 - val_acc: 0.6653\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80304 to 0.80218, saving model to best.model\n",
      "0s - loss: 0.8082 - acc: 0.6606 - val_loss: 0.8022 - val_acc: 0.6671\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80218 to 0.80091, saving model to best.model\n",
      "0s - loss: 0.8058 - acc: 0.6642 - val_loss: 0.8009 - val_acc: 0.6687\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80091 to 0.79839, saving model to best.model\n",
      "0s - loss: 0.8041 - acc: 0.6625 - val_loss: 0.7984 - val_acc: 0.6700\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79839 to 0.79719, saving model to best.model\n",
      "0s - loss: 0.8039 - acc: 0.6638 - val_loss: 0.7972 - val_acc: 0.6707\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79719 to 0.79461, saving model to best.model\n",
      "0s - loss: 0.8013 - acc: 0.6641 - val_loss: 0.7946 - val_acc: 0.6727\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79461 to 0.79385, saving model to best.model\n",
      "0s - loss: 0.8010 - acc: 0.6644 - val_loss: 0.7938 - val_acc: 0.6701\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79385 to 0.79009, saving model to best.model\n",
      "0s - loss: 0.7975 - acc: 0.6663 - val_loss: 0.7901 - val_acc: 0.6733\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79009 to 0.78913, saving model to best.model\n",
      "1s - loss: 0.7962 - acc: 0.6665 - val_loss: 0.7891 - val_acc: 0.6747\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78913 to 0.78773, saving model to best.model\n",
      "1s - loss: 0.7962 - acc: 0.6652 - val_loss: 0.7877 - val_acc: 0.6727\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78773 to 0.78436, saving model to best.model\n",
      "1s - loss: 0.7951 - acc: 0.6670 - val_loss: 0.7844 - val_acc: 0.6789\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78436 to 0.78168, saving model to best.model\n",
      "1s - loss: 0.7909 - acc: 0.6700 - val_loss: 0.7817 - val_acc: 0.6787\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "1s - loss: 0.7904 - acc: 0.6710 - val_loss: 0.7821 - val_acc: 0.6780\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78168 to 0.78133, saving model to best.model\n",
      "1s - loss: 0.7885 - acc: 0.6716 - val_loss: 0.7813 - val_acc: 0.6787\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.78133 to 0.77814, saving model to best.model\n",
      "1s - loss: 0.7870 - acc: 0.6707 - val_loss: 0.7781 - val_acc: 0.6769\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77814 to 0.77591, saving model to best.model\n",
      "1s - loss: 0.7857 - acc: 0.6736 - val_loss: 0.7759 - val_acc: 0.6794\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77591 to 0.77341, saving model to best.model\n",
      "1s - loss: 0.7832 - acc: 0.6720 - val_loss: 0.7734 - val_acc: 0.6803\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77341 to 0.77137, saving model to best.model\n",
      "1s - loss: 0.7824 - acc: 0.6739 - val_loss: 0.7714 - val_acc: 0.6824\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77137 to 0.76757, saving model to best.model\n",
      "1s - loss: 0.7799 - acc: 0.6747 - val_loss: 0.7676 - val_acc: 0.6872\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76757 to 0.76744, saving model to best.model\n",
      "1s - loss: 0.7789 - acc: 0.6759 - val_loss: 0.7674 - val_acc: 0.6850\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76744 to 0.76727, saving model to best.model\n",
      "1s - loss: 0.7777 - acc: 0.6749 - val_loss: 0.7673 - val_acc: 0.6850\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76727 to 0.76199, saving model to best.model\n",
      "1s - loss: 0.7760 - acc: 0.6759 - val_loss: 0.7620 - val_acc: 0.6888\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76199 to 0.76074, saving model to best.model\n",
      "0s - loss: 0.7733 - acc: 0.6791 - val_loss: 0.7607 - val_acc: 0.6903\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76074 to 0.75581, saving model to best.model\n",
      "0s - loss: 0.7718 - acc: 0.6787 - val_loss: 0.7558 - val_acc: 0.6943\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75581 to 0.75428, saving model to best.model\n",
      "0s - loss: 0.7722 - acc: 0.6801 - val_loss: 0.7543 - val_acc: 0.6924\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75428 to 0.75235, saving model to best.model\n",
      "0s - loss: 0.7677 - acc: 0.6802 - val_loss: 0.7523 - val_acc: 0.6930\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75235 to 0.75071, saving model to best.model\n",
      "0s - loss: 0.7670 - acc: 0.6810 - val_loss: 0.7507 - val_acc: 0.6925\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.75071 to 0.74678, saving model to best.model\n",
      "1s - loss: 0.7645 - acc: 0.6828 - val_loss: 0.7468 - val_acc: 0.6966\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74678 to 0.74492, saving model to best.model\n",
      "0s - loss: 0.7614 - acc: 0.6842 - val_loss: 0.7449 - val_acc: 0.6950\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74492 to 0.74184, saving model to best.model\n",
      "0s - loss: 0.7636 - acc: 0.6841 - val_loss: 0.7418 - val_acc: 0.6989\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74184 to 0.73870, saving model to best.model\n",
      "0s - loss: 0.7617 - acc: 0.6836 - val_loss: 0.7387 - val_acc: 0.6989\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73870 to 0.73675, saving model to best.model\n",
      "0s - loss: 0.7595 - acc: 0.6846 - val_loss: 0.7368 - val_acc: 0.6993\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73675 to 0.73010, saving model to best.model\n",
      "1s - loss: 0.7539 - acc: 0.6868 - val_loss: 0.7301 - val_acc: 0.7014\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73010 to 0.72998, saving model to best.model\n",
      "1s - loss: 0.7528 - acc: 0.6897 - val_loss: 0.7300 - val_acc: 0.7021\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72998 to 0.72960, saving model to best.model\n",
      "1s - loss: 0.7493 - acc: 0.6889 - val_loss: 0.7296 - val_acc: 0.7019\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72960 to 0.72583, saving model to best.model\n",
      "1s - loss: 0.7499 - acc: 0.6881 - val_loss: 0.7258 - val_acc: 0.7029\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72583 to 0.72341, saving model to best.model\n",
      "1s - loss: 0.7467 - acc: 0.6908 - val_loss: 0.7234 - val_acc: 0.7035\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72341 to 0.72154, saving model to best.model\n",
      "1s - loss: 0.7482 - acc: 0.6912 - val_loss: 0.7215 - val_acc: 0.7060\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72154 to 0.71684, saving model to best.model\n",
      "1s - loss: 0.7445 - acc: 0.6902 - val_loss: 0.7168 - val_acc: 0.7085\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71684 to 0.71398, saving model to best.model\n",
      "1s - loss: 0.7394 - acc: 0.6958 - val_loss: 0.7140 - val_acc: 0.7109\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71398 to 0.71354, saving model to best.model\n",
      "1s - loss: 0.7381 - acc: 0.6940 - val_loss: 0.7135 - val_acc: 0.7093\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71354 to 0.70996, saving model to best.model\n",
      "1s - loss: 0.7392 - acc: 0.6936 - val_loss: 0.7100 - val_acc: 0.7117\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70996 to 0.70982, saving model to best.model\n",
      "1s - loss: 0.7353 - acc: 0.6953 - val_loss: 0.7098 - val_acc: 0.7100\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70982 to 0.70834, saving model to best.model\n",
      "1s - loss: 0.7358 - acc: 0.6943 - val_loss: 0.7083 - val_acc: 0.7117\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70834 to 0.70699, saving model to best.model\n",
      "1s - loss: 0.7353 - acc: 0.6964 - val_loss: 0.7070 - val_acc: 0.7122\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.7331 - acc: 0.6971 - val_loss: 0.7071 - val_acc: 0.7115\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70699 to 0.69974, saving model to best.model\n",
      "1s - loss: 0.7314 - acc: 0.6974 - val_loss: 0.6997 - val_acc: 0.7153\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69974 to 0.69684, saving model to best.model\n",
      "1s - loss: 0.7290 - acc: 0.6986 - val_loss: 0.6968 - val_acc: 0.7166\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7272 - acc: 0.7006 - val_loss: 0.6975 - val_acc: 0.7158\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69684 to 0.69474, saving model to best.model\n",
      "1s - loss: 0.7233 - acc: 0.7032 - val_loss: 0.6947 - val_acc: 0.7187\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "1s - loss: 0.7258 - acc: 0.6986 - val_loss: 0.6948 - val_acc: 0.7160\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69474 to 0.69340, saving model to best.model\n",
      "1s - loss: 0.7251 - acc: 0.7010 - val_loss: 0.6934 - val_acc: 0.7178\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69340 to 0.69231, saving model to best.model\n",
      "1s - loss: 0.7233 - acc: 0.7007 - val_loss: 0.6923 - val_acc: 0.7203\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69231 to 0.68826, saving model to best.model\n",
      "1s - loss: 0.7213 - acc: 0.7027 - val_loss: 0.6883 - val_acc: 0.7232\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "1s - loss: 0.7195 - acc: 0.7027 - val_loss: 0.6894 - val_acc: 0.7197\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68826 to 0.68549, saving model to best.model\n",
      "1s - loss: 0.7168 - acc: 0.7055 - val_loss: 0.6855 - val_acc: 0.7237\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.7176 - acc: 0.7028 - val_loss: 0.6902 - val_acc: 0.7230\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68549 to 0.68515, saving model to best.model\n",
      "1s - loss: 0.7174 - acc: 0.7043 - val_loss: 0.6852 - val_acc: 0.7238\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.7162 - acc: 0.7058 - val_loss: 0.6860 - val_acc: 0.7222\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68515 to 0.68444, saving model to best.model\n",
      "1s - loss: 0.7162 - acc: 0.7051 - val_loss: 0.6844 - val_acc: 0.7217\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68444 to 0.68005, saving model to best.model\n",
      "1s - loss: 0.7141 - acc: 0.7075 - val_loss: 0.6800 - val_acc: 0.7240\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68005 to 0.67776, saving model to best.model\n",
      "1s - loss: 0.7120 - acc: 0.7067 - val_loss: 0.6778 - val_acc: 0.7256\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7118 - acc: 0.7080 - val_loss: 0.6811 - val_acc: 0.7227\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67776 to 0.67693, saving model to best.model\n",
      "1s - loss: 0.7112 - acc: 0.7071 - val_loss: 0.6769 - val_acc: 0.7247\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.67693 to 0.67565, saving model to best.model\n",
      "1s - loss: 0.7069 - acc: 0.7096 - val_loss: 0.6757 - val_acc: 0.7268\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.67565 to 0.67305, saving model to best.model\n",
      "1s - loss: 0.7066 - acc: 0.7082 - val_loss: 0.6731 - val_acc: 0.7262\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67305 to 0.67220, saving model to best.model\n",
      "1s - loss: 0.7068 - acc: 0.7086 - val_loss: 0.6722 - val_acc: 0.7295\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7067 - acc: 0.7108 - val_loss: 0.6731 - val_acc: 0.7292\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7040 - acc: 0.7109 - val_loss: 0.6726 - val_acc: 0.7261\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67220 to 0.66563, saving model to best.model\n",
      "1s - loss: 0.7011 - acc: 0.7127 - val_loss: 0.6656 - val_acc: 0.7343\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7040 - acc: 0.7092 - val_loss: 0.6662 - val_acc: 0.7308\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.6996 - acc: 0.7113 - val_loss: 0.6667 - val_acc: 0.7304\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7004 - acc: 0.7116 - val_loss: 0.6686 - val_acc: 0.7296\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.6984 - acc: 0.7149 - val_loss: 0.6658 - val_acc: 0.7324\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66563 to 0.66314, saving model to best.model\n",
      "1s - loss: 0.6960 - acc: 0.7145 - val_loss: 0.6631 - val_acc: 0.7350\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.6981 - acc: 0.7169 - val_loss: 0.6636 - val_acc: 0.7321\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66314 to 0.65900, saving model to best.model\n",
      "1s - loss: 0.6974 - acc: 0.7118 - val_loss: 0.6590 - val_acc: 0.7355\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6952 - acc: 0.7145 - val_loss: 0.6621 - val_acc: 0.7369\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6949 - acc: 0.7166 - val_loss: 0.6604 - val_acc: 0.7335\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65900 to 0.65753, saving model to best.model\n",
      "1s - loss: 0.6925 - acc: 0.7177 - val_loss: 0.6575 - val_acc: 0.7352\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65753 to 0.65727, saving model to best.model\n",
      "0s - loss: 0.6931 - acc: 0.7140 - val_loss: 0.6573 - val_acc: 0.7365\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65727 to 0.65655, saving model to best.model\n",
      "0s - loss: 0.6934 - acc: 0.7159 - val_loss: 0.6565 - val_acc: 0.7376\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6898 - acc: 0.7186 - val_loss: 0.6569 - val_acc: 0.7368\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65655 to 0.65350, saving model to best.model\n",
      "0s - loss: 0.6902 - acc: 0.7184 - val_loss: 0.6535 - val_acc: 0.7390\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.6869 - acc: 0.7212 - val_loss: 0.6544 - val_acc: 0.7375\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6881 - acc: 0.7185 - val_loss: 0.6541 - val_acc: 0.7378\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.65350 to 0.65250, saving model to best.model\n",
      "1s - loss: 0.6864 - acc: 0.7187 - val_loss: 0.6525 - val_acc: 0.7382\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65250 to 0.65190, saving model to best.model\n",
      "1s - loss: 0.6854 - acc: 0.7196 - val_loss: 0.6519 - val_acc: 0.7375\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65190 to 0.65073, saving model to best.model\n",
      "1s - loss: 0.6833 - acc: 0.7196 - val_loss: 0.6507 - val_acc: 0.7378\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.65073 to 0.64917, saving model to best.model\n",
      "1s - loss: 0.6830 - acc: 0.7215 - val_loss: 0.6492 - val_acc: 0.7386\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64917 to 0.64775, saving model to best.model\n",
      "1s - loss: 0.6826 - acc: 0.7224 - val_loss: 0.6478 - val_acc: 0.7386\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64775 to 0.64341, saving model to best.model\n",
      "1s - loss: 0.6808 - acc: 0.7217 - val_loss: 0.6434 - val_acc: 0.7412\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6842 - acc: 0.7186 - val_loss: 0.6471 - val_acc: 0.7388\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6819 - acc: 0.7221 - val_loss: 0.6455 - val_acc: 0.7412\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6770 - acc: 0.7215 - val_loss: 0.6471 - val_acc: 0.7395\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6815 - acc: 0.7204 - val_loss: 0.6469 - val_acc: 0.7404\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6774 - acc: 0.7214 - val_loss: 0.6439 - val_acc: 0.7409\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6761 - acc: 0.7221 - val_loss: 0.6437 - val_acc: 0.7413\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64341 to 0.64335, saving model to best.model\n",
      "1s - loss: 0.6776 - acc: 0.7228 - val_loss: 0.6433 - val_acc: 0.7406\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.64335 to 0.64044, saving model to best.model\n",
      "1s - loss: 0.6750 - acc: 0.7237 - val_loss: 0.6404 - val_acc: 0.7417\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6769 - acc: 0.7221 - val_loss: 0.6419 - val_acc: 0.7419\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.64044 to 0.63878, saving model to best.model\n",
      "1s - loss: 0.6775 - acc: 0.7203 - val_loss: 0.6388 - val_acc: 0.7413\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6778 - acc: 0.7222 - val_loss: 0.6390 - val_acc: 0.7424\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6735 - acc: 0.7241 - val_loss: 0.6389 - val_acc: 0.7433\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6755 - acc: 0.7228 - val_loss: 0.6389 - val_acc: 0.7433\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63878 to 0.63858, saving model to best.model\n",
      "1s - loss: 0.6747 - acc: 0.7232 - val_loss: 0.6386 - val_acc: 0.7432\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.63858 to 0.63634, saving model to best.model\n",
      "1s - loss: 0.6722 - acc: 0.7245 - val_loss: 0.6363 - val_acc: 0.7446\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63634 to 0.63394, saving model to best.model\n",
      "1s - loss: 0.6714 - acc: 0.7254 - val_loss: 0.6339 - val_acc: 0.7451\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63394 to 0.63279, saving model to best.model\n",
      "1s - loss: 0.6694 - acc: 0.7273 - val_loss: 0.6328 - val_acc: 0.7464\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6701 - acc: 0.7256 - val_loss: 0.6333 - val_acc: 0.7458\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6732 - acc: 0.7238 - val_loss: 0.6345 - val_acc: 0.7446\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63279 to 0.63221, saving model to best.model\n",
      "1s - loss: 0.6716 - acc: 0.7252 - val_loss: 0.6322 - val_acc: 0.7464\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63221 to 0.63123, saving model to best.model\n",
      "1s - loss: 0.6684 - acc: 0.7289 - val_loss: 0.6312 - val_acc: 0.7465\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6692 - acc: 0.7276 - val_loss: 0.6319 - val_acc: 0.7451\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63123 to 0.62932, saving model to best.model\n",
      "1s - loss: 0.6621 - acc: 0.7293 - val_loss: 0.6293 - val_acc: 0.7450\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6649 - acc: 0.7265 - val_loss: 0.6294 - val_acc: 0.7465\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6671 - acc: 0.7270 - val_loss: 0.6303 - val_acc: 0.7465\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6677 - acc: 0.7265 - val_loss: 0.6312 - val_acc: 0.7457\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6639 - acc: 0.7277 - val_loss: 0.6310 - val_acc: 0.7471\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62932 to 0.62582, saving model to best.model\n",
      "1s - loss: 0.6637 - acc: 0.7261 - val_loss: 0.6258 - val_acc: 0.7480\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6620 - acc: 0.7299 - val_loss: 0.6263 - val_acc: 0.7487\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6591 - acc: 0.7280 - val_loss: 0.6266 - val_acc: 0.7466\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6604 - acc: 0.7301 - val_loss: 0.6263 - val_acc: 0.7498\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62582 to 0.62366, saving model to best.model\n",
      "1s - loss: 0.6595 - acc: 0.7310 - val_loss: 0.6237 - val_acc: 0.7492\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6610 - acc: 0.7300 - val_loss: 0.6256 - val_acc: 0.7499\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6607 - acc: 0.7308 - val_loss: 0.6252 - val_acc: 0.7492\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6617 - acc: 0.7287 - val_loss: 0.6250 - val_acc: 0.7486\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6637 - acc: 0.7272 - val_loss: 0.6266 - val_acc: 0.7508\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62366 to 0.62193, saving model to best.model\n",
      "1s - loss: 0.6619 - acc: 0.7279 - val_loss: 0.6219 - val_acc: 0.7514\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62193 to 0.62175, saving model to best.model\n",
      "1s - loss: 0.6619 - acc: 0.7298 - val_loss: 0.6217 - val_acc: 0.7505\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62175 to 0.62139, saving model to best.model\n",
      "1s - loss: 0.6594 - acc: 0.7315 - val_loss: 0.6214 - val_acc: 0.7515\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.62139 to 0.61978, saving model to best.model\n",
      "1s - loss: 0.6584 - acc: 0.7293 - val_loss: 0.6198 - val_acc: 0.7515\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6602 - acc: 0.7306 - val_loss: 0.6224 - val_acc: 0.7501\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6543 - acc: 0.7334 - val_loss: 0.6208 - val_acc: 0.7528\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61978 to 0.61747, saving model to best.model\n",
      "1s - loss: 0.6507 - acc: 0.7340 - val_loss: 0.6175 - val_acc: 0.7519\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.61747 to 0.61689, saving model to best.model\n",
      "1s - loss: 0.6523 - acc: 0.7327 - val_loss: 0.6169 - val_acc: 0.7515\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6575 - acc: 0.7302 - val_loss: 0.6187 - val_acc: 0.7521\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6563 - acc: 0.7316 - val_loss: 0.6180 - val_acc: 0.7522\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6517 - acc: 0.7339 - val_loss: 0.6174 - val_acc: 0.7528\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61689 to 0.61507, saving model to best.model\n",
      "1s - loss: 0.6513 - acc: 0.7349 - val_loss: 0.6151 - val_acc: 0.7530\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6579 - acc: 0.7292 - val_loss: 0.6155 - val_acc: 0.7536\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6555 - acc: 0.7321 - val_loss: 0.6166 - val_acc: 0.7534\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61507 to 0.61480, saving model to best.model\n",
      "1s - loss: 0.6536 - acc: 0.7315 - val_loss: 0.6148 - val_acc: 0.7534\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6530 - acc: 0.7332 - val_loss: 0.6159 - val_acc: 0.7542\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61480 to 0.61043, saving model to best.model\n",
      "1s - loss: 0.6502 - acc: 0.7345 - val_loss: 0.6104 - val_acc: 0.7546\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6507 - acc: 0.7320 - val_loss: 0.6144 - val_acc: 0.7541\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6509 - acc: 0.7335 - val_loss: 0.6120 - val_acc: 0.7553\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6527 - acc: 0.7328 - val_loss: 0.6139 - val_acc: 0.7542\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6513 - acc: 0.7333 - val_loss: 0.6154 - val_acc: 0.7548\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6495 - acc: 0.7348 - val_loss: 0.6120 - val_acc: 0.7544\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6510 - acc: 0.7351 - val_loss: 0.6136 - val_acc: 0.7515\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6479 - acc: 0.7355 - val_loss: 0.6113 - val_acc: 0.7547\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61043 to 0.60955, saving model to best.model\n",
      "0s - loss: 0.6456 - acc: 0.7373 - val_loss: 0.6095 - val_acc: 0.7569\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6457 - acc: 0.7353 - val_loss: 0.6131 - val_acc: 0.7511\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.6476 - acc: 0.7359 - val_loss: 0.6102 - val_acc: 0.7555\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60955 to 0.60937, saving model to best.model\n",
      "0s - loss: 0.6484 - acc: 0.7337 - val_loss: 0.6094 - val_acc: 0.7542\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6456 - acc: 0.7358 - val_loss: 0.6110 - val_acc: 0.7547\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60937 to 0.60579, saving model to best.model\n",
      "0s - loss: 0.6436 - acc: 0.7367 - val_loss: 0.6058 - val_acc: 0.7562\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.6429 - acc: 0.7346 - val_loss: 0.6068 - val_acc: 0.7564\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6440 - acc: 0.7367 - val_loss: 0.6059 - val_acc: 0.7566\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60579 to 0.60528, saving model to best.model\n",
      "0s - loss: 0.6435 - acc: 0.7358 - val_loss: 0.6053 - val_acc: 0.7585\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.6443 - acc: 0.7367 - val_loss: 0.6061 - val_acc: 0.7581\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60528 to 0.60458, saving model to best.model\n",
      "0s - loss: 0.6421 - acc: 0.7364 - val_loss: 0.6046 - val_acc: 0.7566\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.6452 - acc: 0.7359 - val_loss: 0.6055 - val_acc: 0.7576\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6470 - acc: 0.7355 - val_loss: 0.6051 - val_acc: 0.7582\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60458 to 0.60423, saving model to best.model\n",
      "0s - loss: 0.6439 - acc: 0.7379 - val_loss: 0.6042 - val_acc: 0.7588\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60423 to 0.60278, saving model to best.model\n",
      "0s - loss: 0.6376 - acc: 0.7405 - val_loss: 0.6028 - val_acc: 0.7577\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6422 - acc: 0.7377 - val_loss: 0.6038 - val_acc: 0.7585\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84976, saving model to best.model\n",
      "1s - loss: 0.9026 - acc: 0.6340 - val_loss: 0.8498 - val_acc: 0.6546\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84976 to 0.84876, saving model to best.model\n",
      "1s - loss: 0.8572 - acc: 0.6564 - val_loss: 0.8488 - val_acc: 0.6546\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84876 to 0.84691, saving model to best.model\n",
      "1s - loss: 0.8538 - acc: 0.6565 - val_loss: 0.8469 - val_acc: 0.6546\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84691 to 0.84285, saving model to best.model\n",
      "1s - loss: 0.8482 - acc: 0.6565 - val_loss: 0.8429 - val_acc: 0.6546\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84285 to 0.83627, saving model to best.model\n",
      "1s - loss: 0.8423 - acc: 0.6565 - val_loss: 0.8363 - val_acc: 0.6546\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83627 to 0.83044, saving model to best.model\n",
      "1s - loss: 0.8366 - acc: 0.6565 - val_loss: 0.8304 - val_acc: 0.6546\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83044 to 0.82924, saving model to best.model\n",
      "1s - loss: 0.8336 - acc: 0.6565 - val_loss: 0.8292 - val_acc: 0.6546\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82924 to 0.82693, saving model to best.model\n",
      "1s - loss: 0.8320 - acc: 0.6565 - val_loss: 0.8269 - val_acc: 0.6546\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "1s - loss: 0.8299 - acc: 0.6565 - val_loss: 0.8281 - val_acc: 0.6546\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8288 - acc: 0.6564 - val_loss: 0.8292 - val_acc: 0.6546\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8291 - acc: 0.6565 - val_loss: 0.8281 - val_acc: 0.6546\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82693 to 0.82557, saving model to best.model\n",
      "0s - loss: 0.8275 - acc: 0.6565 - val_loss: 0.8256 - val_acc: 0.6546\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82557 to 0.82478, saving model to best.model\n",
      "0s - loss: 0.8270 - acc: 0.6565 - val_loss: 0.8248 - val_acc: 0.6546\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.8261 - acc: 0.6563 - val_loss: 0.8248 - val_acc: 0.6546\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82478 to 0.82411, saving model to best.model\n",
      "1s - loss: 0.8258 - acc: 0.6565 - val_loss: 0.8241 - val_acc: 0.6546\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82411 to 0.82363, saving model to best.model\n",
      "1s - loss: 0.8252 - acc: 0.6569 - val_loss: 0.8236 - val_acc: 0.6546\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82363 to 0.82286, saving model to best.model\n",
      "1s - loss: 0.8247 - acc: 0.6565 - val_loss: 0.8229 - val_acc: 0.6546\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82286 to 0.82194, saving model to best.model\n",
      "1s - loss: 0.8233 - acc: 0.6571 - val_loss: 0.8219 - val_acc: 0.6546\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.82194 to 0.82122, saving model to best.model\n",
      "1s - loss: 0.8214 - acc: 0.6574 - val_loss: 0.8212 - val_acc: 0.6546\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "1s - loss: 0.8221 - acc: 0.6576 - val_loss: 0.8218 - val_acc: 0.6546\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82122 to 0.81956, saving model to best.model\n",
      "1s - loss: 0.8209 - acc: 0.6583 - val_loss: 0.8196 - val_acc: 0.6567\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81956 to 0.81810, saving model to best.model\n",
      "1s - loss: 0.8198 - acc: 0.6592 - val_loss: 0.8181 - val_acc: 0.6581\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81810 to 0.81755, saving model to best.model\n",
      "1s - loss: 0.8187 - acc: 0.6592 - val_loss: 0.8175 - val_acc: 0.6571\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81755 to 0.81589, saving model to best.model\n",
      "1s - loss: 0.8178 - acc: 0.6598 - val_loss: 0.8159 - val_acc: 0.6577\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81589 to 0.81484, saving model to best.model\n",
      "1s - loss: 0.8157 - acc: 0.6600 - val_loss: 0.8148 - val_acc: 0.6578\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81484 to 0.81364, saving model to best.model\n",
      "1s - loss: 0.8161 - acc: 0.6601 - val_loss: 0.8136 - val_acc: 0.6577\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81364 to 0.81144, saving model to best.model\n",
      "1s - loss: 0.8138 - acc: 0.6609 - val_loss: 0.8114 - val_acc: 0.6588\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81144 to 0.80975, saving model to best.model\n",
      "0s - loss: 0.8111 - acc: 0.6621 - val_loss: 0.8097 - val_acc: 0.6595\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80975 to 0.80848, saving model to best.model\n",
      "1s - loss: 0.8110 - acc: 0.6626 - val_loss: 0.8085 - val_acc: 0.6590\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80848 to 0.80743, saving model to best.model\n",
      "1s - loss: 0.8099 - acc: 0.6629 - val_loss: 0.8074 - val_acc: 0.6594\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80743 to 0.80474, saving model to best.model\n",
      "1s - loss: 0.8086 - acc: 0.6631 - val_loss: 0.8047 - val_acc: 0.6636\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80474 to 0.80153, saving model to best.model\n",
      "1s - loss: 0.8050 - acc: 0.6660 - val_loss: 0.8015 - val_acc: 0.6632\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80153 to 0.80123, saving model to best.model\n",
      "1s - loss: 0.8049 - acc: 0.6654 - val_loss: 0.8012 - val_acc: 0.6636\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80123 to 0.79937, saving model to best.model\n",
      "1s - loss: 0.8037 - acc: 0.6657 - val_loss: 0.7994 - val_acc: 0.6637\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79937 to 0.79689, saving model to best.model\n",
      "1s - loss: 0.8029 - acc: 0.6648 - val_loss: 0.7969 - val_acc: 0.6648\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79689 to 0.79430, saving model to best.model\n",
      "0s - loss: 0.7992 - acc: 0.6681 - val_loss: 0.7943 - val_acc: 0.6676\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79430 to 0.79172, saving model to best.model\n",
      "0s - loss: 0.7980 - acc: 0.6676 - val_loss: 0.7917 - val_acc: 0.6680\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79172 to 0.79130, saving model to best.model\n",
      "1s - loss: 0.7985 - acc: 0.6692 - val_loss: 0.7913 - val_acc: 0.6691\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79130 to 0.78798, saving model to best.model\n",
      "1s - loss: 0.7978 - acc: 0.6688 - val_loss: 0.7880 - val_acc: 0.6700\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78798 to 0.78663, saving model to best.model\n",
      "0s - loss: 0.7959 - acc: 0.6688 - val_loss: 0.7866 - val_acc: 0.6705\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78663 to 0.78486, saving model to best.model\n",
      "1s - loss: 0.7936 - acc: 0.6711 - val_loss: 0.7849 - val_acc: 0.6701\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78486 to 0.78425, saving model to best.model\n",
      "1s - loss: 0.7911 - acc: 0.6717 - val_loss: 0.7842 - val_acc: 0.6690\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78425 to 0.78295, saving model to best.model\n",
      "1s - loss: 0.7909 - acc: 0.6715 - val_loss: 0.7829 - val_acc: 0.6728\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78295 to 0.77915, saving model to best.model\n",
      "1s - loss: 0.7885 - acc: 0.6703 - val_loss: 0.7791 - val_acc: 0.6747\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77915 to 0.77675, saving model to best.model\n",
      "1s - loss: 0.7891 - acc: 0.6729 - val_loss: 0.7767 - val_acc: 0.6782\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "1s - loss: 0.7889 - acc: 0.6720 - val_loss: 0.7777 - val_acc: 0.6752\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77675 to 0.77434, saving model to best.model\n",
      "1s - loss: 0.7859 - acc: 0.6744 - val_loss: 0.7743 - val_acc: 0.6786\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77434 to 0.77154, saving model to best.model\n",
      "1s - loss: 0.7833 - acc: 0.6755 - val_loss: 0.7715 - val_acc: 0.6769\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77154 to 0.76763, saving model to best.model\n",
      "1s - loss: 0.7816 - acc: 0.6746 - val_loss: 0.7676 - val_acc: 0.6843\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76763 to 0.76720, saving model to best.model\n",
      "1s - loss: 0.7810 - acc: 0.6770 - val_loss: 0.7672 - val_acc: 0.6824\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76720 to 0.76314, saving model to best.model\n",
      "1s - loss: 0.7774 - acc: 0.6801 - val_loss: 0.7631 - val_acc: 0.6843\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76314 to 0.75940, saving model to best.model\n",
      "1s - loss: 0.7777 - acc: 0.6782 - val_loss: 0.7594 - val_acc: 0.6872\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "1s - loss: 0.7773 - acc: 0.6793 - val_loss: 0.7600 - val_acc: 0.6862\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75940 to 0.75717, saving model to best.model\n",
      "1s - loss: 0.7737 - acc: 0.6804 - val_loss: 0.7572 - val_acc: 0.6884\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75717 to 0.75338, saving model to best.model\n",
      "1s - loss: 0.7711 - acc: 0.6812 - val_loss: 0.7534 - val_acc: 0.6927\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75338 to 0.75249, saving model to best.model\n",
      "1s - loss: 0.7709 - acc: 0.6814 - val_loss: 0.7525 - val_acc: 0.6923\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75249 to 0.74804, saving model to best.model\n",
      "1s - loss: 0.7689 - acc: 0.6818 - val_loss: 0.7480 - val_acc: 0.6937\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74804 to 0.74595, saving model to best.model\n",
      "1s - loss: 0.7661 - acc: 0.6842 - val_loss: 0.7460 - val_acc: 0.6951\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74595 to 0.74368, saving model to best.model\n",
      "1s - loss: 0.7643 - acc: 0.6849 - val_loss: 0.7437 - val_acc: 0.6975\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74368 to 0.73957, saving model to best.model\n",
      "1s - loss: 0.7624 - acc: 0.6863 - val_loss: 0.7396 - val_acc: 0.6985\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73957 to 0.73588, saving model to best.model\n",
      "1s - loss: 0.7613 - acc: 0.6861 - val_loss: 0.7359 - val_acc: 0.7021\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "1s - loss: 0.7635 - acc: 0.6850 - val_loss: 0.7362 - val_acc: 0.7018\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73588 to 0.73231, saving model to best.model\n",
      "1s - loss: 0.7594 - acc: 0.6893 - val_loss: 0.7323 - val_acc: 0.7027\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73231 to 0.72965, saving model to best.model\n",
      "1s - loss: 0.7571 - acc: 0.6886 - val_loss: 0.7297 - val_acc: 0.7075\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72965 to 0.72768, saving model to best.model\n",
      "1s - loss: 0.7557 - acc: 0.6867 - val_loss: 0.7277 - val_acc: 0.7080\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72768 to 0.72714, saving model to best.model\n",
      "1s - loss: 0.7515 - acc: 0.6918 - val_loss: 0.7271 - val_acc: 0.7073\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72714 to 0.72359, saving model to best.model\n",
      "1s - loss: 0.7502 - acc: 0.6924 - val_loss: 0.7236 - val_acc: 0.7057\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72359 to 0.72014, saving model to best.model\n",
      "1s - loss: 0.7495 - acc: 0.6933 - val_loss: 0.7201 - val_acc: 0.7121\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72014 to 0.71773, saving model to best.model\n",
      "1s - loss: 0.7478 - acc: 0.6912 - val_loss: 0.7177 - val_acc: 0.7160\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71773 to 0.71645, saving model to best.model\n",
      "1s - loss: 0.7474 - acc: 0.6936 - val_loss: 0.7164 - val_acc: 0.7146\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71645 to 0.71517, saving model to best.model\n",
      "1s - loss: 0.7461 - acc: 0.6931 - val_loss: 0.7152 - val_acc: 0.7116\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71517 to 0.71228, saving model to best.model\n",
      "1s - loss: 0.7452 - acc: 0.6931 - val_loss: 0.7123 - val_acc: 0.7170\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71228 to 0.71047, saving model to best.model\n",
      "1s - loss: 0.7417 - acc: 0.6949 - val_loss: 0.7105 - val_acc: 0.7132\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "1s - loss: 0.7413 - acc: 0.6946 - val_loss: 0.7120 - val_acc: 0.7139\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71047 to 0.70722, saving model to best.model\n",
      "1s - loss: 0.7397 - acc: 0.6972 - val_loss: 0.7072 - val_acc: 0.7171\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70722 to 0.70390, saving model to best.model\n",
      "1s - loss: 0.7362 - acc: 0.6981 - val_loss: 0.7039 - val_acc: 0.7151\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70390 to 0.69910, saving model to best.model\n",
      "1s - loss: 0.7359 - acc: 0.6955 - val_loss: 0.6991 - val_acc: 0.7215\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69910 to 0.69652, saving model to best.model\n",
      "1s - loss: 0.7337 - acc: 0.6996 - val_loss: 0.6965 - val_acc: 0.7213\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69652 to 0.69446, saving model to best.model\n",
      "1s - loss: 0.7338 - acc: 0.6993 - val_loss: 0.6945 - val_acc: 0.7211\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69446 to 0.69286, saving model to best.model\n",
      "1s - loss: 0.7302 - acc: 0.7018 - val_loss: 0.6929 - val_acc: 0.7218\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "1s - loss: 0.7305 - acc: 0.7000 - val_loss: 0.6942 - val_acc: 0.7239\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69286 to 0.69162, saving model to best.model\n",
      "1s - loss: 0.7293 - acc: 0.7007 - val_loss: 0.6916 - val_acc: 0.7225\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69162 to 0.69084, saving model to best.model\n",
      "1s - loss: 0.7309 - acc: 0.7003 - val_loss: 0.6908 - val_acc: 0.7206\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69084 to 0.68638, saving model to best.model\n",
      "1s - loss: 0.7247 - acc: 0.7031 - val_loss: 0.6864 - val_acc: 0.7227\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7252 - acc: 0.7030 - val_loss: 0.6874 - val_acc: 0.7222\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68638 to 0.68376, saving model to best.model\n",
      "0s - loss: 0.7231 - acc: 0.7046 - val_loss: 0.6838 - val_acc: 0.7290\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7258 - acc: 0.7036 - val_loss: 0.6845 - val_acc: 0.7221\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68376 to 0.68346, saving model to best.model\n",
      "0s - loss: 0.7219 - acc: 0.7049 - val_loss: 0.6835 - val_acc: 0.7273\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.68346 to 0.68153, saving model to best.model\n",
      "0s - loss: 0.7194 - acc: 0.7049 - val_loss: 0.6815 - val_acc: 0.7259\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68153 to 0.68023, saving model to best.model\n",
      "0s - loss: 0.7194 - acc: 0.7055 - val_loss: 0.6802 - val_acc: 0.7266\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68023 to 0.67986, saving model to best.model\n",
      "0s - loss: 0.7184 - acc: 0.7064 - val_loss: 0.6799 - val_acc: 0.7268\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67986 to 0.67484, saving model to best.model\n",
      "0s - loss: 0.7172 - acc: 0.7069 - val_loss: 0.6748 - val_acc: 0.7293\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.7205 - acc: 0.7058 - val_loss: 0.6777 - val_acc: 0.7234\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.7157 - acc: 0.7078 - val_loss: 0.6777 - val_acc: 0.7273\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67484 to 0.67054, saving model to best.model\n",
      "0s - loss: 0.7134 - acc: 0.7092 - val_loss: 0.6705 - val_acc: 0.7290\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67054 to 0.66765, saving model to best.model\n",
      "0s - loss: 0.7113 - acc: 0.7087 - val_loss: 0.6676 - val_acc: 0.7324\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.7104 - acc: 0.7085 - val_loss: 0.6710 - val_acc: 0.7273\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.7109 - acc: 0.7106 - val_loss: 0.6700 - val_acc: 0.7269\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.7097 - acc: 0.7098 - val_loss: 0.6686 - val_acc: 0.7265\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66765 to 0.66611, saving model to best.model\n",
      "0s - loss: 0.7084 - acc: 0.7093 - val_loss: 0.6661 - val_acc: 0.7341\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66611 to 0.66461, saving model to best.model\n",
      "0s - loss: 0.7075 - acc: 0.7099 - val_loss: 0.6646 - val_acc: 0.7333\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7074 - acc: 0.7117 - val_loss: 0.6654 - val_acc: 0.7302\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.66461 to 0.66452, saving model to best.model\n",
      "1s - loss: 0.7079 - acc: 0.7104 - val_loss: 0.6645 - val_acc: 0.7302\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66452 to 0.66046, saving model to best.model\n",
      "1s - loss: 0.7038 - acc: 0.7146 - val_loss: 0.6605 - val_acc: 0.7344\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7047 - acc: 0.7109 - val_loss: 0.6638 - val_acc: 0.7329\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.66046 to 0.65761, saving model to best.model\n",
      "0s - loss: 0.7025 - acc: 0.7128 - val_loss: 0.6576 - val_acc: 0.7368\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.7034 - acc: 0.7123 - val_loss: 0.6598 - val_acc: 0.7358\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7008 - acc: 0.7163 - val_loss: 0.6580 - val_acc: 0.7364\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65761 to 0.65553, saving model to best.model\n",
      "1s - loss: 0.7011 - acc: 0.7123 - val_loss: 0.6555 - val_acc: 0.7345\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6991 - acc: 0.7137 - val_loss: 0.6578 - val_acc: 0.7329\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6984 - acc: 0.7140 - val_loss: 0.6573 - val_acc: 0.7335\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6976 - acc: 0.7163 - val_loss: 0.6570 - val_acc: 0.7347\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65553 to 0.65237, saving model to best.model\n",
      "1s - loss: 0.6974 - acc: 0.7159 - val_loss: 0.6524 - val_acc: 0.7359\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65237 to 0.65121, saving model to best.model\n",
      "1s - loss: 0.6976 - acc: 0.7162 - val_loss: 0.6512 - val_acc: 0.7359\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "0s - loss: 0.6960 - acc: 0.7164 - val_loss: 0.6523 - val_acc: 0.7351\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.6993 - acc: 0.7162 - val_loss: 0.6530 - val_acc: 0.7363\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "0s - loss: 0.6979 - acc: 0.7147 - val_loss: 0.6534 - val_acc: 0.7385\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65121 to 0.64653, saving model to best.model\n",
      "0s - loss: 0.6900 - acc: 0.7188 - val_loss: 0.6465 - val_acc: 0.7385\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6908 - acc: 0.7176 - val_loss: 0.6483 - val_acc: 0.7389\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64653 to 0.64618, saving model to best.model\n",
      "0s - loss: 0.6900 - acc: 0.7177 - val_loss: 0.6462 - val_acc: 0.7398\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6900 - acc: 0.7181 - val_loss: 0.6482 - val_acc: 0.7416\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6903 - acc: 0.7179 - val_loss: 0.6462 - val_acc: 0.7415\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64618 to 0.64247, saving model to best.model\n",
      "0s - loss: 0.6876 - acc: 0.7186 - val_loss: 0.6425 - val_acc: 0.7416\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6884 - acc: 0.7190 - val_loss: 0.6426 - val_acc: 0.7396\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64247 to 0.64187, saving model to best.model\n",
      "0s - loss: 0.6883 - acc: 0.7180 - val_loss: 0.6419 - val_acc: 0.7416\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.64187 to 0.63957, saving model to best.model\n",
      "0s - loss: 0.6826 - acc: 0.7209 - val_loss: 0.6396 - val_acc: 0.7433\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.6838 - acc: 0.7184 - val_loss: 0.6399 - val_acc: 0.7418\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6848 - acc: 0.7202 - val_loss: 0.6411 - val_acc: 0.7427\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6852 - acc: 0.7184 - val_loss: 0.6397 - val_acc: 0.7422\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6834 - acc: 0.7211 - val_loss: 0.6408 - val_acc: 0.7425\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6833 - acc: 0.7208 - val_loss: 0.6416 - val_acc: 0.7399\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6834 - acc: 0.7212 - val_loss: 0.6411 - val_acc: 0.7378\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63957 to 0.63921, saving model to best.model\n",
      "1s - loss: 0.6818 - acc: 0.7215 - val_loss: 0.6392 - val_acc: 0.7431\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63921 to 0.63383, saving model to best.model\n",
      "1s - loss: 0.6804 - acc: 0.7233 - val_loss: 0.6338 - val_acc: 0.7438\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6786 - acc: 0.7216 - val_loss: 0.6361 - val_acc: 0.7441\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6814 - acc: 0.7220 - val_loss: 0.6378 - val_acc: 0.7422\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6784 - acc: 0.7230 - val_loss: 0.6347 - val_acc: 0.7446\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63383 to 0.63340, saving model to best.model\n",
      "1s - loss: 0.6793 - acc: 0.7222 - val_loss: 0.6334 - val_acc: 0.7431\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6761 - acc: 0.7238 - val_loss: 0.6335 - val_acc: 0.7427\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63340 to 0.63327, saving model to best.model\n",
      "1s - loss: 0.6798 - acc: 0.7231 - val_loss: 0.6333 - val_acc: 0.7426\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.63327 to 0.63165, saving model to best.model\n",
      "1s - loss: 0.6759 - acc: 0.7258 - val_loss: 0.6317 - val_acc: 0.7443\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.63165 to 0.63091, saving model to best.model\n",
      "1s - loss: 0.6751 - acc: 0.7220 - val_loss: 0.6309 - val_acc: 0.7430\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6747 - acc: 0.7249 - val_loss: 0.6319 - val_acc: 0.7427\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.6780 - acc: 0.7235 - val_loss: 0.6316 - val_acc: 0.7451\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63091 to 0.63000, saving model to best.model\n",
      "1s - loss: 0.6740 - acc: 0.7251 - val_loss: 0.6300 - val_acc: 0.7457\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.63000 to 0.62590, saving model to best.model\n",
      "1s - loss: 0.6731 - acc: 0.7270 - val_loss: 0.6259 - val_acc: 0.7491\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6736 - acc: 0.7245 - val_loss: 0.6302 - val_acc: 0.7434\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6755 - acc: 0.7240 - val_loss: 0.6279 - val_acc: 0.7440\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6707 - acc: 0.7250 - val_loss: 0.6261 - val_acc: 0.7450\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.6705 - acc: 0.7275 - val_loss: 0.6269 - val_acc: 0.7424\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6716 - acc: 0.7261 - val_loss: 0.6261 - val_acc: 0.7479\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6745 - acc: 0.7238 - val_loss: 0.6267 - val_acc: 0.7458\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62590 to 0.62460, saving model to best.model\n",
      "0s - loss: 0.6698 - acc: 0.7263 - val_loss: 0.6246 - val_acc: 0.7447\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62460 to 0.62296, saving model to best.model\n",
      "0s - loss: 0.6700 - acc: 0.7292 - val_loss: 0.6230 - val_acc: 0.7460\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62296 to 0.62070, saving model to best.model\n",
      "0s - loss: 0.6685 - acc: 0.7271 - val_loss: 0.6207 - val_acc: 0.7478\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6683 - acc: 0.7264 - val_loss: 0.6243 - val_acc: 0.7451\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6700 - acc: 0.7264 - val_loss: 0.6246 - val_acc: 0.7459\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6689 - acc: 0.7278 - val_loss: 0.6232 - val_acc: 0.7452\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6631 - acc: 0.7295 - val_loss: 0.6214 - val_acc: 0.7488\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6667 - acc: 0.7298 - val_loss: 0.6218 - val_acc: 0.7440\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.62070 to 0.62038, saving model to best.model\n",
      "0s - loss: 0.6652 - acc: 0.7283 - val_loss: 0.6204 - val_acc: 0.7499\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62038 to 0.61949, saving model to best.model\n",
      "1s - loss: 0.6626 - acc: 0.7309 - val_loss: 0.6195 - val_acc: 0.7480\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.61949 to 0.61881, saving model to best.model\n",
      "1s - loss: 0.6598 - acc: 0.7309 - val_loss: 0.6188 - val_acc: 0.7463\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6628 - acc: 0.7299 - val_loss: 0.6206 - val_acc: 0.7436\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.61881 to 0.61836, saving model to best.model\n",
      "1s - loss: 0.6620 - acc: 0.7292 - val_loss: 0.6184 - val_acc: 0.7457\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61836 to 0.61671, saving model to best.model\n",
      "1s - loss: 0.6617 - acc: 0.7299 - val_loss: 0.6167 - val_acc: 0.7495\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61671 to 0.61558, saving model to best.model\n",
      "1s - loss: 0.6631 - acc: 0.7310 - val_loss: 0.6156 - val_acc: 0.7495\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6607 - acc: 0.7309 - val_loss: 0.6168 - val_acc: 0.7458\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6622 - acc: 0.7312 - val_loss: 0.6156 - val_acc: 0.7467\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61558 to 0.61391, saving model to best.model\n",
      "1s - loss: 0.6600 - acc: 0.7312 - val_loss: 0.6139 - val_acc: 0.7473\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.6595 - acc: 0.7303 - val_loss: 0.6157 - val_acc: 0.7467\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61391 to 0.61358, saving model to best.model\n",
      "1s - loss: 0.6623 - acc: 0.7306 - val_loss: 0.6136 - val_acc: 0.7512\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61358 to 0.61227, saving model to best.model\n",
      "1s - loss: 0.6576 - acc: 0.7320 - val_loss: 0.6123 - val_acc: 0.7501\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6579 - acc: 0.7303 - val_loss: 0.6151 - val_acc: 0.7466\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61227 to 0.61141, saving model to best.model\n",
      "1s - loss: 0.6565 - acc: 0.7311 - val_loss: 0.6114 - val_acc: 0.7518\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6551 - acc: 0.7340 - val_loss: 0.6118 - val_acc: 0.7492\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61141 to 0.60958, saving model to best.model\n",
      "1s - loss: 0.6546 - acc: 0.7349 - val_loss: 0.6096 - val_acc: 0.7485\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6562 - acc: 0.7333 - val_loss: 0.6116 - val_acc: 0.7484\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.60958 to 0.60939, saving model to best.model\n",
      "1s - loss: 0.6560 - acc: 0.7325 - val_loss: 0.6094 - val_acc: 0.7495\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.60939 to 0.60873, saving model to best.model\n",
      "0s - loss: 0.6562 - acc: 0.7301 - val_loss: 0.6087 - val_acc: 0.7530\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60873 to 0.60669, saving model to best.model\n",
      "1s - loss: 0.6571 - acc: 0.7326 - val_loss: 0.6067 - val_acc: 0.7516\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6573 - acc: 0.7327 - val_loss: 0.6070 - val_acc: 0.7511\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60669 to 0.60632, saving model to best.model\n",
      "0s - loss: 0.6531 - acc: 0.7344 - val_loss: 0.6063 - val_acc: 0.7539\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6534 - acc: 0.7335 - val_loss: 0.6081 - val_acc: 0.7515\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7322 - val_loss: 0.6064 - val_acc: 0.7530\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6520 - acc: 0.7324 - val_loss: 0.6095 - val_acc: 0.7485\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60632 to 0.60592, saving model to best.model\n",
      "1s - loss: 0.6511 - acc: 0.7350 - val_loss: 0.6059 - val_acc: 0.7521\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6536 - acc: 0.7346 - val_loss: 0.6061 - val_acc: 0.7522\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60592 to 0.60528, saving model to best.model\n",
      "1s - loss: 0.6503 - acc: 0.7355 - val_loss: 0.6053 - val_acc: 0.7519\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6527 - acc: 0.7341 - val_loss: 0.6085 - val_acc: 0.7501\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60528 to 0.60265, saving model to best.model\n",
      "1s - loss: 0.6511 - acc: 0.7360 - val_loss: 0.6026 - val_acc: 0.7549\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.60265 to 0.60195, saving model to best.model\n",
      "1s - loss: 0.6522 - acc: 0.7338 - val_loss: 0.6020 - val_acc: 0.7528\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6503 - acc: 0.7345 - val_loss: 0.6040 - val_acc: 0.7502\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6504 - acc: 0.7359 - val_loss: 0.6030 - val_acc: 0.7507\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6517 - acc: 0.7339 - val_loss: 0.6033 - val_acc: 0.7540\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.60195 to 0.60093, saving model to best.model\n",
      "1s - loss: 0.6492 - acc: 0.7337 - val_loss: 0.6009 - val_acc: 0.7536\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60093 to 0.59959, saving model to best.model\n",
      "1s - loss: 0.6462 - acc: 0.7367 - val_loss: 0.5996 - val_acc: 0.7546\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.59959 to 0.59918, saving model to best.model\n",
      "1s - loss: 0.6496 - acc: 0.7359 - val_loss: 0.5992 - val_acc: 0.7559\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.6499 - acc: 0.7354 - val_loss: 0.6018 - val_acc: 0.7522\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6505 - acc: 0.7353 - val_loss: 0.6004 - val_acc: 0.7530\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84167, saving model to best.model\n",
      "1s - loss: 0.9508 - acc: 0.6066 - val_loss: 0.8417 - val_acc: 0.6609\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84167 to 0.84128, saving model to best.model\n",
      "1s - loss: 0.8648 - acc: 0.6541 - val_loss: 0.8413 - val_acc: 0.6609\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "1s - loss: 0.8540 - acc: 0.6577 - val_loss: 0.8414 - val_acc: 0.6609\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84128 to 0.84110, saving model to best.model\n",
      "1s - loss: 0.8509 - acc: 0.6578 - val_loss: 0.8411 - val_acc: 0.6609\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84110 to 0.83651, saving model to best.model\n",
      "1s - loss: 0.8485 - acc: 0.6578 - val_loss: 0.8365 - val_acc: 0.6609\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83651 to 0.82954, saving model to best.model\n",
      "1s - loss: 0.8429 - acc: 0.6578 - val_loss: 0.8295 - val_acc: 0.6609\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82954 to 0.82559, saving model to best.model\n",
      "1s - loss: 0.8365 - acc: 0.6578 - val_loss: 0.8256 - val_acc: 0.6609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82559 to 0.82325, saving model to best.model\n",
      "1s - loss: 0.8331 - acc: 0.6578 - val_loss: 0.8233 - val_acc: 0.6609\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82325 to 0.82019, saving model to best.model\n",
      "1s - loss: 0.8306 - acc: 0.6578 - val_loss: 0.8202 - val_acc: 0.6609\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82019 to 0.81926, saving model to best.model\n",
      "1s - loss: 0.8291 - acc: 0.6577 - val_loss: 0.8193 - val_acc: 0.6609\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81926 to 0.81893, saving model to best.model\n",
      "0s - loss: 0.8285 - acc: 0.6577 - val_loss: 0.8189 - val_acc: 0.6609\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8269 - acc: 0.6578 - val_loss: 0.8191 - val_acc: 0.6609\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81893 to 0.81856, saving model to best.model\n",
      "1s - loss: 0.8252 - acc: 0.6578 - val_loss: 0.8186 - val_acc: 0.6609\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8261 - acc: 0.6579 - val_loss: 0.8187 - val_acc: 0.6609\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81856 to 0.81814, saving model to best.model\n",
      "1s - loss: 0.8244 - acc: 0.6577 - val_loss: 0.8181 - val_acc: 0.6609\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81814 to 0.81784, saving model to best.model\n",
      "1s - loss: 0.8235 - acc: 0.6577 - val_loss: 0.8178 - val_acc: 0.6609\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.8221 - acc: 0.6575 - val_loss: 0.8183 - val_acc: 0.6609\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.8230 - acc: 0.6579 - val_loss: 0.8184 - val_acc: 0.6609\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81784 to 0.81718, saving model to best.model\n",
      "1s - loss: 0.8230 - acc: 0.6577 - val_loss: 0.8172 - val_acc: 0.6609\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81718 to 0.81635, saving model to best.model\n",
      "1s - loss: 0.8210 - acc: 0.6579 - val_loss: 0.8163 - val_acc: 0.6609\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "1s - loss: 0.8215 - acc: 0.6570 - val_loss: 0.8171 - val_acc: 0.6609\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81635 to 0.81511, saving model to best.model\n",
      "0s - loss: 0.8209 - acc: 0.6582 - val_loss: 0.8151 - val_acc: 0.6609\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81511 to 0.81429, saving model to best.model\n",
      "1s - loss: 0.8199 - acc: 0.6584 - val_loss: 0.8143 - val_acc: 0.6609\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81429 to 0.81338, saving model to best.model\n",
      "1s - loss: 0.8192 - acc: 0.6594 - val_loss: 0.8134 - val_acc: 0.6609\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81338 to 0.81278, saving model to best.model\n",
      "1s - loss: 0.8175 - acc: 0.6590 - val_loss: 0.8128 - val_acc: 0.6619\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81278 to 0.81136, saving model to best.model\n",
      "1s - loss: 0.8172 - acc: 0.6611 - val_loss: 0.8114 - val_acc: 0.6639\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81136 to 0.81073, saving model to best.model\n",
      "1s - loss: 0.8153 - acc: 0.6616 - val_loss: 0.8107 - val_acc: 0.6652\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81073 to 0.80970, saving model to best.model\n",
      "1s - loss: 0.8162 - acc: 0.6606 - val_loss: 0.8097 - val_acc: 0.6637\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80970 to 0.80920, saving model to best.model\n",
      "0s - loss: 0.8140 - acc: 0.6609 - val_loss: 0.8092 - val_acc: 0.6676\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80920 to 0.80674, saving model to best.model\n",
      "1s - loss: 0.8131 - acc: 0.6612 - val_loss: 0.8067 - val_acc: 0.6687\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80674 to 0.80627, saving model to best.model\n",
      "1s - loss: 0.8115 - acc: 0.6627 - val_loss: 0.8063 - val_acc: 0.6671\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80627 to 0.80525, saving model to best.model\n",
      "1s - loss: 0.8112 - acc: 0.6618 - val_loss: 0.8052 - val_acc: 0.6683\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80525 to 0.80362, saving model to best.model\n",
      "1s - loss: 0.8114 - acc: 0.6635 - val_loss: 0.8036 - val_acc: 0.6694\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80362 to 0.80163, saving model to best.model\n",
      "1s - loss: 0.8085 - acc: 0.6654 - val_loss: 0.8016 - val_acc: 0.6684\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80163 to 0.80041, saving model to best.model\n",
      "1s - loss: 0.8077 - acc: 0.6658 - val_loss: 0.8004 - val_acc: 0.6699\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80041 to 0.79964, saving model to best.model\n",
      "1s - loss: 0.8068 - acc: 0.6652 - val_loss: 0.7996 - val_acc: 0.6680\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79964 to 0.79699, saving model to best.model\n",
      "1s - loss: 0.8040 - acc: 0.6656 - val_loss: 0.7970 - val_acc: 0.6704\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79699 to 0.79365, saving model to best.model\n",
      "1s - loss: 0.8039 - acc: 0.6660 - val_loss: 0.7937 - val_acc: 0.6711\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79365 to 0.79172, saving model to best.model\n",
      "1s - loss: 0.8014 - acc: 0.6669 - val_loss: 0.7917 - val_acc: 0.6707\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79172 to 0.78884, saving model to best.model\n",
      "1s - loss: 0.7999 - acc: 0.6674 - val_loss: 0.7888 - val_acc: 0.6719\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78884 to 0.78629, saving model to best.model\n",
      "1s - loss: 0.7979 - acc: 0.6673 - val_loss: 0.7863 - val_acc: 0.6714\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78629 to 0.78493, saving model to best.model\n",
      "1s - loss: 0.7970 - acc: 0.6672 - val_loss: 0.7849 - val_acc: 0.6715\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78493 to 0.78341, saving model to best.model\n",
      "1s - loss: 0.7957 - acc: 0.6683 - val_loss: 0.7834 - val_acc: 0.6718\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78341 to 0.77999, saving model to best.model\n",
      "1s - loss: 0.7941 - acc: 0.6684 - val_loss: 0.7800 - val_acc: 0.6749\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77999 to 0.77867, saving model to best.model\n",
      "1s - loss: 0.7909 - acc: 0.6725 - val_loss: 0.7787 - val_acc: 0.6756\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77867 to 0.77561, saving model to best.model\n",
      "1s - loss: 0.7893 - acc: 0.6712 - val_loss: 0.7756 - val_acc: 0.6772\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77561 to 0.77087, saving model to best.model\n",
      "1s - loss: 0.7893 - acc: 0.6712 - val_loss: 0.7709 - val_acc: 0.6827\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "1s - loss: 0.7850 - acc: 0.6734 - val_loss: 0.7716 - val_acc: 0.6787\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77087 to 0.76888, saving model to best.model\n",
      "1s - loss: 0.7846 - acc: 0.6727 - val_loss: 0.7689 - val_acc: 0.6780\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76888 to 0.76468, saving model to best.model\n",
      "1s - loss: 0.7838 - acc: 0.6725 - val_loss: 0.7647 - val_acc: 0.6838\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76468 to 0.76398, saving model to best.model\n",
      "1s - loss: 0.7806 - acc: 0.6760 - val_loss: 0.7640 - val_acc: 0.6838\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76398 to 0.75794, saving model to best.model\n",
      "1s - loss: 0.7787 - acc: 0.6746 - val_loss: 0.7579 - val_acc: 0.6869\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75794 to 0.75641, saving model to best.model\n",
      "1s - loss: 0.7778 - acc: 0.6763 - val_loss: 0.7564 - val_acc: 0.6881\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75641 to 0.75227, saving model to best.model\n",
      "1s - loss: 0.7746 - acc: 0.6767 - val_loss: 0.7523 - val_acc: 0.6892\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75227 to 0.74822, saving model to best.model\n",
      "0s - loss: 0.7728 - acc: 0.6801 - val_loss: 0.7482 - val_acc: 0.6941\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74822 to 0.74613, saving model to best.model\n",
      "1s - loss: 0.7707 - acc: 0.6813 - val_loss: 0.7461 - val_acc: 0.6926\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74613 to 0.74389, saving model to best.model\n",
      "1s - loss: 0.7672 - acc: 0.6820 - val_loss: 0.7439 - val_acc: 0.6964\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74389 to 0.74151, saving model to best.model\n",
      "1s - loss: 0.7673 - acc: 0.6843 - val_loss: 0.7415 - val_acc: 0.6965\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74151 to 0.73786, saving model to best.model\n",
      "1s - loss: 0.7650 - acc: 0.6800 - val_loss: 0.7379 - val_acc: 0.6958\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73786 to 0.73398, saving model to best.model\n",
      "1s - loss: 0.7638 - acc: 0.6831 - val_loss: 0.7340 - val_acc: 0.6993\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73398 to 0.73135, saving model to best.model\n",
      "1s - loss: 0.7607 - acc: 0.6868 - val_loss: 0.7313 - val_acc: 0.7015\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73135 to 0.72847, saving model to best.model\n",
      "1s - loss: 0.7584 - acc: 0.6869 - val_loss: 0.7285 - val_acc: 0.7049\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "1s - loss: 0.7579 - acc: 0.6861 - val_loss: 0.7295 - val_acc: 0.7049\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72847 to 0.72442, saving model to best.model\n",
      "0s - loss: 0.7544 - acc: 0.6871 - val_loss: 0.7244 - val_acc: 0.7071\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72442 to 0.72137, saving model to best.model\n",
      "1s - loss: 0.7545 - acc: 0.6886 - val_loss: 0.7214 - val_acc: 0.7047\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72137 to 0.71729, saving model to best.model\n",
      "1s - loss: 0.7525 - acc: 0.6888 - val_loss: 0.7173 - val_acc: 0.7095\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "1s - loss: 0.7490 - acc: 0.6898 - val_loss: 0.7186 - val_acc: 0.7036\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71729 to 0.71419, saving model to best.model\n",
      "1s - loss: 0.7472 - acc: 0.6915 - val_loss: 0.7142 - val_acc: 0.7118\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71419 to 0.71220, saving model to best.model\n",
      "1s - loss: 0.7476 - acc: 0.6925 - val_loss: 0.7122 - val_acc: 0.7102\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71220 to 0.71108, saving model to best.model\n",
      "1s - loss: 0.7461 - acc: 0.6929 - val_loss: 0.7111 - val_acc: 0.7123\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71108 to 0.70895, saving model to best.model\n",
      "1s - loss: 0.7447 - acc: 0.6912 - val_loss: 0.7089 - val_acc: 0.7109\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70895 to 0.70637, saving model to best.model\n",
      "1s - loss: 0.7399 - acc: 0.6958 - val_loss: 0.7064 - val_acc: 0.7115\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70637 to 0.70404, saving model to best.model\n",
      "1s - loss: 0.7388 - acc: 0.6962 - val_loss: 0.7040 - val_acc: 0.7137\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70404 to 0.70050, saving model to best.model\n",
      "1s - loss: 0.7387 - acc: 0.6956 - val_loss: 0.7005 - val_acc: 0.7163\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70050 to 0.69768, saving model to best.model\n",
      "1s - loss: 0.7379 - acc: 0.6952 - val_loss: 0.6977 - val_acc: 0.7160\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.7348 - acc: 0.6973 - val_loss: 0.6980 - val_acc: 0.7190\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69768 to 0.69373, saving model to best.model\n",
      "1s - loss: 0.7332 - acc: 0.6987 - val_loss: 0.6937 - val_acc: 0.7185\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69373 to 0.69160, saving model to best.model\n",
      "1s - loss: 0.7339 - acc: 0.6964 - val_loss: 0.6916 - val_acc: 0.7218\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.69160 to 0.68816, saving model to best.model\n",
      "1s - loss: 0.7309 - acc: 0.6997 - val_loss: 0.6882 - val_acc: 0.7210\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "1s - loss: 0.7279 - acc: 0.7022 - val_loss: 0.6884 - val_acc: 0.7262\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68816 to 0.68318, saving model to best.model\n",
      "1s - loss: 0.7277 - acc: 0.7016 - val_loss: 0.6832 - val_acc: 0.7258\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.7267 - acc: 0.7005 - val_loss: 0.6848 - val_acc: 0.7214\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "1s - loss: 0.7236 - acc: 0.7038 - val_loss: 0.6864 - val_acc: 0.7224\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68318 to 0.68091, saving model to best.model\n",
      "1s - loss: 0.7254 - acc: 0.7017 - val_loss: 0.6809 - val_acc: 0.7266\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.68091 to 0.67983, saving model to best.model\n",
      "1s - loss: 0.7225 - acc: 0.7034 - val_loss: 0.6798 - val_acc: 0.7260\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67983 to 0.67917, saving model to best.model\n",
      "1s - loss: 0.7190 - acc: 0.7056 - val_loss: 0.6792 - val_acc: 0.7231\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67917 to 0.67516, saving model to best.model\n",
      "1s - loss: 0.7198 - acc: 0.7051 - val_loss: 0.6752 - val_acc: 0.7258\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7194 - acc: 0.7057 - val_loss: 0.6756 - val_acc: 0.7241\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67516 to 0.67018, saving model to best.model\n",
      "1s - loss: 0.7150 - acc: 0.7071 - val_loss: 0.6702 - val_acc: 0.7319\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.7161 - acc: 0.7058 - val_loss: 0.6720 - val_acc: 0.7297\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67018 to 0.66836, saving model to best.model\n",
      "0s - loss: 0.7145 - acc: 0.7055 - val_loss: 0.6684 - val_acc: 0.7321\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.7141 - acc: 0.7066 - val_loss: 0.6715 - val_acc: 0.7275\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.66836 to 0.66809, saving model to best.model\n",
      "0s - loss: 0.7158 - acc: 0.7069 - val_loss: 0.6681 - val_acc: 0.7304\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.7137 - acc: 0.7080 - val_loss: 0.6694 - val_acc: 0.7292\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.66809 to 0.66375, saving model to best.model\n",
      "0s - loss: 0.7100 - acc: 0.7087 - val_loss: 0.6638 - val_acc: 0.7327\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.7118 - acc: 0.7083 - val_loss: 0.6658 - val_acc: 0.7338\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.7086 - acc: 0.7121 - val_loss: 0.6640 - val_acc: 0.7334\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7079 - acc: 0.7112 - val_loss: 0.6640 - val_acc: 0.7311\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66375 to 0.65914, saving model to best.model\n",
      "1s - loss: 0.7062 - acc: 0.7112 - val_loss: 0.6591 - val_acc: 0.7361\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7070 - acc: 0.7108 - val_loss: 0.6599 - val_acc: 0.7333\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7071 - acc: 0.7104 - val_loss: 0.6611 - val_acc: 0.7320\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.65914 to 0.65553, saving model to best.model\n",
      "0s - loss: 0.7017 - acc: 0.7137 - val_loss: 0.6555 - val_acc: 0.7371\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7044 - acc: 0.7130 - val_loss: 0.6564 - val_acc: 0.7379\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.7038 - acc: 0.7113 - val_loss: 0.6562 - val_acc: 0.7388\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65553 to 0.65479, saving model to best.model\n",
      "1s - loss: 0.7040 - acc: 0.7117 - val_loss: 0.6548 - val_acc: 0.7355\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7011 - acc: 0.7141 - val_loss: 0.6568 - val_acc: 0.7358\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65479 to 0.65400, saving model to best.model\n",
      "1s - loss: 0.7028 - acc: 0.7131 - val_loss: 0.6540 - val_acc: 0.7357\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65400 to 0.64964, saving model to best.model\n",
      "1s - loss: 0.6971 - acc: 0.7157 - val_loss: 0.6496 - val_acc: 0.7385\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.64964 to 0.64673, saving model to best.model\n",
      "1s - loss: 0.6986 - acc: 0.7119 - val_loss: 0.6467 - val_acc: 0.7392\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.7003 - acc: 0.7133 - val_loss: 0.6515 - val_acc: 0.7363\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.6996 - acc: 0.7127 - val_loss: 0.6494 - val_acc: 0.7413\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.64673 to 0.64612, saving model to best.model\n",
      "1s - loss: 0.6946 - acc: 0.7167 - val_loss: 0.6461 - val_acc: 0.7411\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6933 - acc: 0.7172 - val_loss: 0.6485 - val_acc: 0.7370\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "0s - loss: 0.6964 - acc: 0.7159 - val_loss: 0.6481 - val_acc: 0.7400\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.64612 to 0.64297, saving model to best.model\n",
      "0s - loss: 0.6911 - acc: 0.7199 - val_loss: 0.6430 - val_acc: 0.7413\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "0s - loss: 0.6924 - acc: 0.7177 - val_loss: 0.6431 - val_acc: 0.7403\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6903 - acc: 0.7176 - val_loss: 0.6433 - val_acc: 0.7398\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.64297 to 0.64222, saving model to best.model\n",
      "1s - loss: 0.6933 - acc: 0.7149 - val_loss: 0.6422 - val_acc: 0.7393\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.64222 to 0.64137, saving model to best.model\n",
      "1s - loss: 0.6883 - acc: 0.7193 - val_loss: 0.6414 - val_acc: 0.7389\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.6917 - acc: 0.7192 - val_loss: 0.6418 - val_acc: 0.7402\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6902 - acc: 0.7190 - val_loss: 0.6435 - val_acc: 0.7404\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.64137 to 0.63623, saving model to best.model\n",
      "0s - loss: 0.6865 - acc: 0.7184 - val_loss: 0.6362 - val_acc: 0.7426\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6875 - acc: 0.7188 - val_loss: 0.6389 - val_acc: 0.7407\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6890 - acc: 0.7179 - val_loss: 0.6383 - val_acc: 0.7429\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.63623 to 0.63400, saving model to best.model\n",
      "0s - loss: 0.6870 - acc: 0.7193 - val_loss: 0.6340 - val_acc: 0.7424\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6860 - acc: 0.7193 - val_loss: 0.6375 - val_acc: 0.7422\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6831 - acc: 0.7217 - val_loss: 0.6352 - val_acc: 0.7434\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6826 - acc: 0.7230 - val_loss: 0.6365 - val_acc: 0.7407\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.6850 - acc: 0.7208 - val_loss: 0.6350 - val_acc: 0.7431\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63400 to 0.63221, saving model to best.model\n",
      "0s - loss: 0.6784 - acc: 0.7249 - val_loss: 0.6322 - val_acc: 0.7457\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "0s - loss: 0.6847 - acc: 0.7212 - val_loss: 0.6344 - val_acc: 0.7424\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "0s - loss: 0.6831 - acc: 0.7208 - val_loss: 0.6323 - val_acc: 0.7447\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.63221 to 0.63151, saving model to best.model\n",
      "0s - loss: 0.6818 - acc: 0.7221 - val_loss: 0.6315 - val_acc: 0.7465\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6790 - acc: 0.7234 - val_loss: 0.6318 - val_acc: 0.7422\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6804 - acc: 0.7224 - val_loss: 0.6316 - val_acc: 0.7445\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63151 to 0.62959, saving model to best.model\n",
      "0s - loss: 0.6778 - acc: 0.7210 - val_loss: 0.6296 - val_acc: 0.7440\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.6807 - acc: 0.7233 - val_loss: 0.6318 - val_acc: 0.7460\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62959 to 0.62597, saving model to best.model\n",
      "0s - loss: 0.6772 - acc: 0.7228 - val_loss: 0.6260 - val_acc: 0.7454\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62597 to 0.62553, saving model to best.model\n",
      "0s - loss: 0.6744 - acc: 0.7244 - val_loss: 0.6255 - val_acc: 0.7473\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.62553 to 0.62427, saving model to best.model\n",
      "1s - loss: 0.6735 - acc: 0.7253 - val_loss: 0.6243 - val_acc: 0.7475\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62427 to 0.62405, saving model to best.model\n",
      "1s - loss: 0.6735 - acc: 0.7276 - val_loss: 0.6241 - val_acc: 0.7452\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6752 - acc: 0.7238 - val_loss: 0.6253 - val_acc: 0.7463\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6742 - acc: 0.7246 - val_loss: 0.6270 - val_acc: 0.7444\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62405 to 0.62289, saving model to best.model\n",
      "1s - loss: 0.6725 - acc: 0.7276 - val_loss: 0.6229 - val_acc: 0.7492\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.62289 to 0.61915, saving model to best.model\n",
      "1s - loss: 0.6735 - acc: 0.7273 - val_loss: 0.6192 - val_acc: 0.7480\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.6763 - acc: 0.7270 - val_loss: 0.6243 - val_acc: 0.7498\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.6729 - acc: 0.7274 - val_loss: 0.6225 - val_acc: 0.7467\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6719 - acc: 0.7284 - val_loss: 0.6199 - val_acc: 0.7488\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.6732 - acc: 0.7253 - val_loss: 0.6200 - val_acc: 0.7492\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.61915 to 0.61826, saving model to best.model\n",
      "0s - loss: 0.6705 - acc: 0.7270 - val_loss: 0.6183 - val_acc: 0.7498\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.6708 - acc: 0.7274 - val_loss: 0.6183 - val_acc: 0.7493\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.61826 to 0.61577, saving model to best.model\n",
      "1s - loss: 0.6674 - acc: 0.7262 - val_loss: 0.6158 - val_acc: 0.7484\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6701 - acc: 0.7267 - val_loss: 0.6194 - val_acc: 0.7484\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6694 - acc: 0.7267 - val_loss: 0.6161 - val_acc: 0.7506\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.61577 to 0.61546, saving model to best.model\n",
      "1s - loss: 0.6677 - acc: 0.7288 - val_loss: 0.6155 - val_acc: 0.7501\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.61546 to 0.61318, saving model to best.model\n",
      "0s - loss: 0.6681 - acc: 0.7299 - val_loss: 0.6132 - val_acc: 0.7508\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6655 - acc: 0.7280 - val_loss: 0.6154 - val_acc: 0.7501\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6670 - acc: 0.7291 - val_loss: 0.6151 - val_acc: 0.7479\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61318 to 0.61286, saving model to best.model\n",
      "1s - loss: 0.6658 - acc: 0.7291 - val_loss: 0.6129 - val_acc: 0.7502\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6667 - acc: 0.7295 - val_loss: 0.6150 - val_acc: 0.7498\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6655 - acc: 0.7277 - val_loss: 0.6132 - val_acc: 0.7502\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6643 - acc: 0.7290 - val_loss: 0.6145 - val_acc: 0.7495\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6647 - acc: 0.7285 - val_loss: 0.6139 - val_acc: 0.7499\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61286 to 0.61062, saving model to best.model\n",
      "1s - loss: 0.6633 - acc: 0.7305 - val_loss: 0.6106 - val_acc: 0.7525\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6640 - acc: 0.7306 - val_loss: 0.6127 - val_acc: 0.7499\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61062 to 0.60985, saving model to best.model\n",
      "1s - loss: 0.6642 - acc: 0.7284 - val_loss: 0.6098 - val_acc: 0.7516\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.60985 to 0.60793, saving model to best.model\n",
      "1s - loss: 0.6603 - acc: 0.7285 - val_loss: 0.6079 - val_acc: 0.7542\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6639 - acc: 0.7296 - val_loss: 0.6100 - val_acc: 0.7505\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.60793 to 0.60708, saving model to best.model\n",
      "1s - loss: 0.6611 - acc: 0.7305 - val_loss: 0.6071 - val_acc: 0.7512\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.60708 to 0.60691, saving model to best.model\n",
      "1s - loss: 0.6592 - acc: 0.7340 - val_loss: 0.6069 - val_acc: 0.7526\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6616 - acc: 0.7314 - val_loss: 0.6114 - val_acc: 0.7516\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.60691 to 0.60655, saving model to best.model\n",
      "1s - loss: 0.6598 - acc: 0.7309 - val_loss: 0.6066 - val_acc: 0.7523\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6591 - acc: 0.7325 - val_loss: 0.6070 - val_acc: 0.7520\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.60655 to 0.60364, saving model to best.model\n",
      "1s - loss: 0.6565 - acc: 0.7338 - val_loss: 0.6036 - val_acc: 0.7527\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6582 - acc: 0.7332 - val_loss: 0.6069 - val_acc: 0.7511\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.60364 to 0.60290, saving model to best.model\n",
      "1s - loss: 0.6562 - acc: 0.7314 - val_loss: 0.6029 - val_acc: 0.7542\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6545 - acc: 0.7349 - val_loss: 0.6038 - val_acc: 0.7535\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.60290 to 0.60087, saving model to best.model\n",
      "1s - loss: 0.6559 - acc: 0.7326 - val_loss: 0.6009 - val_acc: 0.7570\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.60087 to 0.60074, saving model to best.model\n",
      "1s - loss: 0.6546 - acc: 0.7342 - val_loss: 0.6007 - val_acc: 0.7554\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6580 - acc: 0.7330 - val_loss: 0.6036 - val_acc: 0.7522\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7338 - val_loss: 0.6046 - val_acc: 0.7515\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6541 - acc: 0.7357 - val_loss: 0.6033 - val_acc: 0.7526\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6551 - acc: 0.7327 - val_loss: 0.6029 - val_acc: 0.7528\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60074 to 0.59888, saving model to best.model\n",
      "1s - loss: 0.6517 - acc: 0.7347 - val_loss: 0.5989 - val_acc: 0.7577\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6551 - acc: 0.7339 - val_loss: 0.6009 - val_acc: 0.7550\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6533 - acc: 0.7352 - val_loss: 0.5998 - val_acc: 0.7536\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.59888 to 0.59878, saving model to best.model\n",
      "1s - loss: 0.6514 - acc: 0.7357 - val_loss: 0.5988 - val_acc: 0.7580\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.59878 to 0.59758, saving model to best.model\n",
      "1s - loss: 0.6538 - acc: 0.7346 - val_loss: 0.5976 - val_acc: 0.7575\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7349 - val_loss: 0.5998 - val_acc: 0.7520\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.59758 to 0.59667, saving model to best.model\n",
      "1s - loss: 0.6503 - acc: 0.7374 - val_loss: 0.5967 - val_acc: 0.7581\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6536 - acc: 0.7356 - val_loss: 0.5985 - val_acc: 0.7549\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.59667 to 0.59519, saving model to best.model\n",
      "1s - loss: 0.6502 - acc: 0.7354 - val_loss: 0.5952 - val_acc: 0.7570\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6467 - acc: 0.7366 - val_loss: 0.5959 - val_acc: 0.7562\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6507 - acc: 0.7357 - val_loss: 0.5957 - val_acc: 0.7541\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6503 - acc: 0.7366 - val_loss: 0.5974 - val_acc: 0.7564\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.59519 to 0.59471, saving model to best.model\n",
      "1s - loss: 0.6496 - acc: 0.7380 - val_loss: 0.5947 - val_acc: 0.7559\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.59471 to 0.59353, saving model to best.model\n",
      "1s - loss: 0.6479 - acc: 0.7391 - val_loss: 0.5935 - val_acc: 0.7587\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.59353 to 0.59239, saving model to best.model\n",
      "1s - loss: 0.6462 - acc: 0.7380 - val_loss: 0.5924 - val_acc: 0.7577\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.59239 to 0.59234, saving model to best.model\n",
      "1s - loss: 0.6470 - acc: 0.7387 - val_loss: 0.5923 - val_acc: 0.7605\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6484 - acc: 0.7362 - val_loss: 0.5951 - val_acc: 0.7543\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.86098, saving model to best.model\n",
      "1s - loss: 0.9157 - acc: 0.6291 - val_loss: 0.8610 - val_acc: 0.6479\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.86098 to 0.85754, saving model to best.model\n",
      "1s - loss: 0.8576 - acc: 0.6603 - val_loss: 0.8575 - val_acc: 0.6479\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.85754 to 0.85719, saving model to best.model\n",
      "1s - loss: 0.8496 - acc: 0.6606 - val_loss: 0.8572 - val_acc: 0.6479\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.85719 to 0.85434, saving model to best.model\n",
      "1s - loss: 0.8476 - acc: 0.6606 - val_loss: 0.8543 - val_acc: 0.6479\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.85434 to 0.84642, saving model to best.model\n",
      "1s - loss: 0.8428 - acc: 0.6606 - val_loss: 0.8464 - val_acc: 0.6479\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.84642 to 0.83954, saving model to best.model\n",
      "1s - loss: 0.8335 - acc: 0.6606 - val_loss: 0.8395 - val_acc: 0.6479\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83954 to 0.83638, saving model to best.model\n",
      "1s - loss: 0.8314 - acc: 0.6605 - val_loss: 0.8364 - val_acc: 0.6479\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.83638 to 0.83547, saving model to best.model\n",
      "1s - loss: 0.8280 - acc: 0.6606 - val_loss: 0.8355 - val_acc: 0.6479\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.83547 to 0.83423, saving model to best.model\n",
      "1s - loss: 0.8259 - acc: 0.6606 - val_loss: 0.8342 - val_acc: 0.6479\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8256 - acc: 0.6603 - val_loss: 0.8346 - val_acc: 0.6479\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.83423 to 0.83361, saving model to best.model\n",
      "1s - loss: 0.8241 - acc: 0.6605 - val_loss: 0.8336 - val_acc: 0.6479\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8241 - acc: 0.6605 - val_loss: 0.8337 - val_acc: 0.6479\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.83361 to 0.83273, saving model to best.model\n",
      "1s - loss: 0.8222 - acc: 0.6606 - val_loss: 0.8327 - val_acc: 0.6479\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8213 - acc: 0.6605 - val_loss: 0.8328 - val_acc: 0.6479\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.83273 to 0.83144, saving model to best.model\n",
      "1s - loss: 0.8203 - acc: 0.6606 - val_loss: 0.8314 - val_acc: 0.6479\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.83144 to 0.83132, saving model to best.model\n",
      "1s - loss: 0.8208 - acc: 0.6607 - val_loss: 0.8313 - val_acc: 0.6479\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "1s - loss: 0.8199 - acc: 0.6606 - val_loss: 0.8313 - val_acc: 0.6479\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.83132 to 0.83009, saving model to best.model\n",
      "1s - loss: 0.8190 - acc: 0.6606 - val_loss: 0.8301 - val_acc: 0.6479\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.83009 to 0.82985, saving model to best.model\n",
      "1s - loss: 0.8181 - acc: 0.6604 - val_loss: 0.8299 - val_acc: 0.6479\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82985 to 0.82880, saving model to best.model\n",
      "1s - loss: 0.8181 - acc: 0.6605 - val_loss: 0.8288 - val_acc: 0.6479\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82880 to 0.82750, saving model to best.model\n",
      "1s - loss: 0.8186 - acc: 0.6606 - val_loss: 0.8275 - val_acc: 0.6479\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82750 to 0.82721, saving model to best.model\n",
      "1s - loss: 0.8170 - acc: 0.6606 - val_loss: 0.8272 - val_acc: 0.6479\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.82721 to 0.82539, saving model to best.model\n",
      "1s - loss: 0.8152 - acc: 0.6612 - val_loss: 0.8254 - val_acc: 0.6482\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.82539 to 0.82452, saving model to best.model\n",
      "1s - loss: 0.8144 - acc: 0.6611 - val_loss: 0.8245 - val_acc: 0.6489\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.82452 to 0.82444, saving model to best.model\n",
      "1s - loss: 0.8139 - acc: 0.6628 - val_loss: 0.8244 - val_acc: 0.6480\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.82444 to 0.82244, saving model to best.model\n",
      "1s - loss: 0.8122 - acc: 0.6632 - val_loss: 0.8224 - val_acc: 0.6529\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.82244 to 0.81949, saving model to best.model\n",
      "1s - loss: 0.8104 - acc: 0.6635 - val_loss: 0.8195 - val_acc: 0.6557\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81949 to 0.81769, saving model to best.model\n",
      "1s - loss: 0.8091 - acc: 0.6648 - val_loss: 0.8177 - val_acc: 0.6574\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "1s - loss: 0.8086 - acc: 0.6628 - val_loss: 0.8191 - val_acc: 0.6513\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81769 to 0.81517, saving model to best.model\n",
      "1s - loss: 0.8073 - acc: 0.6634 - val_loss: 0.8152 - val_acc: 0.6546\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.81517 to 0.81359, saving model to best.model\n",
      "1s - loss: 0.8059 - acc: 0.6665 - val_loss: 0.8136 - val_acc: 0.6559\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.81359 to 0.81284, saving model to best.model\n",
      "1s - loss: 0.8052 - acc: 0.6654 - val_loss: 0.8128 - val_acc: 0.6544\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.81284 to 0.80997, saving model to best.model\n",
      "1s - loss: 0.8032 - acc: 0.6652 - val_loss: 0.8100 - val_acc: 0.6560\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80997 to 0.80712, saving model to best.model\n",
      "1s - loss: 0.8002 - acc: 0.6684 - val_loss: 0.8071 - val_acc: 0.6609\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80712 to 0.80485, saving model to best.model\n",
      "1s - loss: 0.7986 - acc: 0.6671 - val_loss: 0.8049 - val_acc: 0.6625\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80485 to 0.80392, saving model to best.model\n",
      "1s - loss: 0.7984 - acc: 0.6675 - val_loss: 0.8039 - val_acc: 0.6629\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.80392 to 0.80185, saving model to best.model\n",
      "1s - loss: 0.7980 - acc: 0.6687 - val_loss: 0.8019 - val_acc: 0.6648\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.80185 to 0.79984, saving model to best.model\n",
      "1s - loss: 0.7952 - acc: 0.6702 - val_loss: 0.7998 - val_acc: 0.6623\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79984 to 0.79842, saving model to best.model\n",
      "1s - loss: 0.7952 - acc: 0.6697 - val_loss: 0.7984 - val_acc: 0.6666\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79842 to 0.79676, saving model to best.model\n",
      "1s - loss: 0.7932 - acc: 0.6688 - val_loss: 0.7968 - val_acc: 0.6635\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79676 to 0.79368, saving model to best.model\n",
      "1s - loss: 0.7916 - acc: 0.6721 - val_loss: 0.7937 - val_acc: 0.6681\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.7888 - acc: 0.6703 - val_loss: 0.7941 - val_acc: 0.6611\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.79368 to 0.79025, saving model to best.model\n",
      "1s - loss: 0.7875 - acc: 0.6718 - val_loss: 0.7902 - val_acc: 0.6685\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.79025 to 0.78894, saving model to best.model\n",
      "1s - loss: 0.7887 - acc: 0.6700 - val_loss: 0.7889 - val_acc: 0.6703\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "1s - loss: 0.7851 - acc: 0.6729 - val_loss: 0.7890 - val_acc: 0.6644\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78894 to 0.78498, saving model to best.model\n",
      "1s - loss: 0.7832 - acc: 0.6734 - val_loss: 0.7850 - val_acc: 0.6713\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.78498 to 0.78418, saving model to best.model\n",
      "1s - loss: 0.7832 - acc: 0.6740 - val_loss: 0.7842 - val_acc: 0.6703\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.78418 to 0.78146, saving model to best.model\n",
      "1s - loss: 0.7834 - acc: 0.6742 - val_loss: 0.7815 - val_acc: 0.6738\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.78146 to 0.77766, saving model to best.model\n",
      "1s - loss: 0.7805 - acc: 0.6747 - val_loss: 0.7777 - val_acc: 0.6759\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77766 to 0.77568, saving model to best.model\n",
      "1s - loss: 0.7796 - acc: 0.6742 - val_loss: 0.7757 - val_acc: 0.6770\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.77568 to 0.77319, saving model to best.model\n",
      "1s - loss: 0.7766 - acc: 0.6771 - val_loss: 0.7732 - val_acc: 0.6755\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.77319 to 0.77162, saving model to best.model\n",
      "1s - loss: 0.7735 - acc: 0.6794 - val_loss: 0.7716 - val_acc: 0.6774\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.77162 to 0.77064, saving model to best.model\n",
      "1s - loss: 0.7740 - acc: 0.6777 - val_loss: 0.7706 - val_acc: 0.6756\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.77064 to 0.76707, saving model to best.model\n",
      "1s - loss: 0.7725 - acc: 0.6786 - val_loss: 0.7671 - val_acc: 0.6793\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "1s - loss: 0.7725 - acc: 0.6773 - val_loss: 0.7672 - val_acc: 0.6754\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76707 to 0.76345, saving model to best.model\n",
      "1s - loss: 0.7692 - acc: 0.6796 - val_loss: 0.7634 - val_acc: 0.6793\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.76345 to 0.76108, saving model to best.model\n",
      "0s - loss: 0.7682 - acc: 0.6782 - val_loss: 0.7611 - val_acc: 0.6810\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.76108 to 0.75626, saving model to best.model\n",
      "1s - loss: 0.7653 - acc: 0.6825 - val_loss: 0.7563 - val_acc: 0.6838\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75626 to 0.75320, saving model to best.model\n",
      "1s - loss: 0.7641 - acc: 0.6811 - val_loss: 0.7532 - val_acc: 0.6847\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.75320 to 0.75155, saving model to best.model\n",
      "1s - loss: 0.7634 - acc: 0.6830 - val_loss: 0.7516 - val_acc: 0.6858\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss did not improve\n",
      "1s - loss: 0.7630 - acc: 0.6837 - val_loss: 0.7541 - val_acc: 0.6789\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.75155 to 0.74736, saving model to best.model\n",
      "1s - loss: 0.7579 - acc: 0.6824 - val_loss: 0.7474 - val_acc: 0.6835\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74736 to 0.74528, saving model to best.model\n",
      "1s - loss: 0.7602 - acc: 0.6828 - val_loss: 0.7453 - val_acc: 0.6844\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.74528 to 0.74509, saving model to best.model\n",
      "1s - loss: 0.7548 - acc: 0.6849 - val_loss: 0.7451 - val_acc: 0.6813\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.74509 to 0.73803, saving model to best.model\n",
      "0s - loss: 0.7523 - acc: 0.6893 - val_loss: 0.7380 - val_acc: 0.6857\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.7542 - acc: 0.6873 - val_loss: 0.7415 - val_acc: 0.6849\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73803 to 0.73190, saving model to best.model\n",
      "1s - loss: 0.7502 - acc: 0.6894 - val_loss: 0.7319 - val_acc: 0.6934\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73190 to 0.73144, saving model to best.model\n",
      "1s - loss: 0.7484 - acc: 0.6906 - val_loss: 0.7314 - val_acc: 0.6896\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.73144 to 0.72939, saving model to best.model\n",
      "0s - loss: 0.7475 - acc: 0.6880 - val_loss: 0.7294 - val_acc: 0.6896\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72939 to 0.72754, saving model to best.model\n",
      "1s - loss: 0.7461 - acc: 0.6907 - val_loss: 0.7275 - val_acc: 0.6892\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72754 to 0.72535, saving model to best.model\n",
      "1s - loss: 0.7423 - acc: 0.6902 - val_loss: 0.7254 - val_acc: 0.6956\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72535 to 0.72215, saving model to best.model\n",
      "1s - loss: 0.7427 - acc: 0.6925 - val_loss: 0.7222 - val_acc: 0.6945\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7406 - acc: 0.6917 - val_loss: 0.7228 - val_acc: 0.6910\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.72215 to 0.71780, saving model to best.model\n",
      "1s - loss: 0.7401 - acc: 0.6938 - val_loss: 0.7178 - val_acc: 0.7018\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "1s - loss: 0.7368 - acc: 0.6936 - val_loss: 0.7180 - val_acc: 0.6933\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71780 to 0.71336, saving model to best.model\n",
      "1s - loss: 0.7338 - acc: 0.6952 - val_loss: 0.7134 - val_acc: 0.6989\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.71336 to 0.71304, saving model to best.model\n",
      "1s - loss: 0.7343 - acc: 0.6969 - val_loss: 0.7130 - val_acc: 0.6958\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.71304 to 0.70715, saving model to best.model\n",
      "1s - loss: 0.7312 - acc: 0.6962 - val_loss: 0.7072 - val_acc: 0.7027\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7318 - acc: 0.6986 - val_loss: 0.7086 - val_acc: 0.6996\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70715 to 0.70360, saving model to best.model\n",
      "1s - loss: 0.7271 - acc: 0.6991 - val_loss: 0.7036 - val_acc: 0.7105\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70360 to 0.69985, saving model to best.model\n",
      "1s - loss: 0.7286 - acc: 0.6969 - val_loss: 0.6999 - val_acc: 0.7109\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7301 - acc: 0.6977 - val_loss: 0.7039 - val_acc: 0.7019\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69985 to 0.69811, saving model to best.model\n",
      "1s - loss: 0.7249 - acc: 0.7010 - val_loss: 0.6981 - val_acc: 0.7103\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69811 to 0.69763, saving model to best.model\n",
      "0s - loss: 0.7262 - acc: 0.7011 - val_loss: 0.6976 - val_acc: 0.7083\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7229 - acc: 0.7036 - val_loss: 0.6981 - val_acc: 0.7055\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.69763 to 0.69271, saving model to best.model\n",
      "1s - loss: 0.7232 - acc: 0.7008 - val_loss: 0.6927 - val_acc: 0.7110\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7204 - acc: 0.7022 - val_loss: 0.6933 - val_acc: 0.7105\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.7189 - acc: 0.7041 - val_loss: 0.6933 - val_acc: 0.7104\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.69271 to 0.69198, saving model to best.model\n",
      "0s - loss: 0.7187 - acc: 0.7018 - val_loss: 0.6920 - val_acc: 0.7156\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69198 to 0.68992, saving model to best.model\n",
      "1s - loss: 0.7163 - acc: 0.7041 - val_loss: 0.6899 - val_acc: 0.7118\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68992 to 0.68845, saving model to best.model\n",
      "1s - loss: 0.7167 - acc: 0.7042 - val_loss: 0.6884 - val_acc: 0.7096\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68845 to 0.68658, saving model to best.model\n",
      "1s - loss: 0.7166 - acc: 0.7029 - val_loss: 0.6866 - val_acc: 0.7121\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68658 to 0.68175, saving model to best.model\n",
      "1s - loss: 0.7132 - acc: 0.7074 - val_loss: 0.6817 - val_acc: 0.7159\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68175 to 0.68141, saving model to best.model\n",
      "1s - loss: 0.7113 - acc: 0.7070 - val_loss: 0.6814 - val_acc: 0.7179\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7100 - acc: 0.7082 - val_loss: 0.6832 - val_acc: 0.7124\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7087 - acc: 0.7075 - val_loss: 0.6823 - val_acc: 0.7148\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7084 - acc: 0.7065 - val_loss: 0.6819 - val_acc: 0.7125\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.68141 to 0.68076, saving model to best.model\n",
      "1s - loss: 0.7068 - acc: 0.7076 - val_loss: 0.6808 - val_acc: 0.7177\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.68076 to 0.67711, saving model to best.model\n",
      "1s - loss: 0.7072 - acc: 0.7114 - val_loss: 0.6771 - val_acc: 0.7152\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.67711 to 0.67661, saving model to best.model\n",
      "1s - loss: 0.7071 - acc: 0.7101 - val_loss: 0.6766 - val_acc: 0.7171\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.67661 to 0.67517, saving model to best.model\n",
      "1s - loss: 0.7050 - acc: 0.7098 - val_loss: 0.6752 - val_acc: 0.7183\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67517 to 0.67048, saving model to best.model\n",
      "1s - loss: 0.7018 - acc: 0.7131 - val_loss: 0.6705 - val_acc: 0.7222\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7026 - acc: 0.7095 - val_loss: 0.6708 - val_acc: 0.7198\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.67048 to 0.66974, saving model to best.model\n",
      "1s - loss: 0.7026 - acc: 0.7095 - val_loss: 0.6697 - val_acc: 0.7212\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.7026 - acc: 0.7114 - val_loss: 0.6707 - val_acc: 0.7222\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.7003 - acc: 0.7131 - val_loss: 0.6698 - val_acc: 0.7230\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.6987 - acc: 0.7131 - val_loss: 0.6716 - val_acc: 0.7191\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.66974 to 0.66751, saving model to best.model\n",
      "1s - loss: 0.6995 - acc: 0.7112 - val_loss: 0.6675 - val_acc: 0.7254\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66751 to 0.66731, saving model to best.model\n",
      "1s - loss: 0.6988 - acc: 0.7118 - val_loss: 0.6673 - val_acc: 0.7185\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66731 to 0.66527, saving model to best.model\n",
      "1s - loss: 0.6974 - acc: 0.7146 - val_loss: 0.6653 - val_acc: 0.7222\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.66527 to 0.66285, saving model to best.model\n",
      "1s - loss: 0.6973 - acc: 0.7117 - val_loss: 0.6628 - val_acc: 0.7227\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.66285 to 0.66265, saving model to best.model\n",
      "1s - loss: 0.6947 - acc: 0.7130 - val_loss: 0.6627 - val_acc: 0.7240\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.66265 to 0.66066, saving model to best.model\n",
      "1s - loss: 0.6954 - acc: 0.7163 - val_loss: 0.6607 - val_acc: 0.7251\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6969 - acc: 0.7128 - val_loss: 0.6622 - val_acc: 0.7242\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.66066 to 0.65751, saving model to best.model\n",
      "1s - loss: 0.6945 - acc: 0.7156 - val_loss: 0.6575 - val_acc: 0.7313\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65751 to 0.65709, saving model to best.model\n",
      "1s - loss: 0.6915 - acc: 0.7163 - val_loss: 0.6571 - val_acc: 0.7252\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.65709 to 0.65701, saving model to best.model\n",
      "1s - loss: 0.6889 - acc: 0.7174 - val_loss: 0.6570 - val_acc: 0.7256\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6904 - acc: 0.7169 - val_loss: 0.6578 - val_acc: 0.7249\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6899 - acc: 0.7150 - val_loss: 0.6574 - val_acc: 0.7249\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65701 to 0.65458, saving model to best.model\n",
      "1s - loss: 0.6879 - acc: 0.7177 - val_loss: 0.6546 - val_acc: 0.7274\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6876 - acc: 0.7188 - val_loss: 0.6550 - val_acc: 0.7323\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "1s - loss: 0.6903 - acc: 0.7188 - val_loss: 0.6575 - val_acc: 0.7232\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6864 - acc: 0.7188 - val_loss: 0.6548 - val_acc: 0.7263\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.65458 to 0.65055, saving model to best.model\n",
      "1s - loss: 0.6833 - acc: 0.7201 - val_loss: 0.6506 - val_acc: 0.7292\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.65055 to 0.64971, saving model to best.model\n",
      "1s - loss: 0.6852 - acc: 0.7187 - val_loss: 0.6497 - val_acc: 0.7286\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.64971 to 0.64481, saving model to best.model\n",
      "1s - loss: 0.6813 - acc: 0.7211 - val_loss: 0.6448 - val_acc: 0.7331\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6831 - acc: 0.7212 - val_loss: 0.6510 - val_acc: 0.7292\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6841 - acc: 0.7195 - val_loss: 0.6520 - val_acc: 0.7287\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "1s - loss: 0.6824 - acc: 0.7189 - val_loss: 0.6459 - val_acc: 0.7315\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64481 to 0.64348, saving model to best.model\n",
      "1s - loss: 0.6822 - acc: 0.7196 - val_loss: 0.6435 - val_acc: 0.7337\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6833 - acc: 0.7197 - val_loss: 0.6470 - val_acc: 0.7296\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6816 - acc: 0.7218 - val_loss: 0.6445 - val_acc: 0.7322\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6803 - acc: 0.7222 - val_loss: 0.6463 - val_acc: 0.7319\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6774 - acc: 0.7219 - val_loss: 0.6452 - val_acc: 0.7333\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.64348 to 0.64301, saving model to best.model\n",
      "1s - loss: 0.6773 - acc: 0.7233 - val_loss: 0.6430 - val_acc: 0.7341\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64301 to 0.64182, saving model to best.model\n",
      "1s - loss: 0.6787 - acc: 0.7237 - val_loss: 0.6418 - val_acc: 0.7335\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "1s - loss: 0.6751 - acc: 0.7248 - val_loss: 0.6441 - val_acc: 0.7329\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6757 - acc: 0.7223 - val_loss: 0.6443 - val_acc: 0.7328\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6783 - acc: 0.7236 - val_loss: 0.6421 - val_acc: 0.7347\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.64182 to 0.64136, saving model to best.model\n",
      "1s - loss: 0.6711 - acc: 0.7273 - val_loss: 0.6414 - val_acc: 0.7349\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.64136 to 0.63867, saving model to best.model\n",
      "1s - loss: 0.6744 - acc: 0.7234 - val_loss: 0.6387 - val_acc: 0.7378\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.6724 - acc: 0.7259 - val_loss: 0.6391 - val_acc: 0.7371\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "1s - loss: 0.6742 - acc: 0.7243 - val_loss: 0.6391 - val_acc: 0.7348\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.63867 to 0.63612, saving model to best.model\n",
      "1s - loss: 0.6742 - acc: 0.7240 - val_loss: 0.6361 - val_acc: 0.7381\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63612 to 0.63558, saving model to best.model\n",
      "1s - loss: 0.6721 - acc: 0.7268 - val_loss: 0.6356 - val_acc: 0.7370\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6716 - acc: 0.7255 - val_loss: 0.6375 - val_acc: 0.7357\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63558 to 0.63396, saving model to best.model\n",
      "1s - loss: 0.6695 - acc: 0.7238 - val_loss: 0.6340 - val_acc: 0.7389\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6715 - acc: 0.7255 - val_loss: 0.6344 - val_acc: 0.7405\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6681 - acc: 0.7268 - val_loss: 0.6354 - val_acc: 0.7363\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6673 - acc: 0.7271 - val_loss: 0.6346 - val_acc: 0.7361\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6671 - acc: 0.7290 - val_loss: 0.6359 - val_acc: 0.7348\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.63396 to 0.63005, saving model to best.model\n",
      "1s - loss: 0.6657 - acc: 0.7302 - val_loss: 0.6301 - val_acc: 0.7399\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6678 - acc: 0.7265 - val_loss: 0.6319 - val_acc: 0.7397\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.63005 to 0.62987, saving model to best.model\n",
      "1s - loss: 0.6664 - acc: 0.7258 - val_loss: 0.6299 - val_acc: 0.7393\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6667 - acc: 0.7276 - val_loss: 0.6315 - val_acc: 0.7385\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6662 - acc: 0.7288 - val_loss: 0.6309 - val_acc: 0.7393\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.62987 to 0.62704, saving model to best.model\n",
      "1s - loss: 0.6633 - acc: 0.7297 - val_loss: 0.6270 - val_acc: 0.7415\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6643 - acc: 0.7267 - val_loss: 0.6327 - val_acc: 0.7388\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62704 to 0.62688, saving model to best.model\n",
      "1s - loss: 0.6633 - acc: 0.7309 - val_loss: 0.6269 - val_acc: 0.7440\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.62688 to 0.62544, saving model to best.model\n",
      "1s - loss: 0.6632 - acc: 0.7311 - val_loss: 0.6254 - val_acc: 0.7419\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "1s - loss: 0.6634 - acc: 0.7282 - val_loss: 0.6280 - val_acc: 0.7411\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62544 to 0.62521, saving model to best.model\n",
      "1s - loss: 0.6608 - acc: 0.7297 - val_loss: 0.6252 - val_acc: 0.7423\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62521 to 0.62473, saving model to best.model\n",
      "1s - loss: 0.6625 - acc: 0.7279 - val_loss: 0.6247 - val_acc: 0.7426\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6590 - acc: 0.7304 - val_loss: 0.6251 - val_acc: 0.7418\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6632 - acc: 0.7306 - val_loss: 0.6274 - val_acc: 0.7405\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.62473 to 0.62382, saving model to best.model\n",
      "1s - loss: 0.6599 - acc: 0.7278 - val_loss: 0.6238 - val_acc: 0.7426\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6580 - acc: 0.7299 - val_loss: 0.6302 - val_acc: 0.7368\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.62382 to 0.62217, saving model to best.model\n",
      "1s - loss: 0.6579 - acc: 0.7316 - val_loss: 0.6222 - val_acc: 0.7430\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6599 - acc: 0.7324 - val_loss: 0.6236 - val_acc: 0.7446\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6600 - acc: 0.7290 - val_loss: 0.6245 - val_acc: 0.7403\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6583 - acc: 0.7312 - val_loss: 0.6241 - val_acc: 0.7440\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.62217 to 0.62014, saving model to best.model\n",
      "1s - loss: 0.6572 - acc: 0.7308 - val_loss: 0.6201 - val_acc: 0.7432\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6567 - acc: 0.7309 - val_loss: 0.6222 - val_acc: 0.7422\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6540 - acc: 0.7343 - val_loss: 0.6213 - val_acc: 0.7420\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6565 - acc: 0.7320 - val_loss: 0.6229 - val_acc: 0.7403\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6535 - acc: 0.7321 - val_loss: 0.6203 - val_acc: 0.7443\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.62014 to 0.61953, saving model to best.model\n",
      "1s - loss: 0.6538 - acc: 0.7327 - val_loss: 0.6195 - val_acc: 0.7423\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61953 to 0.61947, saving model to best.model\n",
      "1s - loss: 0.6578 - acc: 0.7322 - val_loss: 0.6195 - val_acc: 0.7446\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6544 - acc: 0.7331 - val_loss: 0.6222 - val_acc: 0.7420\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61947 to 0.61791, saving model to best.model\n",
      "1s - loss: 0.6529 - acc: 0.7338 - val_loss: 0.6179 - val_acc: 0.7446\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6526 - acc: 0.7346 - val_loss: 0.6179 - val_acc: 0.7458\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.6503 - acc: 0.7326 - val_loss: 0.6201 - val_acc: 0.7420\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.61791 to 0.61456, saving model to best.model\n",
      "0s - loss: 0.6526 - acc: 0.7333 - val_loss: 0.6146 - val_acc: 0.7479\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6496 - acc: 0.7350 - val_loss: 0.6173 - val_acc: 0.7436\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6511 - acc: 0.7362 - val_loss: 0.6149 - val_acc: 0.7477\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.61456 to 0.61374, saving model to best.model\n",
      "0s - loss: 0.6503 - acc: 0.7323 - val_loss: 0.6137 - val_acc: 0.7488\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.61374 to 0.61253, saving model to best.model\n",
      "0s - loss: 0.6463 - acc: 0.7359 - val_loss: 0.6125 - val_acc: 0.7480\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.61253 to 0.61168, saving model to best.model\n",
      "0s - loss: 0.6493 - acc: 0.7357 - val_loss: 0.6117 - val_acc: 0.7509\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6513 - acc: 0.7360 - val_loss: 0.6127 - val_acc: 0.7499\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6533 - acc: 0.7339 - val_loss: 0.6163 - val_acc: 0.7437\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6479 - acc: 0.7362 - val_loss: 0.6143 - val_acc: 0.7450\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6489 - acc: 0.7344 - val_loss: 0.6120 - val_acc: 0.7504\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6520 - acc: 0.7311 - val_loss: 0.6145 - val_acc: 0.7468\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6477 - acc: 0.7370 - val_loss: 0.6129 - val_acc: 0.7457\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.61168 to 0.61076, saving model to best.model\n",
      "1s - loss: 0.6433 - acc: 0.7373 - val_loss: 0.6108 - val_acc: 0.7500\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.61076 to 0.60797, saving model to best.model\n",
      "1s - loss: 0.6478 - acc: 0.7358 - val_loss: 0.6080 - val_acc: 0.7511\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6461 - acc: 0.7370 - val_loss: 0.6126 - val_acc: 0.7450\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6469 - acc: 0.7379 - val_loss: 0.6107 - val_acc: 0.7492\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6452 - acc: 0.7373 - val_loss: 0.6094 - val_acc: 0.7486\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.6473 - acc: 0.7369 - val_loss: 0.6125 - val_acc: 0.7459\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84505, saving model to best.model\n",
      "1s - loss: 0.8885 - acc: 0.6451 - val_loss: 0.8450 - val_acc: 0.6605\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84505 to 0.84233, saving model to best.model\n",
      "1s - loss: 0.8529 - acc: 0.6597 - val_loss: 0.8423 - val_acc: 0.6605\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84233 to 0.84012, saving model to best.model\n",
      "1s - loss: 0.8483 - acc: 0.6597 - val_loss: 0.8401 - val_acc: 0.6605\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84012 to 0.83357, saving model to best.model\n",
      "1s - loss: 0.8428 - acc: 0.6597 - val_loss: 0.8336 - val_acc: 0.6605\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83357 to 0.82542, saving model to best.model\n",
      "1s - loss: 0.8382 - acc: 0.6597 - val_loss: 0.8254 - val_acc: 0.6605\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.82542 to 0.82319, saving model to best.model\n",
      "1s - loss: 0.8325 - acc: 0.6597 - val_loss: 0.8232 - val_acc: 0.6605\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "1s - loss: 0.8315 - acc: 0.6597 - val_loss: 0.8238 - val_acc: 0.6605\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82319 to 0.81990, saving model to best.model\n",
      "0s - loss: 0.8302 - acc: 0.6597 - val_loss: 0.8199 - val_acc: 0.6605\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81990 to 0.81921, saving model to best.model\n",
      "0s - loss: 0.8284 - acc: 0.6597 - val_loss: 0.8192 - val_acc: 0.6605\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.8274 - acc: 0.6597 - val_loss: 0.8197 - val_acc: 0.6605\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81921 to 0.81848, saving model to best.model\n",
      "0s - loss: 0.8264 - acc: 0.6597 - val_loss: 0.8185 - val_acc: 0.6605\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81848 to 0.81818, saving model to best.model\n",
      "1s - loss: 0.8255 - acc: 0.6597 - val_loss: 0.8182 - val_acc: 0.6605\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81818 to 0.81726, saving model to best.model\n",
      "1s - loss: 0.8251 - acc: 0.6598 - val_loss: 0.8173 - val_acc: 0.6605\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.8240 - acc: 0.6598 - val_loss: 0.8188 - val_acc: 0.6605\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.81726 to 0.81585, saving model to best.model\n",
      "1s - loss: 0.8233 - acc: 0.6598 - val_loss: 0.8159 - val_acc: 0.6605\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "1s - loss: 0.8226 - acc: 0.6598 - val_loss: 0.8170 - val_acc: 0.6605\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81585 to 0.81460, saving model to best.model\n",
      "1s - loss: 0.8209 - acc: 0.6607 - val_loss: 0.8146 - val_acc: 0.6603\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81460 to 0.81350, saving model to best.model\n",
      "1s - loss: 0.8214 - acc: 0.6611 - val_loss: 0.8135 - val_acc: 0.6598\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81350 to 0.81319, saving model to best.model\n",
      "1s - loss: 0.8208 - acc: 0.6603 - val_loss: 0.8132 - val_acc: 0.6602\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "1s - loss: 0.8195 - acc: 0.6604 - val_loss: 0.8148 - val_acc: 0.6619\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81319 to 0.81182, saving model to best.model\n",
      "1s - loss: 0.8197 - acc: 0.6609 - val_loss: 0.8118 - val_acc: 0.6636\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81182 to 0.81161, saving model to best.model\n",
      "1s - loss: 0.8174 - acc: 0.6631 - val_loss: 0.8116 - val_acc: 0.6604\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.8166 - acc: 0.6633 - val_loss: 0.8140 - val_acc: 0.6637\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81161 to 0.80887, saving model to best.model\n",
      "1s - loss: 0.8161 - acc: 0.6632 - val_loss: 0.8089 - val_acc: 0.6652\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80887 to 0.80766, saving model to best.model\n",
      "1s - loss: 0.8147 - acc: 0.6634 - val_loss: 0.8077 - val_acc: 0.6672\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80766 to 0.80745, saving model to best.model\n",
      "1s - loss: 0.8134 - acc: 0.6646 - val_loss: 0.8075 - val_acc: 0.6660\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80745 to 0.80603, saving model to best.model\n",
      "1s - loss: 0.8123 - acc: 0.6654 - val_loss: 0.8060 - val_acc: 0.6659\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80603 to 0.80374, saving model to best.model\n",
      "1s - loss: 0.8094 - acc: 0.6661 - val_loss: 0.8037 - val_acc: 0.6681\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80374 to 0.80219, saving model to best.model\n",
      "1s - loss: 0.8081 - acc: 0.6672 - val_loss: 0.8022 - val_acc: 0.6697\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80219 to 0.80133, saving model to best.model\n",
      "1s - loss: 0.8066 - acc: 0.6668 - val_loss: 0.8013 - val_acc: 0.6707\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80133 to 0.80054, saving model to best.model\n",
      "1s - loss: 0.8070 - acc: 0.6669 - val_loss: 0.8005 - val_acc: 0.6651\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80054 to 0.79790, saving model to best.model\n",
      "1s - loss: 0.8061 - acc: 0.6661 - val_loss: 0.7979 - val_acc: 0.6689\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79790 to 0.79584, saving model to best.model\n",
      "1s - loss: 0.8018 - acc: 0.6681 - val_loss: 0.7958 - val_acc: 0.6683\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79584 to 0.79253, saving model to best.model\n",
      "1s - loss: 0.8007 - acc: 0.6679 - val_loss: 0.7925 - val_acc: 0.6725\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79253 to 0.79220, saving model to best.model\n",
      "1s - loss: 0.7997 - acc: 0.6694 - val_loss: 0.7922 - val_acc: 0.6730\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79220 to 0.78872, saving model to best.model\n",
      "1s - loss: 0.7968 - acc: 0.6692 - val_loss: 0.7887 - val_acc: 0.6728\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.78872 to 0.78666, saving model to best.model\n",
      "1s - loss: 0.7967 - acc: 0.6712 - val_loss: 0.7867 - val_acc: 0.6762\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.78666 to 0.78475, saving model to best.model\n",
      "1s - loss: 0.7948 - acc: 0.6700 - val_loss: 0.7847 - val_acc: 0.6762\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78475 to 0.78301, saving model to best.model\n",
      "1s - loss: 0.7936 - acc: 0.6722 - val_loss: 0.7830 - val_acc: 0.6712\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78301 to 0.77947, saving model to best.model\n",
      "1s - loss: 0.7910 - acc: 0.6734 - val_loss: 0.7795 - val_acc: 0.6792\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.77947 to 0.77726, saving model to best.model\n",
      "1s - loss: 0.7880 - acc: 0.6741 - val_loss: 0.7773 - val_acc: 0.6778\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.77726 to 0.77597, saving model to best.model\n",
      "1s - loss: 0.7884 - acc: 0.6738 - val_loss: 0.7760 - val_acc: 0.6809\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77597 to 0.77520, saving model to best.model\n",
      "1s - loss: 0.7855 - acc: 0.6729 - val_loss: 0.7752 - val_acc: 0.6780\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77520 to 0.77189, saving model to best.model\n",
      "1s - loss: 0.7833 - acc: 0.6770 - val_loss: 0.7719 - val_acc: 0.6811\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77189 to 0.76802, saving model to best.model\n",
      "1s - loss: 0.7802 - acc: 0.6765 - val_loss: 0.7680 - val_acc: 0.6826\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "1s - loss: 0.7823 - acc: 0.6787 - val_loss: 0.7691 - val_acc: 0.6801\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.76802 to 0.76459, saving model to best.model\n",
      "1s - loss: 0.7798 - acc: 0.6776 - val_loss: 0.7646 - val_acc: 0.6843\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76459 to 0.76436, saving model to best.model\n",
      "1s - loss: 0.7789 - acc: 0.6801 - val_loss: 0.7644 - val_acc: 0.6849\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76436 to 0.76072, saving model to best.model\n",
      "1s - loss: 0.7733 - acc: 0.6793 - val_loss: 0.7607 - val_acc: 0.6851\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.76072 to 0.75849, saving model to best.model\n",
      "1s - loss: 0.7729 - acc: 0.6797 - val_loss: 0.7585 - val_acc: 0.6855\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.75849 to 0.75743, saving model to best.model\n",
      "1s - loss: 0.7742 - acc: 0.6788 - val_loss: 0.7574 - val_acc: 0.6881\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75743 to 0.75685, saving model to best.model\n",
      "1s - loss: 0.7718 - acc: 0.6813 - val_loss: 0.7569 - val_acc: 0.6876\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75685 to 0.75008, saving model to best.model\n",
      "0s - loss: 0.7668 - acc: 0.6834 - val_loss: 0.7501 - val_acc: 0.6916\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75008 to 0.74801, saving model to best.model\n",
      "1s - loss: 0.7667 - acc: 0.6843 - val_loss: 0.7480 - val_acc: 0.6937\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "1s - loss: 0.7649 - acc: 0.6847 - val_loss: 0.7480 - val_acc: 0.6891\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "1s - loss: 0.7643 - acc: 0.6849 - val_loss: 0.7482 - val_acc: 0.6898\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74801 to 0.74320, saving model to best.model\n",
      "1s - loss: 0.7626 - acc: 0.6868 - val_loss: 0.7432 - val_acc: 0.6963\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74320 to 0.74175, saving model to best.model\n",
      "1s - loss: 0.7598 - acc: 0.6878 - val_loss: 0.7417 - val_acc: 0.6961\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74175 to 0.73681, saving model to best.model\n",
      "1s - loss: 0.7581 - acc: 0.6865 - val_loss: 0.7368 - val_acc: 0.6978\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73681 to 0.73659, saving model to best.model\n",
      "1s - loss: 0.7575 - acc: 0.6886 - val_loss: 0.7366 - val_acc: 0.6974\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73659 to 0.73560, saving model to best.model\n",
      "0s - loss: 0.7529 - acc: 0.6896 - val_loss: 0.7356 - val_acc: 0.6995\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73560 to 0.73212, saving model to best.model\n",
      "1s - loss: 0.7523 - acc: 0.6908 - val_loss: 0.7321 - val_acc: 0.6973\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "1s - loss: 0.7501 - acc: 0.6926 - val_loss: 0.7327 - val_acc: 0.7002\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73212 to 0.72666, saving model to best.model\n",
      "1s - loss: 0.7509 - acc: 0.6914 - val_loss: 0.7267 - val_acc: 0.7036\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72666 to 0.72650, saving model to best.model\n",
      "1s - loss: 0.7475 - acc: 0.6926 - val_loss: 0.7265 - val_acc: 0.7046\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72650 to 0.72130, saving model to best.model\n",
      "1s - loss: 0.7463 - acc: 0.6935 - val_loss: 0.7213 - val_acc: 0.7027\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72130 to 0.71777, saving model to best.model\n",
      "1s - loss: 0.7430 - acc: 0.6943 - val_loss: 0.7178 - val_acc: 0.7053\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71777 to 0.71679, saving model to best.model\n",
      "0s - loss: 0.7430 - acc: 0.6969 - val_loss: 0.7168 - val_acc: 0.7054\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71679 to 0.71624, saving model to best.model\n",
      "1s - loss: 0.7427 - acc: 0.6943 - val_loss: 0.7162 - val_acc: 0.7060\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71624 to 0.71410, saving model to best.model\n",
      "1s - loss: 0.7407 - acc: 0.6957 - val_loss: 0.7141 - val_acc: 0.7063\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71410 to 0.71064, saving model to best.model\n",
      "1s - loss: 0.7357 - acc: 0.6979 - val_loss: 0.7106 - val_acc: 0.7090\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.7372 - acc: 0.6969 - val_loss: 0.7117 - val_acc: 0.7061\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71064 to 0.71035, saving model to best.model\n",
      "1s - loss: 0.7377 - acc: 0.6984 - val_loss: 0.7103 - val_acc: 0.7085\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71035 to 0.70749, saving model to best.model\n",
      "1s - loss: 0.7337 - acc: 0.6996 - val_loss: 0.7075 - val_acc: 0.7116\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70749 to 0.70182, saving model to best.model\n",
      "1s - loss: 0.7287 - acc: 0.6993 - val_loss: 0.7018 - val_acc: 0.7111\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.7306 - acc: 0.7006 - val_loss: 0.7031 - val_acc: 0.7118\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70182 to 0.69762, saving model to best.model\n",
      "1s - loss: 0.7259 - acc: 0.7038 - val_loss: 0.6976 - val_acc: 0.7142\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69762 to 0.69664, saving model to best.model\n",
      "1s - loss: 0.7281 - acc: 0.7033 - val_loss: 0.6966 - val_acc: 0.7145\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.7240 - acc: 0.7034 - val_loss: 0.6969 - val_acc: 0.7187\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69664 to 0.69344, saving model to best.model\n",
      "1s - loss: 0.7231 - acc: 0.7054 - val_loss: 0.6934 - val_acc: 0.7163\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69344 to 0.69275, saving model to best.model\n",
      "1s - loss: 0.7240 - acc: 0.7061 - val_loss: 0.6928 - val_acc: 0.7153\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69275 to 0.68992, saving model to best.model\n",
      "1s - loss: 0.7248 - acc: 0.7022 - val_loss: 0.6899 - val_acc: 0.7186\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68992 to 0.68618, saving model to best.model\n",
      "1s - loss: 0.7181 - acc: 0.7072 - val_loss: 0.6862 - val_acc: 0.7190\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68618 to 0.68245, saving model to best.model\n",
      "1s - loss: 0.7154 - acc: 0.7085 - val_loss: 0.6825 - val_acc: 0.7241\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "1s - loss: 0.7191 - acc: 0.7066 - val_loss: 0.6859 - val_acc: 0.7203\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7158 - acc: 0.7074 - val_loss: 0.6844 - val_acc: 0.7185\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "1s - loss: 0.7161 - acc: 0.7068 - val_loss: 0.6849 - val_acc: 0.7187\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68245 to 0.67924, saving model to best.model\n",
      "1s - loss: 0.7156 - acc: 0.7081 - val_loss: 0.6792 - val_acc: 0.7204\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.7149 - acc: 0.7087 - val_loss: 0.6802 - val_acc: 0.7220\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67924 to 0.67899, saving model to best.model\n",
      "0s - loss: 0.7144 - acc: 0.7101 - val_loss: 0.6790 - val_acc: 0.7230\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.67899 to 0.67556, saving model to best.model\n",
      "1s - loss: 0.7124 - acc: 0.7099 - val_loss: 0.6756 - val_acc: 0.7262\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67556 to 0.67438, saving model to best.model\n",
      "1s - loss: 0.7104 - acc: 0.7101 - val_loss: 0.6744 - val_acc: 0.7244\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7094 - acc: 0.7140 - val_loss: 0.6750 - val_acc: 0.7222\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "1s - loss: 0.7096 - acc: 0.7108 - val_loss: 0.6747 - val_acc: 0.7228\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67438 to 0.67408, saving model to best.model\n",
      "1s - loss: 0.7078 - acc: 0.7088 - val_loss: 0.6741 - val_acc: 0.7259\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67408 to 0.66870, saving model to best.model\n",
      "0s - loss: 0.7034 - acc: 0.7134 - val_loss: 0.6687 - val_acc: 0.7266\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7038 - acc: 0.7129 - val_loss: 0.6693 - val_acc: 0.7261\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7054 - acc: 0.7137 - val_loss: 0.6690 - val_acc: 0.7235\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7010 - acc: 0.7148 - val_loss: 0.6695 - val_acc: 0.7245\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66870 to 0.66565, saving model to best.model\n",
      "1s - loss: 0.7042 - acc: 0.7128 - val_loss: 0.6657 - val_acc: 0.7270\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66565 to 0.66449, saving model to best.model\n",
      "1s - loss: 0.7004 - acc: 0.7140 - val_loss: 0.6645 - val_acc: 0.7259\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.6996 - acc: 0.7153 - val_loss: 0.6672 - val_acc: 0.7261\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.6984 - acc: 0.7157 - val_loss: 0.6659 - val_acc: 0.7232\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66449 to 0.66112, saving model to best.model\n",
      "1s - loss: 0.6988 - acc: 0.7153 - val_loss: 0.6611 - val_acc: 0.7269\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "1s - loss: 0.6999 - acc: 0.7150 - val_loss: 0.6642 - val_acc: 0.7255\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.6969 - acc: 0.7162 - val_loss: 0.6619 - val_acc: 0.7282\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66112 to 0.65960, saving model to best.model\n",
      "1s - loss: 0.6937 - acc: 0.7181 - val_loss: 0.6596 - val_acc: 0.7300\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65960 to 0.65918, saving model to best.model\n",
      "1s - loss: 0.6942 - acc: 0.7163 - val_loss: 0.6592 - val_acc: 0.7319\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65918 to 0.65822, saving model to best.model\n",
      "1s - loss: 0.6925 - acc: 0.7193 - val_loss: 0.6582 - val_acc: 0.7289\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6951 - acc: 0.7177 - val_loss: 0.6608 - val_acc: 0.7276\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.65822 to 0.65684, saving model to best.model\n",
      "1s - loss: 0.6929 - acc: 0.7158 - val_loss: 0.6568 - val_acc: 0.7323\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss did not improve\n",
      "1s - loss: 0.6884 - acc: 0.7220 - val_loss: 0.6574 - val_acc: 0.7297\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65684 to 0.65518, saving model to best.model\n",
      "1s - loss: 0.6898 - acc: 0.7185 - val_loss: 0.6552 - val_acc: 0.7323\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65518 to 0.65395, saving model to best.model\n",
      "1s - loss: 0.6883 - acc: 0.7191 - val_loss: 0.6539 - val_acc: 0.7322\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.65395 to 0.65374, saving model to best.model\n",
      "1s - loss: 0.6910 - acc: 0.7199 - val_loss: 0.6537 - val_acc: 0.7341\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65374 to 0.65271, saving model to best.model\n",
      "1s - loss: 0.6900 - acc: 0.7201 - val_loss: 0.6527 - val_acc: 0.7301\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6897 - acc: 0.7203 - val_loss: 0.6542 - val_acc: 0.7296\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65271 to 0.65074, saving model to best.model\n",
      "1s - loss: 0.6879 - acc: 0.7239 - val_loss: 0.6507 - val_acc: 0.7320\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6889 - acc: 0.7220 - val_loss: 0.6555 - val_acc: 0.7279\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "1s - loss: 0.6864 - acc: 0.7210 - val_loss: 0.6537 - val_acc: 0.7256\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65074 to 0.64940, saving model to best.model\n",
      "1s - loss: 0.6852 - acc: 0.7215 - val_loss: 0.6494 - val_acc: 0.7350\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.64940 to 0.64773, saving model to best.model\n",
      "1s - loss: 0.6831 - acc: 0.7228 - val_loss: 0.6477 - val_acc: 0.7348\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6859 - acc: 0.7225 - val_loss: 0.6492 - val_acc: 0.7357\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.64773 to 0.64591, saving model to best.model\n",
      "1s - loss: 0.6817 - acc: 0.7231 - val_loss: 0.6459 - val_acc: 0.7368\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64591 to 0.64544, saving model to best.model\n",
      "1s - loss: 0.6810 - acc: 0.7244 - val_loss: 0.6454 - val_acc: 0.7369\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.64544 to 0.64476, saving model to best.model\n",
      "1s - loss: 0.6832 - acc: 0.7225 - val_loss: 0.6448 - val_acc: 0.7359\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.6804 - acc: 0.7241 - val_loss: 0.6455 - val_acc: 0.7361\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.6779 - acc: 0.7238 - val_loss: 0.6453 - val_acc: 0.7358\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.64476 to 0.64409, saving model to best.model\n",
      "0s - loss: 0.6757 - acc: 0.7250 - val_loss: 0.6441 - val_acc: 0.7344\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64409 to 0.64287, saving model to best.model\n",
      "1s - loss: 0.6797 - acc: 0.7243 - val_loss: 0.6429 - val_acc: 0.7376\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6774 - acc: 0.7246 - val_loss: 0.6431 - val_acc: 0.7342\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.64287 to 0.64171, saving model to best.model\n",
      "1s - loss: 0.6797 - acc: 0.7254 - val_loss: 0.6417 - val_acc: 0.7367\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6787 - acc: 0.7250 - val_loss: 0.6432 - val_acc: 0.7338\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6799 - acc: 0.7253 - val_loss: 0.6448 - val_acc: 0.7355\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.64171 to 0.63986, saving model to best.model\n",
      "1s - loss: 0.6747 - acc: 0.7269 - val_loss: 0.6399 - val_acc: 0.7377\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6740 - acc: 0.7267 - val_loss: 0.6407 - val_acc: 0.7357\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63986 to 0.63977, saving model to best.model\n",
      "1s - loss: 0.6731 - acc: 0.7250 - val_loss: 0.6398 - val_acc: 0.7364\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.63977 to 0.63761, saving model to best.model\n",
      "1s - loss: 0.6722 - acc: 0.7255 - val_loss: 0.6376 - val_acc: 0.7368\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6731 - acc: 0.7279 - val_loss: 0.6398 - val_acc: 0.7379\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6738 - acc: 0.7266 - val_loss: 0.6378 - val_acc: 0.7402\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6736 - acc: 0.7278 - val_loss: 0.6401 - val_acc: 0.7367\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.63761 to 0.63504, saving model to best.model\n",
      "1s - loss: 0.6713 - acc: 0.7247 - val_loss: 0.6350 - val_acc: 0.7404\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63504 to 0.63488, saving model to best.model\n",
      "1s - loss: 0.6721 - acc: 0.7282 - val_loss: 0.6349 - val_acc: 0.7391\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6730 - acc: 0.7254 - val_loss: 0.6382 - val_acc: 0.7393\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6693 - acc: 0.7292 - val_loss: 0.6351 - val_acc: 0.7392\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.63488 to 0.63232, saving model to best.model\n",
      "0s - loss: 0.6688 - acc: 0.7283 - val_loss: 0.6323 - val_acc: 0.7402\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6691 - acc: 0.7300 - val_loss: 0.6324 - val_acc: 0.7393\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6675 - acc: 0.7293 - val_loss: 0.6334 - val_acc: 0.7390\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63232 to 0.63141, saving model to best.model\n",
      "1s - loss: 0.6682 - acc: 0.7299 - val_loss: 0.6314 - val_acc: 0.7405\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.63141 to 0.62991, saving model to best.model\n",
      "1s - loss: 0.6676 - acc: 0.7269 - val_loss: 0.6299 - val_acc: 0.7407\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "1s - loss: 0.6678 - acc: 0.7299 - val_loss: 0.6321 - val_acc: 0.7389\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6663 - acc: 0.7296 - val_loss: 0.6319 - val_acc: 0.7409\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.62991 to 0.62953, saving model to best.model\n",
      "0s - loss: 0.6658 - acc: 0.7312 - val_loss: 0.6295 - val_acc: 0.7392\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62953 to 0.62871, saving model to best.model\n",
      "0s - loss: 0.6645 - acc: 0.7305 - val_loss: 0.6287 - val_acc: 0.7417\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6647 - acc: 0.7284 - val_loss: 0.6306 - val_acc: 0.7377\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62871 to 0.62739, saving model to best.model\n",
      "0s - loss: 0.6625 - acc: 0.7320 - val_loss: 0.6274 - val_acc: 0.7423\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6653 - acc: 0.7289 - val_loss: 0.6300 - val_acc: 0.7432\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6650 - acc: 0.7299 - val_loss: 0.6278 - val_acc: 0.7415\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62739 to 0.62497, saving model to best.model\n",
      "0s - loss: 0.6589 - acc: 0.7336 - val_loss: 0.6250 - val_acc: 0.7434\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6592 - acc: 0.7319 - val_loss: 0.6270 - val_acc: 0.7412\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6647 - acc: 0.7304 - val_loss: 0.6271 - val_acc: 0.7415\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.62497 to 0.62384, saving model to best.model\n",
      "0s - loss: 0.6625 - acc: 0.7318 - val_loss: 0.6238 - val_acc: 0.7452\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62384 to 0.62369, saving model to best.model\n",
      "0s - loss: 0.6603 - acc: 0.7326 - val_loss: 0.6237 - val_acc: 0.7454\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.62369 to 0.62208, saving model to best.model\n",
      "0s - loss: 0.6614 - acc: 0.7307 - val_loss: 0.6221 - val_acc: 0.7451\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6605 - acc: 0.7309 - val_loss: 0.6245 - val_acc: 0.7440\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "1s - loss: 0.6593 - acc: 0.7336 - val_loss: 0.6259 - val_acc: 0.7432\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6543 - acc: 0.7363 - val_loss: 0.6234 - val_acc: 0.7446\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6596 - acc: 0.7321 - val_loss: 0.6225 - val_acc: 0.7444\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.62208 to 0.62087, saving model to best.model\n",
      "1s - loss: 0.6587 - acc: 0.7330 - val_loss: 0.6209 - val_acc: 0.7457\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7322 - val_loss: 0.6214 - val_acc: 0.7445\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6568 - acc: 0.7332 - val_loss: 0.6228 - val_acc: 0.7446\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6555 - acc: 0.7352 - val_loss: 0.6214 - val_acc: 0.7465\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6547 - acc: 0.7337 - val_loss: 0.6233 - val_acc: 0.7406\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.62087 to 0.61900, saving model to best.model\n",
      "1s - loss: 0.6547 - acc: 0.7354 - val_loss: 0.6190 - val_acc: 0.7470\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6538 - acc: 0.7351 - val_loss: 0.6211 - val_acc: 0.7452\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.61900 to 0.61785, saving model to best.model\n",
      "1s - loss: 0.6538 - acc: 0.7350 - val_loss: 0.6179 - val_acc: 0.7464\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6544 - acc: 0.7361 - val_loss: 0.6224 - val_acc: 0.7427\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6532 - acc: 0.7354 - val_loss: 0.6201 - val_acc: 0.7437\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "1s - loss: 0.6507 - acc: 0.7351 - val_loss: 0.6181 - val_acc: 0.7470\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.61785 to 0.61755, saving model to best.model\n",
      "1s - loss: 0.6539 - acc: 0.7326 - val_loss: 0.6176 - val_acc: 0.7450\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.61755 to 0.61669, saving model to best.model\n",
      "0s - loss: 0.6527 - acc: 0.7351 - val_loss: 0.6167 - val_acc: 0.7481\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.61669 to 0.61599, saving model to best.model\n",
      "1s - loss: 0.6517 - acc: 0.7367 - val_loss: 0.6160 - val_acc: 0.7453\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.61599 to 0.61501, saving model to best.model\n",
      "1s - loss: 0.6505 - acc: 0.7373 - val_loss: 0.6150 - val_acc: 0.7470\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6508 - acc: 0.7368 - val_loss: 0.6152 - val_acc: 0.7461\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61501 to 0.61319, saving model to best.model\n",
      "0s - loss: 0.6470 - acc: 0.7377 - val_loss: 0.6132 - val_acc: 0.7477\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.6510 - acc: 0.7353 - val_loss: 0.6144 - val_acc: 0.7472\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.61319 to 0.61181, saving model to best.model\n",
      "1s - loss: 0.6453 - acc: 0.7382 - val_loss: 0.6118 - val_acc: 0.7471\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6517 - acc: 0.7353 - val_loss: 0.6145 - val_acc: 0.7465\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6487 - acc: 0.7364 - val_loss: 0.6122 - val_acc: 0.7493\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6494 - acc: 0.7349 - val_loss: 0.6128 - val_acc: 0.7480\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.61181 to 0.61150, saving model to best.model\n",
      "1s - loss: 0.6477 - acc: 0.7387 - val_loss: 0.6115 - val_acc: 0.7502\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6447 - acc: 0.7373 - val_loss: 0.6142 - val_acc: 0.7459\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.61150 to 0.60989, saving model to best.model\n",
      "1s - loss: 0.6470 - acc: 0.7394 - val_loss: 0.6099 - val_acc: 0.7498\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60989 to 0.60872, saving model to best.model\n",
      "1s - loss: 0.6469 - acc: 0.7359 - val_loss: 0.6087 - val_acc: 0.7499\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6459 - acc: 0.7373 - val_loss: 0.6104 - val_acc: 0.7487\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.60872 to 0.60830, saving model to best.model\n",
      "1s - loss: 0.6456 - acc: 0.7406 - val_loss: 0.6083 - val_acc: 0.7502\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6456 - acc: 0.7378 - val_loss: 0.6102 - val_acc: 0.7475\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60830 to 0.60811, saving model to best.model\n",
      "1s - loss: 0.6429 - acc: 0.7382 - val_loss: 0.6081 - val_acc: 0.7501\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60811 to 0.60743, saving model to best.model\n",
      "1s - loss: 0.6437 - acc: 0.7396 - val_loss: 0.6074 - val_acc: 0.7523\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6427 - acc: 0.7389 - val_loss: 0.6075 - val_acc: 0.7516\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.85079, saving model to best.model\n",
      "0s - loss: 0.8968 - acc: 0.6404 - val_loss: 0.8508 - val_acc: 0.6550\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.85079 to 0.85003, saving model to best.model\n",
      "1s - loss: 0.8570 - acc: 0.6583 - val_loss: 0.8500 - val_acc: 0.6550\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.85003 to 0.84708, saving model to best.model\n",
      "1s - loss: 0.8523 - acc: 0.6584 - val_loss: 0.8471 - val_acc: 0.6550\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84708 to 0.83997, saving model to best.model\n",
      "1s - loss: 0.8459 - acc: 0.6584 - val_loss: 0.8400 - val_acc: 0.6550\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83997 to 0.83539, saving model to best.model\n",
      "1s - loss: 0.8406 - acc: 0.6584 - val_loss: 0.8354 - val_acc: 0.6550\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83539 to 0.83138, saving model to best.model\n",
      "1s - loss: 0.8353 - acc: 0.6584 - val_loss: 0.8314 - val_acc: 0.6550\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83138 to 0.82976, saving model to best.model\n",
      "1s - loss: 0.8324 - acc: 0.6584 - val_loss: 0.8298 - val_acc: 0.6550\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82976 to 0.82881, saving model to best.model\n",
      "1s - loss: 0.8316 - acc: 0.6584 - val_loss: 0.8288 - val_acc: 0.6550\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82881 to 0.82800, saving model to best.model\n",
      "1s - loss: 0.8303 - acc: 0.6584 - val_loss: 0.8280 - val_acc: 0.6550\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8303 - acc: 0.6584 - val_loss: 0.8284 - val_acc: 0.6550\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.8281 - acc: 0.6584 - val_loss: 0.8280 - val_acc: 0.6550\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82800 to 0.82627, saving model to best.model\n",
      "0s - loss: 0.8269 - acc: 0.6584 - val_loss: 0.8263 - val_acc: 0.6550\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.8272 - acc: 0.6584 - val_loss: 0.8264 - val_acc: 0.6550\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82627 to 0.82561, saving model to best.model\n",
      "0s - loss: 0.8258 - acc: 0.6584 - val_loss: 0.8256 - val_acc: 0.6550\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82561 to 0.82524, saving model to best.model\n",
      "0s - loss: 0.8267 - acc: 0.6584 - val_loss: 0.8252 - val_acc: 0.6550\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82524 to 0.82479, saving model to best.model\n",
      "0s - loss: 0.8245 - acc: 0.6584 - val_loss: 0.8248 - val_acc: 0.6550\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82479 to 0.82471, saving model to best.model\n",
      "0s - loss: 0.8242 - acc: 0.6584 - val_loss: 0.8247 - val_acc: 0.6550\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82471 to 0.82348, saving model to best.model\n",
      "1s - loss: 0.8238 - acc: 0.6584 - val_loss: 0.8235 - val_acc: 0.6550\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.82348 to 0.82292, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6584 - val_loss: 0.8229 - val_acc: 0.6550\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82292 to 0.82155, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6584 - val_loss: 0.8215 - val_acc: 0.6550\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82155 to 0.82111, saving model to best.model\n",
      "1s - loss: 0.8214 - acc: 0.6584 - val_loss: 0.8211 - val_acc: 0.6550\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82111 to 0.82031, saving model to best.model\n",
      "0s - loss: 0.8202 - acc: 0.6585 - val_loss: 0.8203 - val_acc: 0.6550\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.82031 to 0.81869, saving model to best.model\n",
      "0s - loss: 0.8200 - acc: 0.6586 - val_loss: 0.8187 - val_acc: 0.6550\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81869 to 0.81705, saving model to best.model\n",
      "0s - loss: 0.8187 - acc: 0.6585 - val_loss: 0.8170 - val_acc: 0.6550\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81705 to 0.81571, saving model to best.model\n",
      "0s - loss: 0.8168 - acc: 0.6592 - val_loss: 0.8157 - val_acc: 0.6550\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81571 to 0.81424, saving model to best.model\n",
      "0s - loss: 0.8170 - acc: 0.6596 - val_loss: 0.8142 - val_acc: 0.6557\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.8147 - acc: 0.6604 - val_loss: 0.8152 - val_acc: 0.6577\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81424 to 0.81072, saving model to best.model\n",
      "0s - loss: 0.8121 - acc: 0.6610 - val_loss: 0.8107 - val_acc: 0.6560\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81072 to 0.80844, saving model to best.model\n",
      "0s - loss: 0.8106 - acc: 0.6613 - val_loss: 0.8084 - val_acc: 0.6563\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80844 to 0.80672, saving model to best.model\n",
      "0s - loss: 0.8103 - acc: 0.6621 - val_loss: 0.8067 - val_acc: 0.6580\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80672 to 0.80492, saving model to best.model\n",
      "0s - loss: 0.8076 - acc: 0.6655 - val_loss: 0.8049 - val_acc: 0.6611\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80492 to 0.80242, saving model to best.model\n",
      "0s - loss: 0.8076 - acc: 0.6639 - val_loss: 0.8024 - val_acc: 0.6643\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80242 to 0.80176, saving model to best.model\n",
      "0s - loss: 0.8049 - acc: 0.6650 - val_loss: 0.8018 - val_acc: 0.6628\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80176 to 0.79779, saving model to best.model\n",
      "0s - loss: 0.8017 - acc: 0.6666 - val_loss: 0.7978 - val_acc: 0.6665\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79779 to 0.79730, saving model to best.model\n",
      "0s - loss: 0.8010 - acc: 0.6646 - val_loss: 0.7973 - val_acc: 0.6643\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79730 to 0.79505, saving model to best.model\n",
      "0s - loss: 0.7996 - acc: 0.6682 - val_loss: 0.7951 - val_acc: 0.6683\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79505 to 0.79366, saving model to best.model\n",
      "0s - loss: 0.7983 - acc: 0.6668 - val_loss: 0.7937 - val_acc: 0.6657\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79366 to 0.79207, saving model to best.model\n",
      "0s - loss: 0.7978 - acc: 0.6664 - val_loss: 0.7921 - val_acc: 0.6670\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79207 to 0.78970, saving model to best.model\n",
      "1s - loss: 0.7938 - acc: 0.6696 - val_loss: 0.7897 - val_acc: 0.6719\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78970 to 0.78811, saving model to best.model\n",
      "1s - loss: 0.7933 - acc: 0.6702 - val_loss: 0.7881 - val_acc: 0.6694\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78811 to 0.78691, saving model to best.model\n",
      "1s - loss: 0.7923 - acc: 0.6708 - val_loss: 0.7869 - val_acc: 0.6748\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78691 to 0.78373, saving model to best.model\n",
      "1s - loss: 0.7909 - acc: 0.6704 - val_loss: 0.7837 - val_acc: 0.6756\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "1s - loss: 0.7900 - acc: 0.6705 - val_loss: 0.7850 - val_acc: 0.6728\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78373 to 0.78265, saving model to best.model\n",
      "1s - loss: 0.7896 - acc: 0.6714 - val_loss: 0.7827 - val_acc: 0.6760\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78265 to 0.77845, saving model to best.model\n",
      "1s - loss: 0.7848 - acc: 0.6730 - val_loss: 0.7785 - val_acc: 0.6819\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.7839 - acc: 0.6734 - val_loss: 0.7796 - val_acc: 0.6797\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77845 to 0.77513, saving model to best.model\n",
      "1s - loss: 0.7836 - acc: 0.6738 - val_loss: 0.7751 - val_acc: 0.6802\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77513 to 0.77413, saving model to best.model\n",
      "1s - loss: 0.7829 - acc: 0.6733 - val_loss: 0.7741 - val_acc: 0.6790\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "1s - loss: 0.7802 - acc: 0.6779 - val_loss: 0.7744 - val_acc: 0.6735\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77413 to 0.76762, saving model to best.model\n",
      "1s - loss: 0.7782 - acc: 0.6771 - val_loss: 0.7676 - val_acc: 0.6842\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76762 to 0.76704, saving model to best.model\n",
      "1s - loss: 0.7777 - acc: 0.6774 - val_loss: 0.7670 - val_acc: 0.6835\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76704 to 0.76659, saving model to best.model\n",
      "1s - loss: 0.7760 - acc: 0.6777 - val_loss: 0.7666 - val_acc: 0.6803\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76659 to 0.76104, saving model to best.model\n",
      "1s - loss: 0.7725 - acc: 0.6788 - val_loss: 0.7610 - val_acc: 0.6868\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76104 to 0.76091, saving model to best.model\n",
      "1s - loss: 0.7708 - acc: 0.6805 - val_loss: 0.7609 - val_acc: 0.6859\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76091 to 0.75545, saving model to best.model\n",
      "0s - loss: 0.7706 - acc: 0.6799 - val_loss: 0.7555 - val_acc: 0.6925\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75545 to 0.75445, saving model to best.model\n",
      "1s - loss: 0.7682 - acc: 0.6830 - val_loss: 0.7545 - val_acc: 0.6877\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75445 to 0.75147, saving model to best.model\n",
      "1s - loss: 0.7656 - acc: 0.6821 - val_loss: 0.7515 - val_acc: 0.6912\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "1s - loss: 0.7652 - acc: 0.6833 - val_loss: 0.7524 - val_acc: 0.6881\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75147 to 0.74628, saving model to best.model\n",
      "1s - loss: 0.7629 - acc: 0.6853 - val_loss: 0.7463 - val_acc: 0.6951\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74628 to 0.74479, saving model to best.model\n",
      "0s - loss: 0.7600 - acc: 0.6845 - val_loss: 0.7448 - val_acc: 0.6915\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74479 to 0.74353, saving model to best.model\n",
      "0s - loss: 0.7612 - acc: 0.6847 - val_loss: 0.7435 - val_acc: 0.6947\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74353 to 0.74013, saving model to best.model\n",
      "0s - loss: 0.7593 - acc: 0.6835 - val_loss: 0.7401 - val_acc: 0.6946\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74013 to 0.73889, saving model to best.model\n",
      "0s - loss: 0.7556 - acc: 0.6856 - val_loss: 0.7389 - val_acc: 0.6924\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73889 to 0.73667, saving model to best.model\n",
      "0s - loss: 0.7551 - acc: 0.6885 - val_loss: 0.7367 - val_acc: 0.6986\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.73667 to 0.73477, saving model to best.model\n",
      "0s - loss: 0.7534 - acc: 0.6871 - val_loss: 0.7348 - val_acc: 0.6946\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.73477 to 0.73225, saving model to best.model\n",
      "0s - loss: 0.7518 - acc: 0.6895 - val_loss: 0.7323 - val_acc: 0.6936\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73225 to 0.72748, saving model to best.model\n",
      "1s - loss: 0.7498 - acc: 0.6890 - val_loss: 0.7275 - val_acc: 0.7066\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "1s - loss: 0.7492 - acc: 0.6896 - val_loss: 0.7297 - val_acc: 0.6925\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "1s - loss: 0.7485 - acc: 0.6910 - val_loss: 0.7291 - val_acc: 0.6932\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.72748 to 0.72031, saving model to best.model\n",
      "1s - loss: 0.7434 - acc: 0.6925 - val_loss: 0.7203 - val_acc: 0.7056\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72031 to 0.71930, saving model to best.model\n",
      "0s - loss: 0.7426 - acc: 0.6930 - val_loss: 0.7193 - val_acc: 0.7071\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71930 to 0.71912, saving model to best.model\n",
      "0s - loss: 0.7432 - acc: 0.6939 - val_loss: 0.7191 - val_acc: 0.7083\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71912 to 0.71525, saving model to best.model\n",
      "0s - loss: 0.7394 - acc: 0.6941 - val_loss: 0.7152 - val_acc: 0.7096\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.71525 to 0.71478, saving model to best.model\n",
      "0s - loss: 0.7391 - acc: 0.6947 - val_loss: 0.7148 - val_acc: 0.7075\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71478 to 0.71453, saving model to best.model\n",
      "0s - loss: 0.7360 - acc: 0.6987 - val_loss: 0.7145 - val_acc: 0.7064\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.71453 to 0.70831, saving model to best.model\n",
      "0s - loss: 0.7352 - acc: 0.6978 - val_loss: 0.7083 - val_acc: 0.7124\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "1s - loss: 0.7345 - acc: 0.6973 - val_loss: 0.7090 - val_acc: 0.7084\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70831 to 0.70758, saving model to best.model\n",
      "1s - loss: 0.7330 - acc: 0.6979 - val_loss: 0.7076 - val_acc: 0.7110\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70758 to 0.70539, saving model to best.model\n",
      "1s - loss: 0.7306 - acc: 0.6985 - val_loss: 0.7054 - val_acc: 0.7131\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.70539 to 0.70238, saving model to best.model\n",
      "1s - loss: 0.7311 - acc: 0.6983 - val_loss: 0.7024 - val_acc: 0.7153\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.70238 to 0.70009, saving model to best.model\n",
      "1s - loss: 0.7298 - acc: 0.7008 - val_loss: 0.7001 - val_acc: 0.7160\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "1s - loss: 0.7290 - acc: 0.7001 - val_loss: 0.7033 - val_acc: 0.7091\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.70009 to 0.69986, saving model to best.model\n",
      "1s - loss: 0.7286 - acc: 0.6999 - val_loss: 0.6999 - val_acc: 0.7136\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69986 to 0.69836, saving model to best.model\n",
      "1s - loss: 0.7290 - acc: 0.6988 - val_loss: 0.6984 - val_acc: 0.7137\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69836 to 0.69535, saving model to best.model\n",
      "1s - loss: 0.7245 - acc: 0.7013 - val_loss: 0.6953 - val_acc: 0.7160\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.7257 - acc: 0.7026 - val_loss: 0.6959 - val_acc: 0.7164\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.69535 to 0.69059, saving model to best.model\n",
      "1s - loss: 0.7214 - acc: 0.7066 - val_loss: 0.6906 - val_acc: 0.7185\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7229 - acc: 0.7025 - val_loss: 0.6919 - val_acc: 0.7165\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.7213 - acc: 0.7027 - val_loss: 0.6932 - val_acc: 0.7221\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69059 to 0.68879, saving model to best.model\n",
      "1s - loss: 0.7216 - acc: 0.7043 - val_loss: 0.6888 - val_acc: 0.7181\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.68879 to 0.68855, saving model to best.model\n",
      "1s - loss: 0.7181 - acc: 0.7034 - val_loss: 0.6885 - val_acc: 0.7174\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.68855 to 0.68811, saving model to best.model\n",
      "1s - loss: 0.7180 - acc: 0.7039 - val_loss: 0.6881 - val_acc: 0.7183\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.68811 to 0.68578, saving model to best.model\n",
      "0s - loss: 0.7168 - acc: 0.7063 - val_loss: 0.6858 - val_acc: 0.7187\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.68578 to 0.68483, saving model to best.model\n",
      "0s - loss: 0.7142 - acc: 0.7069 - val_loss: 0.6848 - val_acc: 0.7210\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.7142 - acc: 0.7088 - val_loss: 0.6875 - val_acc: 0.7171\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.68483 to 0.68170, saving model to best.model\n",
      "1s - loss: 0.7120 - acc: 0.7092 - val_loss: 0.6817 - val_acc: 0.7210\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.68170 to 0.67964, saving model to best.model\n",
      "1s - loss: 0.7097 - acc: 0.7092 - val_loss: 0.6796 - val_acc: 0.7233\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7115 - acc: 0.7073 - val_loss: 0.6828 - val_acc: 0.7174\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67964 to 0.67561, saving model to best.model\n",
      "1s - loss: 0.7090 - acc: 0.7099 - val_loss: 0.6756 - val_acc: 0.7241\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.67561 to 0.67377, saving model to best.model\n",
      "0s - loss: 0.7085 - acc: 0.7074 - val_loss: 0.6738 - val_acc: 0.7272\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "1s - loss: 0.7072 - acc: 0.7113 - val_loss: 0.6744 - val_acc: 0.7262\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "1s - loss: 0.7067 - acc: 0.7109 - val_loss: 0.6746 - val_acc: 0.7240\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7092 - acc: 0.7072 - val_loss: 0.6747 - val_acc: 0.7218\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.67377 to 0.67251, saving model to best.model\n",
      "1s - loss: 0.7011 - acc: 0.7150 - val_loss: 0.6725 - val_acc: 0.7292\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.67251 to 0.66977, saving model to best.model\n",
      "1s - loss: 0.7041 - acc: 0.7132 - val_loss: 0.6698 - val_acc: 0.7245\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.66977 to 0.66808, saving model to best.model\n",
      "1s - loss: 0.7034 - acc: 0.7110 - val_loss: 0.6681 - val_acc: 0.7290\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "1s - loss: 0.7032 - acc: 0.7119 - val_loss: 0.6695 - val_acc: 0.7272\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7019 - acc: 0.7129 - val_loss: 0.6720 - val_acc: 0.7218\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.66808 to 0.66789, saving model to best.model\n",
      "1s - loss: 0.6995 - acc: 0.7149 - val_loss: 0.6679 - val_acc: 0.7244\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66789 to 0.66772, saving model to best.model\n",
      "1s - loss: 0.7002 - acc: 0.7126 - val_loss: 0.6677 - val_acc: 0.7240\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.7019 - acc: 0.7140 - val_loss: 0.6687 - val_acc: 0.7248\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.66772 to 0.66639, saving model to best.model\n",
      "1s - loss: 0.7005 - acc: 0.7129 - val_loss: 0.6664 - val_acc: 0.7283\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.66639 to 0.66402, saving model to best.model\n",
      "1s - loss: 0.6967 - acc: 0.7163 - val_loss: 0.6640 - val_acc: 0.7288\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.7009 - acc: 0.7138 - val_loss: 0.6653 - val_acc: 0.7295\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6980 - acc: 0.7133 - val_loss: 0.6646 - val_acc: 0.7265\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.66402 to 0.66347, saving model to best.model\n",
      "1s - loss: 0.6937 - acc: 0.7165 - val_loss: 0.6635 - val_acc: 0.7285\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.66347 to 0.66237, saving model to best.model\n",
      "1s - loss: 0.6943 - acc: 0.7158 - val_loss: 0.6624 - val_acc: 0.7261\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.66237 to 0.66036, saving model to best.model\n",
      "1s - loss: 0.6935 - acc: 0.7191 - val_loss: 0.6604 - val_acc: 0.7276\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6930 - acc: 0.7156 - val_loss: 0.6616 - val_acc: 0.7288\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.66036 to 0.65956, saving model to best.model\n",
      "1s - loss: 0.6918 - acc: 0.7176 - val_loss: 0.6596 - val_acc: 0.7283\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65956 to 0.65727, saving model to best.model\n",
      "1s - loss: 0.6914 - acc: 0.7169 - val_loss: 0.6573 - val_acc: 0.7304\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.65727 to 0.65682, saving model to best.model\n",
      "1s - loss: 0.6916 - acc: 0.7174 - val_loss: 0.6568 - val_acc: 0.7323\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6902 - acc: 0.7162 - val_loss: 0.6587 - val_acc: 0.7288\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.65682 to 0.65524, saving model to best.model\n",
      "1s - loss: 0.6926 - acc: 0.7164 - val_loss: 0.6552 - val_acc: 0.7327\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.65524 to 0.65495, saving model to best.model\n",
      "1s - loss: 0.6891 - acc: 0.7183 - val_loss: 0.6550 - val_acc: 0.7311\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6884 - acc: 0.7208 - val_loss: 0.6550 - val_acc: 0.7309\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.65495 to 0.65187, saving model to best.model\n",
      "1s - loss: 0.6847 - acc: 0.7206 - val_loss: 0.6519 - val_acc: 0.7343\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6856 - acc: 0.7199 - val_loss: 0.6536 - val_acc: 0.7310\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.65187 to 0.65026, saving model to best.model\n",
      "1s - loss: 0.6849 - acc: 0.7214 - val_loss: 0.6503 - val_acc: 0.7338\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "1s - loss: 0.6872 - acc: 0.7199 - val_loss: 0.6513 - val_acc: 0.7331\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.65026 to 0.64903, saving model to best.model\n",
      "1s - loss: 0.6840 - acc: 0.7218 - val_loss: 0.6490 - val_acc: 0.7334\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6839 - acc: 0.7205 - val_loss: 0.6503 - val_acc: 0.7349\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.64903 to 0.64751, saving model to best.model\n",
      "1s - loss: 0.6849 - acc: 0.7217 - val_loss: 0.6475 - val_acc: 0.7375\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6841 - acc: 0.7226 - val_loss: 0.6509 - val_acc: 0.7326\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6821 - acc: 0.7227 - val_loss: 0.6514 - val_acc: 0.7300\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6833 - acc: 0.7224 - val_loss: 0.6512 - val_acc: 0.7307\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.64751 to 0.64636, saving model to best.model\n",
      "1s - loss: 0.6803 - acc: 0.7229 - val_loss: 0.6464 - val_acc: 0.7368\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6825 - acc: 0.7202 - val_loss: 0.6476 - val_acc: 0.7338\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6809 - acc: 0.7229 - val_loss: 0.6471 - val_acc: 0.7350\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.64636 to 0.64388, saving model to best.model\n",
      "1s - loss: 0.6765 - acc: 0.7261 - val_loss: 0.6439 - val_acc: 0.7361\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6810 - acc: 0.7224 - val_loss: 0.6447 - val_acc: 0.7359\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.64388 to 0.64367, saving model to best.model\n",
      "1s - loss: 0.6771 - acc: 0.7243 - val_loss: 0.6437 - val_acc: 0.7384\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.64367 to 0.64292, saving model to best.model\n",
      "1s - loss: 0.6760 - acc: 0.7241 - val_loss: 0.6429 - val_acc: 0.7389\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.64292 to 0.64212, saving model to best.model\n",
      "0s - loss: 0.6774 - acc: 0.7248 - val_loss: 0.6421 - val_acc: 0.7392\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.64212 to 0.64057, saving model to best.model\n",
      "1s - loss: 0.6770 - acc: 0.7235 - val_loss: 0.6406 - val_acc: 0.7383\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6762 - acc: 0.7254 - val_loss: 0.6428 - val_acc: 0.7383\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.64057 to 0.63846, saving model to best.model\n",
      "1s - loss: 0.6733 - acc: 0.7256 - val_loss: 0.6385 - val_acc: 0.7395\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6778 - acc: 0.7241 - val_loss: 0.6401 - val_acc: 0.7397\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63846 to 0.63822, saving model to best.model\n",
      "1s - loss: 0.6699 - acc: 0.7277 - val_loss: 0.6382 - val_acc: 0.7395\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.63822 to 0.63747, saving model to best.model\n",
      "1s - loss: 0.6703 - acc: 0.7277 - val_loss: 0.6375 - val_acc: 0.7388\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.63747 to 0.63581, saving model to best.model\n",
      "1s - loss: 0.6717 - acc: 0.7263 - val_loss: 0.6358 - val_acc: 0.7410\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6728 - acc: 0.7278 - val_loss: 0.6383 - val_acc: 0.7365\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6704 - acc: 0.7280 - val_loss: 0.6365 - val_acc: 0.7381\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6708 - acc: 0.7268 - val_loss: 0.6399 - val_acc: 0.7348\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.63581 to 0.63533, saving model to best.model\n",
      "1s - loss: 0.6725 - acc: 0.7249 - val_loss: 0.6353 - val_acc: 0.7402\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.63533 to 0.63406, saving model to best.model\n",
      "1s - loss: 0.6695 - acc: 0.7268 - val_loss: 0.6341 - val_acc: 0.7392\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6703 - acc: 0.7255 - val_loss: 0.6347 - val_acc: 0.7398\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6690 - acc: 0.7285 - val_loss: 0.6359 - val_acc: 0.7407\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.63406 to 0.63365, saving model to best.model\n",
      "0s - loss: 0.6679 - acc: 0.7285 - val_loss: 0.6337 - val_acc: 0.7407\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.63365 to 0.63155, saving model to best.model\n",
      "0s - loss: 0.6632 - acc: 0.7306 - val_loss: 0.6315 - val_acc: 0.7417\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6671 - acc: 0.7299 - val_loss: 0.6319 - val_acc: 0.7425\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6699 - acc: 0.7271 - val_loss: 0.6350 - val_acc: 0.7381\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6674 - acc: 0.7285 - val_loss: 0.6338 - val_acc: 0.7397\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.63155 to 0.63049, saving model to best.model\n",
      "0s - loss: 0.6657 - acc: 0.7306 - val_loss: 0.6305 - val_acc: 0.7404\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6691 - acc: 0.7276 - val_loss: 0.6307 - val_acc: 0.7434\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.63049 to 0.62920, saving model to best.model\n",
      "0s - loss: 0.6636 - acc: 0.7298 - val_loss: 0.6292 - val_acc: 0.7430\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.62920 to 0.62820, saving model to best.model\n",
      "0s - loss: 0.6638 - acc: 0.7297 - val_loss: 0.6282 - val_acc: 0.7451\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.62820 to 0.62727, saving model to best.model\n",
      "0s - loss: 0.6655 - acc: 0.7289 - val_loss: 0.6273 - val_acc: 0.7436\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6659 - acc: 0.7277 - val_loss: 0.6276 - val_acc: 0.7418\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.62727 to 0.62557, saving model to best.model\n",
      "1s - loss: 0.6653 - acc: 0.7292 - val_loss: 0.6256 - val_acc: 0.7430\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6612 - acc: 0.7313 - val_loss: 0.6264 - val_acc: 0.7418\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.62557 to 0.62523, saving model to best.model\n",
      "1s - loss: 0.6605 - acc: 0.7296 - val_loss: 0.6252 - val_acc: 0.7433\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.62523 to 0.62457, saving model to best.model\n",
      "1s - loss: 0.6596 - acc: 0.7323 - val_loss: 0.6246 - val_acc: 0.7444\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6581 - acc: 0.7327 - val_loss: 0.6260 - val_acc: 0.7417\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.62457 to 0.62362, saving model to best.model\n",
      "0s - loss: 0.6587 - acc: 0.7321 - val_loss: 0.6236 - val_acc: 0.7446\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.62362 to 0.62325, saving model to best.model\n",
      "1s - loss: 0.6587 - acc: 0.7319 - val_loss: 0.6233 - val_acc: 0.7429\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7331 - val_loss: 0.6253 - val_acc: 0.7418\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.6601 - acc: 0.7348 - val_loss: 0.6264 - val_acc: 0.7416\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.62325 to 0.62309, saving model to best.model\n",
      "1s - loss: 0.6582 - acc: 0.7324 - val_loss: 0.6231 - val_acc: 0.7432\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.62309 to 0.62178, saving model to best.model\n",
      "1s - loss: 0.6587 - acc: 0.7330 - val_loss: 0.6218 - val_acc: 0.7450\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.62178 to 0.62089, saving model to best.model\n",
      "1s - loss: 0.6581 - acc: 0.7346 - val_loss: 0.6209 - val_acc: 0.7445\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.62089 to 0.61937, saving model to best.model\n",
      "1s - loss: 0.6554 - acc: 0.7349 - val_loss: 0.6194 - val_acc: 0.7468\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6552 - acc: 0.7340 - val_loss: 0.6202 - val_acc: 0.7446\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6551 - acc: 0.7346 - val_loss: 0.6218 - val_acc: 0.7438\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61937 to 0.61797, saving model to best.model\n",
      "1s - loss: 0.6522 - acc: 0.7353 - val_loss: 0.6180 - val_acc: 0.7457\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.61797 to 0.61779, saving model to best.model\n",
      "1s - loss: 0.6573 - acc: 0.7334 - val_loss: 0.6178 - val_acc: 0.7465\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6569 - acc: 0.7330 - val_loss: 0.6200 - val_acc: 0.7502\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "1s - loss: 0.6547 - acc: 0.7347 - val_loss: 0.6192 - val_acc: 0.7456\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.61779 to 0.61503, saving model to best.model\n",
      "1s - loss: 0.6540 - acc: 0.7331 - val_loss: 0.6150 - val_acc: 0.7484\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6542 - acc: 0.7344 - val_loss: 0.6164 - val_acc: 0.7471\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6552 - acc: 0.7318 - val_loss: 0.6173 - val_acc: 0.7461\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6523 - acc: 0.7338 - val_loss: 0.6151 - val_acc: 0.7480\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6523 - acc: 0.7352 - val_loss: 0.6157 - val_acc: 0.7470\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6541 - acc: 0.7338 - val_loss: 0.6167 - val_acc: 0.7470\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.61503 to 0.61483, saving model to best.model\n",
      "1s - loss: 0.6508 - acc: 0.7349 - val_loss: 0.6148 - val_acc: 0.7489\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.61483 to 0.61400, saving model to best.model\n",
      "1s - loss: 0.6476 - acc: 0.7353 - val_loss: 0.6140 - val_acc: 0.7485\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "1s - loss: 0.6510 - acc: 0.7348 - val_loss: 0.6162 - val_acc: 0.7479\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.61400 to 0.61259, saving model to best.model\n",
      "1s - loss: 0.6482 - acc: 0.7375 - val_loss: 0.6126 - val_acc: 0.7491\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6520 - acc: 0.7364 - val_loss: 0.6158 - val_acc: 0.7463\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.61259 to 0.61152, saving model to best.model\n",
      "1s - loss: 0.6494 - acc: 0.7359 - val_loss: 0.6115 - val_acc: 0.7515\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.84601, saving model to best.model\n",
      "1s - loss: 0.9357 - acc: 0.6190 - val_loss: 0.8460 - val_acc: 0.6594\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.84601 to 0.84518, saving model to best.model\n",
      "1s - loss: 0.8571 - acc: 0.6609 - val_loss: 0.8452 - val_acc: 0.6594\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84518 to 0.84424, saving model to best.model\n",
      "1s - loss: 0.8471 - acc: 0.6629 - val_loss: 0.8442 - val_acc: 0.6594\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84424 to 0.84136, saving model to best.model\n",
      "1s - loss: 0.8453 - acc: 0.6629 - val_loss: 0.8414 - val_acc: 0.6594\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84136 to 0.83646, saving model to best.model\n",
      "1s - loss: 0.8363 - acc: 0.6629 - val_loss: 0.8365 - val_acc: 0.6594\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83646 to 0.83004, saving model to best.model\n",
      "0s - loss: 0.8339 - acc: 0.6629 - val_loss: 0.8300 - val_acc: 0.6594\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83004 to 0.82662, saving model to best.model\n",
      "1s - loss: 0.8290 - acc: 0.6629 - val_loss: 0.8266 - val_acc: 0.6594\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82662 to 0.82539, saving model to best.model\n",
      "1s - loss: 0.8257 - acc: 0.6629 - val_loss: 0.8254 - val_acc: 0.6594\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82539 to 0.82372, saving model to best.model\n",
      "1s - loss: 0.8236 - acc: 0.6629 - val_loss: 0.8237 - val_acc: 0.6594\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.82372 to 0.82312, saving model to best.model\n",
      "0s - loss: 0.8215 - acc: 0.6629 - val_loss: 0.8231 - val_acc: 0.6594\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8215 - acc: 0.6629 - val_loss: 0.8233 - val_acc: 0.6594\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8200 - acc: 0.6629 - val_loss: 0.8237 - val_acc: 0.6594\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82312 to 0.82108, saving model to best.model\n",
      "0s - loss: 0.8198 - acc: 0.6629 - val_loss: 0.8211 - val_acc: 0.6594\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "1s - loss: 0.8181 - acc: 0.6630 - val_loss: 0.8213 - val_acc: 0.6594\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82108 to 0.82091, saving model to best.model\n",
      "1s - loss: 0.8176 - acc: 0.6630 - val_loss: 0.8209 - val_acc: 0.6594\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82091 to 0.82001, saving model to best.model\n",
      "1s - loss: 0.8169 - acc: 0.6630 - val_loss: 0.8200 - val_acc: 0.6594\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "1s - loss: 0.8172 - acc: 0.6630 - val_loss: 0.8201 - val_acc: 0.6594\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82001 to 0.81870, saving model to best.model\n",
      "1s - loss: 0.8162 - acc: 0.6631 - val_loss: 0.8187 - val_acc: 0.6594\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81870 to 0.81816, saving model to best.model\n",
      "1s - loss: 0.8160 - acc: 0.6625 - val_loss: 0.8182 - val_acc: 0.6594\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81816 to 0.81806, saving model to best.model\n",
      "1s - loss: 0.8157 - acc: 0.6631 - val_loss: 0.8181 - val_acc: 0.6594\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81806 to 0.81718, saving model to best.model\n",
      "1s - loss: 0.8144 - acc: 0.6628 - val_loss: 0.8172 - val_acc: 0.6594\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81718 to 0.81610, saving model to best.model\n",
      "1s - loss: 0.8142 - acc: 0.6631 - val_loss: 0.8161 - val_acc: 0.6594\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81610 to 0.81509, saving model to best.model\n",
      "1s - loss: 0.8107 - acc: 0.6637 - val_loss: 0.8151 - val_acc: 0.6594\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81509 to 0.81487, saving model to best.model\n",
      "1s - loss: 0.8125 - acc: 0.6634 - val_loss: 0.8149 - val_acc: 0.6594\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81487 to 0.81480, saving model to best.model\n",
      "1s - loss: 0.8098 - acc: 0.6631 - val_loss: 0.8148 - val_acc: 0.6601\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81480 to 0.81250, saving model to best.model\n",
      "1s - loss: 0.8095 - acc: 0.6638 - val_loss: 0.8125 - val_acc: 0.6596\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81250 to 0.81249, saving model to best.model\n",
      "1s - loss: 0.8081 - acc: 0.6636 - val_loss: 0.8125 - val_acc: 0.6601\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81249 to 0.81019, saving model to best.model\n",
      "1s - loss: 0.8086 - acc: 0.6640 - val_loss: 0.8102 - val_acc: 0.6604\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81019 to 0.80881, saving model to best.model\n",
      "1s - loss: 0.8070 - acc: 0.6651 - val_loss: 0.8088 - val_acc: 0.6643\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80881 to 0.80826, saving model to best.model\n",
      "1s - loss: 0.8052 - acc: 0.6661 - val_loss: 0.8083 - val_acc: 0.6617\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80826 to 0.80704, saving model to best.model\n",
      "1s - loss: 0.8040 - acc: 0.6662 - val_loss: 0.8070 - val_acc: 0.6644\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80704 to 0.80530, saving model to best.model\n",
      "1s - loss: 0.8038 - acc: 0.6659 - val_loss: 0.8053 - val_acc: 0.6636\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80530 to 0.80401, saving model to best.model\n",
      "1s - loss: 0.8023 - acc: 0.6665 - val_loss: 0.8040 - val_acc: 0.6639\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80401 to 0.80269, saving model to best.model\n",
      "0s - loss: 0.8009 - acc: 0.6666 - val_loss: 0.8027 - val_acc: 0.6659\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80269 to 0.80098, saving model to best.model\n",
      "1s - loss: 0.7997 - acc: 0.6679 - val_loss: 0.8010 - val_acc: 0.6667\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80098 to 0.79851, saving model to best.model\n",
      "1s - loss: 0.7977 - acc: 0.6680 - val_loss: 0.7985 - val_acc: 0.6697\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79851 to 0.79552, saving model to best.model\n",
      "1s - loss: 0.7967 - acc: 0.6684 - val_loss: 0.7955 - val_acc: 0.6683\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79552 to 0.79366, saving model to best.model\n",
      "1s - loss: 0.7951 - acc: 0.6692 - val_loss: 0.7937 - val_acc: 0.6677\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79366 to 0.79181, saving model to best.model\n",
      "1s - loss: 0.7943 - acc: 0.6685 - val_loss: 0.7918 - val_acc: 0.6678\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79181 to 0.78899, saving model to best.model\n",
      "1s - loss: 0.7933 - acc: 0.6689 - val_loss: 0.7890 - val_acc: 0.6725\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78899 to 0.78706, saving model to best.model\n",
      "1s - loss: 0.7904 - acc: 0.6716 - val_loss: 0.7871 - val_acc: 0.6737\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78706 to 0.78370, saving model to best.model\n",
      "1s - loss: 0.7888 - acc: 0.6719 - val_loss: 0.7837 - val_acc: 0.6738\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.7877 - acc: 0.6703 - val_loss: 0.7843 - val_acc: 0.6692\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78370 to 0.78102, saving model to best.model\n",
      "1s - loss: 0.7858 - acc: 0.6713 - val_loss: 0.7810 - val_acc: 0.6714\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78102 to 0.77857, saving model to best.model\n",
      "1s - loss: 0.7842 - acc: 0.6734 - val_loss: 0.7786 - val_acc: 0.6774\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77857 to 0.77473, saving model to best.model\n",
      "1s - loss: 0.7827 - acc: 0.6722 - val_loss: 0.7747 - val_acc: 0.6782\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77473 to 0.77299, saving model to best.model\n",
      "1s - loss: 0.7806 - acc: 0.6753 - val_loss: 0.7730 - val_acc: 0.6790\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77299 to 0.77040, saving model to best.model\n",
      "1s - loss: 0.7776 - acc: 0.6745 - val_loss: 0.7704 - val_acc: 0.6801\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "1s - loss: 0.7762 - acc: 0.6778 - val_loss: 0.7707 - val_acc: 0.6759\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77040 to 0.76313, saving model to best.model\n",
      "1s - loss: 0.7747 - acc: 0.6758 - val_loss: 0.7631 - val_acc: 0.6828\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76313 to 0.76194, saving model to best.model\n",
      "1s - loss: 0.7732 - acc: 0.6779 - val_loss: 0.7619 - val_acc: 0.6836\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76194 to 0.75924, saving model to best.model\n",
      "1s - loss: 0.7714 - acc: 0.6782 - val_loss: 0.7592 - val_acc: 0.6875\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75924 to 0.75672, saving model to best.model\n",
      "1s - loss: 0.7678 - acc: 0.6798 - val_loss: 0.7567 - val_acc: 0.6889\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75672 to 0.75430, saving model to best.model\n",
      "1s - loss: 0.7675 - acc: 0.6824 - val_loss: 0.7543 - val_acc: 0.6861\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75430 to 0.75219, saving model to best.model\n",
      "1s - loss: 0.7641 - acc: 0.6823 - val_loss: 0.7522 - val_acc: 0.6879\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75219 to 0.74899, saving model to best.model\n",
      "1s - loss: 0.7625 - acc: 0.6820 - val_loss: 0.7490 - val_acc: 0.6872\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74899 to 0.74595, saving model to best.model\n",
      "1s - loss: 0.7607 - acc: 0.6831 - val_loss: 0.7460 - val_acc: 0.6953\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74595 to 0.74391, saving model to best.model\n",
      "1s - loss: 0.7604 - acc: 0.6842 - val_loss: 0.7439 - val_acc: 0.6944\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74391 to 0.73679, saving model to best.model\n",
      "1s - loss: 0.7567 - acc: 0.6851 - val_loss: 0.7368 - val_acc: 0.6960\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73679 to 0.73558, saving model to best.model\n",
      "1s - loss: 0.7565 - acc: 0.6867 - val_loss: 0.7356 - val_acc: 0.6968\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73558 to 0.73461, saving model to best.model\n",
      "1s - loss: 0.7548 - acc: 0.6882 - val_loss: 0.7346 - val_acc: 0.6992\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73461 to 0.73352, saving model to best.model\n",
      "1s - loss: 0.7534 - acc: 0.6868 - val_loss: 0.7335 - val_acc: 0.6965\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73352 to 0.72848, saving model to best.model\n",
      "1s - loss: 0.7517 - acc: 0.6871 - val_loss: 0.7285 - val_acc: 0.6985\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.72848 to 0.72638, saving model to best.model\n",
      "1s - loss: 0.7485 - acc: 0.6897 - val_loss: 0.7264 - val_acc: 0.7030\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72638 to 0.72598, saving model to best.model\n",
      "1s - loss: 0.7461 - acc: 0.6889 - val_loss: 0.7260 - val_acc: 0.6984\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72598 to 0.72494, saving model to best.model\n",
      "1s - loss: 0.7439 - acc: 0.6904 - val_loss: 0.7249 - val_acc: 0.6987\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72494 to 0.72093, saving model to best.model\n",
      "1s - loss: 0.7447 - acc: 0.6916 - val_loss: 0.7209 - val_acc: 0.7023\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "1s - loss: 0.7423 - acc: 0.6909 - val_loss: 0.7214 - val_acc: 0.7022\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72093 to 0.71851, saving model to best.model\n",
      "1s - loss: 0.7429 - acc: 0.6944 - val_loss: 0.7185 - val_acc: 0.7023\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71851 to 0.71587, saving model to best.model\n",
      "1s - loss: 0.7410 - acc: 0.6951 - val_loss: 0.7159 - val_acc: 0.7044\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71587 to 0.71351, saving model to best.model\n",
      "1s - loss: 0.7380 - acc: 0.6965 - val_loss: 0.7135 - val_acc: 0.7037\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.71351 to 0.70914, saving model to best.model\n",
      "1s - loss: 0.7348 - acc: 0.6954 - val_loss: 0.7091 - val_acc: 0.7068\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7367 - acc: 0.6979 - val_loss: 0.7095 - val_acc: 0.7048\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.7351 - acc: 0.6964 - val_loss: 0.7105 - val_acc: 0.7008\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70914 to 0.70120, saving model to best.model\n",
      "1s - loss: 0.7303 - acc: 0.6986 - val_loss: 0.7012 - val_acc: 0.7096\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "1s - loss: 0.7297 - acc: 0.6993 - val_loss: 0.7021 - val_acc: 0.7093\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.70120 to 0.70021, saving model to best.model\n",
      "1s - loss: 0.7310 - acc: 0.6983 - val_loss: 0.7002 - val_acc: 0.7124\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "1s - loss: 0.7293 - acc: 0.6990 - val_loss: 0.7008 - val_acc: 0.7121\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.70021 to 0.69721, saving model to best.model\n",
      "1s - loss: 0.7274 - acc: 0.6991 - val_loss: 0.6972 - val_acc: 0.7105\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69721 to 0.69183, saving model to best.model\n",
      "1s - loss: 0.7220 - acc: 0.7011 - val_loss: 0.6918 - val_acc: 0.7145\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69183 to 0.69142, saving model to best.model\n",
      "1s - loss: 0.7249 - acc: 0.7003 - val_loss: 0.6914 - val_acc: 0.7156\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69142 to 0.69123, saving model to best.model\n",
      "1s - loss: 0.7202 - acc: 0.7035 - val_loss: 0.6912 - val_acc: 0.7148\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69123 to 0.69094, saving model to best.model\n",
      "1s - loss: 0.7215 - acc: 0.7041 - val_loss: 0.6909 - val_acc: 0.7178\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69094 to 0.68860, saving model to best.model\n",
      "1s - loss: 0.7205 - acc: 0.7045 - val_loss: 0.6886 - val_acc: 0.7115\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.68860 to 0.68432, saving model to best.model\n",
      "1s - loss: 0.7171 - acc: 0.7065 - val_loss: 0.6843 - val_acc: 0.7208\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "1s - loss: 0.7180 - acc: 0.7068 - val_loss: 0.6848 - val_acc: 0.7166\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68432 to 0.67955, saving model to best.model\n",
      "1s - loss: 0.7140 - acc: 0.7054 - val_loss: 0.6795 - val_acc: 0.7227\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "1s - loss: 0.7164 - acc: 0.7071 - val_loss: 0.6839 - val_acc: 0.7167\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.67955 to 0.67921, saving model to best.model\n",
      "1s - loss: 0.7156 - acc: 0.7053 - val_loss: 0.6792 - val_acc: 0.7197\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67921 to 0.67682, saving model to best.model\n",
      "1s - loss: 0.7119 - acc: 0.7089 - val_loss: 0.6768 - val_acc: 0.7249\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7087 - acc: 0.7103 - val_loss: 0.6782 - val_acc: 0.7171\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67682 to 0.67398, saving model to best.model\n",
      "1s - loss: 0.7109 - acc: 0.7078 - val_loss: 0.6740 - val_acc: 0.7238\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67398 to 0.67383, saving model to best.model\n",
      "1s - loss: 0.7067 - acc: 0.7116 - val_loss: 0.6738 - val_acc: 0.7279\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.7066 - acc: 0.7128 - val_loss: 0.6739 - val_acc: 0.7196\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67383 to 0.67154, saving model to best.model\n",
      "1s - loss: 0.7049 - acc: 0.7117 - val_loss: 0.6715 - val_acc: 0.7230\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.67154 to 0.66986, saving model to best.model\n",
      "1s - loss: 0.7070 - acc: 0.7093 - val_loss: 0.6699 - val_acc: 0.7247\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "1s - loss: 0.7044 - acc: 0.7114 - val_loss: 0.6712 - val_acc: 0.7198\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7077 - acc: 0.7081 - val_loss: 0.6708 - val_acc: 0.7221\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66986 to 0.66432, saving model to best.model\n",
      "1s - loss: 0.7051 - acc: 0.7111 - val_loss: 0.6643 - val_acc: 0.7311\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.7068 - acc: 0.7108 - val_loss: 0.6678 - val_acc: 0.7255\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.7005 - acc: 0.7117 - val_loss: 0.6671 - val_acc: 0.7218\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66432 to 0.66060, saving model to best.model\n",
      "1s - loss: 0.6992 - acc: 0.7126 - val_loss: 0.6606 - val_acc: 0.7304\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7037 - acc: 0.7132 - val_loss: 0.6615 - val_acc: 0.7307\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66060 to 0.65850, saving model to best.model\n",
      "1s - loss: 0.6982 - acc: 0.7143 - val_loss: 0.6585 - val_acc: 0.7290\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.65850 to 0.65812, saving model to best.model\n",
      "1s - loss: 0.6989 - acc: 0.7134 - val_loss: 0.6581 - val_acc: 0.7351\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "1s - loss: 0.6960 - acc: 0.7158 - val_loss: 0.6607 - val_acc: 0.7244\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65812 to 0.65553, saving model to best.model\n",
      "1s - loss: 0.6956 - acc: 0.7153 - val_loss: 0.6555 - val_acc: 0.7326\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6944 - acc: 0.7150 - val_loss: 0.6578 - val_acc: 0.7288\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.65553 to 0.65243, saving model to best.model\n",
      "1s - loss: 0.6966 - acc: 0.7164 - val_loss: 0.6524 - val_acc: 0.7355\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "1s - loss: 0.6946 - acc: 0.7187 - val_loss: 0.6540 - val_acc: 0.7351\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6943 - acc: 0.7136 - val_loss: 0.6537 - val_acc: 0.7341\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65243 to 0.65048, saving model to best.model\n",
      "1s - loss: 0.6937 - acc: 0.7156 - val_loss: 0.6505 - val_acc: 0.7354\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "0s - loss: 0.6944 - acc: 0.7162 - val_loss: 0.6524 - val_acc: 0.7323\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss did not improve\n",
      "1s - loss: 0.6920 - acc: 0.7166 - val_loss: 0.6528 - val_acc: 0.7336\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6917 - acc: 0.7174 - val_loss: 0.6511 - val_acc: 0.7331\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6911 - acc: 0.7178 - val_loss: 0.6513 - val_acc: 0.7391\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.65048 to 0.64596, saving model to best.model\n",
      "1s - loss: 0.6897 - acc: 0.7148 - val_loss: 0.6460 - val_acc: 0.7393\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.64596 to 0.64471, saving model to best.model\n",
      "1s - loss: 0.6867 - acc: 0.7188 - val_loss: 0.6447 - val_acc: 0.7377\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6869 - acc: 0.7188 - val_loss: 0.6458 - val_acc: 0.7363\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64471 to 0.64392, saving model to best.model\n",
      "1s - loss: 0.6869 - acc: 0.7195 - val_loss: 0.6439 - val_acc: 0.7382\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "0s - loss: 0.6886 - acc: 0.7181 - val_loss: 0.6443 - val_acc: 0.7385\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6832 - acc: 0.7206 - val_loss: 0.6456 - val_acc: 0.7344\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "0s - loss: 0.6874 - acc: 0.7196 - val_loss: 0.6447 - val_acc: 0.7345\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "1s - loss: 0.6869 - acc: 0.7202 - val_loss: 0.6441 - val_acc: 0.7350\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.64392 to 0.64008, saving model to best.model\n",
      "1s - loss: 0.6867 - acc: 0.7189 - val_loss: 0.6401 - val_acc: 0.7372\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.6825 - acc: 0.7221 - val_loss: 0.6406 - val_acc: 0.7411\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6811 - acc: 0.7214 - val_loss: 0.6412 - val_acc: 0.7374\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6799 - acc: 0.7223 - val_loss: 0.6401 - val_acc: 0.7370\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.64008 to 0.63479, saving model to best.model\n",
      "1s - loss: 0.6809 - acc: 0.7210 - val_loss: 0.6348 - val_acc: 0.7423\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63479 to 0.63301, saving model to best.model\n",
      "1s - loss: 0.6804 - acc: 0.7221 - val_loss: 0.6330 - val_acc: 0.7437\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss did not improve\n",
      "1s - loss: 0.6800 - acc: 0.7203 - val_loss: 0.6342 - val_acc: 0.7404\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.63301 to 0.63279, saving model to best.model\n",
      "1s - loss: 0.6780 - acc: 0.7222 - val_loss: 0.6328 - val_acc: 0.7426\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6764 - acc: 0.7237 - val_loss: 0.6331 - val_acc: 0.7412\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.6770 - acc: 0.7254 - val_loss: 0.6347 - val_acc: 0.7385\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6788 - acc: 0.7241 - val_loss: 0.6335 - val_acc: 0.7431\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63279 to 0.63000, saving model to best.model\n",
      "1s - loss: 0.6773 - acc: 0.7251 - val_loss: 0.6300 - val_acc: 0.7446\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63000 to 0.62929, saving model to best.model\n",
      "1s - loss: 0.6732 - acc: 0.7249 - val_loss: 0.6293 - val_acc: 0.7467\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.62929 to 0.62896, saving model to best.model\n",
      "1s - loss: 0.6747 - acc: 0.7245 - val_loss: 0.6290 - val_acc: 0.7465\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.62896 to 0.62801, saving model to best.model\n",
      "1s - loss: 0.6731 - acc: 0.7240 - val_loss: 0.6280 - val_acc: 0.7445\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "1s - loss: 0.6740 - acc: 0.7247 - val_loss: 0.6321 - val_acc: 0.7427\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.62801 to 0.62415, saving model to best.model\n",
      "1s - loss: 0.6697 - acc: 0.7284 - val_loss: 0.6242 - val_acc: 0.7481\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "1s - loss: 0.6731 - acc: 0.7273 - val_loss: 0.6258 - val_acc: 0.7465\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.62415 to 0.62394, saving model to best.model\n",
      "0s - loss: 0.6689 - acc: 0.7266 - val_loss: 0.6239 - val_acc: 0.7474\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.62394 to 0.62254, saving model to best.model\n",
      "0s - loss: 0.6681 - acc: 0.7278 - val_loss: 0.6225 - val_acc: 0.7451\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6683 - acc: 0.7262 - val_loss: 0.6227 - val_acc: 0.7461\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6714 - acc: 0.7241 - val_loss: 0.6266 - val_acc: 0.7424\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6679 - acc: 0.7285 - val_loss: 0.6246 - val_acc: 0.7481\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.62254 to 0.62192, saving model to best.model\n",
      "1s - loss: 0.6670 - acc: 0.7280 - val_loss: 0.6219 - val_acc: 0.7464\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "1s - loss: 0.6674 - acc: 0.7292 - val_loss: 0.6226 - val_acc: 0.7475\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62192 to 0.62023, saving model to best.model\n",
      "1s - loss: 0.6670 - acc: 0.7275 - val_loss: 0.6202 - val_acc: 0.7487\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62023 to 0.62014, saving model to best.model\n",
      "1s - loss: 0.6662 - acc: 0.7268 - val_loss: 0.6201 - val_acc: 0.7501\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62014 to 0.61820, saving model to best.model\n",
      "0s - loss: 0.6650 - acc: 0.7306 - val_loss: 0.6182 - val_acc: 0.7500\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6661 - acc: 0.7284 - val_loss: 0.6195 - val_acc: 0.7461\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.61820 to 0.61812, saving model to best.model\n",
      "1s - loss: 0.6621 - acc: 0.7314 - val_loss: 0.6181 - val_acc: 0.7509\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.61812 to 0.61499, saving model to best.model\n",
      "1s - loss: 0.6630 - acc: 0.7293 - val_loss: 0.6150 - val_acc: 0.7513\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6614 - acc: 0.7298 - val_loss: 0.6160 - val_acc: 0.7488\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.6646 - acc: 0.7289 - val_loss: 0.6198 - val_acc: 0.7481\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6639 - acc: 0.7288 - val_loss: 0.6162 - val_acc: 0.7505\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "1s - loss: 0.6640 - acc: 0.7301 - val_loss: 0.6196 - val_acc: 0.7526\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6617 - acc: 0.7311 - val_loss: 0.6181 - val_acc: 0.7507\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.61499 to 0.61246, saving model to best.model\n",
      "1s - loss: 0.6583 - acc: 0.7324 - val_loss: 0.6125 - val_acc: 0.7541\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.6639 - acc: 0.7316 - val_loss: 0.6141 - val_acc: 0.7530\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.6609 - acc: 0.7294 - val_loss: 0.6153 - val_acc: 0.7530\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.61246 to 0.61018, saving model to best.model\n",
      "0s - loss: 0.6587 - acc: 0.7313 - val_loss: 0.6102 - val_acc: 0.7532\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6604 - acc: 0.7280 - val_loss: 0.6140 - val_acc: 0.7522\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.6614 - acc: 0.7278 - val_loss: 0.6129 - val_acc: 0.7511\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61018 to 0.60809, saving model to best.model\n",
      "0s - loss: 0.6576 - acc: 0.7348 - val_loss: 0.6081 - val_acc: 0.7533\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6543 - acc: 0.7348 - val_loss: 0.6112 - val_acc: 0.7521\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.6570 - acc: 0.7339 - val_loss: 0.6112 - val_acc: 0.7529\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6575 - acc: 0.7310 - val_loss: 0.6082 - val_acc: 0.7528\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.60809 to 0.60744, saving model to best.model\n",
      "0s - loss: 0.6557 - acc: 0.7350 - val_loss: 0.6074 - val_acc: 0.7537\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6548 - acc: 0.7322 - val_loss: 0.6091 - val_acc: 0.7526\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.6562 - acc: 0.7308 - val_loss: 0.6074 - val_acc: 0.7529\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6565 - acc: 0.7304 - val_loss: 0.6085 - val_acc: 0.7540\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.6533 - acc: 0.7336 - val_loss: 0.6094 - val_acc: 0.7521\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.60744 to 0.60676, saving model to best.model\n",
      "0s - loss: 0.6538 - acc: 0.7329 - val_loss: 0.6068 - val_acc: 0.7532\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.60676 to 0.60322, saving model to best.model\n",
      "1s - loss: 0.6540 - acc: 0.7322 - val_loss: 0.6032 - val_acc: 0.7570\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6528 - acc: 0.7351 - val_loss: 0.6037 - val_acc: 0.7594\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.60322 to 0.60303, saving model to best.model\n",
      "1s - loss: 0.6515 - acc: 0.7335 - val_loss: 0.6030 - val_acc: 0.7571\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.60303 to 0.60200, saving model to best.model\n",
      "1s - loss: 0.6540 - acc: 0.7347 - val_loss: 0.6020 - val_acc: 0.7585\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.6533 - acc: 0.7356 - val_loss: 0.6075 - val_acc: 0.7550\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6486 - acc: 0.7347 - val_loss: 0.6063 - val_acc: 0.7528\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6509 - acc: 0.7331 - val_loss: 0.6035 - val_acc: 0.7559\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60200 to 0.59900, saving model to best.model\n",
      "1s - loss: 0.6494 - acc: 0.7350 - val_loss: 0.5990 - val_acc: 0.7580\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "1s - loss: 0.6483 - acc: 0.7355 - val_loss: 0.6011 - val_acc: 0.7574\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6476 - acc: 0.7352 - val_loss: 0.6010 - val_acc: 0.7594\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6491 - acc: 0.7345 - val_loss: 0.6004 - val_acc: 0.7595\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.59900 to 0.59737, saving model to best.model\n",
      "1s - loss: 0.6488 - acc: 0.7376 - val_loss: 0.5974 - val_acc: 0.7584\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6477 - acc: 0.7381 - val_loss: 0.5995 - val_acc: 0.7584\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.6496 - acc: 0.7360 - val_loss: 0.6014 - val_acc: 0.7577\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.59737 to 0.59706, saving model to best.model\n",
      "0s - loss: 0.6455 - acc: 0.7359 - val_loss: 0.5971 - val_acc: 0.7580\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.6457 - acc: 0.7365 - val_loss: 0.5983 - val_acc: 0.7573\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.6468 - acc: 0.7347 - val_loss: 0.5979 - val_acc: 0.7595\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.59706 to 0.59425, saving model to best.model\n",
      "0s - loss: 0.6433 - acc: 0.7377 - val_loss: 0.5942 - val_acc: 0.7612\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "1s - loss: 0.6472 - acc: 0.7374 - val_loss: 0.5985 - val_acc: 0.7585\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.59425 to 0.59414, saving model to best.model\n",
      "1s - loss: 0.6445 - acc: 0.7369 - val_loss: 0.5941 - val_acc: 0.7588\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6454 - acc: 0.7375 - val_loss: 0.5961 - val_acc: 0.7594\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "1s - loss: 0.6434 - acc: 0.7370 - val_loss: 0.5972 - val_acc: 0.7573\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.6437 - acc: 0.7371 - val_loss: 0.5956 - val_acc: 0.7582\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6450 - acc: 0.7378 - val_loss: 0.5967 - val_acc: 0.7564\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.85354, saving model to best.model\n",
      "1s - loss: 0.9352 - acc: 0.6151 - val_loss: 0.8535 - val_acc: 0.6567\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.85354 to 0.84959, saving model to best.model\n",
      "1s - loss: 0.8672 - acc: 0.6536 - val_loss: 0.8496 - val_acc: 0.6567\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.84959 to 0.84750, saving model to best.model\n",
      "1s - loss: 0.8589 - acc: 0.6541 - val_loss: 0.8475 - val_acc: 0.6567\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84750 to 0.84308, saving model to best.model\n",
      "1s - loss: 0.8544 - acc: 0.6542 - val_loss: 0.8431 - val_acc: 0.6567\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84308 to 0.83849, saving model to best.model\n",
      "0s - loss: 0.8488 - acc: 0.6542 - val_loss: 0.8385 - val_acc: 0.6567\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83849 to 0.83197, saving model to best.model\n",
      "1s - loss: 0.8438 - acc: 0.6542 - val_loss: 0.8320 - val_acc: 0.6567\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83197 to 0.82907, saving model to best.model\n",
      "1s - loss: 0.8407 - acc: 0.6542 - val_loss: 0.8291 - val_acc: 0.6567\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82907 to 0.82815, saving model to best.model\n",
      "1s - loss: 0.8382 - acc: 0.6542 - val_loss: 0.8282 - val_acc: 0.6567\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82815 to 0.82614, saving model to best.model\n",
      "1s - loss: 0.8357 - acc: 0.6542 - val_loss: 0.8261 - val_acc: 0.6567\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "1s - loss: 0.8332 - acc: 0.6542 - val_loss: 0.8268 - val_acc: 0.6567\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "1s - loss: 0.8338 - acc: 0.6542 - val_loss: 0.8272 - val_acc: 0.6567\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "1s - loss: 0.8339 - acc: 0.6542 - val_loss: 0.8263 - val_acc: 0.6567\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82614 to 0.82367, saving model to best.model\n",
      "1s - loss: 0.8318 - acc: 0.6542 - val_loss: 0.8237 - val_acc: 0.6567\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82367 to 0.82322, saving model to best.model\n",
      "1s - loss: 0.8315 - acc: 0.6542 - val_loss: 0.8232 - val_acc: 0.6567\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "1s - loss: 0.8304 - acc: 0.6542 - val_loss: 0.8237 - val_acc: 0.6567\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82322 to 0.82274, saving model to best.model\n",
      "1s - loss: 0.8306 - acc: 0.6541 - val_loss: 0.8227 - val_acc: 0.6567\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82274 to 0.82259, saving model to best.model\n",
      "0s - loss: 0.8301 - acc: 0.6542 - val_loss: 0.8226 - val_acc: 0.6567\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.82259 to 0.82257, saving model to best.model\n",
      "1s - loss: 0.8293 - acc: 0.6542 - val_loss: 0.8226 - val_acc: 0.6567\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "1s - loss: 0.8292 - acc: 0.6542 - val_loss: 0.8227 - val_acc: 0.6567\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82257 to 0.82086, saving model to best.model\n",
      "1s - loss: 0.8281 - acc: 0.6542 - val_loss: 0.8209 - val_acc: 0.6567\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82086 to 0.82032, saving model to best.model\n",
      "1s - loss: 0.8274 - acc: 0.6542 - val_loss: 0.8203 - val_acc: 0.6567\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.82032 to 0.81939, saving model to best.model\n",
      "1s - loss: 0.8272 - acc: 0.6542 - val_loss: 0.8194 - val_acc: 0.6567\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.8267 - acc: 0.6543 - val_loss: 0.8194 - val_acc: 0.6567\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81939 to 0.81773, saving model to best.model\n",
      "1s - loss: 0.8258 - acc: 0.6541 - val_loss: 0.8177 - val_acc: 0.6567\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81773 to 0.81627, saving model to best.model\n",
      "1s - loss: 0.8251 - acc: 0.6544 - val_loss: 0.8163 - val_acc: 0.6567\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "1s - loss: 0.8238 - acc: 0.6543 - val_loss: 0.8165 - val_acc: 0.6567\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81627 to 0.81433, saving model to best.model\n",
      "1s - loss: 0.8229 - acc: 0.6551 - val_loss: 0.8143 - val_acc: 0.6578\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "1s - loss: 0.8217 - acc: 0.6557 - val_loss: 0.8149 - val_acc: 0.6575\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81433 to 0.81189, saving model to best.model\n",
      "0s - loss: 0.8211 - acc: 0.6554 - val_loss: 0.8119 - val_acc: 0.6587\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81189 to 0.81103, saving model to best.model\n",
      "0s - loss: 0.8206 - acc: 0.6550 - val_loss: 0.8110 - val_acc: 0.6583\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.81103 to 0.80907, saving model to best.model\n",
      "0s - loss: 0.8190 - acc: 0.6560 - val_loss: 0.8091 - val_acc: 0.6618\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80907 to 0.80741, saving model to best.model\n",
      "1s - loss: 0.8177 - acc: 0.6570 - val_loss: 0.8074 - val_acc: 0.6609\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80741 to 0.80512, saving model to best.model\n",
      "0s - loss: 0.8150 - acc: 0.6575 - val_loss: 0.8051 - val_acc: 0.6623\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80512 to 0.80420, saving model to best.model\n",
      "1s - loss: 0.8131 - acc: 0.6580 - val_loss: 0.8042 - val_acc: 0.6614\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80420 to 0.80161, saving model to best.model\n",
      "1s - loss: 0.8132 - acc: 0.6576 - val_loss: 0.8016 - val_acc: 0.6626\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.80161 to 0.80031, saving model to best.model\n",
      "1s - loss: 0.8116 - acc: 0.6593 - val_loss: 0.8003 - val_acc: 0.6641\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.80031 to 0.79655, saving model to best.model\n",
      "1s - loss: 0.8100 - acc: 0.6604 - val_loss: 0.7966 - val_acc: 0.6673\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79655 to 0.79550, saving model to best.model\n",
      "1s - loss: 0.8052 - acc: 0.6639 - val_loss: 0.7955 - val_acc: 0.6658\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79550 to 0.79405, saving model to best.model\n",
      "1s - loss: 0.8062 - acc: 0.6619 - val_loss: 0.7940 - val_acc: 0.6710\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79405 to 0.78945, saving model to best.model\n",
      "0s - loss: 0.8037 - acc: 0.6645 - val_loss: 0.7895 - val_acc: 0.6719\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78945 to 0.78823, saving model to best.model\n",
      "1s - loss: 0.8017 - acc: 0.6658 - val_loss: 0.7882 - val_acc: 0.6717\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78823 to 0.78676, saving model to best.model\n",
      "1s - loss: 0.8013 - acc: 0.6643 - val_loss: 0.7868 - val_acc: 0.6693\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78676 to 0.78441, saving model to best.model\n",
      "1s - loss: 0.7993 - acc: 0.6658 - val_loss: 0.7844 - val_acc: 0.6744\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78441 to 0.78190, saving model to best.model\n",
      "1s - loss: 0.7985 - acc: 0.6655 - val_loss: 0.7819 - val_acc: 0.6752\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78190 to 0.77949, saving model to best.model\n",
      "1s - loss: 0.7946 - acc: 0.6688 - val_loss: 0.7795 - val_acc: 0.6751\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77949 to 0.77808, saving model to best.model\n",
      "1s - loss: 0.7936 - acc: 0.6671 - val_loss: 0.7781 - val_acc: 0.6733\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77808 to 0.77574, saving model to best.model\n",
      "1s - loss: 0.7923 - acc: 0.6677 - val_loss: 0.7757 - val_acc: 0.6801\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77574 to 0.77370, saving model to best.model\n",
      "1s - loss: 0.7916 - acc: 0.6710 - val_loss: 0.7737 - val_acc: 0.6773\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77370 to 0.77140, saving model to best.model\n",
      "1s - loss: 0.7888 - acc: 0.6727 - val_loss: 0.7714 - val_acc: 0.6770\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.77140 to 0.76878, saving model to best.model\n",
      "0s - loss: 0.7868 - acc: 0.6734 - val_loss: 0.7688 - val_acc: 0.6852\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76878 to 0.76321, saving model to best.model\n",
      "1s - loss: 0.7853 - acc: 0.6720 - val_loss: 0.7632 - val_acc: 0.6874\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76321 to 0.76174, saving model to best.model\n",
      "1s - loss: 0.7834 - acc: 0.6743 - val_loss: 0.7617 - val_acc: 0.6865\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76174 to 0.76020, saving model to best.model\n",
      "1s - loss: 0.7806 - acc: 0.6755 - val_loss: 0.7602 - val_acc: 0.6849\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76020 to 0.75612, saving model to best.model\n",
      "1s - loss: 0.7784 - acc: 0.6775 - val_loss: 0.7561 - val_acc: 0.6890\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.75612 to 0.75408, saving model to best.model\n",
      "1s - loss: 0.7777 - acc: 0.6749 - val_loss: 0.7541 - val_acc: 0.6930\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.75408 to 0.75066, saving model to best.model\n",
      "1s - loss: 0.7745 - acc: 0.6776 - val_loss: 0.7507 - val_acc: 0.6900\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.75066 to 0.74735, saving model to best.model\n",
      "1s - loss: 0.7738 - acc: 0.6796 - val_loss: 0.7474 - val_acc: 0.6930\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74735 to 0.74573, saving model to best.model\n",
      "1s - loss: 0.7711 - acc: 0.6819 - val_loss: 0.7457 - val_acc: 0.6974\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.74573 to 0.74536, saving model to best.model\n",
      "1s - loss: 0.7709 - acc: 0.6797 - val_loss: 0.7454 - val_acc: 0.6903\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.74536 to 0.74118, saving model to best.model\n",
      "1s - loss: 0.7679 - acc: 0.6809 - val_loss: 0.7412 - val_acc: 0.6977\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.74118 to 0.73877, saving model to best.model\n",
      "1s - loss: 0.7647 - acc: 0.6830 - val_loss: 0.7388 - val_acc: 0.6979\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.73877 to 0.73652, saving model to best.model\n",
      "1s - loss: 0.7656 - acc: 0.6839 - val_loss: 0.7365 - val_acc: 0.6933\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.73652 to 0.73139, saving model to best.model\n",
      "1s - loss: 0.7600 - acc: 0.6874 - val_loss: 0.7314 - val_acc: 0.7004\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.73139 to 0.72812, saving model to best.model\n",
      "1s - loss: 0.7593 - acc: 0.6855 - val_loss: 0.7281 - val_acc: 0.7027\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "1s - loss: 0.7560 - acc: 0.6875 - val_loss: 0.7288 - val_acc: 0.7052\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.72812 to 0.72465, saving model to best.model\n",
      "1s - loss: 0.7564 - acc: 0.6869 - val_loss: 0.7247 - val_acc: 0.7007\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.72465 to 0.72115, saving model to best.model\n",
      "1s - loss: 0.7513 - acc: 0.6908 - val_loss: 0.7212 - val_acc: 0.7035\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.72115 to 0.72039, saving model to best.model\n",
      "1s - loss: 0.7534 - acc: 0.6902 - val_loss: 0.7204 - val_acc: 0.7034\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.72039 to 0.71768, saving model to best.model\n",
      "1s - loss: 0.7530 - acc: 0.6895 - val_loss: 0.7177 - val_acc: 0.7068\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.71768 to 0.71324, saving model to best.model\n",
      "1s - loss: 0.7491 - acc: 0.6917 - val_loss: 0.7132 - val_acc: 0.7090\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.71324 to 0.71306, saving model to best.model\n",
      "1s - loss: 0.7477 - acc: 0.6918 - val_loss: 0.7131 - val_acc: 0.7052\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "1s - loss: 0.7458 - acc: 0.6921 - val_loss: 0.7151 - val_acc: 0.7084\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.71306 to 0.70872, saving model to best.model\n",
      "1s - loss: 0.7422 - acc: 0.6943 - val_loss: 0.7087 - val_acc: 0.7094\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70872 to 0.70636, saving model to best.model\n",
      "1s - loss: 0.7420 - acc: 0.6943 - val_loss: 0.7064 - val_acc: 0.7133\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.70636 to 0.70405, saving model to best.model\n",
      "1s - loss: 0.7405 - acc: 0.6955 - val_loss: 0.7040 - val_acc: 0.7133\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.70405 to 0.70159, saving model to best.model\n",
      "0s - loss: 0.7407 - acc: 0.6948 - val_loss: 0.7016 - val_acc: 0.7148\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.7350 - acc: 0.7004 - val_loss: 0.7018 - val_acc: 0.7117\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.70159 to 0.69662, saving model to best.model\n",
      "1s - loss: 0.7339 - acc: 0.6981 - val_loss: 0.6966 - val_acc: 0.7146\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7338 - acc: 0.6978 - val_loss: 0.6987 - val_acc: 0.7153\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.7348 - acc: 0.6983 - val_loss: 0.6987 - val_acc: 0.7098\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.69662 to 0.69307, saving model to best.model\n",
      "1s - loss: 0.7328 - acc: 0.7003 - val_loss: 0.6931 - val_acc: 0.7151\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.69307 to 0.69078, saving model to best.model\n",
      "1s - loss: 0.7327 - acc: 0.6982 - val_loss: 0.6908 - val_acc: 0.7172\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.69078 to 0.69077, saving model to best.model\n",
      "1s - loss: 0.7287 - acc: 0.7031 - val_loss: 0.6908 - val_acc: 0.7166\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.69077 to 0.68863, saving model to best.model\n",
      "1s - loss: 0.7280 - acc: 0.7015 - val_loss: 0.6886 - val_acc: 0.7192\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.68863 to 0.68551, saving model to best.model\n",
      "1s - loss: 0.7253 - acc: 0.7022 - val_loss: 0.6855 - val_acc: 0.7189\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.68551 to 0.68516, saving model to best.model\n",
      "1s - loss: 0.7225 - acc: 0.7034 - val_loss: 0.6852 - val_acc: 0.7189\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.68516 to 0.68301, saving model to best.model\n",
      "1s - loss: 0.7215 - acc: 0.7036 - val_loss: 0.6830 - val_acc: 0.7214\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.68301 to 0.68261, saving model to best.model\n",
      "0s - loss: 0.7241 - acc: 0.7025 - val_loss: 0.6826 - val_acc: 0.7231\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.7220 - acc: 0.7048 - val_loss: 0.6839 - val_acc: 0.7220\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.68261 to 0.67680, saving model to best.model\n",
      "1s - loss: 0.7197 - acc: 0.7060 - val_loss: 0.6768 - val_acc: 0.7241\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7210 - acc: 0.7044 - val_loss: 0.6822 - val_acc: 0.7163\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "1s - loss: 0.7175 - acc: 0.7080 - val_loss: 0.6782 - val_acc: 0.7219\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.67680 to 0.67598, saving model to best.model\n",
      "1s - loss: 0.7181 - acc: 0.7053 - val_loss: 0.6760 - val_acc: 0.7226\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.67598 to 0.67576, saving model to best.model\n",
      "1s - loss: 0.7132 - acc: 0.7079 - val_loss: 0.6758 - val_acc: 0.7248\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.67576 to 0.67216, saving model to best.model\n",
      "1s - loss: 0.7143 - acc: 0.7060 - val_loss: 0.6722 - val_acc: 0.7266\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "1s - loss: 0.7138 - acc: 0.7056 - val_loss: 0.6737 - val_acc: 0.7238\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.67216 to 0.66971, saving model to best.model\n",
      "1s - loss: 0.7127 - acc: 0.7084 - val_loss: 0.6697 - val_acc: 0.7258\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "1s - loss: 0.7116 - acc: 0.7085 - val_loss: 0.6701 - val_acc: 0.7255\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.66971 to 0.66721, saving model to best.model\n",
      "1s - loss: 0.7105 - acc: 0.7076 - val_loss: 0.6672 - val_acc: 0.7265\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "1s - loss: 0.7089 - acc: 0.7092 - val_loss: 0.6675 - val_acc: 0.7255\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66721 to 0.66684, saving model to best.model\n",
      "1s - loss: 0.7092 - acc: 0.7092 - val_loss: 0.6668 - val_acc: 0.7270\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66684 to 0.66552, saving model to best.model\n",
      "1s - loss: 0.7091 - acc: 0.7089 - val_loss: 0.6655 - val_acc: 0.7272\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.7085 - acc: 0.7111 - val_loss: 0.6660 - val_acc: 0.7278\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.66552 to 0.66401, saving model to best.model\n",
      "1s - loss: 0.7069 - acc: 0.7105 - val_loss: 0.6640 - val_acc: 0.7275\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.66401 to 0.66235, saving model to best.model\n",
      "1s - loss: 0.7014 - acc: 0.7137 - val_loss: 0.6624 - val_acc: 0.7281\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.7008 - acc: 0.7142 - val_loss: 0.6642 - val_acc: 0.7258\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.66235 to 0.65945, saving model to best.model\n",
      "1s - loss: 0.7023 - acc: 0.7116 - val_loss: 0.6594 - val_acc: 0.7288\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.7029 - acc: 0.7133 - val_loss: 0.6614 - val_acc: 0.7276\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.7002 - acc: 0.7153 - val_loss: 0.6596 - val_acc: 0.7295\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65945 to 0.65720, saving model to best.model\n",
      "1s - loss: 0.7010 - acc: 0.7121 - val_loss: 0.6572 - val_acc: 0.7308\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.65720 to 0.65676, saving model to best.model\n",
      "1s - loss: 0.6989 - acc: 0.7129 - val_loss: 0.6568 - val_acc: 0.7311\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.65676 to 0.65585, saving model to best.model\n",
      "1s - loss: 0.6993 - acc: 0.7144 - val_loss: 0.6559 - val_acc: 0.7314\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.65585 to 0.65521, saving model to best.model\n",
      "1s - loss: 0.6987 - acc: 0.7119 - val_loss: 0.6552 - val_acc: 0.7307\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.65521 to 0.65442, saving model to best.model\n",
      "0s - loss: 0.6981 - acc: 0.7152 - val_loss: 0.6544 - val_acc: 0.7285\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6952 - acc: 0.7147 - val_loss: 0.6554 - val_acc: 0.7296\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.65442 to 0.65376, saving model to best.model\n",
      "1s - loss: 0.6953 - acc: 0.7163 - val_loss: 0.6538 - val_acc: 0.7307\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.65376 to 0.65166, saving model to best.model\n",
      "1s - loss: 0.6950 - acc: 0.7191 - val_loss: 0.6517 - val_acc: 0.7319\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6935 - acc: 0.7151 - val_loss: 0.6529 - val_acc: 0.7307\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6958 - acc: 0.7134 - val_loss: 0.6522 - val_acc: 0.7297\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.65166 to 0.64941, saving model to best.model\n",
      "0s - loss: 0.6925 - acc: 0.7155 - val_loss: 0.6494 - val_acc: 0.7333\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.64941 to 0.64793, saving model to best.model\n",
      "1s - loss: 0.6926 - acc: 0.7171 - val_loss: 0.6479 - val_acc: 0.7335\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6934 - acc: 0.7169 - val_loss: 0.6493 - val_acc: 0.7321\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.64793 to 0.64710, saving model to best.model\n",
      "0s - loss: 0.6902 - acc: 0.7188 - val_loss: 0.6471 - val_acc: 0.7336\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6883 - acc: 0.7207 - val_loss: 0.6478 - val_acc: 0.7328\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6904 - acc: 0.7163 - val_loss: 0.6474 - val_acc: 0.7322\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6889 - acc: 0.7163 - val_loss: 0.6492 - val_acc: 0.7310\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6883 - acc: 0.7184 - val_loss: 0.6480 - val_acc: 0.7304\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.64710 to 0.64610, saving model to best.model\n",
      "1s - loss: 0.6876 - acc: 0.7185 - val_loss: 0.6461 - val_acc: 0.7340\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.64610 to 0.64302, saving model to best.model\n",
      "1s - loss: 0.6859 - acc: 0.7190 - val_loss: 0.6430 - val_acc: 0.7338\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.64302 to 0.64225, saving model to best.model\n",
      "1s - loss: 0.6866 - acc: 0.7190 - val_loss: 0.6422 - val_acc: 0.7345\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64225 to 0.64091, saving model to best.model\n",
      "1s - loss: 0.6837 - acc: 0.7215 - val_loss: 0.6409 - val_acc: 0.7377\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6838 - acc: 0.7192 - val_loss: 0.6423 - val_acc: 0.7378\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6872 - acc: 0.7196 - val_loss: 0.6416 - val_acc: 0.7354\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "1s - loss: 0.6850 - acc: 0.7204 - val_loss: 0.6421 - val_acc: 0.7363\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.6813 - acc: 0.7222 - val_loss: 0.6418 - val_acc: 0.7392\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.64091 to 0.63942, saving model to best.model\n",
      "0s - loss: 0.6795 - acc: 0.7236 - val_loss: 0.6394 - val_acc: 0.7358\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.63942 to 0.63894, saving model to best.model\n",
      "0s - loss: 0.6820 - acc: 0.7205 - val_loss: 0.6389 - val_acc: 0.7361\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "0s - loss: 0.6797 - acc: 0.7229 - val_loss: 0.6395 - val_acc: 0.7370\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.63894 to 0.63837, saving model to best.model\n",
      "0s - loss: 0.6814 - acc: 0.7197 - val_loss: 0.6384 - val_acc: 0.7369\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63837 to 0.63604, saving model to best.model\n",
      "1s - loss: 0.6788 - acc: 0.7218 - val_loss: 0.6360 - val_acc: 0.7389\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6774 - acc: 0.7239 - val_loss: 0.6372 - val_acc: 0.7371\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.63604 to 0.63406, saving model to best.model\n",
      "1s - loss: 0.6759 - acc: 0.7250 - val_loss: 0.6341 - val_acc: 0.7398\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6755 - acc: 0.7241 - val_loss: 0.6355 - val_acc: 0.7392\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6775 - acc: 0.7216 - val_loss: 0.6373 - val_acc: 0.7354\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.63406 to 0.63107, saving model to best.model\n",
      "1s - loss: 0.6760 - acc: 0.7237 - val_loss: 0.6311 - val_acc: 0.7411\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "1s - loss: 0.6745 - acc: 0.7233 - val_loss: 0.6318 - val_acc: 0.7410\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63107 to 0.63024, saving model to best.model\n",
      "1s - loss: 0.6716 - acc: 0.7252 - val_loss: 0.6302 - val_acc: 0.7420\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.6742 - acc: 0.7229 - val_loss: 0.6337 - val_acc: 0.7376\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63024 to 0.62983, saving model to best.model\n",
      "0s - loss: 0.6726 - acc: 0.7247 - val_loss: 0.6298 - val_acc: 0.7422\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62983 to 0.62925, saving model to best.model\n",
      "0s - loss: 0.6744 - acc: 0.7241 - val_loss: 0.6293 - val_acc: 0.7419\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62925 to 0.62875, saving model to best.model\n",
      "0s - loss: 0.6710 - acc: 0.7252 - val_loss: 0.6287 - val_acc: 0.7420\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.6730 - acc: 0.7244 - val_loss: 0.6294 - val_acc: 0.7411\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6696 - acc: 0.7246 - val_loss: 0.6307 - val_acc: 0.7374\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.62875 to 0.62650, saving model to best.model\n",
      "0s - loss: 0.6690 - acc: 0.7277 - val_loss: 0.6265 - val_acc: 0.7422\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.6714 - acc: 0.7242 - val_loss: 0.6278 - val_acc: 0.7431\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.62650 to 0.62472, saving model to best.model\n",
      "0s - loss: 0.6682 - acc: 0.7267 - val_loss: 0.6247 - val_acc: 0.7440\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.62472 to 0.62342, saving model to best.model\n",
      "0s - loss: 0.6685 - acc: 0.7260 - val_loss: 0.6234 - val_acc: 0.7436\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.6686 - acc: 0.7255 - val_loss: 0.6263 - val_acc: 0.7423\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.6691 - acc: 0.7258 - val_loss: 0.6261 - val_acc: 0.7415\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.6676 - acc: 0.7271 - val_loss: 0.6242 - val_acc: 0.7430\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6684 - acc: 0.7267 - val_loss: 0.6246 - val_acc: 0.7433\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6679 - acc: 0.7269 - val_loss: 0.6239 - val_acc: 0.7424\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6673 - acc: 0.7266 - val_loss: 0.6246 - val_acc: 0.7423\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.62342 to 0.62183, saving model to best.model\n",
      "0s - loss: 0.6659 - acc: 0.7275 - val_loss: 0.6218 - val_acc: 0.7446\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.6659 - acc: 0.7277 - val_loss: 0.6235 - val_acc: 0.7444\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.62183 to 0.62048, saving model to best.model\n",
      "0s - loss: 0.6671 - acc: 0.7249 - val_loss: 0.6205 - val_acc: 0.7446\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.6636 - acc: 0.7298 - val_loss: 0.6222 - val_acc: 0.7437\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6630 - acc: 0.7288 - val_loss: 0.6217 - val_acc: 0.7426\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.62048 to 0.61923, saving model to best.model\n",
      "1s - loss: 0.6623 - acc: 0.7293 - val_loss: 0.6192 - val_acc: 0.7452\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.6654 - acc: 0.7288 - val_loss: 0.6209 - val_acc: 0.7438\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.61923 to 0.61847, saving model to best.model\n",
      "1s - loss: 0.6613 - acc: 0.7318 - val_loss: 0.6185 - val_acc: 0.7454\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61847 to 0.61651, saving model to best.model\n",
      "1s - loss: 0.6621 - acc: 0.7276 - val_loss: 0.6165 - val_acc: 0.7444\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6617 - acc: 0.7288 - val_loss: 0.6178 - val_acc: 0.7453\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6618 - acc: 0.7295 - val_loss: 0.6170 - val_acc: 0.7456\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6605 - acc: 0.7303 - val_loss: 0.6171 - val_acc: 0.7451\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.6589 - acc: 0.7309 - val_loss: 0.6172 - val_acc: 0.7437\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61651 to 0.61601, saving model to best.model\n",
      "1s - loss: 0.6593 - acc: 0.7318 - val_loss: 0.6160 - val_acc: 0.7445\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.61601 to 0.61524, saving model to best.model\n",
      "1s - loss: 0.6586 - acc: 0.7294 - val_loss: 0.6152 - val_acc: 0.7456\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61524 to 0.61259, saving model to best.model\n",
      "1s - loss: 0.6571 - acc: 0.7288 - val_loss: 0.6126 - val_acc: 0.7475\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6591 - acc: 0.7287 - val_loss: 0.6135 - val_acc: 0.7461\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "1s - loss: 0.6578 - acc: 0.7329 - val_loss: 0.6147 - val_acc: 0.7467\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6562 - acc: 0.7306 - val_loss: 0.6130 - val_acc: 0.7461\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "1s - loss: 0.6591 - acc: 0.7282 - val_loss: 0.6150 - val_acc: 0.7461\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.6575 - acc: 0.7295 - val_loss: 0.6131 - val_acc: 0.7460\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.61259 to 0.61022, saving model to best.model\n",
      "1s - loss: 0.6530 - acc: 0.7327 - val_loss: 0.6102 - val_acc: 0.7482\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6550 - acc: 0.7305 - val_loss: 0.6103 - val_acc: 0.7472\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.61022 to 0.61012, saving model to best.model\n",
      "1s - loss: 0.6578 - acc: 0.7320 - val_loss: 0.6101 - val_acc: 0.7482\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.6555 - acc: 0.7319 - val_loss: 0.6108 - val_acc: 0.7474\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.6536 - acc: 0.7316 - val_loss: 0.6114 - val_acc: 0.7475\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.61012 to 0.60930, saving model to best.model\n",
      "1s - loss: 0.6537 - acc: 0.7331 - val_loss: 0.6093 - val_acc: 0.7477\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60930 to 0.60916, saving model to best.model\n",
      "1s - loss: 0.6555 - acc: 0.7305 - val_loss: 0.6092 - val_acc: 0.7489\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.60916 to 0.60891, saving model to best.model\n",
      "1s - loss: 0.6552 - acc: 0.7316 - val_loss: 0.6089 - val_acc: 0.7477\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.60891 to 0.60442, saving model to best.model\n",
      "1s - loss: 0.6509 - acc: 0.7333 - val_loss: 0.6044 - val_acc: 0.7499\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6500 - acc: 0.7331 - val_loss: 0.6065 - val_acc: 0.7500\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.6516 - acc: 0.7340 - val_loss: 0.6067 - val_acc: 0.7499\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6499 - acc: 0.7350 - val_loss: 0.6058 - val_acc: 0.7484\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.6480 - acc: 0.7345 - val_loss: 0.6050 - val_acc: 0.7486\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.6495 - acc: 0.7357 - val_loss: 0.6044 - val_acc: 0.7515\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6486 - acc: 0.7334 - val_loss: 0.6053 - val_acc: 0.7499\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.60442 to 0.60395, saving model to best.model\n",
      "1s - loss: 0.6511 - acc: 0.7346 - val_loss: 0.6039 - val_acc: 0.7500\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.83852, saving model to best.model\n",
      "1s - loss: 0.9191 - acc: 0.6278 - val_loss: 0.8385 - val_acc: 0.6619\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.83852 to 0.83726, saving model to best.model\n",
      "1s - loss: 0.8569 - acc: 0.6610 - val_loss: 0.8373 - val_acc: 0.6619\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "1s - loss: 0.8486 - acc: 0.6615 - val_loss: 0.8379 - val_acc: 0.6619\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.83726 to 0.83575, saving model to best.model\n",
      "1s - loss: 0.8455 - acc: 0.6615 - val_loss: 0.8358 - val_acc: 0.6619\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.83575 to 0.83013, saving model to best.model\n",
      "1s - loss: 0.8416 - acc: 0.6615 - val_loss: 0.8301 - val_acc: 0.6619\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83013 to 0.82425, saving model to best.model\n",
      "1s - loss: 0.8358 - acc: 0.6615 - val_loss: 0.8242 - val_acc: 0.6619\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.82425 to 0.82190, saving model to best.model\n",
      "1s - loss: 0.8311 - acc: 0.6615 - val_loss: 0.8219 - val_acc: 0.6619\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82190 to 0.81913, saving model to best.model\n",
      "1s - loss: 0.8288 - acc: 0.6615 - val_loss: 0.8191 - val_acc: 0.6619\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.81913 to 0.81779, saving model to best.model\n",
      "1s - loss: 0.8278 - acc: 0.6614 - val_loss: 0.8178 - val_acc: 0.6619\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.81779 to 0.81692, saving model to best.model\n",
      "0s - loss: 0.8268 - acc: 0.6614 - val_loss: 0.8169 - val_acc: 0.6619\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.81692 to 0.81610, saving model to best.model\n",
      "0s - loss: 0.8238 - acc: 0.6614 - val_loss: 0.8161 - val_acc: 0.6619\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.81610 to 0.81544, saving model to best.model\n",
      "1s - loss: 0.8233 - acc: 0.6613 - val_loss: 0.8154 - val_acc: 0.6619\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.81544 to 0.81529, saving model to best.model\n",
      "0s - loss: 0.8220 - acc: 0.6613 - val_loss: 0.8153 - val_acc: 0.6619\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.81529 to 0.81408, saving model to best.model\n",
      "1s - loss: 0.8224 - acc: 0.6615 - val_loss: 0.8141 - val_acc: 0.6619\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "1s - loss: 0.8197 - acc: 0.6611 - val_loss: 0.8144 - val_acc: 0.6619\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.81408 to 0.81306, saving model to best.model\n",
      "0s - loss: 0.8203 - acc: 0.6614 - val_loss: 0.8131 - val_acc: 0.6619\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.81306 to 0.81226, saving model to best.model\n",
      "1s - loss: 0.8186 - acc: 0.6617 - val_loss: 0.8123 - val_acc: 0.6619\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.81226 to 0.81167, saving model to best.model\n",
      "1s - loss: 0.8185 - acc: 0.6614 - val_loss: 0.8117 - val_acc: 0.6619\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.81167 to 0.81142, saving model to best.model\n",
      "1s - loss: 0.8159 - acc: 0.6617 - val_loss: 0.8114 - val_acc: 0.6619\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.81142 to 0.81086, saving model to best.model\n",
      "1s - loss: 0.8160 - acc: 0.6622 - val_loss: 0.8109 - val_acc: 0.6623\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.81086 to 0.80938, saving model to best.model\n",
      "1s - loss: 0.8148 - acc: 0.6628 - val_loss: 0.8094 - val_acc: 0.6658\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.80938 to 0.80862, saving model to best.model\n",
      "1s - loss: 0.8142 - acc: 0.6628 - val_loss: 0.8086 - val_acc: 0.6624\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.80862 to 0.80838, saving model to best.model\n",
      "1s - loss: 0.8133 - acc: 0.6636 - val_loss: 0.8084 - val_acc: 0.6644\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.80838 to 0.80797, saving model to best.model\n",
      "1s - loss: 0.8126 - acc: 0.6636 - val_loss: 0.8080 - val_acc: 0.6625\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.80797 to 0.80616, saving model to best.model\n",
      "1s - loss: 0.8113 - acc: 0.6644 - val_loss: 0.8062 - val_acc: 0.6653\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.80616 to 0.80584, saving model to best.model\n",
      "1s - loss: 0.8112 - acc: 0.6652 - val_loss: 0.8058 - val_acc: 0.6642\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.80584 to 0.80430, saving model to best.model\n",
      "1s - loss: 0.8081 - acc: 0.6648 - val_loss: 0.8043 - val_acc: 0.6655\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.80430 to 0.80332, saving model to best.model\n",
      "1s - loss: 0.8084 - acc: 0.6653 - val_loss: 0.8033 - val_acc: 0.6680\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.80332 to 0.80290, saving model to best.model\n",
      "1s - loss: 0.8071 - acc: 0.6656 - val_loss: 0.8029 - val_acc: 0.6687\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.80290 to 0.80144, saving model to best.model\n",
      "1s - loss: 0.8070 - acc: 0.6656 - val_loss: 0.8014 - val_acc: 0.6659\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80144 to 0.79910, saving model to best.model\n",
      "1s - loss: 0.8057 - acc: 0.6660 - val_loss: 0.7991 - val_acc: 0.6713\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.79910 to 0.79829, saving model to best.model\n",
      "1s - loss: 0.8039 - acc: 0.6676 - val_loss: 0.7983 - val_acc: 0.6711\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.79829 to 0.79725, saving model to best.model\n",
      "1s - loss: 0.8032 - acc: 0.6672 - val_loss: 0.7972 - val_acc: 0.6737\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.79725 to 0.79459, saving model to best.model\n",
      "1s - loss: 0.8014 - acc: 0.6666 - val_loss: 0.7946 - val_acc: 0.6735\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.79459 to 0.79270, saving model to best.model\n",
      "1s - loss: 0.8003 - acc: 0.6681 - val_loss: 0.7927 - val_acc: 0.6741\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79270 to 0.79223, saving model to best.model\n",
      "1s - loss: 0.7995 - acc: 0.6675 - val_loss: 0.7922 - val_acc: 0.6714\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79223 to 0.79109, saving model to best.model\n",
      "1s - loss: 0.7973 - acc: 0.6681 - val_loss: 0.7911 - val_acc: 0.6707\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79109 to 0.78846, saving model to best.model\n",
      "1s - loss: 0.7960 - acc: 0.6686 - val_loss: 0.7885 - val_acc: 0.6742\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.78846 to 0.78592, saving model to best.model\n",
      "1s - loss: 0.7940 - acc: 0.6706 - val_loss: 0.7859 - val_acc: 0.6732\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.78592 to 0.78393, saving model to best.model\n",
      "1s - loss: 0.7928 - acc: 0.6703 - val_loss: 0.7839 - val_acc: 0.6728\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.78393 to 0.78014, saving model to best.model\n",
      "1s - loss: 0.7904 - acc: 0.6725 - val_loss: 0.7801 - val_acc: 0.6776\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78014 to 0.77870, saving model to best.model\n",
      "1s - loss: 0.7899 - acc: 0.6726 - val_loss: 0.7787 - val_acc: 0.6800\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.77870 to 0.77645, saving model to best.model\n",
      "1s - loss: 0.7866 - acc: 0.6734 - val_loss: 0.7764 - val_acc: 0.6776\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.77645 to 0.77332, saving model to best.model\n",
      "1s - loss: 0.7858 - acc: 0.6746 - val_loss: 0.7733 - val_acc: 0.6827\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.77332 to 0.77213, saving model to best.model\n",
      "1s - loss: 0.7848 - acc: 0.6730 - val_loss: 0.7721 - val_acc: 0.6793\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.77213 to 0.77050, saving model to best.model\n",
      "1s - loss: 0.7834 - acc: 0.6732 - val_loss: 0.7705 - val_acc: 0.6795\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77050 to 0.76699, saving model to best.model\n",
      "1s - loss: 0.7807 - acc: 0.6765 - val_loss: 0.7670 - val_acc: 0.6862\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.76699 to 0.76372, saving model to best.model\n",
      "1s - loss: 0.7783 - acc: 0.6778 - val_loss: 0.7637 - val_acc: 0.6867\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.76372 to 0.76019, saving model to best.model\n",
      "1s - loss: 0.7753 - acc: 0.6766 - val_loss: 0.7602 - val_acc: 0.6881\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.7786 - acc: 0.6781 - val_loss: 0.7603 - val_acc: 0.6881\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.76019 to 0.75838, saving model to best.model\n",
      "1s - loss: 0.7747 - acc: 0.6776 - val_loss: 0.7584 - val_acc: 0.6852\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.75838 to 0.75256, saving model to best.model\n",
      "1s - loss: 0.7712 - acc: 0.6812 - val_loss: 0.7526 - val_acc: 0.6904\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.75256 to 0.75039, saving model to best.model\n",
      "1s - loss: 0.7700 - acc: 0.6784 - val_loss: 0.7504 - val_acc: 0.6947\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.75039 to 0.74682, saving model to best.model\n",
      "1s - loss: 0.7668 - acc: 0.6817 - val_loss: 0.7468 - val_acc: 0.6934\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.74682 to 0.74423, saving model to best.model\n",
      "1s - loss: 0.7632 - acc: 0.6834 - val_loss: 0.7442 - val_acc: 0.6941\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.74423 to 0.74309, saving model to best.model\n",
      "0s - loss: 0.7646 - acc: 0.6841 - val_loss: 0.7431 - val_acc: 0.6927\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.74309 to 0.74210, saving model to best.model\n",
      "1s - loss: 0.7632 - acc: 0.6854 - val_loss: 0.7421 - val_acc: 0.6989\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.74210 to 0.73460, saving model to best.model\n",
      "1s - loss: 0.7570 - acc: 0.6871 - val_loss: 0.7346 - val_acc: 0.6989\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.73460 to 0.73342, saving model to best.model\n",
      "1s - loss: 0.7572 - acc: 0.6849 - val_loss: 0.7334 - val_acc: 0.6996\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.73342 to 0.73231, saving model to best.model\n",
      "1s - loss: 0.7573 - acc: 0.6870 - val_loss: 0.7323 - val_acc: 0.7012\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.73231 to 0.72879, saving model to best.model\n",
      "1s - loss: 0.7545 - acc: 0.6895 - val_loss: 0.7288 - val_acc: 0.7077\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.72879 to 0.72507, saving model to best.model\n",
      "1s - loss: 0.7532 - acc: 0.6880 - val_loss: 0.7251 - val_acc: 0.7041\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.72507 to 0.72178, saving model to best.model\n",
      "1s - loss: 0.7491 - acc: 0.6915 - val_loss: 0.7218 - val_acc: 0.7037\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "1s - loss: 0.7487 - acc: 0.6908 - val_loss: 0.7224 - val_acc: 0.7046\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.72178 to 0.71963, saving model to best.model\n",
      "1s - loss: 0.7468 - acc: 0.6918 - val_loss: 0.7196 - val_acc: 0.7073\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.71963 to 0.71519, saving model to best.model\n",
      "1s - loss: 0.7458 - acc: 0.6929 - val_loss: 0.7152 - val_acc: 0.7102\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.71519 to 0.71302, saving model to best.model\n",
      "0s - loss: 0.7443 - acc: 0.6941 - val_loss: 0.7130 - val_acc: 0.7135\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.71302 to 0.71195, saving model to best.model\n",
      "1s - loss: 0.7424 - acc: 0.6928 - val_loss: 0.7119 - val_acc: 0.7095\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.71195 to 0.70812, saving model to best.model\n",
      "1s - loss: 0.7393 - acc: 0.6955 - val_loss: 0.7081 - val_acc: 0.7158\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.70812 to 0.70648, saving model to best.model\n",
      "1s - loss: 0.7406 - acc: 0.6949 - val_loss: 0.7065 - val_acc: 0.7121\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.70648 to 0.70362, saving model to best.model\n",
      "1s - loss: 0.7389 - acc: 0.6973 - val_loss: 0.7036 - val_acc: 0.7155\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.70362 to 0.70035, saving model to best.model\n",
      "1s - loss: 0.7348 - acc: 0.6953 - val_loss: 0.7004 - val_acc: 0.7199\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.70035 to 0.70033, saving model to best.model\n",
      "1s - loss: 0.7344 - acc: 0.6977 - val_loss: 0.7003 - val_acc: 0.7174\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.70033 to 0.69716, saving model to best.model\n",
      "1s - loss: 0.7326 - acc: 0.6986 - val_loss: 0.6972 - val_acc: 0.7191\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.69716 to 0.69462, saving model to best.model\n",
      "0s - loss: 0.7309 - acc: 0.6997 - val_loss: 0.6946 - val_acc: 0.7211\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.69462 to 0.69419, saving model to best.model\n",
      "1s - loss: 0.7296 - acc: 0.7008 - val_loss: 0.6942 - val_acc: 0.7215\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.69419 to 0.69136, saving model to best.model\n",
      "1s - loss: 0.7291 - acc: 0.7007 - val_loss: 0.6914 - val_acc: 0.7227\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.69136 to 0.69077, saving model to best.model\n",
      "1s - loss: 0.7261 - acc: 0.7031 - val_loss: 0.6908 - val_acc: 0.7221\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7295 - acc: 0.6988 - val_loss: 0.6911 - val_acc: 0.7190\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.69077 to 0.68885, saving model to best.model\n",
      "1s - loss: 0.7252 - acc: 0.7006 - val_loss: 0.6888 - val_acc: 0.7227\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.68885 to 0.68298, saving model to best.model\n",
      "0s - loss: 0.7241 - acc: 0.7031 - val_loss: 0.6830 - val_acc: 0.7281\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.7210 - acc: 0.7056 - val_loss: 0.6836 - val_acc: 0.7245\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.68298 to 0.68117, saving model to best.model\n",
      "0s - loss: 0.7173 - acc: 0.7055 - val_loss: 0.6812 - val_acc: 0.7288\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.68117 to 0.67945, saving model to best.model\n",
      "0s - loss: 0.7193 - acc: 0.7061 - val_loss: 0.6795 - val_acc: 0.7262\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss did not improve\n",
      "0s - loss: 0.7201 - acc: 0.7061 - val_loss: 0.6812 - val_acc: 0.7254\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.67945 to 0.67880, saving model to best.model\n",
      "0s - loss: 0.7178 - acc: 0.7063 - val_loss: 0.6788 - val_acc: 0.7286\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.67880 to 0.67858, saving model to best.model\n",
      "0s - loss: 0.7134 - acc: 0.7071 - val_loss: 0.6786 - val_acc: 0.7241\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.67858 to 0.67264, saving model to best.model\n",
      "1s - loss: 0.7130 - acc: 0.7084 - val_loss: 0.6726 - val_acc: 0.7306\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.7124 - acc: 0.7066 - val_loss: 0.6736 - val_acc: 0.7288\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.67264 to 0.67048, saving model to best.model\n",
      "1s - loss: 0.7127 - acc: 0.7083 - val_loss: 0.6705 - val_acc: 0.7321\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "1s - loss: 0.7119 - acc: 0.7077 - val_loss: 0.6728 - val_acc: 0.7294\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.67048 to 0.66837, saving model to best.model\n",
      "1s - loss: 0.7100 - acc: 0.7102 - val_loss: 0.6684 - val_acc: 0.7342\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7091 - acc: 0.7087 - val_loss: 0.6693 - val_acc: 0.7336\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.66837 to 0.66525, saving model to best.model\n",
      "1s - loss: 0.7076 - acc: 0.7118 - val_loss: 0.6653 - val_acc: 0.7328\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "1s - loss: 0.7059 - acc: 0.7114 - val_loss: 0.6656 - val_acc: 0.7344\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.66525 to 0.66341, saving model to best.model\n",
      "1s - loss: 0.7067 - acc: 0.7098 - val_loss: 0.6634 - val_acc: 0.7347\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.66341 to 0.66253, saving model to best.model\n",
      "1s - loss: 0.7042 - acc: 0.7119 - val_loss: 0.6625 - val_acc: 0.7334\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.7074 - acc: 0.7099 - val_loss: 0.6665 - val_acc: 0.7280\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "1s - loss: 0.7033 - acc: 0.7136 - val_loss: 0.6634 - val_acc: 0.7326\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.66253 to 0.66211, saving model to best.model\n",
      "1s - loss: 0.7011 - acc: 0.7128 - val_loss: 0.6621 - val_acc: 0.7340\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.66211 to 0.66190, saving model to best.model\n",
      "1s - loss: 0.7021 - acc: 0.7120 - val_loss: 0.6619 - val_acc: 0.7335\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.66190 to 0.65825, saving model to best.model\n",
      "1s - loss: 0.7011 - acc: 0.7155 - val_loss: 0.6582 - val_acc: 0.7342\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.65825 to 0.65604, saving model to best.model\n",
      "1s - loss: 0.6999 - acc: 0.7140 - val_loss: 0.6560 - val_acc: 0.7361\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "1s - loss: 0.6985 - acc: 0.7174 - val_loss: 0.6565 - val_acc: 0.7370\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.6984 - acc: 0.7151 - val_loss: 0.6592 - val_acc: 0.7348\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.65604 to 0.65328, saving model to best.model\n",
      "1s - loss: 0.6975 - acc: 0.7141 - val_loss: 0.6533 - val_acc: 0.7367\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.65328 to 0.65313, saving model to best.model\n",
      "1s - loss: 0.6948 - acc: 0.7166 - val_loss: 0.6531 - val_acc: 0.7368\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.65313 to 0.65140, saving model to best.model\n",
      "1s - loss: 0.6930 - acc: 0.7168 - val_loss: 0.6514 - val_acc: 0.7367\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "1s - loss: 0.6960 - acc: 0.7158 - val_loss: 0.6532 - val_acc: 0.7375\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.65140 to 0.64942, saving model to best.model\n",
      "1s - loss: 0.6931 - acc: 0.7170 - val_loss: 0.6494 - val_acc: 0.7362\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.64942 to 0.64920, saving model to best.model\n",
      "1s - loss: 0.6933 - acc: 0.7163 - val_loss: 0.6492 - val_acc: 0.7381\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.64920 to 0.64912, saving model to best.model\n",
      "1s - loss: 0.6908 - acc: 0.7196 - val_loss: 0.6491 - val_acc: 0.7370\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss did not improve\n",
      "1s - loss: 0.6927 - acc: 0.7176 - val_loss: 0.6498 - val_acc: 0.7370\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.64912 to 0.64599, saving model to best.model\n",
      "1s - loss: 0.6868 - acc: 0.7211 - val_loss: 0.6460 - val_acc: 0.7385\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6907 - acc: 0.7190 - val_loss: 0.6484 - val_acc: 0.7376\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.64599 to 0.64499, saving model to best.model\n",
      "1s - loss: 0.6869 - acc: 0.7198 - val_loss: 0.6450 - val_acc: 0.7403\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.64499 to 0.64417, saving model to best.model\n",
      "1s - loss: 0.6873 - acc: 0.7200 - val_loss: 0.6442 - val_acc: 0.7384\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss did not improve\n",
      "1s - loss: 0.6905 - acc: 0.7180 - val_loss: 0.6454 - val_acc: 0.7399\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "1s - loss: 0.6857 - acc: 0.7204 - val_loss: 0.6476 - val_acc: 0.7365\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.64417 to 0.64194, saving model to best.model\n",
      "1s - loss: 0.6802 - acc: 0.7219 - val_loss: 0.6419 - val_acc: 0.7393\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss did not improve\n",
      "1s - loss: 0.6844 - acc: 0.7210 - val_loss: 0.6454 - val_acc: 0.7376\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.64194 to 0.63948, saving model to best.model\n",
      "1s - loss: 0.6813 - acc: 0.7212 - val_loss: 0.6395 - val_acc: 0.7389\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.63948 to 0.63792, saving model to best.model\n",
      "1s - loss: 0.6813 - acc: 0.7226 - val_loss: 0.6379 - val_acc: 0.7393\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.63792 to 0.63791, saving model to best.model\n",
      "1s - loss: 0.6800 - acc: 0.7202 - val_loss: 0.6379 - val_acc: 0.7392\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss did not improve\n",
      "1s - loss: 0.6831 - acc: 0.7224 - val_loss: 0.6405 - val_acc: 0.7390\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.6786 - acc: 0.7239 - val_loss: 0.6380 - val_acc: 0.7403\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.63791 to 0.63640, saving model to best.model\n",
      "1s - loss: 0.6792 - acc: 0.7227 - val_loss: 0.6364 - val_acc: 0.7402\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "1s - loss: 0.6803 - acc: 0.7244 - val_loss: 0.6414 - val_acc: 0.7365\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.63640 to 0.63639, saving model to best.model\n",
      "1s - loss: 0.6777 - acc: 0.7248 - val_loss: 0.6364 - val_acc: 0.7392\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.63639 to 0.63387, saving model to best.model\n",
      "1s - loss: 0.6795 - acc: 0.7253 - val_loss: 0.6339 - val_acc: 0.7426\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.63387 to 0.63326, saving model to best.model\n",
      "1s - loss: 0.6780 - acc: 0.7245 - val_loss: 0.6333 - val_acc: 0.7415\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6754 - acc: 0.7249 - val_loss: 0.6360 - val_acc: 0.7386\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6754 - acc: 0.7244 - val_loss: 0.6333 - val_acc: 0.7418\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.63326 to 0.63268, saving model to best.model\n",
      "1s - loss: 0.6756 - acc: 0.7245 - val_loss: 0.6327 - val_acc: 0.7415\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6750 - acc: 0.7232 - val_loss: 0.6334 - val_acc: 0.7417\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.63268 to 0.62958, saving model to best.model\n",
      "1s - loss: 0.6735 - acc: 0.7249 - val_loss: 0.6296 - val_acc: 0.7420\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.62958 to 0.62918, saving model to best.model\n",
      "1s - loss: 0.6710 - acc: 0.7274 - val_loss: 0.6292 - val_acc: 0.7440\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6757 - acc: 0.7251 - val_loss: 0.6304 - val_acc: 0.7440\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "0s - loss: 0.6728 - acc: 0.7251 - val_loss: 0.6303 - val_acc: 0.7438\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss did not improve\n",
      "0s - loss: 0.6738 - acc: 0.7276 - val_loss: 0.6303 - val_acc: 0.7440\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6705 - acc: 0.7274 - val_loss: 0.6306 - val_acc: 0.7447\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.62918 to 0.62641, saving model to best.model\n",
      "1s - loss: 0.6691 - acc: 0.7288 - val_loss: 0.6264 - val_acc: 0.7443\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.6727 - acc: 0.7272 - val_loss: 0.6300 - val_acc: 0.7444\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6695 - acc: 0.7282 - val_loss: 0.6278 - val_acc: 0.7438\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6688 - acc: 0.7282 - val_loss: 0.6312 - val_acc: 0.7416\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.62641 to 0.62593, saving model to best.model\n",
      "1s - loss: 0.6643 - acc: 0.7319 - val_loss: 0.6259 - val_acc: 0.7438\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "1s - loss: 0.6699 - acc: 0.7272 - val_loss: 0.6278 - val_acc: 0.7434\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6665 - acc: 0.7285 - val_loss: 0.6263 - val_acc: 0.7443\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.62593 to 0.62241, saving model to best.model\n",
      "1s - loss: 0.6666 - acc: 0.7297 - val_loss: 0.6224 - val_acc: 0.7443\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "1s - loss: 0.6630 - acc: 0.7298 - val_loss: 0.6228 - val_acc: 0.7454\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62241 to 0.61879, saving model to best.model\n",
      "1s - loss: 0.6632 - acc: 0.7291 - val_loss: 0.6188 - val_acc: 0.7471\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "1s - loss: 0.6654 - acc: 0.7291 - val_loss: 0.6202 - val_acc: 0.7464\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.6639 - acc: 0.7281 - val_loss: 0.6203 - val_acc: 0.7457\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6668 - acc: 0.7282 - val_loss: 0.6211 - val_acc: 0.7454\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "1s - loss: 0.6645 - acc: 0.7306 - val_loss: 0.6198 - val_acc: 0.7448\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6632 - acc: 0.7302 - val_loss: 0.6226 - val_acc: 0.7468\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.61879 to 0.61851, saving model to best.model\n",
      "1s - loss: 0.6634 - acc: 0.7314 - val_loss: 0.6185 - val_acc: 0.7471\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.61851 to 0.61739, saving model to best.model\n",
      "1s - loss: 0.6616 - acc: 0.7307 - val_loss: 0.6174 - val_acc: 0.7464\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.61739 to 0.61643, saving model to best.model\n",
      "1s - loss: 0.6601 - acc: 0.7321 - val_loss: 0.6164 - val_acc: 0.7465\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6610 - acc: 0.7292 - val_loss: 0.6194 - val_acc: 0.7472\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.61643 to 0.61625, saving model to best.model\n",
      "1s - loss: 0.6588 - acc: 0.7306 - val_loss: 0.6163 - val_acc: 0.7489\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.61625 to 0.61560, saving model to best.model\n",
      "1s - loss: 0.6567 - acc: 0.7304 - val_loss: 0.6156 - val_acc: 0.7473\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "1s - loss: 0.6600 - acc: 0.7317 - val_loss: 0.6169 - val_acc: 0.7466\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "1s - loss: 0.6597 - acc: 0.7313 - val_loss: 0.6167 - val_acc: 0.7475\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6589 - acc: 0.7335 - val_loss: 0.6186 - val_acc: 0.7474\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.61560 to 0.61437, saving model to best.model\n",
      "1s - loss: 0.6561 - acc: 0.7341 - val_loss: 0.6144 - val_acc: 0.7479\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.61437 to 0.61268, saving model to best.model\n",
      "1s - loss: 0.6568 - acc: 0.7319 - val_loss: 0.6127 - val_acc: 0.7487\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.6572 - acc: 0.7301 - val_loss: 0.6136 - val_acc: 0.7495\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "1s - loss: 0.6571 - acc: 0.7322 - val_loss: 0.6139 - val_acc: 0.7485\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "1s - loss: 0.6568 - acc: 0.7333 - val_loss: 0.6157 - val_acc: 0.7481\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6556 - acc: 0.7326 - val_loss: 0.6153 - val_acc: 0.7479\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.61268 to 0.61148, saving model to best.model\n",
      "1s - loss: 0.6522 - acc: 0.7338 - val_loss: 0.6115 - val_acc: 0.7501\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.61148 to 0.60882, saving model to best.model\n",
      "1s - loss: 0.6521 - acc: 0.7353 - val_loss: 0.6088 - val_acc: 0.7501\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.6546 - acc: 0.7340 - val_loss: 0.6115 - val_acc: 0.7491\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "1s - loss: 0.6510 - acc: 0.7349 - val_loss: 0.6112 - val_acc: 0.7501\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6545 - acc: 0.7340 - val_loss: 0.6105 - val_acc: 0.7489\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.60882 to 0.60843, saving model to best.model\n",
      "1s - loss: 0.6533 - acc: 0.7326 - val_loss: 0.6084 - val_acc: 0.7508\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.60843 to 0.60771, saving model to best.model\n",
      "1s - loss: 0.6505 - acc: 0.7357 - val_loss: 0.6077 - val_acc: 0.7508\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.6505 - acc: 0.7345 - val_loss: 0.6085 - val_acc: 0.7507\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6490 - acc: 0.7355 - val_loss: 0.6079 - val_acc: 0.7499\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60771 to 0.60745, saving model to best.model\n",
      "1s - loss: 0.6456 - acc: 0.7366 - val_loss: 0.6075 - val_acc: 0.7498\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6501 - acc: 0.7328 - val_loss: 0.6098 - val_acc: 0.7494\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.6518 - acc: 0.7352 - val_loss: 0.6077 - val_acc: 0.7502\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.60745 to 0.60600, saving model to best.model\n",
      "1s - loss: 0.6521 - acc: 0.7336 - val_loss: 0.6060 - val_acc: 0.7509\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.6450 - acc: 0.7361 - val_loss: 0.6073 - val_acc: 0.7508\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6512 - acc: 0.7337 - val_loss: 0.6065 - val_acc: 0.7525\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.60600 to 0.60549, saving model to best.model\n",
      "1s - loss: 0.6487 - acc: 0.7350 - val_loss: 0.6055 - val_acc: 0.7515\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60549 to 0.60513, saving model to best.model\n",
      "1s - loss: 0.6455 - acc: 0.7348 - val_loss: 0.6051 - val_acc: 0.7528\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.60513 to 0.60396, saving model to best.model\n",
      "1s - loss: 0.6459 - acc: 0.7361 - val_loss: 0.6040 - val_acc: 0.7520\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.60396 to 0.60246, saving model to best.model\n",
      "1s - loss: 0.6438 - acc: 0.7389 - val_loss: 0.6025 - val_acc: 0.7532\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "1s - loss: 0.6460 - acc: 0.7360 - val_loss: 0.6040 - val_acc: 0.7521\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "1s - loss: 0.6489 - acc: 0.7357 - val_loss: 0.6093 - val_acc: 0.7500\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6442 - acc: 0.7383 - val_loss: 0.6036 - val_acc: 0.7527\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.60246 to 0.60185, saving model to best.model\n",
      "1s - loss: 0.6446 - acc: 0.7384 - val_loss: 0.6018 - val_acc: 0.7530\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60185 to 0.60169, saving model to best.model\n",
      "1s - loss: 0.6427 - acc: 0.7393 - val_loss: 0.6017 - val_acc: 0.7533\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "1s - loss: 0.6441 - acc: 0.7376 - val_loss: 0.6051 - val_acc: 0.7499\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60169 to 0.60111, saving model to best.model\n",
      "1s - loss: 0.6421 - acc: 0.7385 - val_loss: 0.6011 - val_acc: 0.7526\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60111 to 0.60052, saving model to best.model\n",
      "1s - loss: 0.6442 - acc: 0.7404 - val_loss: 0.6005 - val_acc: 0.7542\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.60052 to 0.60022, saving model to best.model\n",
      "1s - loss: 0.6451 - acc: 0.7358 - val_loss: 0.6002 - val_acc: 0.7541\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.60022 to 0.59974, saving model to best.model\n",
      "1s - loss: 0.6427 - acc: 0.7371 - val_loss: 0.5997 - val_acc: 0.7544\n",
      "Train on 34156 samples, validate on 8540 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.85650, saving model to best.model\n",
      "1s - loss: 0.8911 - acc: 0.6445 - val_loss: 0.8565 - val_acc: 0.6529\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.85650 to 0.85086, saving model to best.model\n",
      "1s - loss: 0.8466 - acc: 0.6648 - val_loss: 0.8509 - val_acc: 0.6529\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.85086 to 0.84960, saving model to best.model\n",
      "1s - loss: 0.8424 - acc: 0.6648 - val_loss: 0.8496 - val_acc: 0.6529\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.84960 to 0.84258, saving model to best.model\n",
      "1s - loss: 0.8378 - acc: 0.6648 - val_loss: 0.8426 - val_acc: 0.6529\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.84258 to 0.83447, saving model to best.model\n",
      "1s - loss: 0.8310 - acc: 0.6648 - val_loss: 0.8345 - val_acc: 0.6529\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.83447 to 0.83052, saving model to best.model\n",
      "1s - loss: 0.8268 - acc: 0.6648 - val_loss: 0.8305 - val_acc: 0.6529\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.83052 to 0.82813, saving model to best.model\n",
      "1s - loss: 0.8223 - acc: 0.6648 - val_loss: 0.8281 - val_acc: 0.6529\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.82813 to 0.82742, saving model to best.model\n",
      "1s - loss: 0.8220 - acc: 0.6648 - val_loss: 0.8274 - val_acc: 0.6529\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.82742 to 0.82652, saving model to best.model\n",
      "1s - loss: 0.8192 - acc: 0.6649 - val_loss: 0.8265 - val_acc: 0.6529\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.8182 - acc: 0.6648 - val_loss: 0.8273 - val_acc: 0.6529\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.82652 to 0.82539, saving model to best.model\n",
      "1s - loss: 0.8179 - acc: 0.6649 - val_loss: 0.8254 - val_acc: 0.6529\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.82539 to 0.82515, saving model to best.model\n",
      "1s - loss: 0.8168 - acc: 0.6647 - val_loss: 0.8251 - val_acc: 0.6529\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.82515 to 0.82506, saving model to best.model\n",
      "1s - loss: 0.8162 - acc: 0.6649 - val_loss: 0.8251 - val_acc: 0.6529\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.82506 to 0.82424, saving model to best.model\n",
      "1s - loss: 0.8149 - acc: 0.6648 - val_loss: 0.8242 - val_acc: 0.6529\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.82424 to 0.82386, saving model to best.model\n",
      "1s - loss: 0.8151 - acc: 0.6648 - val_loss: 0.8239 - val_acc: 0.6529\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.82386 to 0.82354, saving model to best.model\n",
      "1s - loss: 0.8147 - acc: 0.6648 - val_loss: 0.8235 - val_acc: 0.6529\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.82354 to 0.82260, saving model to best.model\n",
      "1s - loss: 0.8139 - acc: 0.6648 - val_loss: 0.8226 - val_acc: 0.6529\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "1s - loss: 0.8119 - acc: 0.6649 - val_loss: 0.8228 - val_acc: 0.6529\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.82260 to 0.82102, saving model to best.model\n",
      "1s - loss: 0.8127 - acc: 0.6649 - val_loss: 0.8210 - val_acc: 0.6529\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.82102 to 0.82059, saving model to best.model\n",
      "0s - loss: 0.8118 - acc: 0.6649 - val_loss: 0.8206 - val_acc: 0.6529\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.82059 to 0.81881, saving model to best.model\n",
      "1s - loss: 0.8108 - acc: 0.6648 - val_loss: 0.8188 - val_acc: 0.6529\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.81881 to 0.81796, saving model to best.model\n",
      "1s - loss: 0.8090 - acc: 0.6646 - val_loss: 0.8180 - val_acc: 0.6529\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.81796 to 0.81724, saving model to best.model\n",
      "1s - loss: 0.8090 - acc: 0.6652 - val_loss: 0.8172 - val_acc: 0.6529\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.81724 to 0.81696, saving model to best.model\n",
      "1s - loss: 0.8084 - acc: 0.6647 - val_loss: 0.8170 - val_acc: 0.6529\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.81696 to 0.81515, saving model to best.model\n",
      "0s - loss: 0.8076 - acc: 0.6649 - val_loss: 0.8152 - val_acc: 0.6529\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.81515 to 0.81320, saving model to best.model\n",
      "0s - loss: 0.8057 - acc: 0.6659 - val_loss: 0.8132 - val_acc: 0.6543\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.81320 to 0.81269, saving model to best.model\n",
      "0s - loss: 0.8052 - acc: 0.6651 - val_loss: 0.8127 - val_acc: 0.6549\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.81269 to 0.81110, saving model to best.model\n",
      "0s - loss: 0.8039 - acc: 0.6660 - val_loss: 0.8111 - val_acc: 0.6555\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.81110 to 0.81040, saving model to best.model\n",
      "0s - loss: 0.8036 - acc: 0.6671 - val_loss: 0.8104 - val_acc: 0.6556\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.81040 to 0.80911, saving model to best.model\n",
      "0s - loss: 0.8026 - acc: 0.6675 - val_loss: 0.8091 - val_acc: 0.6559\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.80911 to 0.80635, saving model to best.model\n",
      "0s - loss: 0.8012 - acc: 0.6673 - val_loss: 0.8064 - val_acc: 0.6615\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.80635 to 0.80518, saving model to best.model\n",
      "0s - loss: 0.7984 - acc: 0.6677 - val_loss: 0.8052 - val_acc: 0.6585\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.80518 to 0.80355, saving model to best.model\n",
      "0s - loss: 0.7980 - acc: 0.6683 - val_loss: 0.8035 - val_acc: 0.6595\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.80355 to 0.80137, saving model to best.model\n",
      "0s - loss: 0.7954 - acc: 0.6700 - val_loss: 0.8014 - val_acc: 0.6644\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.80137 to 0.79929, saving model to best.model\n",
      "0s - loss: 0.7952 - acc: 0.6690 - val_loss: 0.7993 - val_acc: 0.6637\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.79929 to 0.79832, saving model to best.model\n",
      "0s - loss: 0.7930 - acc: 0.6707 - val_loss: 0.7983 - val_acc: 0.6624\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.79832 to 0.79821, saving model to best.model\n",
      "0s - loss: 0.7932 - acc: 0.6712 - val_loss: 0.7982 - val_acc: 0.6617\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.79821 to 0.79718, saving model to best.model\n",
      "1s - loss: 0.7921 - acc: 0.6723 - val_loss: 0.7972 - val_acc: 0.6616\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.79718 to 0.79403, saving model to best.model\n",
      "1s - loss: 0.7891 - acc: 0.6721 - val_loss: 0.7940 - val_acc: 0.6624\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.79403 to 0.79029, saving model to best.model\n",
      "1s - loss: 0.7890 - acc: 0.6727 - val_loss: 0.7903 - val_acc: 0.6664\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.79029 to 0.78895, saving model to best.model\n",
      "1s - loss: 0.7857 - acc: 0.6743 - val_loss: 0.7890 - val_acc: 0.6671\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.78895 to 0.78724, saving model to best.model\n",
      "1s - loss: 0.7861 - acc: 0.6746 - val_loss: 0.7872 - val_acc: 0.6684\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.78724 to 0.78461, saving model to best.model\n",
      "1s - loss: 0.7818 - acc: 0.6753 - val_loss: 0.7846 - val_acc: 0.6698\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.78461 to 0.78212, saving model to best.model\n",
      "1s - loss: 0.7814 - acc: 0.6758 - val_loss: 0.7821 - val_acc: 0.6742\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.78212 to 0.78072, saving model to best.model\n",
      "1s - loss: 0.7810 - acc: 0.6739 - val_loss: 0.7807 - val_acc: 0.6744\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.78072 to 0.77826, saving model to best.model\n",
      "1s - loss: 0.7802 - acc: 0.6775 - val_loss: 0.7783 - val_acc: 0.6768\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.77826 to 0.77454, saving model to best.model\n",
      "1s - loss: 0.7793 - acc: 0.6778 - val_loss: 0.7745 - val_acc: 0.6813\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.77454 to 0.77338, saving model to best.model\n",
      "1s - loss: 0.7772 - acc: 0.6778 - val_loss: 0.7734 - val_acc: 0.6843\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.77338 to 0.76979, saving model to best.model\n",
      "1s - loss: 0.7747 - acc: 0.6801 - val_loss: 0.7698 - val_acc: 0.6859\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "1s - loss: 0.7730 - acc: 0.6797 - val_loss: 0.7721 - val_acc: 0.6780\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "1s - loss: 0.7718 - acc: 0.6800 - val_loss: 0.7717 - val_acc: 0.6767\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.76979 to 0.76816, saving model to best.model\n",
      "1s - loss: 0.7718 - acc: 0.6803 - val_loss: 0.7682 - val_acc: 0.6810\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.76816 to 0.76430, saving model to best.model\n",
      "1s - loss: 0.7689 - acc: 0.6811 - val_loss: 0.7643 - val_acc: 0.6859\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.76430 to 0.76415, saving model to best.model\n",
      "1s - loss: 0.7674 - acc: 0.6829 - val_loss: 0.7642 - val_acc: 0.6836\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.76415 to 0.76293, saving model to best.model\n",
      "1s - loss: 0.7657 - acc: 0.6818 - val_loss: 0.7629 - val_acc: 0.6795\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.76293 to 0.76034, saving model to best.model\n",
      "1s - loss: 0.7653 - acc: 0.6841 - val_loss: 0.7603 - val_acc: 0.6841\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.76034 to 0.75834, saving model to best.model\n",
      "1s - loss: 0.7616 - acc: 0.6856 - val_loss: 0.7583 - val_acc: 0.6869\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.75834 to 0.75721, saving model to best.model\n",
      "1s - loss: 0.7640 - acc: 0.6842 - val_loss: 0.7572 - val_acc: 0.6879\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.75721 to 0.75453, saving model to best.model\n",
      "1s - loss: 0.7611 - acc: 0.6854 - val_loss: 0.7545 - val_acc: 0.6864\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.75453 to 0.75190, saving model to best.model\n",
      "1s - loss: 0.7616 - acc: 0.6851 - val_loss: 0.7519 - val_acc: 0.6896\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.75190 to 0.74977, saving model to best.model\n",
      "1s - loss: 0.7565 - acc: 0.6885 - val_loss: 0.7498 - val_acc: 0.6895\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.74977 to 0.74692, saving model to best.model\n",
      "1s - loss: 0.7542 - acc: 0.6891 - val_loss: 0.7469 - val_acc: 0.6900\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.74692 to 0.74324, saving model to best.model\n",
      "1s - loss: 0.7548 - acc: 0.6892 - val_loss: 0.7432 - val_acc: 0.6929\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.74324 to 0.74255, saving model to best.model\n",
      "1s - loss: 0.7530 - acc: 0.6915 - val_loss: 0.7426 - val_acc: 0.6920\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.74255 to 0.74089, saving model to best.model\n",
      "1s - loss: 0.7523 - acc: 0.6910 - val_loss: 0.7409 - val_acc: 0.6939\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.74089 to 0.73545, saving model to best.model\n",
      "1s - loss: 0.7478 - acc: 0.6927 - val_loss: 0.7355 - val_acc: 0.6968\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.73545 to 0.73424, saving model to best.model\n",
      "1s - loss: 0.7490 - acc: 0.6917 - val_loss: 0.7342 - val_acc: 0.6961\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.73424 to 0.73349, saving model to best.model\n",
      "1s - loss: 0.7460 - acc: 0.6921 - val_loss: 0.7335 - val_acc: 0.6944\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.73349 to 0.72936, saving model to best.model\n",
      "1s - loss: 0.7445 - acc: 0.6932 - val_loss: 0.7294 - val_acc: 0.7090\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "1s - loss: 0.7456 - acc: 0.6938 - val_loss: 0.7296 - val_acc: 0.6972\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.72936 to 0.72470, saving model to best.model\n",
      "1s - loss: 0.7420 - acc: 0.6946 - val_loss: 0.7247 - val_acc: 0.7009\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.72470 to 0.72056, saving model to best.model\n",
      "1s - loss: 0.7405 - acc: 0.6979 - val_loss: 0.7206 - val_acc: 0.7032\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "1s - loss: 0.7382 - acc: 0.6974 - val_loss: 0.7210 - val_acc: 0.7000\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.72056 to 0.71919, saving model to best.model\n",
      "1s - loss: 0.7381 - acc: 0.6957 - val_loss: 0.7192 - val_acc: 0.7052\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.71919 to 0.71734, saving model to best.model\n",
      "1s - loss: 0.7366 - acc: 0.6970 - val_loss: 0.7173 - val_acc: 0.7041\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "1s - loss: 0.7367 - acc: 0.6968 - val_loss: 0.7181 - val_acc: 0.7009\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.71734 to 0.71178, saving model to best.model\n",
      "1s - loss: 0.7331 - acc: 0.6974 - val_loss: 0.7118 - val_acc: 0.7085\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "1s - loss: 0.7329 - acc: 0.6984 - val_loss: 0.7169 - val_acc: 0.7009\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "1s - loss: 0.7302 - acc: 0.7033 - val_loss: 0.7139 - val_acc: 0.7006\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.71178 to 0.71156, saving model to best.model\n",
      "1s - loss: 0.7313 - acc: 0.7022 - val_loss: 0.7116 - val_acc: 0.7029\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.71156 to 0.71115, saving model to best.model\n",
      "1s - loss: 0.7278 - acc: 0.7033 - val_loss: 0.7111 - val_acc: 0.7025\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.71115 to 0.70106, saving model to best.model\n",
      "1s - loss: 0.7254 - acc: 0.7046 - val_loss: 0.7011 - val_acc: 0.7148\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.7273 - acc: 0.7013 - val_loss: 0.7069 - val_acc: 0.7074\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.70106 to 0.69932, saving model to best.model\n",
      "0s - loss: 0.7249 - acc: 0.7023 - val_loss: 0.6993 - val_acc: 0.7142\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.69932 to 0.69780, saving model to best.model\n",
      "0s - loss: 0.7223 - acc: 0.7034 - val_loss: 0.6978 - val_acc: 0.7155\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.7217 - acc: 0.7061 - val_loss: 0.6990 - val_acc: 0.7156\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.7222 - acc: 0.7042 - val_loss: 0.6985 - val_acc: 0.7125\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.7189 - acc: 0.7065 - val_loss: 0.7001 - val_acc: 0.7101\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "1s - loss: 0.7180 - acc: 0.7072 - val_loss: 0.6980 - val_acc: 0.7093\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.69780 to 0.69360, saving model to best.model\n",
      "1s - loss: 0.7166 - acc: 0.7080 - val_loss: 0.6936 - val_acc: 0.7136\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.69360 to 0.69059, saving model to best.model\n",
      "1s - loss: 0.7159 - acc: 0.7094 - val_loss: 0.6906 - val_acc: 0.7164\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.69059 to 0.68641, saving model to best.model\n",
      "1s - loss: 0.7146 - acc: 0.7090 - val_loss: 0.6864 - val_acc: 0.7197\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "1s - loss: 0.7148 - acc: 0.7103 - val_loss: 0.6907 - val_acc: 0.7150\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.7105 - acc: 0.7106 - val_loss: 0.6891 - val_acc: 0.7157\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.68641 to 0.68416, saving model to best.model\n",
      "1s - loss: 0.7111 - acc: 0.7102 - val_loss: 0.6842 - val_acc: 0.7196\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.68416 to 0.68204, saving model to best.model\n",
      "1s - loss: 0.7098 - acc: 0.7109 - val_loss: 0.6820 - val_acc: 0.7205\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.68204 to 0.68161, saving model to best.model\n",
      "1s - loss: 0.7068 - acc: 0.7096 - val_loss: 0.6816 - val_acc: 0.7192\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.68161 to 0.67914, saving model to best.model\n",
      "1s - loss: 0.7088 - acc: 0.7108 - val_loss: 0.6791 - val_acc: 0.7220\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.67914 to 0.67790, saving model to best.model\n",
      "1s - loss: 0.7072 - acc: 0.7134 - val_loss: 0.6779 - val_acc: 0.7230\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.67790 to 0.67751, saving model to best.model\n",
      "1s - loss: 0.7057 - acc: 0.7127 - val_loss: 0.6775 - val_acc: 0.7294\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.67751 to 0.67564, saving model to best.model\n",
      "1s - loss: 0.7027 - acc: 0.7145 - val_loss: 0.6756 - val_acc: 0.7222\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.67564 to 0.67465, saving model to best.model\n",
      "1s - loss: 0.7008 - acc: 0.7162 - val_loss: 0.6746 - val_acc: 0.7226\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "1s - loss: 0.7049 - acc: 0.7125 - val_loss: 0.6770 - val_acc: 0.7190\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.67465 to 0.67289, saving model to best.model\n",
      "1s - loss: 0.7035 - acc: 0.7138 - val_loss: 0.6729 - val_acc: 0.7241\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.67289 to 0.67217, saving model to best.model\n",
      "1s - loss: 0.6988 - acc: 0.7158 - val_loss: 0.6722 - val_acc: 0.7225\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.67217 to 0.67130, saving model to best.model\n",
      "1s - loss: 0.6965 - acc: 0.7165 - val_loss: 0.6713 - val_acc: 0.7217\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.67130 to 0.67045, saving model to best.model\n",
      "1s - loss: 0.6981 - acc: 0.7144 - val_loss: 0.6705 - val_acc: 0.7231\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "1s - loss: 0.6964 - acc: 0.7167 - val_loss: 0.6721 - val_acc: 0.7208\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.67045 to 0.66859, saving model to best.model\n",
      "1s - loss: 0.6974 - acc: 0.7165 - val_loss: 0.6686 - val_acc: 0.7235\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.66859 to 0.66759, saving model to best.model\n",
      "1s - loss: 0.6975 - acc: 0.7144 - val_loss: 0.6676 - val_acc: 0.7268\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "1s - loss: 0.6987 - acc: 0.7154 - val_loss: 0.6677 - val_acc: 0.7259\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.66759 to 0.66679, saving model to best.model\n",
      "1s - loss: 0.6958 - acc: 0.7188 - val_loss: 0.6668 - val_acc: 0.7241\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.66679 to 0.66185, saving model to best.model\n",
      "1s - loss: 0.6920 - acc: 0.7192 - val_loss: 0.6618 - val_acc: 0.7280\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.66185 to 0.65846, saving model to best.model\n",
      "1s - loss: 0.6905 - acc: 0.7182 - val_loss: 0.6585 - val_acc: 0.7310\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss did not improve\n",
      "1s - loss: 0.6902 - acc: 0.7171 - val_loss: 0.6625 - val_acc: 0.7275\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss did not improve\n",
      "1s - loss: 0.6906 - acc: 0.7184 - val_loss: 0.6608 - val_acc: 0.7266\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss did not improve\n",
      "1s - loss: 0.6884 - acc: 0.7185 - val_loss: 0.6643 - val_acc: 0.7238\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.65846 to 0.65464, saving model to best.model\n",
      "1s - loss: 0.6885 - acc: 0.7198 - val_loss: 0.6546 - val_acc: 0.7304\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss did not improve\n",
      "0s - loss: 0.6887 - acc: 0.7196 - val_loss: 0.6560 - val_acc: 0.7321\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss did not improve\n",
      "0s - loss: 0.6867 - acc: 0.7202 - val_loss: 0.6582 - val_acc: 0.7287\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.65464 to 0.65122, saving model to best.model\n",
      "0s - loss: 0.6859 - acc: 0.7198 - val_loss: 0.6512 - val_acc: 0.7329\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss did not improve\n",
      "0s - loss: 0.6878 - acc: 0.7210 - val_loss: 0.6578 - val_acc: 0.7282\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss did not improve\n",
      "1s - loss: 0.6837 - acc: 0.7222 - val_loss: 0.6559 - val_acc: 0.7282\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss did not improve\n",
      "0s - loss: 0.6815 - acc: 0.7218 - val_loss: 0.6518 - val_acc: 0.7327\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.65122 to 0.65084, saving model to best.model\n",
      "1s - loss: 0.6826 - acc: 0.7237 - val_loss: 0.6508 - val_acc: 0.7324\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "1s - loss: 0.6823 - acc: 0.7217 - val_loss: 0.6519 - val_acc: 0.7314\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "1s - loss: 0.6839 - acc: 0.7216 - val_loss: 0.6513 - val_acc: 0.7328\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.65084 to 0.65043, saving model to best.model\n",
      "1s - loss: 0.6803 - acc: 0.7246 - val_loss: 0.6504 - val_acc: 0.7315\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.65043 to 0.64642, saving model to best.model\n",
      "1s - loss: 0.6796 - acc: 0.7235 - val_loss: 0.6464 - val_acc: 0.7340\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.6794 - acc: 0.7234 - val_loss: 0.6471 - val_acc: 0.7334\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.64642 to 0.64528, saving model to best.model\n",
      "1s - loss: 0.6785 - acc: 0.7229 - val_loss: 0.6453 - val_acc: 0.7348\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss did not improve\n",
      "1s - loss: 0.6777 - acc: 0.7253 - val_loss: 0.6467 - val_acc: 0.7337\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "1s - loss: 0.6802 - acc: 0.7243 - val_loss: 0.6459 - val_acc: 0.7335\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.64528 to 0.64122, saving model to best.model\n",
      "1s - loss: 0.6749 - acc: 0.7273 - val_loss: 0.6412 - val_acc: 0.7381\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "1s - loss: 0.6748 - acc: 0.7257 - val_loss: 0.6441 - val_acc: 0.7327\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "1s - loss: 0.6740 - acc: 0.7237 - val_loss: 0.6435 - val_acc: 0.7326\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.64122 to 0.63995, saving model to best.model\n",
      "1s - loss: 0.6746 - acc: 0.7278 - val_loss: 0.6399 - val_acc: 0.7349\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss did not improve\n",
      "1s - loss: 0.6694 - acc: 0.7276 - val_loss: 0.6404 - val_acc: 0.7349\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss did not improve\n",
      "1s - loss: 0.6756 - acc: 0.7245 - val_loss: 0.6404 - val_acc: 0.7352\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.63995 to 0.63933, saving model to best.model\n",
      "1s - loss: 0.6727 - acc: 0.7258 - val_loss: 0.6393 - val_acc: 0.7369\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss did not improve\n",
      "1s - loss: 0.6715 - acc: 0.7286 - val_loss: 0.6412 - val_acc: 0.7351\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.63933 to 0.63791, saving model to best.model\n",
      "0s - loss: 0.6713 - acc: 0.7277 - val_loss: 0.6379 - val_acc: 0.7364\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.63791 to 0.63586, saving model to best.model\n",
      "1s - loss: 0.6693 - acc: 0.7283 - val_loss: 0.6359 - val_acc: 0.7371\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "1s - loss: 0.6713 - acc: 0.7267 - val_loss: 0.6392 - val_acc: 0.7344\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "1s - loss: 0.6664 - acc: 0.7281 - val_loss: 0.6380 - val_acc: 0.7365\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.63586 to 0.63297, saving model to best.model\n",
      "1s - loss: 0.6651 - acc: 0.7318 - val_loss: 0.6330 - val_acc: 0.7383\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.63297 to 0.63124, saving model to best.model\n",
      "1s - loss: 0.6668 - acc: 0.7284 - val_loss: 0.6312 - val_acc: 0.7400\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "1s - loss: 0.6636 - acc: 0.7302 - val_loss: 0.6328 - val_acc: 0.7381\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.63124 to 0.62958, saving model to best.model\n",
      "1s - loss: 0.6645 - acc: 0.7280 - val_loss: 0.6296 - val_acc: 0.7404\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.62958 to 0.62934, saving model to best.model\n",
      "1s - loss: 0.6675 - acc: 0.7296 - val_loss: 0.6293 - val_acc: 0.7429\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.62934 to 0.62902, saving model to best.model\n",
      "1s - loss: 0.6641 - acc: 0.7304 - val_loss: 0.6290 - val_acc: 0.7398\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.62902 to 0.62687, saving model to best.model\n",
      "1s - loss: 0.6636 - acc: 0.7299 - val_loss: 0.6269 - val_acc: 0.7429\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "1s - loss: 0.6656 - acc: 0.7287 - val_loss: 0.6314 - val_acc: 0.7379\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "1s - loss: 0.6621 - acc: 0.7299 - val_loss: 0.6293 - val_acc: 0.7396\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.62687 to 0.62279, saving model to best.model\n",
      "1s - loss: 0.6592 - acc: 0.7309 - val_loss: 0.6228 - val_acc: 0.7446\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "1s - loss: 0.6613 - acc: 0.7328 - val_loss: 0.6261 - val_acc: 0.7400\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "1s - loss: 0.6619 - acc: 0.7303 - val_loss: 0.6269 - val_acc: 0.7402\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "1s - loss: 0.6626 - acc: 0.7291 - val_loss: 0.6272 - val_acc: 0.7400\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.62279 to 0.62088, saving model to best.model\n",
      "1s - loss: 0.6602 - acc: 0.7301 - val_loss: 0.6209 - val_acc: 0.7431\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "1s - loss: 0.6587 - acc: 0.7334 - val_loss: 0.6253 - val_acc: 0.7399\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.6573 - acc: 0.7311 - val_loss: 0.6230 - val_acc: 0.7417\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "1s - loss: 0.6554 - acc: 0.7329 - val_loss: 0.6251 - val_acc: 0.7403\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.62088 to 0.62064, saving model to best.model\n",
      "1s - loss: 0.6560 - acc: 0.7330 - val_loss: 0.6206 - val_acc: 0.7436\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.6553 - acc: 0.7350 - val_loss: 0.6225 - val_acc: 0.7423\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "1s - loss: 0.6579 - acc: 0.7321 - val_loss: 0.6217 - val_acc: 0.7418\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.62064 to 0.61774, saving model to best.model\n",
      "1s - loss: 0.6531 - acc: 0.7334 - val_loss: 0.6177 - val_acc: 0.7477\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "1s - loss: 0.6572 - acc: 0.7334 - val_loss: 0.6202 - val_acc: 0.7434\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "1s - loss: 0.6559 - acc: 0.7339 - val_loss: 0.6188 - val_acc: 0.7430\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.61774 to 0.61626, saving model to best.model\n",
      "1s - loss: 0.6545 - acc: 0.7350 - val_loss: 0.6163 - val_acc: 0.7445\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.61626 to 0.61594, saving model to best.model\n",
      "1s - loss: 0.6542 - acc: 0.7345 - val_loss: 0.6159 - val_acc: 0.7459\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "1s - loss: 0.6526 - acc: 0.7342 - val_loss: 0.6167 - val_acc: 0.7440\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.6537 - acc: 0.7362 - val_loss: 0.6160 - val_acc: 0.7451\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "1s - loss: 0.6510 - acc: 0.7355 - val_loss: 0.6174 - val_acc: 0.7426\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "1s - loss: 0.6506 - acc: 0.7366 - val_loss: 0.6182 - val_acc: 0.7441\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.61594 to 0.61381, saving model to best.model\n",
      "1s - loss: 0.6508 - acc: 0.7381 - val_loss: 0.6138 - val_acc: 0.7477\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "1s - loss: 0.6485 - acc: 0.7362 - val_loss: 0.6178 - val_acc: 0.7427\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.61381 to 0.61134, saving model to best.model\n",
      "1s - loss: 0.6468 - acc: 0.7370 - val_loss: 0.6113 - val_acc: 0.7480\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "1s - loss: 0.6484 - acc: 0.7378 - val_loss: 0.6162 - val_acc: 0.7412\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.61134 to 0.60941, saving model to best.model\n",
      "1s - loss: 0.6496 - acc: 0.7354 - val_loss: 0.6094 - val_acc: 0.7496\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "1s - loss: 0.6516 - acc: 0.7362 - val_loss: 0.6124 - val_acc: 0.7463\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.60941 to 0.60924, saving model to best.model\n",
      "1s - loss: 0.6479 - acc: 0.7379 - val_loss: 0.6092 - val_acc: 0.7486\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "1s - loss: 0.6464 - acc: 0.7386 - val_loss: 0.6098 - val_acc: 0.7468\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.60924 to 0.60889, saving model to best.model\n",
      "1s - loss: 0.6446 - acc: 0.7414 - val_loss: 0.6089 - val_acc: 0.7488\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "1s - loss: 0.6448 - acc: 0.7395 - val_loss: 0.6105 - val_acc: 0.7481\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.60889 to 0.60772, saving model to best.model\n",
      "1s - loss: 0.6472 - acc: 0.7379 - val_loss: 0.6077 - val_acc: 0.7520\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "1s - loss: 0.6456 - acc: 0.7388 - val_loss: 0.6137 - val_acc: 0.7447\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "1s - loss: 0.6453 - acc: 0.7386 - val_loss: 0.6124 - val_acc: 0.7457\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.60772 to 0.60722, saving model to best.model\n",
      "1s - loss: 0.6421 - acc: 0.7390 - val_loss: 0.6072 - val_acc: 0.7486\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "1s - loss: 0.6448 - acc: 0.7364 - val_loss: 0.6107 - val_acc: 0.7456\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "1s - loss: 0.6458 - acc: 0.7378 - val_loss: 0.6075 - val_acc: 0.7525\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.60722 to 0.60656, saving model to best.model\n",
      "1s - loss: 0.6456 - acc: 0.7377 - val_loss: 0.6066 - val_acc: 0.7506\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.60656 to 0.60448, saving model to best.model\n",
      "1s - loss: 0.6435 - acc: 0.7389 - val_loss: 0.6045 - val_acc: 0.7507\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "1s - loss: 0.6413 - acc: 0.7398 - val_loss: 0.6058 - val_acc: 0.7535\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "1s - loss: 0.6450 - acc: 0.7387 - val_loss: 0.6074 - val_acc: 0.7491\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.60448 to 0.60373, saving model to best.model\n",
      "1s - loss: 0.6427 - acc: 0.7386 - val_loss: 0.6037 - val_acc: 0.7512\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.60373 to 0.60309, saving model to best.model\n",
      "1s - loss: 0.6391 - acc: 0.7425 - val_loss: 0.6031 - val_acc: 0.7505\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.60309 to 0.60037, saving model to best.model\n",
      "1s - loss: 0.6404 - acc: 0.7411 - val_loss: 0.6004 - val_acc: 0.7541\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.60037 to 0.59932, saving model to best.model\n",
      "1s - loss: 0.6377 - acc: 0.7425 - val_loss: 0.5993 - val_acc: 0.7546\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "1s - loss: 0.6391 - acc: 0.7387 - val_loss: 0.6020 - val_acc: 0.7507\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "1s - loss: 0.6390 - acc: 0.7423 - val_loss: 0.6007 - val_acc: 0.7526\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for i in range(50):\n",
    "    y_pred=train_nn_simple(train,X_val,y_val)\n",
    "    result.append(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
