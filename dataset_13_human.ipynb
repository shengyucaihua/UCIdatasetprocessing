{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/Users/bruce/Desktop/datasets/human/train.csv\")\n",
    "test=pd.read_csv(\"/Users/bruce/Desktop/datasets/human/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X    ...     \\\n",
       "0         -0.923527         -0.934724    ...      \n",
       "1         -0.957686         -0.943068    ...      \n",
       "2         -0.977469         -0.938692    ...      \n",
       "3         -0.989302         -0.938692    ...      \n",
       "4         -0.990441         -0.942469    ...      \n",
       "\n",
       "   fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                        -0.710304                    -0.112754   \n",
       "1                        -0.861499                     0.053477   \n",
       "2                        -0.760104                    -0.118559   \n",
       "3                        -0.482845                    -0.036788   \n",
       "4                        -0.699205                     0.123320   \n",
       "\n",
       "   angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                              0.030400                         -0.464761   \n",
       "1                             -0.007435                         -0.732626   \n",
       "2                              0.177899                          0.100699   \n",
       "3                             -0.012892                          0.640011   \n",
       "4                              0.122542                          0.693578   \n",
       "\n",
       "   angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                             -0.018446             -0.841247   \n",
       "1                              0.703511             -0.844788   \n",
       "2                              0.808529             -0.848933   \n",
       "3                             -0.485366             -0.848649   \n",
       "4                             -0.615971             -0.847865   \n",
       "\n",
       "   angle(Y,gravityMean)  angle(Z,gravityMean)  subject  Activity  \n",
       "0              0.179941             -0.058627        1  STANDING  \n",
       "1              0.180289             -0.054317        1  STANDING  \n",
       "2              0.180637             -0.049118        1  STANDING  \n",
       "3              0.181935             -0.047663        1  STANDING  \n",
       "4              0.185151             -0.043892        1  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 563)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2947, 563)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=train.drop(\"Activity\",axis=1)\n",
    "Y_train=train[\"Activity\"].values\n",
    "X_test=test.drop(\"Activity\",axis=1)\n",
    "Y_test=test[\"Activity\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LAYING', 'SITTING', 'STANDING', 'WALKING', 'WALKING_DOWNSTAIRS',\n",
       "       'WALKING_UPSTAIRS'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train)\n",
    "\n",
    "encoder.fit(Y_test)\n",
    "Y_test = encoder.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranom Forest         91.05 (+/-) 4.40 \n"
     ]
    }
   ],
   "source": [
    "model=RandomForestClassifier(n_estimators=100)\n",
    "kfold = KFold(n_splits=10, random_state=0)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "results=[\"Ranom Forest\",cv_result.mean(),cv_result.std()]\n",
    "\n",
    "print('{:20s} {:2.2f} (+/-) {:2.2f} '.format(results[0] , results[1] * 100, results[2] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[537   0   0   0   0   0]\n",
      " [  0 392  99   0   0   0]\n",
      " [  0 107 425   0   0   0]\n",
      " [  0   0   0 477  11   8]\n",
      " [  0   0   0  54 280  86]\n",
      " [  0   0   0  42   6 423]]\n",
      "85.9857482185\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       537\n",
      "          1       0.79      0.80      0.79       491\n",
      "          2       0.81      0.80      0.80       532\n",
      "          3       0.83      0.96      0.89       496\n",
      "          4       0.94      0.67      0.78       420\n",
      "          5       0.82      0.90      0.86       471\n",
      "\n",
      "avg / total       0.86      0.86      0.86      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from matplotlib import pylab\n",
    "\n",
    "final_model = RandomForestClassifier(n_estimators=200,max_features=None,bootstrap=True,oob_score=True,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[537   0   0   0   0   0]\n",
      " [  0 400  91   0   0   0]\n",
      " [  0 113 419   0   0   0]\n",
      " [  0   0   0 470  10  16]\n",
      " [  0   0   0  49 266 105]\n",
      " [  0   0   0  86   6 379]]\n",
      "83.8479809976\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       537\n",
      "          1       0.78      0.81      0.80       491\n",
      "          2       0.82      0.79      0.80       532\n",
      "          3       0.78      0.95      0.85       496\n",
      "          4       0.94      0.63      0.76       420\n",
      "          5       0.76      0.80      0.78       471\n",
      "\n",
      "avg / total       0.85      0.84      0.84      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=1,max_features=None,bootstrap=False,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[537   0   0   0   0   0]\n",
      " [  0 391 100   0   0   0]\n",
      " [  0  43 489   0   0   0]\n",
      " [  0   0   0 484  10   2]\n",
      " [  0   0   0  62 305  53]\n",
      " [  0   0   0  42   6 423]]\n",
      "89.2093654564\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       537\n",
      "          1       0.90      0.80      0.85       491\n",
      "          2       0.83      0.92      0.87       532\n",
      "          3       0.82      0.98      0.89       496\n",
      "          4       0.95      0.73      0.82       420\n",
      "          5       0.88      0.90      0.89       471\n",
      "\n",
      "avg / total       0.90      0.89      0.89      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=200,max_features='sqrt',bootstrap=True,max_depth=5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[537   0   0   0   0   0]\n",
      " [  0 432  57   0   0   2]\n",
      " [  0  28 504   0   0   0]\n",
      " [  0   0   0 487   4   5]\n",
      " [  0   0   0   7 380  33]\n",
      " [  0   0   0  34   5 432]]\n",
      "94.0617577197\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       537\n",
      "          1       0.94      0.88      0.91       491\n",
      "          2       0.90      0.95      0.92       532\n",
      "          3       0.92      0.98      0.95       496\n",
      "          4       0.98      0.90      0.94       420\n",
      "          5       0.92      0.92      0.92       471\n",
      "\n",
      "avg / total       0.94      0.94      0.94      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "final_model = XGBClassifier(n_estimators=100,num_boost_round=1,max_depth=5,subsample=0.632,colsample_bytree=0.5)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[537   0   0   0   0   0]\n",
      " [  0 379 108   0   0   4]\n",
      " [  0  83 444   0   1   4]\n",
      " [  0   2   0 453  26  15]\n",
      " [  0   5   0  40 340  35]\n",
      " [  0   9   0  80  30 352]]\n",
      "85.0016966407\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       537\n",
      "          1       0.79      0.77      0.78       491\n",
      "          2       0.80      0.83      0.82       532\n",
      "          3       0.79      0.91      0.85       496\n",
      "          4       0.86      0.81      0.83       420\n",
      "          5       0.86      0.75      0.80       471\n",
      "\n",
      "avg / total       0.85      0.85      0.85      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "final_model = XGBClassifier(n_estimators=1,num_boost_round=1,max_depth=5,subsample=1,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[537   0   0   0   0   0]\n",
      " [  0 421  68   0   0   2]\n",
      " [  0  29 503   0   0   0]\n",
      " [  0   0   0 488   4   4]\n",
      " [  0   0   0   8 382  30]\n",
      " [  0   0   0  29   5 437]]\n",
      "93.9260264676\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       537\n",
      "          1       0.94      0.86      0.89       491\n",
      "          2       0.88      0.95      0.91       532\n",
      "          3       0.93      0.98      0.96       496\n",
      "          4       0.98      0.91      0.94       420\n",
      "          5       0.92      0.93      0.93       471\n",
      "\n",
      "avg / total       0.94      0.94      0.94      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "final_model = XGBClassifier(n_estimators=100,num_boost_round=1,max_depth=5,subsample=0.632,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,History\n",
    "from keras.layers import Dense, Activation, Dropout, Input\n",
    "from keras import optimizers\n",
    "history=History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(len(np.unique(Y_train)), activation='softmax'))\n",
    "    \n",
    "m.compile(\n",
    "    optimizer=optimizers.adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6616 samples, validate on 736 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.62317, saving model to best.model\n",
      "0s - loss: 1.9051 - acc: 0.2030 - val_loss: 1.6232 - val_acc: 0.3492\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.62317 to 1.21547, saving model to best.model\n",
      "0s - loss: 1.5445 - acc: 0.3049 - val_loss: 1.2155 - val_acc: 0.3546\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.21547 to 1.12036, saving model to best.model\n",
      "0s - loss: 1.2638 - acc: 0.3420 - val_loss: 1.1204 - val_acc: 0.3750\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 1.12036 to 1.09218, saving model to best.model\n",
      "0s - loss: 1.1921 - acc: 0.3543 - val_loss: 1.0922 - val_acc: 0.4049\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 1.09218 to 1.05918, saving model to best.model\n",
      "0s - loss: 1.1523 - acc: 0.3774 - val_loss: 1.0592 - val_acc: 0.5394\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 1.05918 to 0.98676, saving model to best.model\n",
      "0s - loss: 1.1044 - acc: 0.4141 - val_loss: 0.9868 - val_acc: 0.5856\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.98676 to 0.83761, saving model to best.model\n",
      "0s - loss: 1.0101 - acc: 0.5101 - val_loss: 0.8376 - val_acc: 0.7609\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.83761 to 0.68704, saving model to best.model\n",
      "0s - loss: 0.8829 - acc: 0.5816 - val_loss: 0.6870 - val_acc: 0.7568\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss improved from 0.68704 to 0.59642, saving model to best.model\n",
      "0s - loss: 0.7678 - acc: 0.6301 - val_loss: 0.5964 - val_acc: 0.8057\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss improved from 0.59642 to 0.51864, saving model to best.model\n",
      "0s - loss: 0.7025 - acc: 0.6518 - val_loss: 0.5186 - val_acc: 0.8274\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.51864 to 0.46483, saving model to best.model\n",
      "0s - loss: 0.6341 - acc: 0.6947 - val_loss: 0.4648 - val_acc: 0.8438\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.46483 to 0.41741, saving model to best.model\n",
      "0s - loss: 0.5884 - acc: 0.7183 - val_loss: 0.4174 - val_acc: 0.8981\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.41741 to 0.36845, saving model to best.model\n",
      "0s - loss: 0.5440 - acc: 0.7444 - val_loss: 0.3684 - val_acc: 0.8750\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.36845 to 0.31279, saving model to best.model\n",
      "0s - loss: 0.4917 - acc: 0.7780 - val_loss: 0.3128 - val_acc: 0.8641\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.31279 to 0.26663, saving model to best.model\n",
      "0s - loss: 0.4401 - acc: 0.8041 - val_loss: 0.2666 - val_acc: 0.9022\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.26663 to 0.24057, saving model to best.model\n",
      "0s - loss: 0.3908 - acc: 0.8245 - val_loss: 0.2406 - val_acc: 0.9429\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.24057 to 0.21657, saving model to best.model\n",
      "0s - loss: 0.3624 - acc: 0.8436 - val_loss: 0.2166 - val_acc: 0.9633\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.21657 to 0.19632, saving model to best.model\n",
      "0s - loss: 0.3350 - acc: 0.8544 - val_loss: 0.1963 - val_acc: 0.9606\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.19632 to 0.17394, saving model to best.model\n",
      "0s - loss: 0.3081 - acc: 0.8727 - val_loss: 0.1739 - val_acc: 0.9592\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.17394 to 0.17058, saving model to best.model\n",
      "0s - loss: 0.2886 - acc: 0.8830 - val_loss: 0.1706 - val_acc: 0.9524\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.17058 to 0.14625, saving model to best.model\n",
      "0s - loss: 0.2665 - acc: 0.8940 - val_loss: 0.1462 - val_acc: 0.9620\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.14625 to 0.13066, saving model to best.model\n",
      "0s - loss: 0.2454 - acc: 0.9052 - val_loss: 0.1307 - val_acc: 0.9565\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.13066 to 0.12373, saving model to best.model\n",
      "0s - loss: 0.2336 - acc: 0.9095 - val_loss: 0.1237 - val_acc: 0.9592\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.12373 to 0.12236, saving model to best.model\n",
      "0s - loss: 0.2147 - acc: 0.9205 - val_loss: 0.1224 - val_acc: 0.9579\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.12236 to 0.11933, saving model to best.model\n",
      "0s - loss: 0.1959 - acc: 0.9282 - val_loss: 0.1193 - val_acc: 0.9565\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.1958 - acc: 0.9305 - val_loss: 0.1211 - val_acc: 0.9497\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.11933 to 0.11038, saving model to best.model\n",
      "0s - loss: 0.1804 - acc: 0.9361 - val_loss: 0.1104 - val_acc: 0.9511\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.11038 to 0.10371, saving model to best.model\n",
      "0s - loss: 0.1686 - acc: 0.9380 - val_loss: 0.1037 - val_acc: 0.9579\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.1733 - acc: 0.9344 - val_loss: 0.1109 - val_acc: 0.9524\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.1525 - acc: 0.9445 - val_loss: 0.1256 - val_acc: 0.9443\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.10371 to 0.10339, saving model to best.model\n",
      "0s - loss: 0.1491 - acc: 0.9491 - val_loss: 0.1034 - val_acc: 0.9552\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.1507 - acc: 0.9451 - val_loss: 0.1442 - val_acc: 0.9375\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.1444 - acc: 0.9480 - val_loss: 0.1161 - val_acc: 0.9484\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.10339 to 0.09115, saving model to best.model\n",
      "0s - loss: 0.1335 - acc: 0.9498 - val_loss: 0.0912 - val_acc: 0.9592\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.1355 - acc: 0.9503 - val_loss: 0.1122 - val_acc: 0.9538\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.1247 - acc: 0.9553 - val_loss: 0.1037 - val_acc: 0.9538\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.1207 - acc: 0.9575 - val_loss: 0.0923 - val_acc: 0.9579\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.1257 - acc: 0.9536 - val_loss: 0.1298 - val_acc: 0.9443\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.1150 - acc: 0.9587 - val_loss: 0.0927 - val_acc: 0.9552\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.1143 - acc: 0.9613 - val_loss: 0.1530 - val_acc: 0.9389\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.1229 - acc: 0.9551 - val_loss: 0.1285 - val_acc: 0.9470\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.1092 - acc: 0.9613 - val_loss: 0.1545 - val_acc: 0.9348\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.1066 - acc: 0.9619 - val_loss: 0.1107 - val_acc: 0.9538\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.09115 to 0.08402, saving model to best.model\n",
      "0s - loss: 0.1038 - acc: 0.9649 - val_loss: 0.0840 - val_acc: 0.9606\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.0928 - acc: 0.9664 - val_loss: 0.0931 - val_acc: 0.9552\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.08402 to 0.07570, saving model to best.model\n",
      "0s - loss: 0.0995 - acc: 0.9667 - val_loss: 0.0757 - val_acc: 0.9660\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.1045 - acc: 0.9651 - val_loss: 0.0842 - val_acc: 0.9606\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.0975 - acc: 0.9643 - val_loss: 0.0899 - val_acc: 0.9565\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.0991 - acc: 0.9646 - val_loss: 0.1230 - val_acc: 0.9497\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.0955 - acc: 0.9674 - val_loss: 0.1321 - val_acc: 0.9497\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.0904 - acc: 0.9669 - val_loss: 0.0878 - val_acc: 0.9565\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.0878 - acc: 0.9689 - val_loss: 0.0964 - val_acc: 0.9579\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.0886 - acc: 0.9678 - val_loss: 0.1089 - val_acc: 0.9579\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.0903 - acc: 0.9655 - val_loss: 0.1126 - val_acc: 0.9552\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.0808 - acc: 0.9723 - val_loss: 0.0759 - val_acc: 0.9674\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.0839 - acc: 0.9722 - val_loss: 0.0993 - val_acc: 0.9524\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.0823 - acc: 0.9710 - val_loss: 0.0889 - val_acc: 0.9592\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.0914 - acc: 0.9681 - val_loss: 0.0946 - val_acc: 0.9552\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.0856 - acc: 0.9695 - val_loss: 0.1154 - val_acc: 0.9511\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss did not improve\n",
      "0s - loss: 0.0810 - acc: 0.9702 - val_loss: 0.0786 - val_acc: 0.9633\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.07570 to 0.07409, saving model to best.model\n",
      "0s - loss: 0.0836 - acc: 0.9722 - val_loss: 0.0741 - val_acc: 0.9674\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss did not improve\n",
      "0s - loss: 0.0823 - acc: 0.9699 - val_loss: 0.1370 - val_acc: 0.9497\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss did not improve\n",
      "0s - loss: 0.0765 - acc: 0.9722 - val_loss: 0.1181 - val_acc: 0.9497\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss did not improve\n",
      "0s - loss: 0.0745 - acc: 0.9719 - val_loss: 0.0867 - val_acc: 0.9606\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss did not improve\n",
      "0s - loss: 0.0739 - acc: 0.9742 - val_loss: 0.1090 - val_acc: 0.9538\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss did not improve\n",
      "0s - loss: 0.0770 - acc: 0.9720 - val_loss: 0.1065 - val_acc: 0.9579\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss did not improve\n",
      "0s - loss: 0.0823 - acc: 0.9699 - val_loss: 0.0916 - val_acc: 0.9606\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.0750 - acc: 0.9725 - val_loss: 0.0971 - val_acc: 0.9606\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss did not improve\n",
      "0s - loss: 0.0768 - acc: 0.9720 - val_loss: 0.0789 - val_acc: 0.9647\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss did not improve\n",
      "0s - loss: 0.0706 - acc: 0.9751 - val_loss: 0.1044 - val_acc: 0.9592\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.07409 to 0.06961, saving model to best.model\n",
      "0s - loss: 0.0710 - acc: 0.9766 - val_loss: 0.0696 - val_acc: 0.9687\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss did not improve\n",
      "0s - loss: 0.0714 - acc: 0.9740 - val_loss: 0.0728 - val_acc: 0.9674\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss did not improve\n",
      "0s - loss: 0.0757 - acc: 0.9716 - val_loss: 0.1099 - val_acc: 0.9552\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss did not improve\n",
      "0s - loss: 0.0726 - acc: 0.9752 - val_loss: 0.0788 - val_acc: 0.9660\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss did not improve\n",
      "0s - loss: 0.0724 - acc: 0.9731 - val_loss: 0.0779 - val_acc: 0.9620\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss did not improve\n",
      "0s - loss: 0.0708 - acc: 0.9767 - val_loss: 0.0941 - val_acc: 0.9592\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss did not improve\n",
      "0s - loss: 0.0631 - acc: 0.9779 - val_loss: 0.1027 - val_acc: 0.9579\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss did not improve\n",
      "0s - loss: 0.0677 - acc: 0.9755 - val_loss: 0.1064 - val_acc: 0.9579\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss did not improve\n",
      "0s - loss: 0.0678 - acc: 0.9764 - val_loss: 0.0772 - val_acc: 0.9674\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss did not improve\n",
      "0s - loss: 0.0656 - acc: 0.9748 - val_loss: 0.0978 - val_acc: 0.9565\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss did not improve\n",
      "0s - loss: 0.0697 - acc: 0.9761 - val_loss: 0.0942 - val_acc: 0.9592\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss did not improve\n",
      "0s - loss: 0.0604 - acc: 0.9776 - val_loss: 0.1421 - val_acc: 0.9484\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss did not improve\n",
      "0s - loss: 0.0623 - acc: 0.9766 - val_loss: 0.1099 - val_acc: 0.9579\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.0629 - acc: 0.9761 - val_loss: 0.0696 - val_acc: 0.9687\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.06961 to 0.05049, saving model to best.model\n",
      "0s - loss: 0.0671 - acc: 0.9769 - val_loss: 0.0505 - val_acc: 0.9769\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss did not improve\n",
      "0s - loss: 0.0707 - acc: 0.9745 - val_loss: 0.0783 - val_acc: 0.9647\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss did not improve\n",
      "0s - loss: 0.0609 - acc: 0.9773 - val_loss: 0.0706 - val_acc: 0.9660\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss did not improve\n",
      "0s - loss: 0.0642 - acc: 0.9761 - val_loss: 0.1075 - val_acc: 0.9579\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss did not improve\n",
      "0s - loss: 0.0636 - acc: 0.9794 - val_loss: 0.0880 - val_acc: 0.9606\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss did not improve\n",
      "0s - loss: 0.0640 - acc: 0.9793 - val_loss: 0.0825 - val_acc: 0.9620\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss did not improve\n",
      "0s - loss: 0.0657 - acc: 0.9745 - val_loss: 0.0953 - val_acc: 0.9606\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss did not improve\n",
      "0s - loss: 0.0630 - acc: 0.9770 - val_loss: 0.0818 - val_acc: 0.9633\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss did not improve\n",
      "0s - loss: 0.0575 - acc: 0.9793 - val_loss: 0.0538 - val_acc: 0.9728\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss did not improve\n",
      "0s - loss: 0.0676 - acc: 0.9751 - val_loss: 0.0800 - val_acc: 0.9620\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss did not improve\n",
      "0s - loss: 0.0560 - acc: 0.9799 - val_loss: 0.0895 - val_acc: 0.9606\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss did not improve\n",
      "0s - loss: 0.0636 - acc: 0.9764 - val_loss: 0.1048 - val_acc: 0.9579\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss did not improve\n",
      "0s - loss: 0.0685 - acc: 0.9757 - val_loss: 0.0698 - val_acc: 0.9674\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss did not improve\n",
      "0s - loss: 0.0627 - acc: 0.9754 - val_loss: 0.0864 - val_acc: 0.9620\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss did not improve\n",
      "0s - loss: 0.0603 - acc: 0.9773 - val_loss: 0.1035 - val_acc: 0.9592\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss did not improve\n",
      "0s - loss: 0.0586 - acc: 0.9754 - val_loss: 0.1312 - val_acc: 0.9552\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss did not improve\n",
      "0s - loss: 0.0618 - acc: 0.9787 - val_loss: 0.1135 - val_acc: 0.9565\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss did not improve\n",
      "0s - loss: 0.0611 - acc: 0.9763 - val_loss: 0.0840 - val_acc: 0.9647\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss did not improve\n",
      "0s - loss: 0.0591 - acc: 0.9772 - val_loss: 0.0739 - val_acc: 0.9674\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss did not improve\n",
      "0s - loss: 0.0598 - acc: 0.9776 - val_loss: 0.0928 - val_acc: 0.9606\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss did not improve\n",
      "0s - loss: 0.0596 - acc: 0.9779 - val_loss: 0.0678 - val_acc: 0.9701\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss did not improve\n",
      "0s - loss: 0.0564 - acc: 0.9785 - val_loss: 0.0691 - val_acc: 0.9674\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss did not improve\n",
      "0s - loss: 0.0562 - acc: 0.9805 - val_loss: 0.0830 - val_acc: 0.9647\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss did not improve\n",
      "0s - loss: 0.0563 - acc: 0.9793 - val_loss: 0.0756 - val_acc: 0.9647\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss did not improve\n",
      "0s - loss: 0.0562 - acc: 0.9788 - val_loss: 0.1144 - val_acc: 0.9579\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss did not improve\n",
      "0s - loss: 0.0561 - acc: 0.9791 - val_loss: 0.0764 - val_acc: 0.9660\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss did not improve\n",
      "0s - loss: 0.0522 - acc: 0.9804 - val_loss: 0.0970 - val_acc: 0.9592\n"
     ]
    }
   ],
   "source": [
    "hist=m.fit(\n",
    "    # Feature matrix\n",
    "    X_train.values, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    \n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFlCAYAAADyLnFSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2xWPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121e4c990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.load_weights(\"best.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in m.predict(X_test.values).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[526   0   0   0   0  11]\n",
      " [  0 385 104   0   0   2]\n",
      " [  0  10 522   0   0   0]\n",
      " [  0   0   0 474  19   3]\n",
      " [  0   0   0   9 384  27]\n",
      " [  0   0   0  19  12 440]]\n",
      "92.6705123855\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99       537\n",
      "          1       0.97      0.78      0.87       491\n",
      "          2       0.83      0.98      0.90       532\n",
      "          3       0.94      0.96      0.95       496\n",
      "          4       0.93      0.91      0.92       420\n",
      "          5       0.91      0.93      0.92       471\n",
      "\n",
      "avg / total       0.93      0.93      0.93      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
