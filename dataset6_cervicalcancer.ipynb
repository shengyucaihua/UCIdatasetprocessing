{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/00383/risk_factors_cervical_cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of sexual partners</th>\n",
       "      <th>First sexual intercourse</th>\n",
       "      <th>Num of pregnancies</th>\n",
       "      <th>Smokes</th>\n",
       "      <th>Smokes (years)</th>\n",
       "      <th>Smokes (packs/year)</th>\n",
       "      <th>Hormonal Contraceptives</th>\n",
       "      <th>Hormonal Contraceptives (years)</th>\n",
       "      <th>IUD</th>\n",
       "      <th>...</th>\n",
       "      <th>STDs: Time since first diagnosis</th>\n",
       "      <th>STDs: Time since last diagnosis</th>\n",
       "      <th>Dx:Cancer</th>\n",
       "      <th>Dx:CIN</th>\n",
       "      <th>Dx:HPV</th>\n",
       "      <th>Dx</th>\n",
       "      <th>Hinselmann</th>\n",
       "      <th>Schiller</th>\n",
       "      <th>Citology</th>\n",
       "      <th>Biopsy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Number of sexual partners First sexual intercourse Num of pregnancies  \\\n",
       "0   18                       4.0                     15.0                1.0   \n",
       "1   15                       1.0                     14.0                1.0   \n",
       "2   34                       1.0                        ?                1.0   \n",
       "3   52                       5.0                     16.0                4.0   \n",
       "4   46                       3.0                     21.0                4.0   \n",
       "\n",
       "  Smokes Smokes (years) Smokes (packs/year) Hormonal Contraceptives  \\\n",
       "0    0.0            0.0                 0.0                     0.0   \n",
       "1    0.0            0.0                 0.0                     0.0   \n",
       "2    0.0            0.0                 0.0                     0.0   \n",
       "3    1.0           37.0                37.0                     1.0   \n",
       "4    0.0            0.0                 0.0                     1.0   \n",
       "\n",
       "  Hormonal Contraceptives (years)  IUD  ...    \\\n",
       "0                             0.0  0.0  ...     \n",
       "1                             0.0  0.0  ...     \n",
       "2                             0.0  0.0  ...     \n",
       "3                             3.0  0.0  ...     \n",
       "4                            15.0  0.0  ...     \n",
       "\n",
       "  STDs: Time since first diagnosis STDs: Time since last diagnosis Dx:Cancer  \\\n",
       "0                                ?                               ?         0   \n",
       "1                                ?                               ?         0   \n",
       "2                                ?                               ?         0   \n",
       "3                                ?                               ?         1   \n",
       "4                                ?                               ?         0   \n",
       "\n",
       "  Dx:CIN Dx:HPV Dx Hinselmann Schiller Citology Biopsy  \n",
       "0      0      0  0          0        0        0      0  \n",
       "1      0      0  0          0        0        0      0  \n",
       "2      0      0  0          0        0        0      0  \n",
       "3      0      1  0          0        0        0      0  \n",
       "4      0      0  0          0        0        0      0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(858, 36)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for each in train.columns:\n",
    "    train[each] = train[each].replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                     0\n",
       "Number of sexual partners              26\n",
       "First sexual intercourse                7\n",
       "Num of pregnancies                     56\n",
       "Smokes                                 13\n",
       "Smokes (years)                         13\n",
       "Smokes (packs/year)                    13\n",
       "Hormonal Contraceptives               108\n",
       "Hormonal Contraceptives (years)       108\n",
       "IUD                                   117\n",
       "IUD (years)                           117\n",
       "STDs                                  105\n",
       "STDs (number)                         105\n",
       "STDs:condylomatosis                   105\n",
       "STDs:cervical condylomatosis          105\n",
       "STDs:vaginal condylomatosis           105\n",
       "STDs:vulvo-perineal condylomatosis    105\n",
       "STDs:syphilis                         105\n",
       "STDs:pelvic inflammatory disease      105\n",
       "STDs:genital herpes                   105\n",
       "STDs:molluscum contagiosum            105\n",
       "STDs:AIDS                             105\n",
       "STDs:HIV                              105\n",
       "STDs:Hepatitis B                      105\n",
       "STDs:HPV                              105\n",
       "STDs: Number of diagnosis               0\n",
       "STDs: Time since first diagnosis      787\n",
       "STDs: Time since last diagnosis       787\n",
       "Dx:Cancer                               0\n",
       "Dx:CIN                                  0\n",
       "Dx:HPV                                  0\n",
       "Dx                                      0\n",
       "Hinselmann                              0\n",
       "Schiller                                0\n",
       "Citology                                0\n",
       "Biopsy                                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_drop=['STDs: Time since first diagnosis','STDs: Time since last diagnosis',\n",
    "                  'Hormonal Contraceptives (years)','IUD (years)','STDs (number)',\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=train.drop(features_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['First sexual intercourse'].fillna(train['First sexual intercourse'].median(), inplace=True)\n",
    "train['Number of sexual partners'].fillna(train['Number of sexual partners'].median(), inplace=True)\n",
    "train['Num of pregnancies'].fillna(train['Num of pregnancies'].median(), inplace=True)\n",
    "train['Smokes'].fillna(train['Smokes'].median(), inplace=True)\n",
    "train['Smokes (years)'].fillna(train['Smokes (years)'].median(), inplace=True)\n",
    "train['Smokes (packs/year)'].fillna(train['Smokes (packs/year)'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                     0\n",
       "Number of sexual partners               0\n",
       "First sexual intercourse                0\n",
       "Num of pregnancies                      0\n",
       "Smokes                                  0\n",
       "Smokes (years)                          0\n",
       "Smokes (packs/year)                     0\n",
       "Hormonal Contraceptives               108\n",
       "IUD                                   117\n",
       "STDs                                  105\n",
       "STDs:condylomatosis                   105\n",
       "STDs:cervical condylomatosis          105\n",
       "STDs:vaginal condylomatosis           105\n",
       "STDs:vulvo-perineal condylomatosis    105\n",
       "STDs:syphilis                         105\n",
       "STDs:pelvic inflammatory disease      105\n",
       "STDs:genital herpes                   105\n",
       "STDs:molluscum contagiosum            105\n",
       "STDs:AIDS                             105\n",
       "STDs:HIV                              105\n",
       "STDs:Hepatitis B                      105\n",
       "STDs:HPV                              105\n",
       "STDs: Number of diagnosis               0\n",
       "Dx:Cancer                               0\n",
       "Dx:CIN                                  0\n",
       "Dx:HPV                                  0\n",
       "Dx                                      0\n",
       "Hinselmann                              0\n",
       "Schiller                                0\n",
       "Citology                                0\n",
       "Biopsy                                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for each in train.columns:\n",
    "    train[each] = train[each].replace(np.nan, '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_predict_unknown(trainX, trainY, testX):\n",
    "    forest = RandomForestClassifier(n_estimators=100)\n",
    "    forest = forest.fit(trainX, trainY)\n",
    "    test_predictY = forest.predict(testX)\n",
    "    return pd.DataFrame(test_predictY,index=testX.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruce/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "fill_attrs=['Hormonal Contraceptives','IUD','STDs','STDs:condylomatosis','STDs:cervical condylomatosis',\n",
    "            'STDs:vaginal condylomatosis','STDs:vulvo-perineal condylomatosis','STDs:syphilis',\n",
    "            'STDs:pelvic inflammatory disease','STDs:genital herpes','STDs:molluscum contagiosum',\n",
    "            'STDs:AIDS','STDs:HIV','STDs:Hepatitis B','STDs:HPV'\n",
    "            ]\n",
    "\n",
    "for i in fill_attrs:     \n",
    "    test_data = train[train[i] == '?']\n",
    "    testX = test_data.drop(fill_attrs, axis=1)\n",
    "    train_data = train[train[i] != '?']       \n",
    "    trainY = list(train_data[i].values)\n",
    "    trainX = train_data.drop(fill_attrs, axis=1)    \n",
    "    test_data[i] = train_predict_unknown(trainX, trainY, testX)\n",
    "    train = pd.concat([train_data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFGCAYAAAC7euwcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAMIMgAABjwv02Klgk5sdJTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a3acb90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['Biopsy'].value_counts().sort_index().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                                   0\n",
       "Number of sexual partners             0\n",
       "First sexual intercourse              0\n",
       "Num of pregnancies                    0\n",
       "Smokes                                0\n",
       "Smokes (years)                        0\n",
       "Smokes (packs/year)                   0\n",
       "Hormonal Contraceptives               0\n",
       "IUD                                   0\n",
       "STDs                                  0\n",
       "STDs:condylomatosis                   0\n",
       "STDs:cervical condylomatosis          0\n",
       "STDs:vaginal condylomatosis           0\n",
       "STDs:vulvo-perineal condylomatosis    0\n",
       "STDs:syphilis                         0\n",
       "STDs:pelvic inflammatory disease      0\n",
       "STDs:genital herpes                   0\n",
       "STDs:molluscum contagiosum            0\n",
       "STDs:AIDS                             0\n",
       "STDs:HIV                              0\n",
       "STDs:Hepatitis B                      0\n",
       "STDs:HPV                              0\n",
       "STDs: Number of diagnosis             0\n",
       "Dx:Cancer                             0\n",
       "Dx:CIN                                0\n",
       "Dx:HPV                                0\n",
       "Dx                                    0\n",
       "Hinselmann                            0\n",
       "Schiller                              0\n",
       "Citology                              0\n",
       "Biopsy                                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features=train.drop('Biopsy',axis=1)\n",
    "outcomes=train['Biopsy'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, outcomes, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# random forest by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranom Forest         93.78 (+/-) 3.26 \n"
     ]
    }
   ],
   "source": [
    "model=RandomForestClassifier(n_estimators=5)\n",
    "kfold = KFold(n_splits=10, random_state=0)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "results=[\"Ranom Forest\",cv_result.mean(),cv_result.std()]\n",
    "\n",
    "print('{:20s} {:2.2f} (+/-) {:2.2f} '.format(results[0] , results[1] * 100, results[2] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best accuracy : ', 0.95489891135303262)\n",
      "('Best parameters :', {'max_features': None, 'n_estimators': 300, 'max_depth': 3})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = RandomForestClassifier()\n",
    "paramaters = [\n",
    "             {'n_estimators' : [100, 200, 300, 500, 1000], \n",
    "              'max_features' : ['auto','log2',None],\n",
    "              'max_depth':[3,4,5]\n",
    "             }                                       \n",
    "             ]\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train,Y_train)\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[202   2]\n",
      " [  1  10]]\n",
      "98.6046511628\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99       204\n",
      "          1       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.99      0.99      0.99       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=300,max_features=None,bootstrap=True,oob_score=True,max_depth=3)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[202   2]\n",
      " [  1  10]]\n",
      "98.6046511628\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99       204\n",
      "          1       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.99      0.99      0.99       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=1,max_features=None,bootstrap=False,max_depth=3)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest by Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[204   0]\n",
      " [  0  11]]\n",
      "100.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       204\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=100,num_boost_round=1,max_depth=4,subsample=0.632,colsample_bytree=0.375)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree by xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[203   1]\n",
      " [  1  10]]\n",
      "99.0697674419\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       204\n",
      "          1       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.99      0.99      0.99       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=1,num_boost_round=1,max_depth=4,subsample=1,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagged decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[203   1]\n",
      " [  1  10]]\n",
      "99.0697674419\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       204\n",
      "          1       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.99      0.99      0.99       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = XGBClassifier(n_estimators=100,num_boost_round=1,max_depth=4,subsample=0.632,colsample_bytree=1)\n",
    "final_model.fit(X_train.values, Y_train.astype(int))\n",
    "y_pred = final_model.predict(X_test.values)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,History\n",
    "from keras.layers import Dense, Activation, Dropout,Input\n",
    "from keras import optimizers\n",
    "history=History()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(128, activation='sigmoid'))\n",
    "m.add(Dropout(0.5))\n",
    "m.add(Dense(len(np.unique(Y_train)), activation='softmax'))\n",
    "    \n",
    "m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 129 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.10788, saving model to best.model\n",
      "0s - loss: 0.1216 - acc: 0.9533 - val_loss: 0.1079 - val_acc: 0.9457\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.1189 - acc: 0.9591 - val_loss: 0.1152 - val_acc: 0.9380\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.1102 - acc: 0.9630 - val_loss: 0.1213 - val_acc: 0.9302\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.1117 - acc: 0.9494 - val_loss: 0.1231 - val_acc: 0.9380\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.1033 - acc: 0.9572 - val_loss: 0.1203 - val_acc: 0.9457\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.1081 - acc: 0.9533 - val_loss: 0.1192 - val_acc: 0.9457\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.1141 - acc: 0.9514 - val_loss: 0.1172 - val_acc: 0.9457\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.1023 - acc: 0.9533 - val_loss: 0.1158 - val_acc: 0.9457\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.1228 - acc: 0.9475 - val_loss: 0.1153 - val_acc: 0.9535\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.1138 - acc: 0.9572 - val_loss: 0.1146 - val_acc: 0.9535\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.1182 - acc: 0.9591 - val_loss: 0.1140 - val_acc: 0.9457\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.0942 - acc: 0.9650 - val_loss: 0.1155 - val_acc: 0.9535\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.1037 - acc: 0.9591 - val_loss: 0.1193 - val_acc: 0.9457\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.1013 - acc: 0.9572 - val_loss: 0.1261 - val_acc: 0.9457\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.1169 - acc: 0.9572 - val_loss: 0.1316 - val_acc: 0.9457\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.1123 - acc: 0.9553 - val_loss: 0.1295 - val_acc: 0.9457\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.1135 - acc: 0.9630 - val_loss: 0.1215 - val_acc: 0.9457\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.1136 - acc: 0.9572 - val_loss: 0.1183 - val_acc: 0.9457\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.1044 - acc: 0.9669 - val_loss: 0.1181 - val_acc: 0.9457\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.0976 - acc: 0.9591 - val_loss: 0.1185 - val_acc: 0.9457\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.1038 - acc: 0.9591 - val_loss: 0.1199 - val_acc: 0.9457\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.1037 - acc: 0.9591 - val_loss: 0.1232 - val_acc: 0.9457\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.1062 - acc: 0.9591 - val_loss: 0.1252 - val_acc: 0.9457\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.1063 - acc: 0.9611 - val_loss: 0.1276 - val_acc: 0.9457\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.1083 - acc: 0.9630 - val_loss: 0.1259 - val_acc: 0.9457\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.1011 - acc: 0.9591 - val_loss: 0.1195 - val_acc: 0.9457\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.1030 - acc: 0.9611 - val_loss: 0.1160 - val_acc: 0.9535\n"
     ]
    }
   ],
   "source": [
    "hist=m.fit(\n",
    "    # Feature matrix\n",
    "    X_train.values, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=132, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFlCAYAAADs50HhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXuvYXywAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122e46a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.load_weights(\"best.model\")\n",
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in m.predict(X_test.values).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[199   5]\n",
      " [  0  11]]\n",
      "97.6744186047\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99       204\n",
      "          1       0.69      1.00      0.81        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 578 samples, validate on 65 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.04157, saving model to best.model\n",
      "0s - loss: 0.4789 - acc: 0.9239 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.04157 to 0.03905, saving model to best.model\n",
      "0s - loss: 0.2963 - acc: 0.9239 - val_loss: 0.0391 - val_acc: 1.0000\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2775 - acc: 0.9239 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2755 - acc: 0.9239 - val_loss: 0.0715 - val_acc: 1.0000\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2797 - acc: 0.9239 - val_loss: 0.0763 - val_acc: 1.0000\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.03905 to 0.03130, saving model to best.model\n",
      "0s - loss: 0.2750 - acc: 0.9239 - val_loss: 0.0313 - val_acc: 1.0000\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.03130 to 0.03037, saving model to best.model\n",
      "0s - loss: 0.2848 - acc: 0.9239 - val_loss: 0.0304 - val_acc: 1.0000\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.03037 to 0.00440, saving model to best.model\n",
      "0s - loss: 0.3093 - acc: 0.9239 - val_loss: 0.0044 - val_acc: 1.0000\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3074 - acc: 0.9239 - val_loss: 0.0590 - val_acc: 1.0000\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2896 - acc: 0.9239 - val_loss: 0.0308 - val_acc: 1.0000\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2779 - acc: 0.9239 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2949 - acc: 0.9239 - val_loss: 0.0330 - val_acc: 1.0000\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2738 - acc: 0.9239 - val_loss: 0.0874 - val_acc: 1.0000\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2752 - acc: 0.9239 - val_loss: 0.0493 - val_acc: 1.0000\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2743 - acc: 0.9239 - val_loss: 0.0902 - val_acc: 1.0000\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2820 - acc: 0.9239 - val_loss: 0.1297 - val_acc: 1.0000\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2743 - acc: 0.9239 - val_loss: 0.0659 - val_acc: 1.0000\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2760 - acc: 0.9239 - val_loss: 0.0706 - val_acc: 1.0000\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2773 - acc: 0.9239 - val_loss: 0.0907 - val_acc: 1.0000\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2769 - acc: 0.9239 - val_loss: 0.0509 - val_acc: 1.0000\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2784 - acc: 0.9239 - val_loss: 0.0280 - val_acc: 1.0000\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2749 - acc: 0.9239 - val_loss: 0.0556 - val_acc: 1.0000\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2804 - acc: 0.9239 - val_loss: 0.0972 - val_acc: 1.0000\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2743 - acc: 0.9239 - val_loss: 0.1089 - val_acc: 1.0000\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2712 - acc: 0.9239 - val_loss: 0.0538 - val_acc: 1.0000\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2772 - acc: 0.9239 - val_loss: 0.1206 - val_acc: 1.0000\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3008 - acc: 0.9239 - val_loss: 0.1655 - val_acc: 1.0000\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2843 - acc: 0.9239 - val_loss: 0.0351 - val_acc: 1.0000\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2910 - acc: 0.9239 - val_loss: 0.0668 - val_acc: 1.0000\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.3064 - acc: 0.9239 - val_loss: 0.0318 - val_acc: 1.0000\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.3289 - acc: 0.9239 - val_loss: 0.0615 - val_acc: 1.0000\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.3165 - acc: 0.9239 - val_loss: 0.1343 - val_acc: 1.0000\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2859 - acc: 0.9239 - val_loss: 0.0556 - val_acc: 1.0000\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2820 - acc: 0.9239 - val_loss: 0.1209 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,History\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "import keras\n",
    "history = History()\n",
    "\n",
    "\n",
    "input_1 = Input(shape=(1,))\n",
    "input_2 = Input(shape=(1,))\n",
    "input_3 = Input(shape=(1,))\n",
    "input_4 = Input(shape=(1,))\n",
    "input_5 = Input(shape=(1,))\n",
    "input_6 = Input(shape=(1,))\n",
    "input_7 = Input(shape=(1,))\n",
    "input_8 = Input(shape=(1,))\n",
    "input_9 = Input(shape=(1,))\n",
    "input_10 = Input(shape=(1,))\n",
    "input_11 = Input(shape=(1,))\n",
    "input_12= Input(shape=(1,))\n",
    "input_13= Input(shape=(1,))\n",
    "input_14= Input(shape=(1,))\n",
    "input_15 = Input(shape=(1,))\n",
    "input_16= Input(shape=(1,))\n",
    "input_17= Input(shape=(1,))\n",
    "input_18= Input(shape=(1,))\n",
    "input_19= Input(shape=(1,))\n",
    "input_20= Input(shape=(1,))\n",
    "input_21= Input(shape=(1,))\n",
    "input_22= Input(shape=(1,))\n",
    "input_23= Input(shape=(1,))\n",
    "input_24= Input(shape=(1,))\n",
    "input_25= Input(shape=(1,))\n",
    "input_26 = Input(shape=(1,))\n",
    "input_27= Input(shape=(1,))\n",
    "input_28= Input(shape=(1,))\n",
    "input_29= Input(shape=(1,))\n",
    "input_30= Input(shape=(1,))\n",
    "input_31= Input(shape=(1,))\n",
    "input_32= Input(shape=(1,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden_1 = Dense(32, activation='sigmoid')(input_1)\n",
    "hidden_2 = Dense(32, activation='sigmoid')(input_2)\n",
    "hidden_3 = Dense(32, activation='sigmoid')(input_3)\n",
    "hidden_4 = Dense(32, activation='sigmoid')(input_4)\n",
    "hidden_5 = Dense(32, activation='sigmoid')(input_5)\n",
    "hidden_6 = Dense(32, activation='sigmoid')(input_6)\n",
    "hidden_7 = Dense(32, activation='sigmoid')(input_7)\n",
    "hidden_8 = Dense(32, activation='sigmoid')(input_8)\n",
    "hidden_9 = Dense(32, activation='sigmoid')(input_9)\n",
    "hidden_10 = Dense(32, activation='sigmoid')(input_10)\n",
    "hidden_11= Dense(32, activation='sigmoid')(input_11)\n",
    "hidden_12= Dense(32, activation='sigmoid')(input_12)\n",
    "hidden_13= Dense(32, activation='sigmoid')(input_13)\n",
    "hidden_14= Dense(32, activation='sigmoid')(input_14)\n",
    "hidden_15= Dense(32, activation='sigmoid')(input_15)\n",
    "hidden_16= Dense(32, activation='sigmoid')(input_16)\n",
    "hidden_17 = Dense(32, activation='sigmoid')(input_17)\n",
    "hidden_18= Dense(32, activation='sigmoid')(input_18)\n",
    "hidden_19= Dense(32, activation='sigmoid')(input_19)\n",
    "hidden_20 = Dense(32, activation='sigmoid')(input_20)\n",
    "hidden_21= Dense(32, activation='sigmoid')(input_21)\n",
    "hidden_22= Dense(32, activation='sigmoid')(input_22)\n",
    "hidden_23= Dense(32, activation='sigmoid')(input_23)\n",
    "hidden_24= Dense(32, activation='sigmoid')(input_24)\n",
    "hidden_25= Dense(32, activation='sigmoid')(input_25)\n",
    "hidden_26= Dense(32, activation='sigmoid')(input_26)\n",
    "hidden_27= Dense(32, activation='sigmoid')(input_27)\n",
    "hidden_28= Dense(32, activation='sigmoid')(input_28)\n",
    "hidden_29= Dense(32, activation='sigmoid')(input_29)\n",
    "hidden_30= Dense(32, activation='sigmoid')(input_30)\n",
    "hidden_31= Dense(32, activation='sigmoid')(input_31)\n",
    "hidden_32= Dense(32, activation='sigmoid')(input_32)\n",
    "\n",
    "\n",
    "value_list=[X_train[['Age']].values,\n",
    "            X_train[['Number of sexual partners']].values,\n",
    "            X_train[['First sexual intercourse']].values,\n",
    "            X_train[['Num of pregnancies']].values,\n",
    "            X_train[['Smokes']].values,\n",
    "            X_train[['Smokes (years)']].values,\n",
    "            X_train[['Smokes (packs/year)']].values,\n",
    "            X_train[['Hormonal Contraceptives']].values,\n",
    "            X_train[['IUD']].values,\n",
    "            X_train[['STDs']].values,\n",
    "            X_train[['STDs:condylomatosis']].values,\n",
    "            X_train[['STDs:cervical condylomatosis']].values,\n",
    "            X_train[['STDs:vaginal condylomatosis']].values,\n",
    "            X_train[['STDs:vulvo-perineal condylomatosis']].values,\n",
    "            X_train[['STDs:syphilis']].values,\n",
    "            X_train[['STDs:pelvic inflammatory disease']].values,\n",
    "            X_train[['STDs:genital herpes']].values,\n",
    "            X_train[['STDs:molluscum contagiosum']].values,\n",
    "            X_train[['STDs:AIDS']].values,\n",
    "            X_train[['STDs:HIV']].values,\n",
    "            X_train[['STDs:Hepatitis B']].values,\n",
    "            X_train[['STDs:HPV']].values,\n",
    "            X_train[['STDs: Number of diagnosis']].values,\n",
    "            X_train[['Hormonal Contraceptives']].values,\n",
    "            X_train[['Dx:Cancer']].values,\n",
    "            X_train[['Dx:CIN']].values,\n",
    "            X_train[['Dx:HPV']].values,\n",
    "            X_train[['Dx']].values,\n",
    "            X_train[['Hinselmann']].values,\n",
    "            X_train[['Schiller']].values,\n",
    "            X_train[['Citology']].values,\n",
    "            X_train[['Hormonal Contraceptives']].values\n",
    "           ]\n",
    "\n",
    "value_list_test=[X_test[['Age']].values,\n",
    "                X_test[['Number of sexual partners']].values,\n",
    "                X_test[['First sexual intercourse']].values,\n",
    "                X_test[['Num of pregnancies']].values,\n",
    "                X_test[['Smokes']].values,\n",
    "                X_test[['Smokes (years)']].values,\n",
    "                X_test[['Smokes (packs/year)']].values,\n",
    "                X_test[['Hormonal Contraceptives']].values,\n",
    "                X_test[['IUD']].values,\n",
    "                X_test[['STDs']].values,\n",
    "                X_test[['STDs:condylomatosis']].values,\n",
    "                X_test[['STDs:cervical condylomatosis']].values,\n",
    "                X_test[['STDs:vaginal condylomatosis']].values,\n",
    "                X_test[['STDs:vulvo-perineal condylomatosis']].values,\n",
    "                X_test[['STDs:syphilis']].values,\n",
    "                X_test[['STDs:pelvic inflammatory disease']].values,\n",
    "                X_test[['STDs:genital herpes']].values,\n",
    "                X_test[['STDs:molluscum contagiosum']].values,\n",
    "                X_test[['STDs:AIDS']].values,\n",
    "                X_test[['STDs:HIV']].values,\n",
    "                X_test[['STDs:Hepatitis B']].values,\n",
    "                X_test[['STDs:HPV']].values,\n",
    "                X_test[['STDs: Number of diagnosis']].values,\n",
    "                X_test[['Hormonal Contraceptives']].values,\n",
    "                X_test[['Dx:Cancer']].values,\n",
    "                X_test[['Dx:CIN']].values,\n",
    "                X_test[['Dx:HPV']].values,\n",
    "                X_test[['Dx']].values,\n",
    "                X_test[['Hinselmann']].values,\n",
    "                X_test[['Schiller']].values,\n",
    "                X_test[['Citology']].values,\n",
    "                X_test[['Hormonal Contraceptives']].values\n",
    "                \n",
    "                ]\n",
    "\n",
    "x = keras.layers.concatenate([hidden_1, hidden_2,hidden_3,hidden_4,hidden_5,hidden_6,hidden_7,hidden_8,\n",
    "                              hidden_9, hidden_10,hidden_11,hidden_12,hidden_13,hidden_14,hidden_15,hidden_16,\n",
    "                              hidden_17, hidden_18,hidden_19,hidden_20,hidden_21,hidden_22,hidden_23,hidden_24,\n",
    "                              hidden_25, hidden_26,hidden_27,hidden_28,hidden_29,hidden_30,hidden_31,hidden_32,\n",
    "                             ])\n",
    "\n",
    "x = Dense(96, activation='sigmoid')(x)\n",
    "output = Dense(len(np.unique(Y_train)), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[input_1,input_2,input_3,input_4,input_5,input_6,input_7,input_8,\n",
    "                     input_9,input_10,input_11,input_12,input_13,input_14,input_15,input_16,\n",
    "                     input_17,input_18,input_19,input_20,input_21,input_22,input_23,input_24,\n",
    "                     input_25,input_26,input_27,input_28,input_29,input_30,input_31,input_32], outputs=[output])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "hist=model.fit(\n",
    "    # Feature matrix\n",
    "    value_list, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        history,\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.1,\n",
    "    batch_size=25, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njCRvIYQQopOR5C2EEEJ0Mv8PdsLwoY2h95QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f80f650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best.model\")\n",
    "mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(Y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    ")\n",
    "y_pred_nn = [mapping[pred] for pred in model.predict(value_list_test).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[204   0]\n",
      " [ 11   0]]\n",
      "94.8837209302\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97       204\n",
      "          1       0.00      0.00      0.00        11\n",
      "\n",
      "avg / total       0.90      0.95      0.92       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(Y_test.astype(int), y_pred_nn)\n",
    "print(cf)\n",
    "print(accuracy_score(Y_test.astype(int), y_pred_nn) * 100) \n",
    "\n",
    "# #Update. 19Jun2017. Included classiifcation report.\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test.astype(int), y_pred_nn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# neural network ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_train,data_val=train_test_split(train,test_size=0.25, random_state=10)\n",
    "X_val=data_val.drop(['Biopsy'], axis=1).values\n",
    "y_val=data_val['Biopsy'].ravel()\n",
    "\n",
    "def train_nn_simple(data_train,X_val,y_val):\n",
    "    \n",
    "\n",
    "    data_train_new=data_train.sample(frac=0.632,replace=True)\n",
    "    X_train=data_train_new.drop(['Biopsy'], axis=1).values\n",
    "    y_train=data_train_new['Biopsy'].ravel()\n",
    "    \n",
    "    m = Sequential()\n",
    "    m.add(Dense(128, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(128, activation='sigmoid'))\n",
    "    m.add(Dropout(0.5))\n",
    "    m.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "    \n",
    "    m.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    m.fit(\n",
    "    # Feature matrix\n",
    "    X_train, \n",
    "    # Target class one-hot-encoded\n",
    "    pd.get_dummies(pd.DataFrame(y_train), columns=[0]).as_matrix(),\n",
    "    # Iterations to be run if not stopped by EarlyStopping\n",
    "    epochs=200, \n",
    "    callbacks=[\n",
    "        # Stop iterations when validation loss has not improved\n",
    "        EarlyStopping(monitor='val_loss', patience=25),\n",
    "        # Nice for keeping the last model before overfitting occurs\n",
    "        ModelCheckpoint(\n",
    "            'best.model', \n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=2,\n",
    "    validation_split=0.2,\n",
    "    batch_size=256, \n",
    "    )\n",
    "    m.load_weights(\"best.model\")\n",
    "    mapping = (\n",
    "    pd.get_dummies(pd.DataFrame(y_train), columns=[0], prefix='', prefix_sep='')\n",
    "    .columns.astype(int).values\n",
    "    )\n",
    "    y_pred = [mapping[pred] for pred in m.predict(X_val).argmax(axis=1)]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.48415, saving model to best.model\n",
      "0s - loss: 0.9601 - acc: 0.4249 - val_loss: 0.4842 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.48415 to 0.31091, saving model to best.model\n",
      "0s - loss: 0.5794 - acc: 0.6998 - val_loss: 0.3109 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.31091 to 0.26375, saving model to best.model\n",
      "0s - loss: 0.4422 - acc: 0.8129 - val_loss: 0.2637 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.3605 - acc: 0.9030 - val_loss: 0.2685 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3541 - acc: 0.9169 - val_loss: 0.2875 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3132 - acc: 0.9169 - val_loss: 0.3064 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3710 - acc: 0.9122 - val_loss: 0.3211 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3693 - acc: 0.9169 - val_loss: 0.3303 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3635 - acc: 0.9169 - val_loss: 0.3340 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3395 - acc: 0.9169 - val_loss: 0.3335 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3788 - acc: 0.9169 - val_loss: 0.3292 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3804 - acc: 0.9122 - val_loss: 0.3215 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3828 - acc: 0.9145 - val_loss: 0.3127 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3439 - acc: 0.9169 - val_loss: 0.3028 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3597 - acc: 0.9169 - val_loss: 0.2926 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3617 - acc: 0.9145 - val_loss: 0.2835 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3447 - acc: 0.9122 - val_loss: 0.2759 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3462 - acc: 0.9145 - val_loss: 0.2702 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3243 - acc: 0.9099 - val_loss: 0.2665 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3415 - acc: 0.9145 - val_loss: 0.2644 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.26375 to 0.26324, saving model to best.model\n",
      "0s - loss: 0.3166 - acc: 0.9145 - val_loss: 0.2632 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.26324 to 0.26274, saving model to best.model\n",
      "0s - loss: 0.2949 - acc: 0.9122 - val_loss: 0.2627 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.26274 to 0.26259, saving model to best.model\n",
      "0s - loss: 0.3208 - acc: 0.9145 - val_loss: 0.2626 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.3195 - acc: 0.9076 - val_loss: 0.2627 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.3203 - acc: 0.9122 - val_loss: 0.2630 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.3285 - acc: 0.9145 - val_loss: 0.2636 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3346 - acc: 0.9145 - val_loss: 0.2644 - val_acc: 0.9266\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.3282 - acc: 0.9169 - val_loss: 0.2653 - val_acc: 0.9266\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.3289 - acc: 0.9145 - val_loss: 0.2662 - val_acc: 0.9266\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.3132 - acc: 0.9192 - val_loss: 0.2669 - val_acc: 0.9266\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.3405 - acc: 0.9122 - val_loss: 0.2673 - val_acc: 0.9266\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.3305 - acc: 0.9145 - val_loss: 0.2676 - val_acc: 0.9266\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.3275 - acc: 0.9169 - val_loss: 0.2674 - val_acc: 0.9266\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.3235 - acc: 0.9169 - val_loss: 0.2671 - val_acc: 0.9266\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.3414 - acc: 0.9169 - val_loss: 0.2666 - val_acc: 0.9266\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.3171 - acc: 0.9145 - val_loss: 0.2662 - val_acc: 0.9266\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.3177 - acc: 0.9145 - val_loss: 0.2659 - val_acc: 0.9266\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.3051 - acc: 0.9169 - val_loss: 0.2655 - val_acc: 0.9266\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.3069 - acc: 0.9169 - val_loss: 0.2654 - val_acc: 0.9266\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.3130 - acc: 0.9169 - val_loss: 0.2651 - val_acc: 0.9266\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.3268 - acc: 0.9145 - val_loss: 0.2649 - val_acc: 0.9266\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.3029 - acc: 0.9169 - val_loss: 0.2645 - val_acc: 0.9266\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.3021 - acc: 0.9169 - val_loss: 0.2641 - val_acc: 0.9266\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.3048 - acc: 0.9145 - val_loss: 0.2639 - val_acc: 0.9266\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.3423 - acc: 0.9145 - val_loss: 0.2638 - val_acc: 0.9266\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.3362 - acc: 0.9169 - val_loss: 0.2637 - val_acc: 0.9266\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.3322 - acc: 0.9169 - val_loss: 0.2635 - val_acc: 0.9266\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2902 - acc: 0.9145 - val_loss: 0.2634 - val_acc: 0.9266\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.3245 - acc: 0.9145 - val_loss: 0.2634 - val_acc: 0.9266\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.28542, saving model to best.model\n",
      "0s - loss: 0.3291 - acc: 0.9099 - val_loss: 0.2854 - val_acc: 0.9174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.2601 - acc: 0.9353 - val_loss: 0.3023 - val_acc: 0.9174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2773 - acc: 0.9353 - val_loss: 0.3233 - val_acc: 0.9174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2624 - acc: 0.9376 - val_loss: 0.3343 - val_acc: 0.9174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2592 - acc: 0.9376 - val_loss: 0.3374 - val_acc: 0.9174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2428 - acc: 0.9376 - val_loss: 0.3338 - val_acc: 0.9174\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2843 - acc: 0.9376 - val_loss: 0.3242 - val_acc: 0.9174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2744 - acc: 0.9376 - val_loss: 0.3134 - val_acc: 0.9174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2518 - acc: 0.9376 - val_loss: 0.3042 - val_acc: 0.9174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2853 - acc: 0.9376 - val_loss: 0.2966 - val_acc: 0.9174\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2644 - acc: 0.9376 - val_loss: 0.2926 - val_acc: 0.9174\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2543 - acc: 0.9376 - val_loss: 0.2904 - val_acc: 0.9174\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2525 - acc: 0.9376 - val_loss: 0.2904 - val_acc: 0.9174\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2478 - acc: 0.9376 - val_loss: 0.2919 - val_acc: 0.9174\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2640 - acc: 0.9376 - val_loss: 0.2944 - val_acc: 0.9174\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2698 - acc: 0.9376 - val_loss: 0.2968 - val_acc: 0.9174\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2438 - acc: 0.9376 - val_loss: 0.2988 - val_acc: 0.9174\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2456 - acc: 0.9376 - val_loss: 0.3010 - val_acc: 0.9174\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2406 - acc: 0.9376 - val_loss: 0.3024 - val_acc: 0.9174\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2418 - acc: 0.9376 - val_loss: 0.3017 - val_acc: 0.9174\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2495 - acc: 0.9376 - val_loss: 0.3002 - val_acc: 0.9174\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2558 - acc: 0.9376 - val_loss: 0.2982 - val_acc: 0.9174\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2568 - acc: 0.9353 - val_loss: 0.2966 - val_acc: 0.9174\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2464 - acc: 0.9376 - val_loss: 0.2958 - val_acc: 0.9174\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2397 - acc: 0.9376 - val_loss: 0.2958 - val_acc: 0.9174\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2507 - acc: 0.9376 - val_loss: 0.2959 - val_acc: 0.9174\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2333 - acc: 0.9376 - val_loss: 0.2965 - val_acc: 0.9174\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.55749, saving model to best.model\n",
      "0s - loss: 1.0805 - acc: 0.3557 - val_loss: 0.5575 - val_acc: 0.9541\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.55749 to 0.31419, saving model to best.model\n",
      "0s - loss: 0.6427 - acc: 0.6305 - val_loss: 0.3142 - val_acc: 0.9541\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.31419 to 0.21554, saving model to best.model\n",
      "0s - loss: 0.4468 - acc: 0.8106 - val_loss: 0.2155 - val_acc: 0.9541\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.21554 to 0.18778, saving model to best.model\n",
      "0s - loss: 0.2998 - acc: 0.9169 - val_loss: 0.1878 - val_acc: 0.9541\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2696 - acc: 0.9353 - val_loss: 0.1878 - val_acc: 0.9541\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2663 - acc: 0.9400 - val_loss: 0.1965 - val_acc: 0.9541\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2531 - acc: 0.9423 - val_loss: 0.2065 - val_acc: 0.9541\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2392 - acc: 0.9423 - val_loss: 0.2151 - val_acc: 0.9541\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2602 - acc: 0.9423 - val_loss: 0.2216 - val_acc: 0.9541\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2712 - acc: 0.9423 - val_loss: 0.2255 - val_acc: 0.9541\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3080 - acc: 0.9423 - val_loss: 0.2274 - val_acc: 0.9541\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2947 - acc: 0.9423 - val_loss: 0.2275 - val_acc: 0.9541\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2629 - acc: 0.9423 - val_loss: 0.2261 - val_acc: 0.9541\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2842 - acc: 0.9423 - val_loss: 0.2234 - val_acc: 0.9541\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2887 - acc: 0.9423 - val_loss: 0.2198 - val_acc: 0.9541\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2922 - acc: 0.9423 - val_loss: 0.2160 - val_acc: 0.9541\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2532 - acc: 0.9423 - val_loss: 0.2114 - val_acc: 0.9541\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2988 - acc: 0.9423 - val_loss: 0.2067 - val_acc: 0.9541\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2677 - acc: 0.9423 - val_loss: 0.2021 - val_acc: 0.9541\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2654 - acc: 0.9423 - val_loss: 0.1980 - val_acc: 0.9541\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2598 - acc: 0.9423 - val_loss: 0.1943 - val_acc: 0.9541\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2484 - acc: 0.9423 - val_loss: 0.1915 - val_acc: 0.9541\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2622 - acc: 0.9400 - val_loss: 0.1892 - val_acc: 0.9541\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.18778 to 0.18757, saving model to best.model\n",
      "0s - loss: 0.2514 - acc: 0.9423 - val_loss: 0.1876 - val_acc: 0.9541\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.18757 to 0.18660, saving model to best.model\n",
      "0s - loss: 0.2711 - acc: 0.9400 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.18660 to 0.18610, saving model to best.model\n",
      "0s - loss: 0.2394 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.18610 to 0.18587, saving model to best.model\n",
      "0s - loss: 0.2557 - acc: 0.9376 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.18587 to 0.18579, saving model to best.model\n",
      "0s - loss: 0.2595 - acc: 0.9376 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2525 - acc: 0.9400 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2360 - acc: 0.9423 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2569 - acc: 0.9400 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2528 - acc: 0.9376 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2535 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2444 - acc: 0.9423 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2426 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2526 - acc: 0.9400 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2606 - acc: 0.9400 - val_loss: 0.1868 - val_acc: 0.9541\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2488 - acc: 0.9423 - val_loss: 0.1869 - val_acc: 0.9541\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2510 - acc: 0.9423 - val_loss: 0.1871 - val_acc: 0.9541\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2368 - acc: 0.9423 - val_loss: 0.1872 - val_acc: 0.9541\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2383 - acc: 0.9423 - val_loss: 0.1873 - val_acc: 0.9541\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2463 - acc: 0.9423 - val_loss: 0.1874 - val_acc: 0.9541\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2623 - acc: 0.9423 - val_loss: 0.1873 - val_acc: 0.9541\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2270 - acc: 0.9423 - val_loss: 0.1872 - val_acc: 0.9541\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2526 - acc: 0.9423 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2465 - acc: 0.9400 - val_loss: 0.1869 - val_acc: 0.9541\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2422 - acc: 0.9423 - val_loss: 0.1868 - val_acc: 0.9541\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2212 - acc: 0.9423 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2367 - acc: 0.9423 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2458 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.2528 - acc: 0.9423 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.2384 - acc: 0.9423 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.2461 - acc: 0.9423 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.2448 - acc: 0.9423 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.60927, saving model to best.model\n",
      "0s - loss: 1.0925 - acc: 0.3256 - val_loss: 0.6093 - val_acc: 0.8807\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.60927 to 0.41621, saving model to best.model\n",
      "0s - loss: 0.7018 - acc: 0.6005 - val_loss: 0.4162 - val_acc: 0.8807\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.41621 to 0.36564, saving model to best.model\n",
      "0s - loss: 0.4430 - acc: 0.8129 - val_loss: 0.3656 - val_acc: 0.8807\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.3296 - acc: 0.9122 - val_loss: 0.3819 - val_acc: 0.8807\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3143 - acc: 0.9261 - val_loss: 0.4200 - val_acc: 0.8807\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2801 - acc: 0.9284 - val_loss: 0.4600 - val_acc: 0.8807\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2867 - acc: 0.9284 - val_loss: 0.4939 - val_acc: 0.8807\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3039 - acc: 0.9307 - val_loss: 0.5194 - val_acc: 0.8807\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3068 - acc: 0.9307 - val_loss: 0.5365 - val_acc: 0.8807\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2960 - acc: 0.9307 - val_loss: 0.5463 - val_acc: 0.8807\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3317 - acc: 0.9307 - val_loss: 0.5500 - val_acc: 0.8807\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3164 - acc: 0.9307 - val_loss: 0.5476 - val_acc: 0.8807\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3151 - acc: 0.9307 - val_loss: 0.5411 - val_acc: 0.8807\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3322 - acc: 0.9307 - val_loss: 0.5305 - val_acc: 0.8807\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3058 - acc: 0.9307 - val_loss: 0.5168 - val_acc: 0.8807\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2942 - acc: 0.9307 - val_loss: 0.5011 - val_acc: 0.8807\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3121 - acc: 0.9307 - val_loss: 0.4844 - val_acc: 0.8807\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3158 - acc: 0.9307 - val_loss: 0.4681 - val_acc: 0.8807\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3107 - acc: 0.9284 - val_loss: 0.4527 - val_acc: 0.8807\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2861 - acc: 0.9307 - val_loss: 0.4385 - val_acc: 0.8807\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3081 - acc: 0.9307 - val_loss: 0.4263 - val_acc: 0.8807\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3056 - acc: 0.9307 - val_loss: 0.4164 - val_acc: 0.8807\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2782 - acc: 0.9307 - val_loss: 0.4087 - val_acc: 0.8807\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2679 - acc: 0.9307 - val_loss: 0.4029 - val_acc: 0.8807\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2971 - acc: 0.9284 - val_loss: 0.3988 - val_acc: 0.8807\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2804 - acc: 0.9284 - val_loss: 0.3969 - val_acc: 0.8807\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2888 - acc: 0.9284 - val_loss: 0.3962 - val_acc: 0.8807\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2798 - acc: 0.9284 - val_loss: 0.3970 - val_acc: 0.8807\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2853 - acc: 0.9284 - val_loss: 0.3981 - val_acc: 0.8807\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.47936, saving model to best.model\n",
      "0s - loss: 0.8598 - acc: 0.4734 - val_loss: 0.4794 - val_acc: 0.9541\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.47936 to 0.28397, saving model to best.model\n",
      "0s - loss: 0.5486 - acc: 0.7113 - val_loss: 0.2840 - val_acc: 0.9541\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.28397 to 0.20632, saving model to best.model\n",
      "0s - loss: 0.3581 - acc: 0.8730 - val_loss: 0.2063 - val_acc: 0.9541\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.20632 to 0.18686, saving model to best.model\n",
      "0s - loss: 0.2996 - acc: 0.9284 - val_loss: 0.1869 - val_acc: 0.9541\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2355 - acc: 0.9446 - val_loss: 0.1898 - val_acc: 0.9541\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2801 - acc: 0.9400 - val_loss: 0.1991 - val_acc: 0.9541\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2406 - acc: 0.9446 - val_loss: 0.2090 - val_acc: 0.9541\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2602 - acc: 0.9446 - val_loss: 0.2171 - val_acc: 0.9541\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2559 - acc: 0.9446 - val_loss: 0.2225 - val_acc: 0.9541\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2894 - acc: 0.9446 - val_loss: 0.2257 - val_acc: 0.9541\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2578 - acc: 0.9446 - val_loss: 0.2268 - val_acc: 0.9541\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2591 - acc: 0.9446 - val_loss: 0.2260 - val_acc: 0.9541\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2776 - acc: 0.9446 - val_loss: 0.2236 - val_acc: 0.9541\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2635 - acc: 0.9446 - val_loss: 0.2202 - val_acc: 0.9541\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2832 - acc: 0.9446 - val_loss: 0.2159 - val_acc: 0.9541\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2935 - acc: 0.9446 - val_loss: 0.2111 - val_acc: 0.9541\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2634 - acc: 0.9446 - val_loss: 0.2062 - val_acc: 0.9541\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2430 - acc: 0.9446 - val_loss: 0.2015 - val_acc: 0.9541\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2427 - acc: 0.9446 - val_loss: 0.1973 - val_acc: 0.9541\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2792 - acc: 0.9446 - val_loss: 0.1937 - val_acc: 0.9541\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2311 - acc: 0.9446 - val_loss: 0.1909 - val_acc: 0.9541\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2311 - acc: 0.9446 - val_loss: 0.1889 - val_acc: 0.9541\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2397 - acc: 0.9446 - val_loss: 0.1876 - val_acc: 0.9541\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.18686 to 0.18678, saving model to best.model\n",
      "0s - loss: 0.2335 - acc: 0.9423 - val_loss: 0.1868 - val_acc: 0.9541\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.18678 to 0.18636, saving model to best.model\n",
      "0s - loss: 0.2363 - acc: 0.9446 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.18636 to 0.18615, saving model to best.model\n",
      "0s - loss: 0.2181 - acc: 0.9446 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.18615 to 0.18605, saving model to best.model\n",
      "0s - loss: 0.2466 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.18605 to 0.18599, saving model to best.model\n",
      "0s - loss: 0.2565 - acc: 0.9446 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.18599 to 0.18596, saving model to best.model\n",
      "0s - loss: 0.2235 - acc: 0.9446 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.18596 to 0.18591, saving model to best.model\n",
      "0s - loss: 0.2413 - acc: 0.9446 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.18591 to 0.18590, saving model to best.model\n",
      "0s - loss: 0.2400 - acc: 0.9446 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2501 - acc: 0.9446 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2270 - acc: 0.9446 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2419 - acc: 0.9400 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2273 - acc: 0.9446 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2464 - acc: 0.9446 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2377 - acc: 0.9446 - val_loss: 0.1868 - val_acc: 0.9541\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2488 - acc: 0.9423 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2342 - acc: 0.9446 - val_loss: 0.1872 - val_acc: 0.9541\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2276 - acc: 0.9446 - val_loss: 0.1872 - val_acc: 0.9541\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2380 - acc: 0.9446 - val_loss: 0.1871 - val_acc: 0.9541\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2356 - acc: 0.9446 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2331 - acc: 0.9446 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2274 - acc: 0.9446 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2348 - acc: 0.9446 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2191 - acc: 0.9446 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.18590 to 0.18583, saving model to best.model\n",
      "0s - loss: 0.2456 - acc: 0.9446 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.18583 to 0.18568, saving model to best.model\n",
      "0s - loss: 0.2396 - acc: 0.9446 - val_loss: 0.1857 - val_acc: 0.9541\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.18568 to 0.18541, saving model to best.model\n",
      "0s - loss: 0.2389 - acc: 0.9446 - val_loss: 0.1854 - val_acc: 0.9541\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.18541 to 0.18511, saving model to best.model\n",
      "0s - loss: 0.2489 - acc: 0.9446 - val_loss: 0.1851 - val_acc: 0.9541\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.18511 to 0.18483, saving model to best.model\n",
      "0s - loss: 0.2384 - acc: 0.9446 - val_loss: 0.1848 - val_acc: 0.9541\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.18483 to 0.18452, saving model to best.model\n",
      "0s - loss: 0.2299 - acc: 0.9446 - val_loss: 0.1845 - val_acc: 0.9541\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.18452 to 0.18431, saving model to best.model\n",
      "0s - loss: 0.2197 - acc: 0.9446 - val_loss: 0.1843 - val_acc: 0.9541\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.18431 to 0.18413, saving model to best.model\n",
      "0s - loss: 0.2243 - acc: 0.9446 - val_loss: 0.1841 - val_acc: 0.9541\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.18413 to 0.18398, saving model to best.model\n",
      "0s - loss: 0.2270 - acc: 0.9446 - val_loss: 0.1840 - val_acc: 0.9541\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.18398 to 0.18385, saving model to best.model\n",
      "0s - loss: 0.2209 - acc: 0.9423 - val_loss: 0.1838 - val_acc: 0.9541\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.18385 to 0.18373, saving model to best.model\n",
      "0s - loss: 0.2367 - acc: 0.9446 - val_loss: 0.1837 - val_acc: 0.9541\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.18373 to 0.18369, saving model to best.model\n",
      "0s - loss: 0.2466 - acc: 0.9446 - val_loss: 0.1837 - val_acc: 0.9541\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.18369 to 0.18354, saving model to best.model\n",
      "0s - loss: 0.2263 - acc: 0.9446 - val_loss: 0.1835 - val_acc: 0.9541\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.18354 to 0.18325, saving model to best.model\n",
      "0s - loss: 0.2335 - acc: 0.9446 - val_loss: 0.1832 - val_acc: 0.9541\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.18325 to 0.18293, saving model to best.model\n",
      "0s - loss: 0.2201 - acc: 0.9446 - val_loss: 0.1829 - val_acc: 0.9541\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.18293 to 0.18258, saving model to best.model\n",
      "0s - loss: 0.2157 - acc: 0.9446 - val_loss: 0.1826 - val_acc: 0.9541\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.18258 to 0.18234, saving model to best.model\n",
      "0s - loss: 0.2265 - acc: 0.9446 - val_loss: 0.1823 - val_acc: 0.9541\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.18234 to 0.18195, saving model to best.model\n",
      "0s - loss: 0.2376 - acc: 0.9446 - val_loss: 0.1820 - val_acc: 0.9541\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.18195 to 0.18167, saving model to best.model\n",
      "0s - loss: 0.2294 - acc: 0.9446 - val_loss: 0.1817 - val_acc: 0.9541\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.18167 to 0.18127, saving model to best.model\n",
      "0s - loss: 0.2269 - acc: 0.9446 - val_loss: 0.1813 - val_acc: 0.9541\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.18127 to 0.18095, saving model to best.model\n",
      "0s - loss: 0.2068 - acc: 0.9446 - val_loss: 0.1810 - val_acc: 0.9541\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.18095 to 0.18059, saving model to best.model\n",
      "0s - loss: 0.2159 - acc: 0.9446 - val_loss: 0.1806 - val_acc: 0.9541\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.18059 to 0.18023, saving model to best.model\n",
      "0s - loss: 0.2232 - acc: 0.9446 - val_loss: 0.1802 - val_acc: 0.9541\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.18023 to 0.17981, saving model to best.model\n",
      "0s - loss: 0.2059 - acc: 0.9423 - val_loss: 0.1798 - val_acc: 0.9541\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.17981 to 0.17946, saving model to best.model\n",
      "0s - loss: 0.2141 - acc: 0.9446 - val_loss: 0.1795 - val_acc: 0.9541\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.17946 to 0.17916, saving model to best.model\n",
      "0s - loss: 0.2258 - acc: 0.9446 - val_loss: 0.1792 - val_acc: 0.9541\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.17916 to 0.17876, saving model to best.model\n",
      "0s - loss: 0.2210 - acc: 0.9446 - val_loss: 0.1788 - val_acc: 0.9541\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.17876 to 0.17850, saving model to best.model\n",
      "0s - loss: 0.2298 - acc: 0.9446 - val_loss: 0.1785 - val_acc: 0.9541\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.17850 to 0.17816, saving model to best.model\n",
      "0s - loss: 0.2149 - acc: 0.9446 - val_loss: 0.1782 - val_acc: 0.9541\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.17816 to 0.17765, saving model to best.model\n",
      "0s - loss: 0.2095 - acc: 0.9446 - val_loss: 0.1776 - val_acc: 0.9541\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.17765 to 0.17740, saving model to best.model\n",
      "0s - loss: 0.2112 - acc: 0.9446 - val_loss: 0.1774 - val_acc: 0.9541\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.17740 to 0.17724, saving model to best.model\n",
      "0s - loss: 0.2046 - acc: 0.9469 - val_loss: 0.1772 - val_acc: 0.9541\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.17724 to 0.17685, saving model to best.model\n",
      "0s - loss: 0.2143 - acc: 0.9446 - val_loss: 0.1768 - val_acc: 0.9541\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.17685 to 0.17639, saving model to best.model\n",
      "0s - loss: 0.1987 - acc: 0.9446 - val_loss: 0.1764 - val_acc: 0.9541\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.17639 to 0.17580, saving model to best.model\n",
      "0s - loss: 0.2040 - acc: 0.9446 - val_loss: 0.1758 - val_acc: 0.9541\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.17580 to 0.17532, saving model to best.model\n",
      "0s - loss: 0.2019 - acc: 0.9446 - val_loss: 0.1753 - val_acc: 0.9541\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.17532 to 0.17476, saving model to best.model\n",
      "0s - loss: 0.2066 - acc: 0.9446 - val_loss: 0.1748 - val_acc: 0.9541\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.17476 to 0.17408, saving model to best.model\n",
      "0s - loss: 0.2310 - acc: 0.9446 - val_loss: 0.1741 - val_acc: 0.9541\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.17408 to 0.17332, saving model to best.model\n",
      "0s - loss: 0.2090 - acc: 0.9446 - val_loss: 0.1733 - val_acc: 0.9541\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.17332 to 0.17254, saving model to best.model\n",
      "0s - loss: 0.2109 - acc: 0.9446 - val_loss: 0.1725 - val_acc: 0.9541\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.17254 to 0.17179, saving model to best.model\n",
      "0s - loss: 0.2021 - acc: 0.9446 - val_loss: 0.1718 - val_acc: 0.9541\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.17179 to 0.17099, saving model to best.model\n",
      "0s - loss: 0.2066 - acc: 0.9446 - val_loss: 0.1710 - val_acc: 0.9541\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.17099 to 0.17025, saving model to best.model\n",
      "0s - loss: 0.2190 - acc: 0.9446 - val_loss: 0.1702 - val_acc: 0.9541\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.17025 to 0.16956, saving model to best.model\n",
      "0s - loss: 0.2168 - acc: 0.9446 - val_loss: 0.1696 - val_acc: 0.9541\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.16956 to 0.16881, saving model to best.model\n",
      "0s - loss: 0.1946 - acc: 0.9446 - val_loss: 0.1688 - val_acc: 0.9541\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.16881 to 0.16808, saving model to best.model\n",
      "0s - loss: 0.2052 - acc: 0.9446 - val_loss: 0.1681 - val_acc: 0.9541\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.16808 to 0.16750, saving model to best.model\n",
      "0s - loss: 0.2052 - acc: 0.9446 - val_loss: 0.1675 - val_acc: 0.9541\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.16750 to 0.16694, saving model to best.model\n",
      "0s - loss: 0.2055 - acc: 0.9469 - val_loss: 0.1669 - val_acc: 0.9541\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.16694 to 0.16628, saving model to best.model\n",
      "0s - loss: 0.2037 - acc: 0.9446 - val_loss: 0.1663 - val_acc: 0.9541\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.16628 to 0.16582, saving model to best.model\n",
      "0s - loss: 0.2062 - acc: 0.9446 - val_loss: 0.1658 - val_acc: 0.9541\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.16582 to 0.16477, saving model to best.model\n",
      "0s - loss: 0.2033 - acc: 0.9446 - val_loss: 0.1648 - val_acc: 0.9541\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.16477 to 0.16343, saving model to best.model\n",
      "0s - loss: 0.1957 - acc: 0.9469 - val_loss: 0.1634 - val_acc: 0.9541\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.16343 to 0.16234, saving model to best.model\n",
      "0s - loss: 0.1769 - acc: 0.9446 - val_loss: 0.1623 - val_acc: 0.9541\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.16234 to 0.16106, saving model to best.model\n",
      "0s - loss: 0.1872 - acc: 0.9446 - val_loss: 0.1611 - val_acc: 0.9541\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.16106 to 0.15984, saving model to best.model\n",
      "0s - loss: 0.1960 - acc: 0.9446 - val_loss: 0.1598 - val_acc: 0.9541\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.15984 to 0.15881, saving model to best.model\n",
      "0s - loss: 0.1854 - acc: 0.9446 - val_loss: 0.1588 - val_acc: 0.9541\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.15881 to 0.15778, saving model to best.model\n",
      "0s - loss: 0.2068 - acc: 0.9446 - val_loss: 0.1578 - val_acc: 0.9541\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.15778 to 0.15652, saving model to best.model\n",
      "0s - loss: 0.1916 - acc: 0.9446 - val_loss: 0.1565 - val_acc: 0.9541\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.15652 to 0.15505, saving model to best.model\n",
      "0s - loss: 0.1881 - acc: 0.9423 - val_loss: 0.1550 - val_acc: 0.9541\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.15505 to 0.15351, saving model to best.model\n",
      "0s - loss: 0.1784 - acc: 0.9446 - val_loss: 0.1535 - val_acc: 0.9541\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.15351 to 0.15189, saving model to best.model\n",
      "0s - loss: 0.1908 - acc: 0.9446 - val_loss: 0.1519 - val_acc: 0.9541\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.15189 to 0.15044, saving model to best.model\n",
      "0s - loss: 0.1796 - acc: 0.9469 - val_loss: 0.1504 - val_acc: 0.9541\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.15044 to 0.14874, saving model to best.model\n",
      "0s - loss: 0.1989 - acc: 0.9469 - val_loss: 0.1487 - val_acc: 0.9541\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.14874 to 0.14679, saving model to best.model\n",
      "0s - loss: 0.1787 - acc: 0.9446 - val_loss: 0.1468 - val_acc: 0.9541\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.14679 to 0.14533, saving model to best.model\n",
      "0s - loss: 0.1831 - acc: 0.9492 - val_loss: 0.1453 - val_acc: 0.9541\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.14533 to 0.14418, saving model to best.model\n",
      "0s - loss: 0.1841 - acc: 0.9469 - val_loss: 0.1442 - val_acc: 0.9541\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.14418 to 0.14296, saving model to best.model\n",
      "0s - loss: 0.1789 - acc: 0.9423 - val_loss: 0.1430 - val_acc: 0.9541\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.14296 to 0.14132, saving model to best.model\n",
      "0s - loss: 0.1887 - acc: 0.9446 - val_loss: 0.1413 - val_acc: 0.9541\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.14132 to 0.14003, saving model to best.model\n",
      "0s - loss: 0.1777 - acc: 0.9446 - val_loss: 0.1400 - val_acc: 0.9541\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.14003 to 0.13878, saving model to best.model\n",
      "0s - loss: 0.1746 - acc: 0.9446 - val_loss: 0.1388 - val_acc: 0.9541\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.13878 to 0.13695, saving model to best.model\n",
      "0s - loss: 0.1664 - acc: 0.9469 - val_loss: 0.1369 - val_acc: 0.9541\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.13695 to 0.13425, saving model to best.model\n",
      "0s - loss: 0.1755 - acc: 0.9446 - val_loss: 0.1342 - val_acc: 0.9541\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.13425 to 0.13147, saving model to best.model\n",
      "0s - loss: 0.1821 - acc: 0.9515 - val_loss: 0.1315 - val_acc: 0.9541\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.13147 to 0.12879, saving model to best.model\n",
      "0s - loss: 0.1857 - acc: 0.9446 - val_loss: 0.1288 - val_acc: 0.9541\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.12879 to 0.12648, saving model to best.model\n",
      "0s - loss: 0.1722 - acc: 0.9469 - val_loss: 0.1265 - val_acc: 0.9541\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.12648 to 0.12473, saving model to best.model\n",
      "0s - loss: 0.1595 - acc: 0.9469 - val_loss: 0.1247 - val_acc: 0.9541\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.12473 to 0.12336, saving model to best.model\n",
      "0s - loss: 0.1754 - acc: 0.9446 - val_loss: 0.1234 - val_acc: 0.9541\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.12336 to 0.12221, saving model to best.model\n",
      "0s - loss: 0.1743 - acc: 0.9400 - val_loss: 0.1222 - val_acc: 0.9541\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.12221 to 0.12182, saving model to best.model\n",
      "0s - loss: 0.1627 - acc: 0.9515 - val_loss: 0.1218 - val_acc: 0.9541\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.12182 to 0.12153, saving model to best.model\n",
      "0s - loss: 0.1649 - acc: 0.9469 - val_loss: 0.1215 - val_acc: 0.9541\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.12153 to 0.12005, saving model to best.model\n",
      "0s - loss: 0.1553 - acc: 0.9515 - val_loss: 0.1200 - val_acc: 0.9541\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.12005 to 0.11777, saving model to best.model\n",
      "0s - loss: 0.1345 - acc: 0.9538 - val_loss: 0.1178 - val_acc: 0.9541\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.11777 to 0.11453, saving model to best.model\n",
      "0s - loss: 0.1571 - acc: 0.9446 - val_loss: 0.1145 - val_acc: 0.9541\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.11453 to 0.11119, saving model to best.model\n",
      "0s - loss: 0.1551 - acc: 0.9469 - val_loss: 0.1112 - val_acc: 0.9541\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.11119 to 0.10797, saving model to best.model\n",
      "0s - loss: 0.1474 - acc: 0.9538 - val_loss: 0.1080 - val_acc: 0.9541\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.10797 to 0.10552, saving model to best.model\n",
      "0s - loss: 0.1516 - acc: 0.9469 - val_loss: 0.1055 - val_acc: 0.9541\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.10552 to 0.10354, saving model to best.model\n",
      "0s - loss: 0.1492 - acc: 0.9423 - val_loss: 0.1035 - val_acc: 0.9541\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.10354 to 0.10225, saving model to best.model\n",
      "0s - loss: 0.1532 - acc: 0.9469 - val_loss: 0.1023 - val_acc: 0.9541\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.10225 to 0.10028, saving model to best.model\n",
      "0s - loss: 0.1582 - acc: 0.9469 - val_loss: 0.1003 - val_acc: 0.9541\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.10028 to 0.09840, saving model to best.model\n",
      "0s - loss: 0.1472 - acc: 0.9492 - val_loss: 0.0984 - val_acc: 0.9541\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.09840 to 0.09654, saving model to best.model\n",
      "0s - loss: 0.1435 - acc: 0.9469 - val_loss: 0.0965 - val_acc: 0.9541\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.09654 to 0.09447, saving model to best.model\n",
      "0s - loss: 0.1385 - acc: 0.9607 - val_loss: 0.0945 - val_acc: 0.9541\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.09447 to 0.09255, saving model to best.model\n",
      "0s - loss: 0.1440 - acc: 0.9515 - val_loss: 0.0925 - val_acc: 0.9541\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.09255 to 0.09077, saving model to best.model\n",
      "0s - loss: 0.1143 - acc: 0.9561 - val_loss: 0.0908 - val_acc: 0.9541\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.09077 to 0.08844, saving model to best.model\n",
      "0s - loss: 0.1256 - acc: 0.9515 - val_loss: 0.0884 - val_acc: 0.9541\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.08844 to 0.08556, saving model to best.model\n",
      "0s - loss: 0.1458 - acc: 0.9492 - val_loss: 0.0856 - val_acc: 0.9541\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.08556 to 0.08403, saving model to best.model\n",
      "0s - loss: 0.1298 - acc: 0.9515 - val_loss: 0.0840 - val_acc: 0.9541\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.08403 to 0.08260, saving model to best.model\n",
      "0s - loss: 0.1439 - acc: 0.9446 - val_loss: 0.0826 - val_acc: 0.9633\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.08260 to 0.08152, saving model to best.model\n",
      "0s - loss: 0.1477 - acc: 0.9561 - val_loss: 0.0815 - val_acc: 0.9633\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.08152 to 0.08098, saving model to best.model\n",
      "0s - loss: 0.1329 - acc: 0.9469 - val_loss: 0.0810 - val_acc: 0.9633\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.08098 to 0.08019, saving model to best.model\n",
      "0s - loss: 0.1161 - acc: 0.9561 - val_loss: 0.0802 - val_acc: 0.9633\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.08019 to 0.07854, saving model to best.model\n",
      "0s - loss: 0.1246 - acc: 0.9561 - val_loss: 0.0785 - val_acc: 0.9633\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.07854 to 0.07624, saving model to best.model\n",
      "0s - loss: 0.1212 - acc: 0.9538 - val_loss: 0.0762 - val_acc: 0.9633\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.07624 to 0.07403, saving model to best.model\n",
      "0s - loss: 0.1349 - acc: 0.9423 - val_loss: 0.0740 - val_acc: 0.9633\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.07403 to 0.07329, saving model to best.model\n",
      "0s - loss: 0.1164 - acc: 0.9630 - val_loss: 0.0733 - val_acc: 0.9633\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.1161 - acc: 0.9584 - val_loss: 0.0736 - val_acc: 0.9633\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.07329 to 0.07233, saving model to best.model\n",
      "0s - loss: 0.1042 - acc: 0.9515 - val_loss: 0.0723 - val_acc: 0.9633\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.07233 to 0.06978, saving model to best.model\n",
      "0s - loss: 0.1191 - acc: 0.9446 - val_loss: 0.0698 - val_acc: 0.9725\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.06978 to 0.06830, saving model to best.model\n",
      "0s - loss: 0.1126 - acc: 0.9515 - val_loss: 0.0683 - val_acc: 0.9725\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.06830 to 0.06654, saving model to best.model\n",
      "0s - loss: 0.1207 - acc: 0.9446 - val_loss: 0.0665 - val_acc: 0.9725\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.06654 to 0.06573, saving model to best.model\n",
      "0s - loss: 0.1054 - acc: 0.9561 - val_loss: 0.0657 - val_acc: 0.9725\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.0950 - acc: 0.9654 - val_loss: 0.0661 - val_acc: 0.9725\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.1224 - acc: 0.9492 - val_loss: 0.0685 - val_acc: 0.9725\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.1139 - acc: 0.9561 - val_loss: 0.0678 - val_acc: 0.9725\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.1061 - acc: 0.9607 - val_loss: 0.0684 - val_acc: 0.9725\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.1040 - acc: 0.9584 - val_loss: 0.0666 - val_acc: 0.9725\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.06573 to 0.06460, saving model to best.model\n",
      "0s - loss: 0.1151 - acc: 0.9538 - val_loss: 0.0646 - val_acc: 0.9725\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.06460 to 0.06275, saving model to best.model\n",
      "0s - loss: 0.1047 - acc: 0.9446 - val_loss: 0.0628 - val_acc: 0.9725\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.06275 to 0.06207, saving model to best.model\n",
      "0s - loss: 0.1045 - acc: 0.9607 - val_loss: 0.0621 - val_acc: 0.9725\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.1000 - acc: 0.9538 - val_loss: 0.0623 - val_acc: 0.9725\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.1008 - acc: 0.9538 - val_loss: 0.0623 - val_acc: 0.9725\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.06207 to 0.06167, saving model to best.model\n",
      "0s - loss: 0.0947 - acc: 0.9584 - val_loss: 0.0617 - val_acc: 0.9725\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.06167 to 0.06005, saving model to best.model\n",
      "0s - loss: 0.0922 - acc: 0.9607 - val_loss: 0.0601 - val_acc: 0.9725\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.06005 to 0.05880, saving model to best.model\n",
      "0s - loss: 0.1103 - acc: 0.9515 - val_loss: 0.0588 - val_acc: 0.9725\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.0913 - acc: 0.9677 - val_loss: 0.0589 - val_acc: 0.9725\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.0933 - acc: 0.9677 - val_loss: 0.0596 - val_acc: 0.9725\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.0943 - acc: 0.9700 - val_loss: 0.0594 - val_acc: 0.9725\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.0954 - acc: 0.9561 - val_loss: 0.0589 - val_acc: 0.9725\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.05880 to 0.05733, saving model to best.model\n",
      "0s - loss: 0.1188 - acc: 0.9561 - val_loss: 0.0573 - val_acc: 0.9725\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.05733 to 0.05571, saving model to best.model\n",
      "0s - loss: 0.0860 - acc: 0.9654 - val_loss: 0.0557 - val_acc: 0.9725\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.0878 - acc: 0.9677 - val_loss: 0.0565 - val_acc: 0.9725\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.0979 - acc: 0.9630 - val_loss: 0.0575 - val_acc: 0.9725\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.0803 - acc: 0.9746 - val_loss: 0.0589 - val_acc: 0.9725\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.0968 - acc: 0.9630 - val_loss: 0.0596 - val_acc: 0.9725\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.0927 - acc: 0.9607 - val_loss: 0.0560 - val_acc: 0.9725\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.05571 to 0.05313, saving model to best.model\n",
      "0s - loss: 0.0981 - acc: 0.9561 - val_loss: 0.0531 - val_acc: 0.9908\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.05313 to 0.05192, saving model to best.model\n",
      "0s - loss: 0.1009 - acc: 0.9584 - val_loss: 0.0519 - val_acc: 0.9908\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.05192 to 0.05191, saving model to best.model\n",
      "0s - loss: 0.0982 - acc: 0.9630 - val_loss: 0.0519 - val_acc: 0.9908\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.0907 - acc: 0.9607 - val_loss: 0.0541 - val_acc: 0.9908\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.0911 - acc: 0.9700 - val_loss: 0.0550 - val_acc: 0.9908\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.0894 - acc: 0.9677 - val_loss: 0.0542 - val_acc: 0.9908\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.0828 - acc: 0.9700 - val_loss: 0.0541 - val_acc: 0.9908\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.0783 - acc: 0.9723 - val_loss: 0.0543 - val_acc: 0.9908\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.0835 - acc: 0.9630 - val_loss: 0.0538 - val_acc: 0.9908\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.1077 - acc: 0.9584 - val_loss: 0.0531 - val_acc: 0.9908\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.05191 to 0.05185, saving model to best.model\n",
      "0s - loss: 0.0910 - acc: 0.9630 - val_loss: 0.0519 - val_acc: 0.9908\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.05185 to 0.05144, saving model to best.model\n",
      "0s - loss: 0.0906 - acc: 0.9677 - val_loss: 0.0514 - val_acc: 0.9908\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.05144 to 0.05013, saving model to best.model\n",
      "0s - loss: 0.1188 - acc: 0.9469 - val_loss: 0.0501 - val_acc: 0.9908\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.1146 - acc: 0.9515 - val_loss: 0.0503 - val_acc: 0.9908\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.0908 - acc: 0.9607 - val_loss: 0.0507 - val_acc: 0.9908\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.0836 - acc: 0.9723 - val_loss: 0.0518 - val_acc: 0.9908\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.0867 - acc: 0.9723 - val_loss: 0.0539 - val_acc: 0.9908\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.0849 - acc: 0.9746 - val_loss: 0.0554 - val_acc: 0.9908\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.0985 - acc: 0.9654 - val_loss: 0.0561 - val_acc: 0.9908\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.26583, saving model to best.model\n",
      "0s - loss: 0.3918 - acc: 0.8684 - val_loss: 0.2658 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.3103 - acc: 0.9192 - val_loss: 0.2718 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2688 - acc: 0.9353 - val_loss: 0.2999 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2551 - acc: 0.9376 - val_loss: 0.3261 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2587 - acc: 0.9423 - val_loss: 0.3419 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2820 - acc: 0.9423 - val_loss: 0.3487 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2871 - acc: 0.9423 - val_loss: 0.3478 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2896 - acc: 0.9423 - val_loss: 0.3394 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2792 - acc: 0.9423 - val_loss: 0.3273 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2712 - acc: 0.9423 - val_loss: 0.3137 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2548 - acc: 0.9423 - val_loss: 0.3006 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2639 - acc: 0.9423 - val_loss: 0.2888 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2576 - acc: 0.9400 - val_loss: 0.2803 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2512 - acc: 0.9423 - val_loss: 0.2742 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2608 - acc: 0.9400 - val_loss: 0.2710 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2663 - acc: 0.9376 - val_loss: 0.2692 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2569 - acc: 0.9376 - val_loss: 0.2693 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2445 - acc: 0.9423 - val_loss: 0.2701 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2441 - acc: 0.9400 - val_loss: 0.2716 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2733 - acc: 0.9400 - val_loss: 0.2743 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2513 - acc: 0.9423 - val_loss: 0.2766 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2632 - acc: 0.9423 - val_loss: 0.2776 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2751 - acc: 0.9423 - val_loss: 0.2780 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2539 - acc: 0.9423 - val_loss: 0.2782 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2480 - acc: 0.9400 - val_loss: 0.2785 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2447 - acc: 0.9400 - val_loss: 0.2789 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2432 - acc: 0.9423 - val_loss: 0.2785 - val_acc: 0.9266\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.32724, saving model to best.model\n",
      "0s - loss: 0.4134 - acc: 0.8152 - val_loss: 0.3272 - val_acc: 0.8991\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.2906 - acc: 0.9215 - val_loss: 0.3504 - val_acc: 0.8991\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2414 - acc: 0.9492 - val_loss: 0.3977 - val_acc: 0.8991\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2385 - acc: 0.9492 - val_loss: 0.4408 - val_acc: 0.8991\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2379 - acc: 0.9492 - val_loss: 0.4718 - val_acc: 0.8991\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2421 - acc: 0.9515 - val_loss: 0.4899 - val_acc: 0.8991\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2270 - acc: 0.9515 - val_loss: 0.4962 - val_acc: 0.8991\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2425 - acc: 0.9515 - val_loss: 0.4937 - val_acc: 0.8991\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2512 - acc: 0.9515 - val_loss: 0.4841 - val_acc: 0.8991\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2426 - acc: 0.9515 - val_loss: 0.4698 - val_acc: 0.8991\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2274 - acc: 0.9515 - val_loss: 0.4530 - val_acc: 0.8991\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2398 - acc: 0.9515 - val_loss: 0.4342 - val_acc: 0.8991\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2172 - acc: 0.9515 - val_loss: 0.4161 - val_acc: 0.8991\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2171 - acc: 0.9515 - val_loss: 0.3995 - val_acc: 0.8991\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2325 - acc: 0.9492 - val_loss: 0.3850 - val_acc: 0.8991\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2060 - acc: 0.9515 - val_loss: 0.3731 - val_acc: 0.8991\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2096 - acc: 0.9515 - val_loss: 0.3667 - val_acc: 0.8991\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2046 - acc: 0.9515 - val_loss: 0.3630 - val_acc: 0.8991\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2340 - acc: 0.9515 - val_loss: 0.3616 - val_acc: 0.8991\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2231 - acc: 0.9515 - val_loss: 0.3629 - val_acc: 0.8991\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2187 - acc: 0.9515 - val_loss: 0.3663 - val_acc: 0.8991\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2265 - acc: 0.9469 - val_loss: 0.3706 - val_acc: 0.8991\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2242 - acc: 0.9515 - val_loss: 0.3757 - val_acc: 0.8991\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2260 - acc: 0.9515 - val_loss: 0.3798 - val_acc: 0.8991\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2146 - acc: 0.9515 - val_loss: 0.3825 - val_acc: 0.8991\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2035 - acc: 0.9515 - val_loss: 0.3852 - val_acc: 0.8991\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2109 - acc: 0.9515 - val_loss: 0.3872 - val_acc: 0.8991\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.53873, saving model to best.model\n",
      "0s - loss: 0.9141 - acc: 0.3972 - val_loss: 0.5387 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.53873 to 0.34276, saving model to best.model\n",
      "0s - loss: 0.6047 - acc: 0.6674 - val_loss: 0.3428 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.34276 to 0.27276, saving model to best.model\n",
      "0s - loss: 0.4223 - acc: 0.8268 - val_loss: 0.2728 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.27276 to 0.26268, saving model to best.model\n",
      "0s - loss: 0.3382 - acc: 0.8984 - val_loss: 0.2627 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2975 - acc: 0.9261 - val_loss: 0.2750 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2695 - acc: 0.9261 - val_loss: 0.2921 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2695 - acc: 0.9261 - val_loss: 0.3072 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3210 - acc: 0.9261 - val_loss: 0.3188 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3033 - acc: 0.9238 - val_loss: 0.3262 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3114 - acc: 0.9261 - val_loss: 0.3296 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3246 - acc: 0.9261 - val_loss: 0.3296 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3399 - acc: 0.9261 - val_loss: 0.3268 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3273 - acc: 0.9261 - val_loss: 0.3218 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3364 - acc: 0.9261 - val_loss: 0.3152 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3209 - acc: 0.9261 - val_loss: 0.3078 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3320 - acc: 0.9238 - val_loss: 0.2999 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3162 - acc: 0.9238 - val_loss: 0.2922 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3088 - acc: 0.9261 - val_loss: 0.2849 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2982 - acc: 0.9261 - val_loss: 0.2782 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2970 - acc: 0.9238 - val_loss: 0.2729 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2803 - acc: 0.9261 - val_loss: 0.2688 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2949 - acc: 0.9238 - val_loss: 0.2660 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2910 - acc: 0.9261 - val_loss: 0.2642 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2859 - acc: 0.9261 - val_loss: 0.2631 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.26268 to 0.26259, saving model to best.model\n",
      "0s - loss: 0.2973 - acc: 0.9261 - val_loss: 0.2626 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.26259 to 0.26238, saving model to best.model\n",
      "0s - loss: 0.3114 - acc: 0.9261 - val_loss: 0.2624 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3139 - acc: 0.9238 - val_loss: 0.2624 - val_acc: 0.9266\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2981 - acc: 0.9238 - val_loss: 0.2625 - val_acc: 0.9266\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2872 - acc: 0.9238 - val_loss: 0.2627 - val_acc: 0.9266\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.3030 - acc: 0.9261 - val_loss: 0.2630 - val_acc: 0.9266\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2889 - acc: 0.9261 - val_loss: 0.2635 - val_acc: 0.9266\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2849 - acc: 0.9238 - val_loss: 0.2640 - val_acc: 0.9266\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2963 - acc: 0.9215 - val_loss: 0.2646 - val_acc: 0.9266\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2864 - acc: 0.9261 - val_loss: 0.2653 - val_acc: 0.9266\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2777 - acc: 0.9261 - val_loss: 0.2661 - val_acc: 0.9266\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2829 - acc: 0.9261 - val_loss: 0.2667 - val_acc: 0.9266\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2943 - acc: 0.9261 - val_loss: 0.2673 - val_acc: 0.9266\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2852 - acc: 0.9261 - val_loss: 0.2676 - val_acc: 0.9266\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.3178 - acc: 0.9261 - val_loss: 0.2674 - val_acc: 0.9266\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2747 - acc: 0.9261 - val_loss: 0.2674 - val_acc: 0.9266\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2810 - acc: 0.9261 - val_loss: 0.2673 - val_acc: 0.9266\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2911 - acc: 0.9261 - val_loss: 0.2671 - val_acc: 0.9266\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2694 - acc: 0.9261 - val_loss: 0.2668 - val_acc: 0.9266\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2795 - acc: 0.9238 - val_loss: 0.2664 - val_acc: 0.9266\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2852 - acc: 0.9261 - val_loss: 0.2659 - val_acc: 0.9266\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2667 - acc: 0.9261 - val_loss: 0.2654 - val_acc: 0.9266\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2849 - acc: 0.9261 - val_loss: 0.2650 - val_acc: 0.9266\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2749 - acc: 0.9261 - val_loss: 0.2644 - val_acc: 0.9266\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2915 - acc: 0.9261 - val_loss: 0.2641 - val_acc: 0.9266\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2843 - acc: 0.9238 - val_loss: 0.2640 - val_acc: 0.9266\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.2737 - acc: 0.9261 - val_loss: 0.2638 - val_acc: 0.9266\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.3013 - acc: 0.9261 - val_loss: 0.2639 - val_acc: 0.9266\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.45849, saving model to best.model\n",
      "0s - loss: 0.9106 - acc: 0.4734 - val_loss: 0.4585 - val_acc: 0.9633\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.45849 to 0.25362, saving model to best.model\n",
      "0s - loss: 0.5557 - acc: 0.7206 - val_loss: 0.2536 - val_acc: 0.9633\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.25362 to 0.17790, saving model to best.model\n",
      "0s - loss: 0.4464 - acc: 0.8245 - val_loss: 0.1779 - val_acc: 0.9633\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.17790 to 0.15869, saving model to best.model\n",
      "0s - loss: 0.3497 - acc: 0.9053 - val_loss: 0.1587 - val_acc: 0.9633\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3274 - acc: 0.9145 - val_loss: 0.1591 - val_acc: 0.9633\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3313 - acc: 0.9215 - val_loss: 0.1644 - val_acc: 0.9633\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3547 - acc: 0.9215 - val_loss: 0.1695 - val_acc: 0.9633\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3517 - acc: 0.9192 - val_loss: 0.1725 - val_acc: 0.9633\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3681 - acc: 0.9215 - val_loss: 0.1734 - val_acc: 0.9633\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3586 - acc: 0.9215 - val_loss: 0.1725 - val_acc: 0.9633\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3756 - acc: 0.9215 - val_loss: 0.1704 - val_acc: 0.9633\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3683 - acc: 0.9215 - val_loss: 0.1675 - val_acc: 0.9633\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3566 - acc: 0.9215 - val_loss: 0.1645 - val_acc: 0.9633\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3605 - acc: 0.9215 - val_loss: 0.1615 - val_acc: 0.9633\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3029 - acc: 0.9215 - val_loss: 0.1593 - val_acc: 0.9633\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.15869 to 0.15797, saving model to best.model\n",
      "0s - loss: 0.3528 - acc: 0.9215 - val_loss: 0.1580 - val_acc: 0.9633\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.15797 to 0.15786, saving model to best.model\n",
      "0s - loss: 0.3223 - acc: 0.9192 - val_loss: 0.1579 - val_acc: 0.9633\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3308 - acc: 0.9192 - val_loss: 0.1588 - val_acc: 0.9633\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3228 - acc: 0.9169 - val_loss: 0.1606 - val_acc: 0.9633\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3030 - acc: 0.9169 - val_loss: 0.1628 - val_acc: 0.9633\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3324 - acc: 0.9192 - val_loss: 0.1647 - val_acc: 0.9633\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3027 - acc: 0.9215 - val_loss: 0.1661 - val_acc: 0.9633\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.3203 - acc: 0.9145 - val_loss: 0.1665 - val_acc: 0.9633\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.3186 - acc: 0.9215 - val_loss: 0.1658 - val_acc: 0.9633\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.3090 - acc: 0.9215 - val_loss: 0.1648 - val_acc: 0.9633\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.3481 - acc: 0.9169 - val_loss: 0.1633 - val_acc: 0.9633\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3201 - acc: 0.9145 - val_loss: 0.1617 - val_acc: 0.9633\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.3199 - acc: 0.9192 - val_loss: 0.1604 - val_acc: 0.9633\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.3066 - acc: 0.9145 - val_loss: 0.1594 - val_acc: 0.9633\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.3032 - acc: 0.9192 - val_loss: 0.1588 - val_acc: 0.9633\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2954 - acc: 0.9215 - val_loss: 0.1585 - val_acc: 0.9633\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.3308 - acc: 0.9192 - val_loss: 0.1584 - val_acc: 0.9633\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.3296 - acc: 0.9215 - val_loss: 0.1584 - val_acc: 0.9633\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.3034 - acc: 0.9192 - val_loss: 0.1586 - val_acc: 0.9633\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.3117 - acc: 0.9169 - val_loss: 0.1589 - val_acc: 0.9633\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.3096 - acc: 0.9215 - val_loss: 0.1592 - val_acc: 0.9633\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.3190 - acc: 0.9215 - val_loss: 0.1595 - val_acc: 0.9633\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.3008 - acc: 0.9192 - val_loss: 0.1600 - val_acc: 0.9633\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.3083 - acc: 0.9215 - val_loss: 0.1606 - val_acc: 0.9633\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.3149 - acc: 0.9215 - val_loss: 0.1610 - val_acc: 0.9633\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.3245 - acc: 0.9145 - val_loss: 0.1613 - val_acc: 0.9633\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.3128 - acc: 0.9215 - val_loss: 0.1618 - val_acc: 0.9633\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.3062 - acc: 0.9145 - val_loss: 0.1623 - val_acc: 0.9633\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.93898, saving model to best.model\n",
      "0s - loss: 1.5641 - acc: 0.1547 - val_loss: 0.9390 - val_acc: 0.0642\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.93898 to 0.54600, saving model to best.model\n",
      "0s - loss: 0.9520 - acc: 0.4018 - val_loss: 0.5460 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.54600 to 0.33956, saving model to best.model\n",
      "0s - loss: 0.6259 - acc: 0.6582 - val_loss: 0.3396 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.33956 to 0.25790, saving model to best.model\n",
      "0s - loss: 0.4074 - acc: 0.8291 - val_loss: 0.2579 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.25790 to 0.23953, saving model to best.model\n",
      "0s - loss: 0.2838 - acc: 0.9215 - val_loss: 0.2395 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2550 - acc: 0.9446 - val_loss: 0.2471 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2154 - acc: 0.9492 - val_loss: 0.2633 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2129 - acc: 0.9538 - val_loss: 0.2804 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2395 - acc: 0.9538 - val_loss: 0.2956 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2066 - acc: 0.9538 - val_loss: 0.3078 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2272 - acc: 0.9538 - val_loss: 0.3171 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2463 - acc: 0.9538 - val_loss: 0.3238 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2386 - acc: 0.9538 - val_loss: 0.3280 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2311 - acc: 0.9538 - val_loss: 0.3301 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2444 - acc: 0.9538 - val_loss: 0.3303 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2429 - acc: 0.9538 - val_loss: 0.3288 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2163 - acc: 0.9538 - val_loss: 0.3264 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2447 - acc: 0.9538 - val_loss: 0.3230 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2375 - acc: 0.9538 - val_loss: 0.3186 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2437 - acc: 0.9538 - val_loss: 0.3137 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2235 - acc: 0.9538 - val_loss: 0.3080 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2375 - acc: 0.9538 - val_loss: 0.3022 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2395 - acc: 0.9538 - val_loss: 0.2963 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.1967 - acc: 0.9538 - val_loss: 0.2903 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2338 - acc: 0.9538 - val_loss: 0.2846 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2354 - acc: 0.9538 - val_loss: 0.2788 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.1962 - acc: 0.9538 - val_loss: 0.2737 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2070 - acc: 0.9538 - val_loss: 0.2691 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.1916 - acc: 0.9538 - val_loss: 0.2649 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.1962 - acc: 0.9538 - val_loss: 0.2615 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2118 - acc: 0.9538 - val_loss: 0.2585 - val_acc: 0.9358\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.34239, saving model to best.model\n",
      "0s - loss: 0.5366 - acc: 0.7390 - val_loss: 0.3424 - val_acc: 0.9174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.34239 to 0.28638, saving model to best.model\n",
      "0s - loss: 0.3895 - acc: 0.8591 - val_loss: 0.2864 - val_acc: 0.9174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.3453 - acc: 0.9099 - val_loss: 0.2945 - val_acc: 0.9174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.3166 - acc: 0.9192 - val_loss: 0.3184 - val_acc: 0.9174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3343 - acc: 0.9215 - val_loss: 0.3402 - val_acc: 0.9174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3463 - acc: 0.9215 - val_loss: 0.3540 - val_acc: 0.9174\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3267 - acc: 0.9215 - val_loss: 0.3584 - val_acc: 0.9174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3368 - acc: 0.9215 - val_loss: 0.3571 - val_acc: 0.9174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3685 - acc: 0.9215 - val_loss: 0.3493 - val_acc: 0.9174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3457 - acc: 0.9215 - val_loss: 0.3381 - val_acc: 0.9174\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3179 - acc: 0.9215 - val_loss: 0.3268 - val_acc: 0.9174\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3071 - acc: 0.9192 - val_loss: 0.3151 - val_acc: 0.9174\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3286 - acc: 0.9215 - val_loss: 0.3049 - val_acc: 0.9174\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3358 - acc: 0.9169 - val_loss: 0.2967 - val_acc: 0.9174\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3336 - acc: 0.9169 - val_loss: 0.2908 - val_acc: 0.9174\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3096 - acc: 0.9215 - val_loss: 0.2876 - val_acc: 0.9174\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.28638 to 0.28608, saving model to best.model\n",
      "0s - loss: 0.2982 - acc: 0.9192 - val_loss: 0.2861 - val_acc: 0.9174\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.28608 to 0.28568, saving model to best.model\n",
      "0s - loss: 0.3297 - acc: 0.9169 - val_loss: 0.2857 - val_acc: 0.9174\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2947 - acc: 0.9215 - val_loss: 0.2858 - val_acc: 0.9174\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3107 - acc: 0.9169 - val_loss: 0.2863 - val_acc: 0.9174\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3148 - acc: 0.9169 - val_loss: 0.2874 - val_acc: 0.9174\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3167 - acc: 0.9215 - val_loss: 0.2894 - val_acc: 0.9174\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.3099 - acc: 0.9192 - val_loss: 0.2914 - val_acc: 0.9174\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2992 - acc: 0.9192 - val_loss: 0.2935 - val_acc: 0.9174\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2905 - acc: 0.9215 - val_loss: 0.2944 - val_acc: 0.9174\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.3102 - acc: 0.9215 - val_loss: 0.2947 - val_acc: 0.9174\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3046 - acc: 0.9215 - val_loss: 0.2944 - val_acc: 0.9174\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2923 - acc: 0.9215 - val_loss: 0.2939 - val_acc: 0.9174\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.3007 - acc: 0.9192 - val_loss: 0.2927 - val_acc: 0.9174\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.3221 - acc: 0.9215 - val_loss: 0.2909 - val_acc: 0.9174\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2979 - acc: 0.9215 - val_loss: 0.2893 - val_acc: 0.9174\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2860 - acc: 0.9192 - val_loss: 0.2882 - val_acc: 0.9174\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2914 - acc: 0.9215 - val_loss: 0.2875 - val_acc: 0.9174\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.3050 - acc: 0.9192 - val_loss: 0.2867 - val_acc: 0.9174\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2892 - acc: 0.9215 - val_loss: 0.2864 - val_acc: 0.9174\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.3120 - acc: 0.9215 - val_loss: 0.2864 - val_acc: 0.9174\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.3118 - acc: 0.9215 - val_loss: 0.2865 - val_acc: 0.9174\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.3105 - acc: 0.9192 - val_loss: 0.2869 - val_acc: 0.9174\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.3027 - acc: 0.9215 - val_loss: 0.2872 - val_acc: 0.9174\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2946 - acc: 0.9215 - val_loss: 0.2875 - val_acc: 0.9174\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2905 - acc: 0.9215 - val_loss: 0.2879 - val_acc: 0.9174\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2906 - acc: 0.9215 - val_loss: 0.2880 - val_acc: 0.9174\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2748 - acc: 0.9192 - val_loss: 0.2883 - val_acc: 0.9174\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.3059 - acc: 0.9215 - val_loss: 0.2881 - val_acc: 0.9174\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.38052, saving model to best.model\n",
      "0s - loss: 0.7029 - acc: 0.6397 - val_loss: 0.3805 - val_acc: 0.9083\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.38052 to 0.30892, saving model to best.model\n",
      "0s - loss: 0.4158 - acc: 0.8360 - val_loss: 0.3089 - val_acc: 0.9083\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2809 - acc: 0.9261 - val_loss: 0.3174 - val_acc: 0.9083\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2448 - acc: 0.9492 - val_loss: 0.3510 - val_acc: 0.9083\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2400 - acc: 0.9492 - val_loss: 0.3871 - val_acc: 0.9083\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2116 - acc: 0.9515 - val_loss: 0.4179 - val_acc: 0.9083\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2374 - acc: 0.9515 - val_loss: 0.4408 - val_acc: 0.9083\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2481 - acc: 0.9515 - val_loss: 0.4562 - val_acc: 0.9083\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2454 - acc: 0.9515 - val_loss: 0.4646 - val_acc: 0.9083\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2570 - acc: 0.9515 - val_loss: 0.4668 - val_acc: 0.9083\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2400 - acc: 0.9515 - val_loss: 0.4648 - val_acc: 0.9083\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2271 - acc: 0.9515 - val_loss: 0.4586 - val_acc: 0.9083\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2353 - acc: 0.9515 - val_loss: 0.4493 - val_acc: 0.9083\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2381 - acc: 0.9515 - val_loss: 0.4378 - val_acc: 0.9083\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2632 - acc: 0.9515 - val_loss: 0.4246 - val_acc: 0.9083\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2423 - acc: 0.9515 - val_loss: 0.4104 - val_acc: 0.9083\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2238 - acc: 0.9515 - val_loss: 0.3959 - val_acc: 0.9083\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2380 - acc: 0.9515 - val_loss: 0.3820 - val_acc: 0.9083\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2405 - acc: 0.9515 - val_loss: 0.3698 - val_acc: 0.9083\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2168 - acc: 0.9515 - val_loss: 0.3600 - val_acc: 0.9083\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2320 - acc: 0.9492 - val_loss: 0.3516 - val_acc: 0.9083\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2382 - acc: 0.9515 - val_loss: 0.3453 - val_acc: 0.9083\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2332 - acc: 0.9469 - val_loss: 0.3411 - val_acc: 0.9083\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2246 - acc: 0.9515 - val_loss: 0.3386 - val_acc: 0.9083\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2238 - acc: 0.9492 - val_loss: 0.3367 - val_acc: 0.9083\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2337 - acc: 0.9492 - val_loss: 0.3362 - val_acc: 0.9083\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2235 - acc: 0.9446 - val_loss: 0.3365 - val_acc: 0.9083\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2474 - acc: 0.9515 - val_loss: 0.3379 - val_acc: 0.9083\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.94494, saving model to best.model\n",
      "0s - loss: 1.4567 - acc: 0.1686 - val_loss: 0.9449 - val_acc: 0.0459\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.94494 to 0.54232, saving model to best.model\n",
      "0s - loss: 1.0040 - acc: 0.3510 - val_loss: 0.5423 - val_acc: 0.9541\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.54232 to 0.32147, saving model to best.model\n",
      "0s - loss: 0.6104 - acc: 0.6628 - val_loss: 0.3215 - val_acc: 0.9541\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.32147 to 0.22455, saving model to best.model\n",
      "0s - loss: 0.4266 - acc: 0.8383 - val_loss: 0.2246 - val_acc: 0.9541\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.22455 to 0.19158, saving model to best.model\n",
      "0s - loss: 0.3117 - acc: 0.9099 - val_loss: 0.1916 - val_acc: 0.9541\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.19158 to 0.18639, saving model to best.model\n",
      "0s - loss: 0.2994 - acc: 0.9238 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2691 - acc: 0.9330 - val_loss: 0.1916 - val_acc: 0.9541\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2742 - acc: 0.9353 - val_loss: 0.1996 - val_acc: 0.9541\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2864 - acc: 0.9353 - val_loss: 0.2069 - val_acc: 0.9541\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3002 - acc: 0.9353 - val_loss: 0.2128 - val_acc: 0.9541\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3199 - acc: 0.9353 - val_loss: 0.2167 - val_acc: 0.9541\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2878 - acc: 0.9353 - val_loss: 0.2188 - val_acc: 0.9541\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2921 - acc: 0.9353 - val_loss: 0.2194 - val_acc: 0.9541\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3044 - acc: 0.9353 - val_loss: 0.2188 - val_acc: 0.9541\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2760 - acc: 0.9353 - val_loss: 0.2169 - val_acc: 0.9541\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2948 - acc: 0.9353 - val_loss: 0.2140 - val_acc: 0.9541\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2760 - acc: 0.9353 - val_loss: 0.2108 - val_acc: 0.9541\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2818 - acc: 0.9353 - val_loss: 0.2074 - val_acc: 0.9541\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2880 - acc: 0.9353 - val_loss: 0.2039 - val_acc: 0.9541\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2849 - acc: 0.9353 - val_loss: 0.2004 - val_acc: 0.9541\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2806 - acc: 0.9353 - val_loss: 0.1972 - val_acc: 0.9541\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2813 - acc: 0.9353 - val_loss: 0.1943 - val_acc: 0.9541\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2723 - acc: 0.9353 - val_loss: 0.1917 - val_acc: 0.9541\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2553 - acc: 0.9353 - val_loss: 0.1895 - val_acc: 0.9541\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2578 - acc: 0.9353 - val_loss: 0.1880 - val_acc: 0.9541\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2759 - acc: 0.9353 - val_loss: 0.1871 - val_acc: 0.9541\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2406 - acc: 0.9353 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.18639 to 0.18624, saving model to best.model\n",
      "0s - loss: 0.2606 - acc: 0.9330 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.18624 to 0.18613, saving model to best.model\n",
      "0s - loss: 0.2718 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.18613 to 0.18611, saving model to best.model\n",
      "0s - loss: 0.2639 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2666 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2509 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2544 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.18611 to 0.18611, saving model to best.model\n",
      "0s - loss: 0.2607 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.18611 to 0.18610, saving model to best.model\n",
      "0s - loss: 0.2612 - acc: 0.9330 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2594 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2697 - acc: 0.9330 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2594 - acc: 0.9353 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2747 - acc: 0.9376 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2650 - acc: 0.9330 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2631 - acc: 0.9353 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2608 - acc: 0.9330 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2396 - acc: 0.9353 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2542 - acc: 0.9353 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2705 - acc: 0.9330 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2571 - acc: 0.9330 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2797 - acc: 0.9353 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2554 - acc: 0.9353 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2538 - acc: 0.9353 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2717 - acc: 0.9353 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.2724 - acc: 0.9353 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.2485 - acc: 0.9353 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.2723 - acc: 0.9353 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.2420 - acc: 0.9353 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.2598 - acc: 0.9353 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.2598 - acc: 0.9353 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.2421 - acc: 0.9353 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.2591 - acc: 0.9330 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.2644 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.18610 to 0.18609, saving model to best.model\n",
      "0s - loss: 0.2797 - acc: 0.9330 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.18609 to 0.18604, saving model to best.model\n",
      "0s - loss: 0.2436 - acc: 0.9353 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.18604 to 0.18600, saving model to best.model\n",
      "0s - loss: 0.2516 - acc: 0.9353 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.18600 to 0.18597, saving model to best.model\n",
      "0s - loss: 0.2664 - acc: 0.9330 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.18597 to 0.18593, saving model to best.model\n",
      "0s - loss: 0.2585 - acc: 0.9353 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.18593 to 0.18590, saving model to best.model\n",
      "0s - loss: 0.2680 - acc: 0.9353 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.18590 to 0.18588, saving model to best.model\n",
      "0s - loss: 0.2668 - acc: 0.9353 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.18588 to 0.18587, saving model to best.model\n",
      "0s - loss: 0.2488 - acc: 0.9353 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.18587 to 0.18585, saving model to best.model\n",
      "0s - loss: 0.2587 - acc: 0.9330 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.18585 to 0.18584, saving model to best.model\n",
      "0s - loss: 0.2393 - acc: 0.9353 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.18584 to 0.18584, saving model to best.model\n",
      "0s - loss: 0.2694 - acc: 0.9353 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.18584 to 0.18583, saving model to best.model\n",
      "0s - loss: 0.2598 - acc: 0.9353 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.18583 to 0.18580, saving model to best.model\n",
      "0s - loss: 0.2537 - acc: 0.9353 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.18580 to 0.18577, saving model to best.model\n",
      "0s - loss: 0.2542 - acc: 0.9353 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.18577 to 0.18574, saving model to best.model\n",
      "0s - loss: 0.2409 - acc: 0.9353 - val_loss: 0.1857 - val_acc: 0.9541\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.18574 to 0.18570, saving model to best.model\n",
      "0s - loss: 0.2680 - acc: 0.9353 - val_loss: 0.1857 - val_acc: 0.9541\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.18570 to 0.18566, saving model to best.model\n",
      "0s - loss: 0.2694 - acc: 0.9330 - val_loss: 0.1857 - val_acc: 0.9541\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.18566 to 0.18562, saving model to best.model\n",
      "0s - loss: 0.2420 - acc: 0.9353 - val_loss: 0.1856 - val_acc: 0.9541\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.18562 to 0.18558, saving model to best.model\n",
      "0s - loss: 0.2600 - acc: 0.9353 - val_loss: 0.1856 - val_acc: 0.9541\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.18558 to 0.18553, saving model to best.model\n",
      "0s - loss: 0.2411 - acc: 0.9353 - val_loss: 0.1855 - val_acc: 0.9541\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.18553 to 0.18548, saving model to best.model\n",
      "0s - loss: 0.2439 - acc: 0.9353 - val_loss: 0.1855 - val_acc: 0.9541\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.18548 to 0.18544, saving model to best.model\n",
      "0s - loss: 0.2572 - acc: 0.9353 - val_loss: 0.1854 - val_acc: 0.9541\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.18544 to 0.18539, saving model to best.model\n",
      "0s - loss: 0.2739 - acc: 0.9353 - val_loss: 0.1854 - val_acc: 0.9541\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.18539 to 0.18533, saving model to best.model\n",
      "0s - loss: 0.2617 - acc: 0.9353 - val_loss: 0.1853 - val_acc: 0.9541\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.18533 to 0.18527, saving model to best.model\n",
      "0s - loss: 0.2535 - acc: 0.9353 - val_loss: 0.1853 - val_acc: 0.9541\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.18527 to 0.18521, saving model to best.model\n",
      "0s - loss: 0.2541 - acc: 0.9353 - val_loss: 0.1852 - val_acc: 0.9541\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.18521 to 0.18514, saving model to best.model\n",
      "0s - loss: 0.2532 - acc: 0.9353 - val_loss: 0.1851 - val_acc: 0.9541\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.18514 to 0.18506, saving model to best.model\n",
      "0s - loss: 0.2298 - acc: 0.9353 - val_loss: 0.1851 - val_acc: 0.9541\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.18506 to 0.18499, saving model to best.model\n",
      "0s - loss: 0.2392 - acc: 0.9353 - val_loss: 0.1850 - val_acc: 0.9541\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.18499 to 0.18491, saving model to best.model\n",
      "0s - loss: 0.2488 - acc: 0.9353 - val_loss: 0.1849 - val_acc: 0.9541\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.18491 to 0.18482, saving model to best.model\n",
      "0s - loss: 0.2481 - acc: 0.9353 - val_loss: 0.1848 - val_acc: 0.9541\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.18482 to 0.18472, saving model to best.model\n",
      "0s - loss: 0.2376 - acc: 0.9353 - val_loss: 0.1847 - val_acc: 0.9541\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.18472 to 0.18457, saving model to best.model\n",
      "0s - loss: 0.2550 - acc: 0.9353 - val_loss: 0.1846 - val_acc: 0.9541\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.18457 to 0.18443, saving model to best.model\n",
      "0s - loss: 0.2558 - acc: 0.9353 - val_loss: 0.1844 - val_acc: 0.9541\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.18443 to 0.18429, saving model to best.model\n",
      "0s - loss: 0.2399 - acc: 0.9353 - val_loss: 0.1843 - val_acc: 0.9541\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.18429 to 0.18415, saving model to best.model\n",
      "0s - loss: 0.2728 - acc: 0.9353 - val_loss: 0.1841 - val_acc: 0.9541\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.18415 to 0.18399, saving model to best.model\n",
      "0s - loss: 0.2491 - acc: 0.9353 - val_loss: 0.1840 - val_acc: 0.9541\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.18399 to 0.18382, saving model to best.model\n",
      "0s - loss: 0.2524 - acc: 0.9353 - val_loss: 0.1838 - val_acc: 0.9541\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.18382 to 0.18367, saving model to best.model\n",
      "0s - loss: 0.2373 - acc: 0.9353 - val_loss: 0.1837 - val_acc: 0.9541\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.18367 to 0.18353, saving model to best.model\n",
      "0s - loss: 0.2393 - acc: 0.9330 - val_loss: 0.1835 - val_acc: 0.9541\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.18353 to 0.18340, saving model to best.model\n",
      "0s - loss: 0.2507 - acc: 0.9353 - val_loss: 0.1834 - val_acc: 0.9541\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.18340 to 0.18326, saving model to best.model\n",
      "0s - loss: 0.2399 - acc: 0.9353 - val_loss: 0.1833 - val_acc: 0.9541\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.18326 to 0.18315, saving model to best.model\n",
      "0s - loss: 0.2501 - acc: 0.9353 - val_loss: 0.1832 - val_acc: 0.9541\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.18315 to 0.18303, saving model to best.model\n",
      "0s - loss: 0.2470 - acc: 0.9353 - val_loss: 0.1830 - val_acc: 0.9541\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.18303 to 0.18288, saving model to best.model\n",
      "0s - loss: 0.2274 - acc: 0.9353 - val_loss: 0.1829 - val_acc: 0.9541\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.18288 to 0.18270, saving model to best.model\n",
      "0s - loss: 0.2275 - acc: 0.9353 - val_loss: 0.1827 - val_acc: 0.9541\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.18270 to 0.18250, saving model to best.model\n",
      "0s - loss: 0.2379 - acc: 0.9353 - val_loss: 0.1825 - val_acc: 0.9541\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.18250 to 0.18228, saving model to best.model\n",
      "0s - loss: 0.2303 - acc: 0.9353 - val_loss: 0.1823 - val_acc: 0.9541\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.18228 to 0.18206, saving model to best.model\n",
      "0s - loss: 0.2437 - acc: 0.9353 - val_loss: 0.1821 - val_acc: 0.9541\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.18206 to 0.18184, saving model to best.model\n",
      "0s - loss: 0.2242 - acc: 0.9353 - val_loss: 0.1818 - val_acc: 0.9541\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.18184 to 0.18162, saving model to best.model\n",
      "0s - loss: 0.2149 - acc: 0.9353 - val_loss: 0.1816 - val_acc: 0.9541\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.18162 to 0.18140, saving model to best.model\n",
      "0s - loss: 0.2389 - acc: 0.9353 - val_loss: 0.1814 - val_acc: 0.9541\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.18140 to 0.18115, saving model to best.model\n",
      "0s - loss: 0.2506 - acc: 0.9353 - val_loss: 0.1811 - val_acc: 0.9541\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.18115 to 0.18089, saving model to best.model\n",
      "0s - loss: 0.2404 - acc: 0.9353 - val_loss: 0.1809 - val_acc: 0.9541\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.18089 to 0.18064, saving model to best.model\n",
      "0s - loss: 0.2338 - acc: 0.9353 - val_loss: 0.1806 - val_acc: 0.9541\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.18064 to 0.18036, saving model to best.model\n",
      "0s - loss: 0.2388 - acc: 0.9353 - val_loss: 0.1804 - val_acc: 0.9541\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.18036 to 0.18004, saving model to best.model\n",
      "0s - loss: 0.2380 - acc: 0.9353 - val_loss: 0.1800 - val_acc: 0.9541\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.18004 to 0.17969, saving model to best.model\n",
      "0s - loss: 0.2362 - acc: 0.9353 - val_loss: 0.1797 - val_acc: 0.9541\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.17969 to 0.17933, saving model to best.model\n",
      "0s - loss: 0.2357 - acc: 0.9353 - val_loss: 0.1793 - val_acc: 0.9541\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.17933 to 0.17891, saving model to best.model\n",
      "0s - loss: 0.2343 - acc: 0.9353 - val_loss: 0.1789 - val_acc: 0.9541\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.17891 to 0.17852, saving model to best.model\n",
      "0s - loss: 0.2430 - acc: 0.9353 - val_loss: 0.1785 - val_acc: 0.9541\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.17852 to 0.17811, saving model to best.model\n",
      "0s - loss: 0.2333 - acc: 0.9353 - val_loss: 0.1781 - val_acc: 0.9541\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.17811 to 0.17761, saving model to best.model\n",
      "0s - loss: 0.2365 - acc: 0.9353 - val_loss: 0.1776 - val_acc: 0.9541\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.17761 to 0.17705, saving model to best.model\n",
      "0s - loss: 0.2302 - acc: 0.9353 - val_loss: 0.1770 - val_acc: 0.9541\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.17705 to 0.17646, saving model to best.model\n",
      "0s - loss: 0.2231 - acc: 0.9353 - val_loss: 0.1765 - val_acc: 0.9541\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.17646 to 0.17591, saving model to best.model\n",
      "0s - loss: 0.2375 - acc: 0.9330 - val_loss: 0.1759 - val_acc: 0.9541\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.17591 to 0.17538, saving model to best.model\n",
      "0s - loss: 0.2271 - acc: 0.9353 - val_loss: 0.1754 - val_acc: 0.9541\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.17538 to 0.17490, saving model to best.model\n",
      "0s - loss: 0.2286 - acc: 0.9353 - val_loss: 0.1749 - val_acc: 0.9541\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.17490 to 0.17445, saving model to best.model\n",
      "0s - loss: 0.2250 - acc: 0.9353 - val_loss: 0.1745 - val_acc: 0.9541\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.17445 to 0.17387, saving model to best.model\n",
      "0s - loss: 0.2236 - acc: 0.9353 - val_loss: 0.1739 - val_acc: 0.9541\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.17387 to 0.17328, saving model to best.model\n",
      "0s - loss: 0.2279 - acc: 0.9353 - val_loss: 0.1733 - val_acc: 0.9541\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.17328 to 0.17265, saving model to best.model\n",
      "0s - loss: 0.2251 - acc: 0.9353 - val_loss: 0.1726 - val_acc: 0.9541\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.17265 to 0.17202, saving model to best.model\n",
      "0s - loss: 0.2113 - acc: 0.9376 - val_loss: 0.1720 - val_acc: 0.9541\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.17202 to 0.17138, saving model to best.model\n",
      "0s - loss: 0.2102 - acc: 0.9353 - val_loss: 0.1714 - val_acc: 0.9541\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.17138 to 0.17072, saving model to best.model\n",
      "0s - loss: 0.2023 - acc: 0.9353 - val_loss: 0.1707 - val_acc: 0.9541\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.17072 to 0.17007, saving model to best.model\n",
      "0s - loss: 0.2137 - acc: 0.9353 - val_loss: 0.1701 - val_acc: 0.9541\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.17007 to 0.16941, saving model to best.model\n",
      "0s - loss: 0.2122 - acc: 0.9376 - val_loss: 0.1694 - val_acc: 0.9541\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.16941 to 0.16864, saving model to best.model\n",
      "0s - loss: 0.2065 - acc: 0.9376 - val_loss: 0.1686 - val_acc: 0.9541\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.16864 to 0.16781, saving model to best.model\n",
      "0s - loss: 0.2000 - acc: 0.9376 - val_loss: 0.1678 - val_acc: 0.9541\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.16781 to 0.16701, saving model to best.model\n",
      "0s - loss: 0.2110 - acc: 0.9353 - val_loss: 0.1670 - val_acc: 0.9541\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.16701 to 0.16636, saving model to best.model\n",
      "0s - loss: 0.2194 - acc: 0.9353 - val_loss: 0.1664 - val_acc: 0.9541\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.16636 to 0.16563, saving model to best.model\n",
      "0s - loss: 0.2144 - acc: 0.9353 - val_loss: 0.1656 - val_acc: 0.9541\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.16563 to 0.16486, saving model to best.model\n",
      "0s - loss: 0.2102 - acc: 0.9353 - val_loss: 0.1649 - val_acc: 0.9541\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.16486 to 0.16385, saving model to best.model\n",
      "0s - loss: 0.2029 - acc: 0.9353 - val_loss: 0.1639 - val_acc: 0.9541\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.16385 to 0.16292, saving model to best.model\n",
      "0s - loss: 0.1983 - acc: 0.9400 - val_loss: 0.1629 - val_acc: 0.9541\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.16292 to 0.16212, saving model to best.model\n",
      "0s - loss: 0.2003 - acc: 0.9353 - val_loss: 0.1621 - val_acc: 0.9541\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.16212 to 0.16139, saving model to best.model\n",
      "0s - loss: 0.2068 - acc: 0.9353 - val_loss: 0.1614 - val_acc: 0.9541\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.16139 to 0.16099, saving model to best.model\n",
      "0s - loss: 0.2151 - acc: 0.9353 - val_loss: 0.1610 - val_acc: 0.9541\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.16099 to 0.16070, saving model to best.model\n",
      "0s - loss: 0.1878 - acc: 0.9353 - val_loss: 0.1607 - val_acc: 0.9541\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.16070 to 0.16027, saving model to best.model\n",
      "0s - loss: 0.2099 - acc: 0.9353 - val_loss: 0.1603 - val_acc: 0.9541\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.16027 to 0.15921, saving model to best.model\n",
      "0s - loss: 0.1963 - acc: 0.9330 - val_loss: 0.1592 - val_acc: 0.9541\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.15921 to 0.15791, saving model to best.model\n",
      "0s - loss: 0.1823 - acc: 0.9423 - val_loss: 0.1579 - val_acc: 0.9541\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.15791 to 0.15699, saving model to best.model\n",
      "0s - loss: 0.2061 - acc: 0.9376 - val_loss: 0.1570 - val_acc: 0.9541\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.15699 to 0.15620, saving model to best.model\n",
      "0s - loss: 0.1900 - acc: 0.9353 - val_loss: 0.1562 - val_acc: 0.9541\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.15620 to 0.15555, saving model to best.model\n",
      "0s - loss: 0.1873 - acc: 0.9400 - val_loss: 0.1556 - val_acc: 0.9541\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.15555 to 0.15461, saving model to best.model\n",
      "0s - loss: 0.2017 - acc: 0.9376 - val_loss: 0.1546 - val_acc: 0.9541\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.15461 to 0.15299, saving model to best.model\n",
      "0s - loss: 0.1800 - acc: 0.9353 - val_loss: 0.1530 - val_acc: 0.9541\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.15299 to 0.15160, saving model to best.model\n",
      "0s - loss: 0.1571 - acc: 0.9423 - val_loss: 0.1516 - val_acc: 0.9541\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.15160 to 0.15041, saving model to best.model\n",
      "0s - loss: 0.1815 - acc: 0.9400 - val_loss: 0.1504 - val_acc: 0.9541\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.15041 to 0.14903, saving model to best.model\n",
      "0s - loss: 0.1786 - acc: 0.9400 - val_loss: 0.1490 - val_acc: 0.9541\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.14903 to 0.14819, saving model to best.model\n",
      "0s - loss: 0.1811 - acc: 0.9400 - val_loss: 0.1482 - val_acc: 0.9541\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.14819 to 0.14795, saving model to best.model\n",
      "0s - loss: 0.1834 - acc: 0.9400 - val_loss: 0.1480 - val_acc: 0.9541\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.1745 - acc: 0.9400 - val_loss: 0.1483 - val_acc: 0.9541\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.14795 to 0.14791, saving model to best.model\n",
      "0s - loss: 0.1594 - acc: 0.9400 - val_loss: 0.1479 - val_acc: 0.9541\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.14791 to 0.14680, saving model to best.model\n",
      "0s - loss: 0.1526 - acc: 0.9400 - val_loss: 0.1468 - val_acc: 0.9541\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.14680 to 0.14452, saving model to best.model\n",
      "0s - loss: 0.1751 - acc: 0.9446 - val_loss: 0.1445 - val_acc: 0.9541\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.14452 to 0.14259, saving model to best.model\n",
      "0s - loss: 0.1656 - acc: 0.9446 - val_loss: 0.1426 - val_acc: 0.9541\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.14259 to 0.14136, saving model to best.model\n",
      "0s - loss: 0.1688 - acc: 0.9446 - val_loss: 0.1414 - val_acc: 0.9541\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.14136 to 0.14092, saving model to best.model\n",
      "0s - loss: 0.1583 - acc: 0.9446 - val_loss: 0.1409 - val_acc: 0.9541\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.14092 to 0.14014, saving model to best.model\n",
      "0s - loss: 0.1598 - acc: 0.9492 - val_loss: 0.1401 - val_acc: 0.9541\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.14014 to 0.13860, saving model to best.model\n",
      "0s - loss: 0.1562 - acc: 0.9492 - val_loss: 0.1386 - val_acc: 0.9541\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.13860 to 0.13692, saving model to best.model\n",
      "0s - loss: 0.1580 - acc: 0.9423 - val_loss: 0.1369 - val_acc: 0.9541\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.13692 to 0.13634, saving model to best.model\n",
      "0s - loss: 0.1523 - acc: 0.9538 - val_loss: 0.1363 - val_acc: 0.9541\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.13634 to 0.13610, saving model to best.model\n",
      "0s - loss: 0.1400 - acc: 0.9538 - val_loss: 0.1361 - val_acc: 0.9541\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.13610 to 0.13541, saving model to best.model\n",
      "0s - loss: 0.1359 - acc: 0.9584 - val_loss: 0.1354 - val_acc: 0.9541\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.13541 to 0.13485, saving model to best.model\n",
      "0s - loss: 0.1626 - acc: 0.9423 - val_loss: 0.1348 - val_acc: 0.9541\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.13485 to 0.13392, saving model to best.model\n",
      "0s - loss: 0.1572 - acc: 0.9469 - val_loss: 0.1339 - val_acc: 0.9541\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.13392 to 0.13253, saving model to best.model\n",
      "0s - loss: 0.1546 - acc: 0.9538 - val_loss: 0.1325 - val_acc: 0.9541\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.13253 to 0.13193, saving model to best.model\n",
      "0s - loss: 0.1394 - acc: 0.9515 - val_loss: 0.1319 - val_acc: 0.9541\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.13193 to 0.13122, saving model to best.model\n",
      "0s - loss: 0.1544 - acc: 0.9469 - val_loss: 0.1312 - val_acc: 0.9541\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.13122 to 0.13051, saving model to best.model\n",
      "0s - loss: 0.1520 - acc: 0.9515 - val_loss: 0.1305 - val_acc: 0.9541\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.13051 to 0.12802, saving model to best.model\n",
      "0s - loss: 0.1575 - acc: 0.9492 - val_loss: 0.1280 - val_acc: 0.9541\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.12802 to 0.12607, saving model to best.model\n",
      "0s - loss: 0.1503 - acc: 0.9423 - val_loss: 0.1261 - val_acc: 0.9541\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.12607 to 0.12477, saving model to best.model\n",
      "0s - loss: 0.1448 - acc: 0.9515 - val_loss: 0.1248 - val_acc: 0.9541\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.12477 to 0.12359, saving model to best.model\n",
      "0s - loss: 0.1496 - acc: 0.9538 - val_loss: 0.1236 - val_acc: 0.9541\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.12359 to 0.12225, saving model to best.model\n",
      "0s - loss: 0.1486 - acc: 0.9561 - val_loss: 0.1222 - val_acc: 0.9541\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.1322 - acc: 0.9538 - val_loss: 0.1226 - val_acc: 0.9541\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.1439 - acc: 0.9515 - val_loss: 0.1235 - val_acc: 0.9541\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.1191 - acc: 0.9677 - val_loss: 0.1241 - val_acc: 0.9541\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.1392 - acc: 0.9584 - val_loss: 0.1234 - val_acc: 0.9541\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.12225 to 0.12083, saving model to best.model\n",
      "0s - loss: 0.1314 - acc: 0.9538 - val_loss: 0.1208 - val_acc: 0.9541\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.12083 to 0.11742, saving model to best.model\n",
      "0s - loss: 0.1363 - acc: 0.9469 - val_loss: 0.1174 - val_acc: 0.9541\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.11742 to 0.11613, saving model to best.model\n",
      "0s - loss: 0.1212 - acc: 0.9630 - val_loss: 0.1161 - val_acc: 0.9633\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.11613 to 0.11596, saving model to best.model\n",
      "0s - loss: 0.1304 - acc: 0.9561 - val_loss: 0.1160 - val_acc: 0.9633\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.1256 - acc: 0.9584 - val_loss: 0.1169 - val_acc: 0.9633\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.1512 - acc: 0.9515 - val_loss: 0.1179 - val_acc: 0.9541\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.1251 - acc: 0.9515 - val_loss: 0.1199 - val_acc: 0.9541\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.1206 - acc: 0.9607 - val_loss: 0.1222 - val_acc: 0.9541\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.1277 - acc: 0.9584 - val_loss: 0.1213 - val_acc: 0.9541\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.1183 - acc: 0.9561 - val_loss: 0.1180 - val_acc: 0.9633\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.11596 to 0.11425, saving model to best.model\n",
      "0s - loss: 0.1210 - acc: 0.9538 - val_loss: 0.1142 - val_acc: 0.9541\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.43121, saving model to best.model\n",
      "0s - loss: 0.8384 - acc: 0.4919 - val_loss: 0.4312 - val_acc: 0.9450\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.43121 to 0.27143, saving model to best.model\n",
      "0s - loss: 0.5417 - acc: 0.7413 - val_loss: 0.2714 - val_acc: 0.9450\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.27143 to 0.21926, saving model to best.model\n",
      "0s - loss: 0.3497 - acc: 0.8753 - val_loss: 0.2193 - val_acc: 0.9450\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.21926 to 0.21359, saving model to best.model\n",
      "0s - loss: 0.2698 - acc: 0.9261 - val_loss: 0.2136 - val_acc: 0.9450\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2504 - acc: 0.9400 - val_loss: 0.2238 - val_acc: 0.9450\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2682 - acc: 0.9400 - val_loss: 0.2371 - val_acc: 0.9450\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2537 - acc: 0.9376 - val_loss: 0.2488 - val_acc: 0.9450\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2617 - acc: 0.9400 - val_loss: 0.2576 - val_acc: 0.9450\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2768 - acc: 0.9400 - val_loss: 0.2629 - val_acc: 0.9450\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2941 - acc: 0.9400 - val_loss: 0.2654 - val_acc: 0.9450\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2614 - acc: 0.9400 - val_loss: 0.2654 - val_acc: 0.9450\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3086 - acc: 0.9400 - val_loss: 0.2630 - val_acc: 0.9450\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3081 - acc: 0.9400 - val_loss: 0.2587 - val_acc: 0.9450\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2752 - acc: 0.9400 - val_loss: 0.2535 - val_acc: 0.9450\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2911 - acc: 0.9400 - val_loss: 0.2473 - val_acc: 0.9450\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2960 - acc: 0.9400 - val_loss: 0.2410 - val_acc: 0.9450\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2855 - acc: 0.9400 - val_loss: 0.2352 - val_acc: 0.9450\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2399 - acc: 0.9400 - val_loss: 0.2295 - val_acc: 0.9450\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2605 - acc: 0.9376 - val_loss: 0.2246 - val_acc: 0.9450\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2450 - acc: 0.9400 - val_loss: 0.2205 - val_acc: 0.9450\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2577 - acc: 0.9400 - val_loss: 0.2175 - val_acc: 0.9450\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2522 - acc: 0.9400 - val_loss: 0.2153 - val_acc: 0.9450\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2480 - acc: 0.9400 - val_loss: 0.2139 - val_acc: 0.9450\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.21359 to 0.21312, saving model to best.model\n",
      "0s - loss: 0.2388 - acc: 0.9376 - val_loss: 0.2131 - val_acc: 0.9450\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.21312 to 0.21267, saving model to best.model\n",
      "0s - loss: 0.2509 - acc: 0.9400 - val_loss: 0.2127 - val_acc: 0.9450\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.21267 to 0.21250, saving model to best.model\n",
      "0s - loss: 0.2360 - acc: 0.9400 - val_loss: 0.2125 - val_acc: 0.9450\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.21250 to 0.21247, saving model to best.model\n",
      "0s - loss: 0.2504 - acc: 0.9376 - val_loss: 0.2125 - val_acc: 0.9450\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2440 - acc: 0.9400 - val_loss: 0.2126 - val_acc: 0.9450\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2629 - acc: 0.9376 - val_loss: 0.2129 - val_acc: 0.9450\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2652 - acc: 0.9400 - val_loss: 0.2132 - val_acc: 0.9450\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2438 - acc: 0.9353 - val_loss: 0.2137 - val_acc: 0.9450\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2668 - acc: 0.9376 - val_loss: 0.2141 - val_acc: 0.9450\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2595 - acc: 0.9376 - val_loss: 0.2146 - val_acc: 0.9450\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2654 - acc: 0.9376 - val_loss: 0.2150 - val_acc: 0.9450\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2361 - acc: 0.9376 - val_loss: 0.2154 - val_acc: 0.9450\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2550 - acc: 0.9353 - val_loss: 0.2156 - val_acc: 0.9450\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2645 - acc: 0.9400 - val_loss: 0.2156 - val_acc: 0.9450\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2516 - acc: 0.9400 - val_loss: 0.2155 - val_acc: 0.9450\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2403 - acc: 0.9400 - val_loss: 0.2154 - val_acc: 0.9450\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2561 - acc: 0.9376 - val_loss: 0.2152 - val_acc: 0.9450\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2417 - acc: 0.9400 - val_loss: 0.2150 - val_acc: 0.9450\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2483 - acc: 0.9400 - val_loss: 0.2148 - val_acc: 0.9450\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2373 - acc: 0.9400 - val_loss: 0.2146 - val_acc: 0.9450\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2498 - acc: 0.9400 - val_loss: 0.2144 - val_acc: 0.9450\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2514 - acc: 0.9400 - val_loss: 0.2140 - val_acc: 0.9450\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2724 - acc: 0.9400 - val_loss: 0.2136 - val_acc: 0.9450\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2412 - acc: 0.9400 - val_loss: 0.2130 - val_acc: 0.9450\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2537 - acc: 0.9400 - val_loss: 0.2125 - val_acc: 0.9450\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.21247 to 0.21207, saving model to best.model\n",
      "0s - loss: 0.2412 - acc: 0.9400 - val_loss: 0.2121 - val_acc: 0.9450\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.21207 to 0.21166, saving model to best.model\n",
      "0s - loss: 0.2520 - acc: 0.9376 - val_loss: 0.2117 - val_acc: 0.9450\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.21166 to 0.21137, saving model to best.model\n",
      "0s - loss: 0.2427 - acc: 0.9400 - val_loss: 0.2114 - val_acc: 0.9450\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.21137 to 0.21112, saving model to best.model\n",
      "0s - loss: 0.2388 - acc: 0.9400 - val_loss: 0.2111 - val_acc: 0.9450\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.21112 to 0.21084, saving model to best.model\n",
      "0s - loss: 0.2445 - acc: 0.9400 - val_loss: 0.2108 - val_acc: 0.9450\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.21084 to 0.21048, saving model to best.model\n",
      "0s - loss: 0.2424 - acc: 0.9400 - val_loss: 0.2105 - val_acc: 0.9450\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.21048 to 0.21015, saving model to best.model\n",
      "0s - loss: 0.2325 - acc: 0.9400 - val_loss: 0.2102 - val_acc: 0.9450\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.21015 to 0.20985, saving model to best.model\n",
      "0s - loss: 0.2398 - acc: 0.9400 - val_loss: 0.2098 - val_acc: 0.9450\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.20985 to 0.20967, saving model to best.model\n",
      "0s - loss: 0.2356 - acc: 0.9400 - val_loss: 0.2097 - val_acc: 0.9450\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.20967 to 0.20964, saving model to best.model\n",
      "0s - loss: 0.2385 - acc: 0.9400 - val_loss: 0.2096 - val_acc: 0.9450\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.20964 to 0.20962, saving model to best.model\n",
      "0s - loss: 0.2324 - acc: 0.9400 - val_loss: 0.2096 - val_acc: 0.9450\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.20962 to 0.20939, saving model to best.model\n",
      "0s - loss: 0.2542 - acc: 0.9400 - val_loss: 0.2094 - val_acc: 0.9450\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.20939 to 0.20914, saving model to best.model\n",
      "0s - loss: 0.2260 - acc: 0.9400 - val_loss: 0.2091 - val_acc: 0.9450\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.20914 to 0.20898, saving model to best.model\n",
      "0s - loss: 0.2355 - acc: 0.9400 - val_loss: 0.2090 - val_acc: 0.9450\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.20898 to 0.20868, saving model to best.model\n",
      "0s - loss: 0.2410 - acc: 0.9376 - val_loss: 0.2087 - val_acc: 0.9450\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.20868 to 0.20829, saving model to best.model\n",
      "0s - loss: 0.2503 - acc: 0.9400 - val_loss: 0.2083 - val_acc: 0.9450\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.20829 to 0.20815, saving model to best.model\n",
      "0s - loss: 0.2283 - acc: 0.9376 - val_loss: 0.2081 - val_acc: 0.9450\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.20815 to 0.20780, saving model to best.model\n",
      "0s - loss: 0.2417 - acc: 0.9400 - val_loss: 0.2078 - val_acc: 0.9450\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.20780 to 0.20731, saving model to best.model\n",
      "0s - loss: 0.2331 - acc: 0.9400 - val_loss: 0.2073 - val_acc: 0.9450\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.20731 to 0.20713, saving model to best.model\n",
      "0s - loss: 0.2342 - acc: 0.9400 - val_loss: 0.2071 - val_acc: 0.9450\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.20713 to 0.20674, saving model to best.model\n",
      "0s - loss: 0.2342 - acc: 0.9400 - val_loss: 0.2067 - val_acc: 0.9450\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.20674 to 0.20621, saving model to best.model\n",
      "0s - loss: 0.2313 - acc: 0.9400 - val_loss: 0.2062 - val_acc: 0.9450\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.20621 to 0.20581, saving model to best.model\n",
      "0s - loss: 0.2318 - acc: 0.9400 - val_loss: 0.2058 - val_acc: 0.9450\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.20581 to 0.20555, saving model to best.model\n",
      "0s - loss: 0.2404 - acc: 0.9400 - val_loss: 0.2055 - val_acc: 0.9450\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.20555 to 0.20526, saving model to best.model\n",
      "0s - loss: 0.2148 - acc: 0.9400 - val_loss: 0.2053 - val_acc: 0.9450\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.20526 to 0.20486, saving model to best.model\n",
      "0s - loss: 0.2442 - acc: 0.9400 - val_loss: 0.2049 - val_acc: 0.9450\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.20486 to 0.20447, saving model to best.model\n",
      "0s - loss: 0.2289 - acc: 0.9400 - val_loss: 0.2045 - val_acc: 0.9450\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.20447 to 0.20388, saving model to best.model\n",
      "0s - loss: 0.2302 - acc: 0.9400 - val_loss: 0.2039 - val_acc: 0.9450\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.20388 to 0.20337, saving model to best.model\n",
      "0s - loss: 0.2499 - acc: 0.9400 - val_loss: 0.2034 - val_acc: 0.9450\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.20337 to 0.20286, saving model to best.model\n",
      "0s - loss: 0.2261 - acc: 0.9400 - val_loss: 0.2029 - val_acc: 0.9450\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.20286 to 0.20239, saving model to best.model\n",
      "0s - loss: 0.2386 - acc: 0.9400 - val_loss: 0.2024 - val_acc: 0.9450\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.20239 to 0.20178, saving model to best.model\n",
      "0s - loss: 0.2299 - acc: 0.9400 - val_loss: 0.2018 - val_acc: 0.9450\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.20178 to 0.20143, saving model to best.model\n",
      "0s - loss: 0.2153 - acc: 0.9400 - val_loss: 0.2014 - val_acc: 0.9450\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.20143 to 0.20111, saving model to best.model\n",
      "0s - loss: 0.2347 - acc: 0.9400 - val_loss: 0.2011 - val_acc: 0.9450\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.20111 to 0.20061, saving model to best.model\n",
      "0s - loss: 0.2386 - acc: 0.9400 - val_loss: 0.2006 - val_acc: 0.9450\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.20061 to 0.20012, saving model to best.model\n",
      "0s - loss: 0.2192 - acc: 0.9400 - val_loss: 0.2001 - val_acc: 0.9450\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.20012 to 0.19995, saving model to best.model\n",
      "0s - loss: 0.2179 - acc: 0.9400 - val_loss: 0.2000 - val_acc: 0.9450\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.19995 to 0.19977, saving model to best.model\n",
      "0s - loss: 0.2257 - acc: 0.9400 - val_loss: 0.1998 - val_acc: 0.9450\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.19977 to 0.19917, saving model to best.model\n",
      "0s - loss: 0.2185 - acc: 0.9400 - val_loss: 0.1992 - val_acc: 0.9450\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.19917 to 0.19857, saving model to best.model\n",
      "0s - loss: 0.2073 - acc: 0.9400 - val_loss: 0.1986 - val_acc: 0.9450\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.19857 to 0.19815, saving model to best.model\n",
      "0s - loss: 0.2061 - acc: 0.9400 - val_loss: 0.1981 - val_acc: 0.9450\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.19815 to 0.19748, saving model to best.model\n",
      "0s - loss: 0.2157 - acc: 0.9400 - val_loss: 0.1975 - val_acc: 0.9450\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.19748 to 0.19696, saving model to best.model\n",
      "0s - loss: 0.2209 - acc: 0.9400 - val_loss: 0.1970 - val_acc: 0.9450\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.19696 to 0.19643, saving model to best.model\n",
      "0s - loss: 0.2267 - acc: 0.9400 - val_loss: 0.1964 - val_acc: 0.9450\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.19643 to 0.19573, saving model to best.model\n",
      "0s - loss: 0.2404 - acc: 0.9400 - val_loss: 0.1957 - val_acc: 0.9450\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.19573 to 0.19494, saving model to best.model\n",
      "0s - loss: 0.2130 - acc: 0.9400 - val_loss: 0.1949 - val_acc: 0.9450\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.19494 to 0.19384, saving model to best.model\n",
      "0s - loss: 0.2251 - acc: 0.9400 - val_loss: 0.1938 - val_acc: 0.9450\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.19384 to 0.19294, saving model to best.model\n",
      "0s - loss: 0.2121 - acc: 0.9400 - val_loss: 0.1929 - val_acc: 0.9450\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.19294 to 0.19215, saving model to best.model\n",
      "0s - loss: 0.2258 - acc: 0.9400 - val_loss: 0.1922 - val_acc: 0.9450\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.19215 to 0.19155, saving model to best.model\n",
      "0s - loss: 0.2166 - acc: 0.9400 - val_loss: 0.1916 - val_acc: 0.9450\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.19155 to 0.19057, saving model to best.model\n",
      "0s - loss: 0.2237 - acc: 0.9400 - val_loss: 0.1906 - val_acc: 0.9450\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.19057 to 0.18963, saving model to best.model\n",
      "0s - loss: 0.2201 - acc: 0.9400 - val_loss: 0.1896 - val_acc: 0.9450\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.18963 to 0.18895, saving model to best.model\n",
      "0s - loss: 0.2135 - acc: 0.9400 - val_loss: 0.1889 - val_acc: 0.9450\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.18895 to 0.18817, saving model to best.model\n",
      "0s - loss: 0.2061 - acc: 0.9400 - val_loss: 0.1882 - val_acc: 0.9450\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.18817 to 0.18744, saving model to best.model\n",
      "0s - loss: 0.2138 - acc: 0.9400 - val_loss: 0.1874 - val_acc: 0.9450\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.18744 to 0.18698, saving model to best.model\n",
      "0s - loss: 0.2025 - acc: 0.9400 - val_loss: 0.1870 - val_acc: 0.9450\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.18698 to 0.18622, saving model to best.model\n",
      "0s - loss: 0.2193 - acc: 0.9400 - val_loss: 0.1862 - val_acc: 0.9450\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.18622 to 0.18509, saving model to best.model\n",
      "0s - loss: 0.2009 - acc: 0.9400 - val_loss: 0.1851 - val_acc: 0.9450\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.18509 to 0.18361, saving model to best.model\n",
      "0s - loss: 0.2069 - acc: 0.9400 - val_loss: 0.1836 - val_acc: 0.9450\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.18361 to 0.18229, saving model to best.model\n",
      "0s - loss: 0.1999 - acc: 0.9400 - val_loss: 0.1823 - val_acc: 0.9450\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.18229 to 0.18143, saving model to best.model\n",
      "0s - loss: 0.1960 - acc: 0.9400 - val_loss: 0.1814 - val_acc: 0.9450\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.18143 to 0.18058, saving model to best.model\n",
      "0s - loss: 0.2100 - acc: 0.9376 - val_loss: 0.1806 - val_acc: 0.9450\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.18058 to 0.17942, saving model to best.model\n",
      "0s - loss: 0.1989 - acc: 0.9400 - val_loss: 0.1794 - val_acc: 0.9450\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.17942 to 0.17825, saving model to best.model\n",
      "0s - loss: 0.1961 - acc: 0.9400 - val_loss: 0.1783 - val_acc: 0.9450\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.17825 to 0.17722, saving model to best.model\n",
      "0s - loss: 0.2023 - acc: 0.9400 - val_loss: 0.1772 - val_acc: 0.9450\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.17722 to 0.17637, saving model to best.model\n",
      "0s - loss: 0.1885 - acc: 0.9400 - val_loss: 0.1764 - val_acc: 0.9450\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.17637 to 0.17557, saving model to best.model\n",
      "0s - loss: 0.1975 - acc: 0.9400 - val_loss: 0.1756 - val_acc: 0.9450\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.17557 to 0.17490, saving model to best.model\n",
      "0s - loss: 0.1975 - acc: 0.9400 - val_loss: 0.1749 - val_acc: 0.9450\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.17490 to 0.17428, saving model to best.model\n",
      "0s - loss: 0.1886 - acc: 0.9400 - val_loss: 0.1743 - val_acc: 0.9450\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.17428 to 0.17318, saving model to best.model\n",
      "0s - loss: 0.1771 - acc: 0.9400 - val_loss: 0.1732 - val_acc: 0.9450\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.17318 to 0.17196, saving model to best.model\n",
      "0s - loss: 0.1862 - acc: 0.9400 - val_loss: 0.1720 - val_acc: 0.9450\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.17196 to 0.17032, saving model to best.model\n",
      "0s - loss: 0.1920 - acc: 0.9400 - val_loss: 0.1703 - val_acc: 0.9450\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.17032 to 0.16876, saving model to best.model\n",
      "0s - loss: 0.1849 - acc: 0.9400 - val_loss: 0.1688 - val_acc: 0.9450\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.16876 to 0.16727, saving model to best.model\n",
      "0s - loss: 0.1981 - acc: 0.9376 - val_loss: 0.1673 - val_acc: 0.9450\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.16727 to 0.16583, saving model to best.model\n",
      "0s - loss: 0.1803 - acc: 0.9423 - val_loss: 0.1658 - val_acc: 0.9450\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.16583 to 0.16427, saving model to best.model\n",
      "0s - loss: 0.1973 - acc: 0.9423 - val_loss: 0.1643 - val_acc: 0.9450\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.16427 to 0.16282, saving model to best.model\n",
      "0s - loss: 0.1907 - acc: 0.9400 - val_loss: 0.1628 - val_acc: 0.9450\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.16282 to 0.16182, saving model to best.model\n",
      "0s - loss: 0.1883 - acc: 0.9469 - val_loss: 0.1618 - val_acc: 0.9450\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.16182 to 0.16050, saving model to best.model\n",
      "0s - loss: 0.1822 - acc: 0.9423 - val_loss: 0.1605 - val_acc: 0.9450\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.16050 to 0.15981, saving model to best.model\n",
      "0s - loss: 0.1662 - acc: 0.9469 - val_loss: 0.1598 - val_acc: 0.9450\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.15981 to 0.15937, saving model to best.model\n",
      "0s - loss: 0.1826 - acc: 0.9330 - val_loss: 0.1594 - val_acc: 0.9450\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.15937 to 0.15876, saving model to best.model\n",
      "0s - loss: 0.1711 - acc: 0.9400 - val_loss: 0.1588 - val_acc: 0.9450\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.15876 to 0.15755, saving model to best.model\n",
      "0s - loss: 0.1599 - acc: 0.9376 - val_loss: 0.1575 - val_acc: 0.9450\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.15755 to 0.15484, saving model to best.model\n",
      "0s - loss: 0.1687 - acc: 0.9400 - val_loss: 0.1548 - val_acc: 0.9450\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.15484 to 0.15180, saving model to best.model\n",
      "0s - loss: 0.1693 - acc: 0.9400 - val_loss: 0.1518 - val_acc: 0.9450\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.15180 to 0.14984, saving model to best.model\n",
      "0s - loss: 0.1436 - acc: 0.9492 - val_loss: 0.1498 - val_acc: 0.9450\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.14984 to 0.14812, saving model to best.model\n",
      "0s - loss: 0.1682 - acc: 0.9469 - val_loss: 0.1481 - val_acc: 0.9450\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.14812 to 0.14722, saving model to best.model\n",
      "0s - loss: 0.1792 - acc: 0.9353 - val_loss: 0.1472 - val_acc: 0.9450\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.14722 to 0.14655, saving model to best.model\n",
      "0s - loss: 0.1702 - acc: 0.9423 - val_loss: 0.1465 - val_acc: 0.9450\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.14655 to 0.14534, saving model to best.model\n",
      "0s - loss: 0.1624 - acc: 0.9423 - val_loss: 0.1453 - val_acc: 0.9450\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.14534 to 0.14413, saving model to best.model\n",
      "0s - loss: 0.1520 - acc: 0.9423 - val_loss: 0.1441 - val_acc: 0.9450\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.14413 to 0.14272, saving model to best.model\n",
      "0s - loss: 0.1533 - acc: 0.9446 - val_loss: 0.1427 - val_acc: 0.9450\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.14272 to 0.14017, saving model to best.model\n",
      "0s - loss: 0.1569 - acc: 0.9492 - val_loss: 0.1402 - val_acc: 0.9450\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.14017 to 0.13735, saving model to best.model\n",
      "0s - loss: 0.1564 - acc: 0.9515 - val_loss: 0.1373 - val_acc: 0.9541\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.13735 to 0.13491, saving model to best.model\n",
      "0s - loss: 0.1521 - acc: 0.9515 - val_loss: 0.1349 - val_acc: 0.9541\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.13491 to 0.13296, saving model to best.model\n",
      "0s - loss: 0.1466 - acc: 0.9515 - val_loss: 0.1330 - val_acc: 0.9541\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.13296 to 0.13218, saving model to best.model\n",
      "0s - loss: 0.1585 - acc: 0.9400 - val_loss: 0.1322 - val_acc: 0.9541\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.13218 to 0.13117, saving model to best.model\n",
      "0s - loss: 0.1461 - acc: 0.9376 - val_loss: 0.1312 - val_acc: 0.9541\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.13117 to 0.12909, saving model to best.model\n",
      "0s - loss: 0.1431 - acc: 0.9561 - val_loss: 0.1291 - val_acc: 0.9541\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.12909 to 0.12807, saving model to best.model\n",
      "0s - loss: 0.1578 - acc: 0.9400 - val_loss: 0.1281 - val_acc: 0.9541\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.12807 to 0.12684, saving model to best.model\n",
      "0s - loss: 0.1210 - acc: 0.9515 - val_loss: 0.1268 - val_acc: 0.9541\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.12684 to 0.12555, saving model to best.model\n",
      "0s - loss: 0.1439 - acc: 0.9515 - val_loss: 0.1255 - val_acc: 0.9541\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.12555 to 0.12426, saving model to best.model\n",
      "0s - loss: 0.1336 - acc: 0.9515 - val_loss: 0.1243 - val_acc: 0.9541\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.12426 to 0.12271, saving model to best.model\n",
      "0s - loss: 0.1404 - acc: 0.9423 - val_loss: 0.1227 - val_acc: 0.9633\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.12271 to 0.12145, saving model to best.model\n",
      "0s - loss: 0.1419 - acc: 0.9469 - val_loss: 0.1215 - val_acc: 0.9633\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.12145 to 0.12048, saving model to best.model\n",
      "0s - loss: 0.1256 - acc: 0.9607 - val_loss: 0.1205 - val_acc: 0.9633\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.12048 to 0.11935, saving model to best.model\n",
      "0s - loss: 0.1389 - acc: 0.9538 - val_loss: 0.1193 - val_acc: 0.9725\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.11935 to 0.11785, saving model to best.model\n",
      "0s - loss: 0.1513 - acc: 0.9469 - val_loss: 0.1179 - val_acc: 0.9725\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.11785 to 0.11658, saving model to best.model\n",
      "0s - loss: 0.1416 - acc: 0.9492 - val_loss: 0.1166 - val_acc: 0.9725\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.11658 to 0.11566, saving model to best.model\n",
      "0s - loss: 0.1531 - acc: 0.9423 - val_loss: 0.1157 - val_acc: 0.9725\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.11566 to 0.11427, saving model to best.model\n",
      "0s - loss: 0.1322 - acc: 0.9492 - val_loss: 0.1143 - val_acc: 0.9725\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.11427 to 0.11280, saving model to best.model\n",
      "0s - loss: 0.1231 - acc: 0.9515 - val_loss: 0.1128 - val_acc: 0.9725\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.11280 to 0.11152, saving model to best.model\n",
      "0s - loss: 0.1248 - acc: 0.9630 - val_loss: 0.1115 - val_acc: 0.9725\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.11152 to 0.11050, saving model to best.model\n",
      "0s - loss: 0.1235 - acc: 0.9630 - val_loss: 0.1105 - val_acc: 0.9725\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.1323 - acc: 0.9538 - val_loss: 0.1106 - val_acc: 0.9725\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.1271 - acc: 0.9607 - val_loss: 0.1111 - val_acc: 0.9725\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.1259 - acc: 0.9584 - val_loss: 0.1118 - val_acc: 0.9725\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.1319 - acc: 0.9538 - val_loss: 0.1116 - val_acc: 0.9725\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.11050 to 0.11018, saving model to best.model\n",
      "0s - loss: 0.1358 - acc: 0.9469 - val_loss: 0.1102 - val_acc: 0.9725\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.11018 to 0.10797, saving model to best.model\n",
      "0s - loss: 0.1272 - acc: 0.9654 - val_loss: 0.1080 - val_acc: 0.9725\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.10797 to 0.10498, saving model to best.model\n",
      "0s - loss: 0.1386 - acc: 0.9538 - val_loss: 0.1050 - val_acc: 0.9725\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.10498 to 0.10301, saving model to best.model\n",
      "0s - loss: 0.1242 - acc: 0.9561 - val_loss: 0.1030 - val_acc: 0.9725\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.10301 to 0.10147, saving model to best.model\n",
      "0s - loss: 0.1092 - acc: 0.9538 - val_loss: 0.1015 - val_acc: 0.9725\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.10147 to 0.10098, saving model to best.model\n",
      "0s - loss: 0.1245 - acc: 0.9515 - val_loss: 0.1010 - val_acc: 0.9725\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.1135 - acc: 0.9630 - val_loss: 0.1015 - val_acc: 0.9725\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.1201 - acc: 0.9584 - val_loss: 0.1020 - val_acc: 0.9725\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss did not improve\n",
      "0s - loss: 0.1267 - acc: 0.9561 - val_loss: 0.1027 - val_acc: 0.9725\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss did not improve\n",
      "0s - loss: 0.1245 - acc: 0.9561 - val_loss: 0.1017 - val_acc: 0.9725\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.10098 to 0.09882, saving model to best.model\n",
      "0s - loss: 0.1139 - acc: 0.9584 - val_loss: 0.0988 - val_acc: 0.9725\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.09882 to 0.09754, saving model to best.model\n",
      "0s - loss: 0.1188 - acc: 0.9630 - val_loss: 0.0975 - val_acc: 0.9817\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.09754 to 0.09710, saving model to best.model\n",
      "0s - loss: 0.1105 - acc: 0.9654 - val_loss: 0.0971 - val_acc: 0.9817\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.1160 - acc: 0.9654 - val_loss: 0.0972 - val_acc: 0.9817\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.1106 - acc: 0.9584 - val_loss: 0.0980 - val_acc: 0.9725\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss did not improve\n",
      "0s - loss: 0.1105 - acc: 0.9607 - val_loss: 0.0994 - val_acc: 0.9725\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss did not improve\n",
      "0s - loss: 0.0995 - acc: 0.9561 - val_loss: 0.0997 - val_acc: 0.9725\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss did not improve\n",
      "0s - loss: 0.0876 - acc: 0.9723 - val_loss: 0.0985 - val_acc: 0.9725\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.09710 to 0.09636, saving model to best.model\n",
      "0s - loss: 0.1050 - acc: 0.9630 - val_loss: 0.0964 - val_acc: 0.9817\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.09636 to 0.09468, saving model to best.model\n",
      "0s - loss: 0.1195 - acc: 0.9654 - val_loss: 0.0947 - val_acc: 0.9817\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.09468 to 0.09362, saving model to best.model\n",
      "0s - loss: 0.1094 - acc: 0.9746 - val_loss: 0.0936 - val_acc: 0.9817\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.09362 to 0.09323, saving model to best.model\n",
      "0s - loss: 0.1060 - acc: 0.9677 - val_loss: 0.0932 - val_acc: 0.9817\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.09323 to 0.09286, saving model to best.model\n",
      "0s - loss: 0.1189 - acc: 0.9561 - val_loss: 0.0929 - val_acc: 0.9817\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.0978 - acc: 0.9700 - val_loss: 0.0934 - val_acc: 0.9817\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.1232 - acc: 0.9630 - val_loss: 0.0952 - val_acc: 0.9817\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.1160 - acc: 0.9584 - val_loss: 0.0967 - val_acc: 0.9725\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.1132 - acc: 0.9538 - val_loss: 0.0955 - val_acc: 0.9817\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.1028 - acc: 0.9607 - val_loss: 0.0935 - val_acc: 0.9817\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.09286 to 0.09226, saving model to best.model\n",
      "0s - loss: 0.1111 - acc: 0.9607 - val_loss: 0.0923 - val_acc: 0.9817\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.09226 to 0.09190, saving model to best.model\n",
      "0s - loss: 0.1054 - acc: 0.9607 - val_loss: 0.0919 - val_acc: 0.9817\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.09190 to 0.09179, saving model to best.model\n",
      "0s - loss: 0.1057 - acc: 0.9607 - val_loss: 0.0918 - val_acc: 0.9817\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.1016 - acc: 0.9654 - val_loss: 0.0918 - val_acc: 0.9817\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.0999 - acc: 0.9677 - val_loss: 0.0919 - val_acc: 0.9817\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.1062 - acc: 0.9630 - val_loss: 0.0919 - val_acc: 0.9817\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.65523, saving model to best.model\n",
      "0s - loss: 1.1582 - acc: 0.3580 - val_loss: 0.6552 - val_acc: 0.9450\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.65523 to 0.35943, saving model to best.model\n",
      "0s - loss: 0.7156 - acc: 0.5935 - val_loss: 0.3594 - val_acc: 0.9450\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.35943 to 0.24248, saving model to best.model\n",
      "0s - loss: 0.4873 - acc: 0.7829 - val_loss: 0.2425 - val_acc: 0.9450\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.24248 to 0.21363, saving model to best.model\n",
      "0s - loss: 0.3495 - acc: 0.8938 - val_loss: 0.2136 - val_acc: 0.9450\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3049 - acc: 0.9169 - val_loss: 0.2179 - val_acc: 0.9450\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3069 - acc: 0.9215 - val_loss: 0.2315 - val_acc: 0.9450\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3227 - acc: 0.9261 - val_loss: 0.2453 - val_acc: 0.9450\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3347 - acc: 0.9261 - val_loss: 0.2562 - val_acc: 0.9450\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3283 - acc: 0.9261 - val_loss: 0.2633 - val_acc: 0.9450\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3474 - acc: 0.9238 - val_loss: 0.2668 - val_acc: 0.9450\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3517 - acc: 0.9261 - val_loss: 0.2667 - val_acc: 0.9450\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3285 - acc: 0.9261 - val_loss: 0.2642 - val_acc: 0.9450\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3303 - acc: 0.9261 - val_loss: 0.2598 - val_acc: 0.9450\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3663 - acc: 0.9261 - val_loss: 0.2540 - val_acc: 0.9450\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3507 - acc: 0.9261 - val_loss: 0.2476 - val_acc: 0.9450\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3286 - acc: 0.9261 - val_loss: 0.2409 - val_acc: 0.9450\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3058 - acc: 0.9261 - val_loss: 0.2342 - val_acc: 0.9450\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2820 - acc: 0.9261 - val_loss: 0.2283 - val_acc: 0.9450\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2915 - acc: 0.9261 - val_loss: 0.2233 - val_acc: 0.9450\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3077 - acc: 0.9261 - val_loss: 0.2192 - val_acc: 0.9450\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2952 - acc: 0.9261 - val_loss: 0.2163 - val_acc: 0.9450\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2931 - acc: 0.9261 - val_loss: 0.2144 - val_acc: 0.9450\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.21363 to 0.21334, saving model to best.model\n",
      "0s - loss: 0.2869 - acc: 0.9238 - val_loss: 0.2133 - val_acc: 0.9450\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.21334 to 0.21276, saving model to best.model\n",
      "0s - loss: 0.2946 - acc: 0.9238 - val_loss: 0.2128 - val_acc: 0.9450\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.21276 to 0.21254, saving model to best.model\n",
      "0s - loss: 0.3082 - acc: 0.9215 - val_loss: 0.2125 - val_acc: 0.9450\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.21254 to 0.21250, saving model to best.model\n",
      "0s - loss: 0.3067 - acc: 0.9215 - val_loss: 0.2125 - val_acc: 0.9450\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.21250 to 0.21249, saving model to best.model\n",
      "0s - loss: 0.3127 - acc: 0.9215 - val_loss: 0.2125 - val_acc: 0.9450\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.21249 to 0.21246, saving model to best.model\n",
      "0s - loss: 0.2822 - acc: 0.9215 - val_loss: 0.2125 - val_acc: 0.9450\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.21246 to 0.21243, saving model to best.model\n",
      "0s - loss: 0.2968 - acc: 0.9284 - val_loss: 0.2124 - val_acc: 0.9450\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2969 - acc: 0.9238 - val_loss: 0.2124 - val_acc: 0.9450\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.3004 - acc: 0.9238 - val_loss: 0.2126 - val_acc: 0.9450\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2854 - acc: 0.9238 - val_loss: 0.2128 - val_acc: 0.9450\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.3130 - acc: 0.9261 - val_loss: 0.2130 - val_acc: 0.9450\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.3200 - acc: 0.9238 - val_loss: 0.2132 - val_acc: 0.9450\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2974 - acc: 0.9169 - val_loss: 0.2133 - val_acc: 0.9450\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2936 - acc: 0.9261 - val_loss: 0.2134 - val_acc: 0.9450\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.3066 - acc: 0.9215 - val_loss: 0.2134 - val_acc: 0.9450\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.3049 - acc: 0.9238 - val_loss: 0.2134 - val_acc: 0.9450\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2770 - acc: 0.9215 - val_loss: 0.2133 - val_acc: 0.9450\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2862 - acc: 0.9238 - val_loss: 0.2130 - val_acc: 0.9450\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.3047 - acc: 0.9238 - val_loss: 0.2129 - val_acc: 0.9450\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2855 - acc: 0.9284 - val_loss: 0.2128 - val_acc: 0.9450\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.3034 - acc: 0.9261 - val_loss: 0.2127 - val_acc: 0.9450\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2868 - acc: 0.9215 - val_loss: 0.2125 - val_acc: 0.9450\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.21243 to 0.21242, saving model to best.model\n",
      "0s - loss: 0.2968 - acc: 0.9238 - val_loss: 0.2124 - val_acc: 0.9450\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.21242 to 0.21230, saving model to best.model\n",
      "0s - loss: 0.2967 - acc: 0.9238 - val_loss: 0.2123 - val_acc: 0.9450\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.21230 to 0.21226, saving model to best.model\n",
      "0s - loss: 0.2888 - acc: 0.9215 - val_loss: 0.2123 - val_acc: 0.9450\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.21226 to 0.21223, saving model to best.model\n",
      "0s - loss: 0.2735 - acc: 0.9215 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2852 - acc: 0.9261 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.21223 to 0.21221, saving model to best.model\n",
      "0s - loss: 0.2992 - acc: 0.9238 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.21221 to 0.21219, saving model to best.model\n",
      "0s - loss: 0.2644 - acc: 0.9261 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.21219 to 0.21216, saving model to best.model\n",
      "0s - loss: 0.2990 - acc: 0.9261 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.21216 to 0.21214, saving model to best.model\n",
      "0s - loss: 0.3146 - acc: 0.9261 - val_loss: 0.2121 - val_acc: 0.9450\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.21214 to 0.21212, saving model to best.model\n",
      "0s - loss: 0.2836 - acc: 0.9261 - val_loss: 0.2121 - val_acc: 0.9450\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.21212 to 0.21208, saving model to best.model\n",
      "0s - loss: 0.2997 - acc: 0.9261 - val_loss: 0.2121 - val_acc: 0.9450\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.21208 to 0.21192, saving model to best.model\n",
      "0s - loss: 0.2809 - acc: 0.9261 - val_loss: 0.2119 - val_acc: 0.9450\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.21192 to 0.21175, saving model to best.model\n",
      "0s - loss: 0.3005 - acc: 0.9261 - val_loss: 0.2117 - val_acc: 0.9450\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.21175 to 0.21160, saving model to best.model\n",
      "0s - loss: 0.2941 - acc: 0.9261 - val_loss: 0.2116 - val_acc: 0.9450\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.21160 to 0.21148, saving model to best.model\n",
      "0s - loss: 0.2922 - acc: 0.9261 - val_loss: 0.2115 - val_acc: 0.9450\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.21148 to 0.21138, saving model to best.model\n",
      "0s - loss: 0.2824 - acc: 0.9261 - val_loss: 0.2114 - val_acc: 0.9450\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.21138 to 0.21130, saving model to best.model\n",
      "0s - loss: 0.2908 - acc: 0.9261 - val_loss: 0.2113 - val_acc: 0.9450\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.21130 to 0.21122, saving model to best.model\n",
      "0s - loss: 0.2981 - acc: 0.9192 - val_loss: 0.2112 - val_acc: 0.9450\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.21122 to 0.21114, saving model to best.model\n",
      "0s - loss: 0.2887 - acc: 0.9261 - val_loss: 0.2111 - val_acc: 0.9450\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.21114 to 0.21107, saving model to best.model\n",
      "0s - loss: 0.2812 - acc: 0.9215 - val_loss: 0.2111 - val_acc: 0.9450\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.21107 to 0.21102, saving model to best.model\n",
      "0s - loss: 0.2730 - acc: 0.9261 - val_loss: 0.2110 - val_acc: 0.9450\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.21102 to 0.21098, saving model to best.model\n",
      "0s - loss: 0.2800 - acc: 0.9284 - val_loss: 0.2110 - val_acc: 0.9450\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.21098 to 0.21096, saving model to best.model\n",
      "0s - loss: 0.2905 - acc: 0.9238 - val_loss: 0.2110 - val_acc: 0.9450\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss did not improve\n",
      "0s - loss: 0.2932 - acc: 0.9261 - val_loss: 0.2110 - val_acc: 0.9450\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.21096 to 0.21095, saving model to best.model\n",
      "0s - loss: 0.2816 - acc: 0.9238 - val_loss: 0.2109 - val_acc: 0.9450\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.21095 to 0.21087, saving model to best.model\n",
      "0s - loss: 0.2859 - acc: 0.9261 - val_loss: 0.2109 - val_acc: 0.9450\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.21087 to 0.21066, saving model to best.model\n",
      "0s - loss: 0.2940 - acc: 0.9261 - val_loss: 0.2107 - val_acc: 0.9450\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.21066 to 0.21048, saving model to best.model\n",
      "0s - loss: 0.2846 - acc: 0.9261 - val_loss: 0.2105 - val_acc: 0.9450\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.21048 to 0.21033, saving model to best.model\n",
      "0s - loss: 0.2826 - acc: 0.9261 - val_loss: 0.2103 - val_acc: 0.9450\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.21033 to 0.21019, saving model to best.model\n",
      "0s - loss: 0.2778 - acc: 0.9261 - val_loss: 0.2102 - val_acc: 0.9450\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.21019 to 0.21007, saving model to best.model\n",
      "0s - loss: 0.2662 - acc: 0.9261 - val_loss: 0.2101 - val_acc: 0.9450\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.21007 to 0.20992, saving model to best.model\n",
      "0s - loss: 0.2794 - acc: 0.9261 - val_loss: 0.2099 - val_acc: 0.9450\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.20992 to 0.20976, saving model to best.model\n",
      "0s - loss: 0.2822 - acc: 0.9261 - val_loss: 0.2098 - val_acc: 0.9450\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.20976 to 0.20959, saving model to best.model\n",
      "0s - loss: 0.2583 - acc: 0.9261 - val_loss: 0.2096 - val_acc: 0.9450\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.20959 to 0.20940, saving model to best.model\n",
      "0s - loss: 0.2797 - acc: 0.9261 - val_loss: 0.2094 - val_acc: 0.9450\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.20940 to 0.20919, saving model to best.model\n",
      "0s - loss: 0.2713 - acc: 0.9261 - val_loss: 0.2092 - val_acc: 0.9450\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.20919 to 0.20899, saving model to best.model\n",
      "0s - loss: 0.2758 - acc: 0.9261 - val_loss: 0.2090 - val_acc: 0.9450\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.20899 to 0.20881, saving model to best.model\n",
      "0s - loss: 0.2796 - acc: 0.9261 - val_loss: 0.2088 - val_acc: 0.9450\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.20881 to 0.20862, saving model to best.model\n",
      "0s - loss: 0.2967 - acc: 0.9261 - val_loss: 0.2086 - val_acc: 0.9450\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.20862 to 0.20841, saving model to best.model\n",
      "0s - loss: 0.2818 - acc: 0.9261 - val_loss: 0.2084 - val_acc: 0.9450\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.20841 to 0.20817, saving model to best.model\n",
      "0s - loss: 0.2760 - acc: 0.9261 - val_loss: 0.2082 - val_acc: 0.9450\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.20817 to 0.20795, saving model to best.model\n",
      "0s - loss: 0.2851 - acc: 0.9261 - val_loss: 0.2079 - val_acc: 0.9450\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.20795 to 0.20773, saving model to best.model\n",
      "0s - loss: 0.2879 - acc: 0.9261 - val_loss: 0.2077 - val_acc: 0.9450\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.20773 to 0.20751, saving model to best.model\n",
      "0s - loss: 0.2706 - acc: 0.9261 - val_loss: 0.2075 - val_acc: 0.9450\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.20751 to 0.20730, saving model to best.model\n",
      "0s - loss: 0.2780 - acc: 0.9261 - val_loss: 0.2073 - val_acc: 0.9450\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.20730 to 0.20710, saving model to best.model\n",
      "0s - loss: 0.2891 - acc: 0.9261 - val_loss: 0.2071 - val_acc: 0.9450\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.20710 to 0.20690, saving model to best.model\n",
      "0s - loss: 0.2703 - acc: 0.9261 - val_loss: 0.2069 - val_acc: 0.9450\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.20690 to 0.20666, saving model to best.model\n",
      "0s - loss: 0.2767 - acc: 0.9261 - val_loss: 0.2067 - val_acc: 0.9450\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.20666 to 0.20644, saving model to best.model\n",
      "0s - loss: 0.2763 - acc: 0.9261 - val_loss: 0.2064 - val_acc: 0.9450\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.20644 to 0.20623, saving model to best.model\n",
      "0s - loss: 0.2825 - acc: 0.9261 - val_loss: 0.2062 - val_acc: 0.9450\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.20623 to 0.20599, saving model to best.model\n",
      "0s - loss: 0.2594 - acc: 0.9261 - val_loss: 0.2060 - val_acc: 0.9450\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.20599 to 0.20576, saving model to best.model\n",
      "0s - loss: 0.2707 - acc: 0.9261 - val_loss: 0.2058 - val_acc: 0.9450\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.20576 to 0.20544, saving model to best.model\n",
      "0s - loss: 0.2736 - acc: 0.9261 - val_loss: 0.2054 - val_acc: 0.9450\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.20544 to 0.20508, saving model to best.model\n",
      "0s - loss: 0.2670 - acc: 0.9261 - val_loss: 0.2051 - val_acc: 0.9450\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.20508 to 0.20472, saving model to best.model\n",
      "0s - loss: 0.2639 - acc: 0.9261 - val_loss: 0.2047 - val_acc: 0.9450\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.20472 to 0.20442, saving model to best.model\n",
      "0s - loss: 0.2643 - acc: 0.9261 - val_loss: 0.2044 - val_acc: 0.9450\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.20442 to 0.20413, saving model to best.model\n",
      "0s - loss: 0.2773 - acc: 0.9261 - val_loss: 0.2041 - val_acc: 0.9450\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.20413 to 0.20382, saving model to best.model\n",
      "0s - loss: 0.2642 - acc: 0.9261 - val_loss: 0.2038 - val_acc: 0.9450\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.20382 to 0.20350, saving model to best.model\n",
      "0s - loss: 0.2706 - acc: 0.9261 - val_loss: 0.2035 - val_acc: 0.9450\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.20350 to 0.20317, saving model to best.model\n",
      "0s - loss: 0.2591 - acc: 0.9284 - val_loss: 0.2032 - val_acc: 0.9450\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.20317 to 0.20285, saving model to best.model\n",
      "0s - loss: 0.2780 - acc: 0.9261 - val_loss: 0.2028 - val_acc: 0.9450\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.20285 to 0.20256, saving model to best.model\n",
      "0s - loss: 0.2719 - acc: 0.9261 - val_loss: 0.2026 - val_acc: 0.9450\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.20256 to 0.20229, saving model to best.model\n",
      "0s - loss: 0.2664 - acc: 0.9261 - val_loss: 0.2023 - val_acc: 0.9450\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.20229 to 0.20189, saving model to best.model\n",
      "0s - loss: 0.2580 - acc: 0.9261 - val_loss: 0.2019 - val_acc: 0.9450\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.20189 to 0.20142, saving model to best.model\n",
      "0s - loss: 0.2643 - acc: 0.9261 - val_loss: 0.2014 - val_acc: 0.9450\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.20142 to 0.20094, saving model to best.model\n",
      "0s - loss: 0.2636 - acc: 0.9261 - val_loss: 0.2009 - val_acc: 0.9450\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.20094 to 0.20045, saving model to best.model\n",
      "0s - loss: 0.2530 - acc: 0.9261 - val_loss: 0.2004 - val_acc: 0.9450\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.20045 to 0.19998, saving model to best.model\n",
      "0s - loss: 0.2716 - acc: 0.9261 - val_loss: 0.2000 - val_acc: 0.9450\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.19998 to 0.19951, saving model to best.model\n",
      "0s - loss: 0.2641 - acc: 0.9261 - val_loss: 0.1995 - val_acc: 0.9450\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.19951 to 0.19905, saving model to best.model\n",
      "0s - loss: 0.2686 - acc: 0.9261 - val_loss: 0.1990 - val_acc: 0.9450\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.19905 to 0.19854, saving model to best.model\n",
      "0s - loss: 0.2765 - acc: 0.9261 - val_loss: 0.1985 - val_acc: 0.9450\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.19854 to 0.19806, saving model to best.model\n",
      "0s - loss: 0.2641 - acc: 0.9261 - val_loss: 0.1981 - val_acc: 0.9450\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.19806 to 0.19753, saving model to best.model\n",
      "0s - loss: 0.2733 - acc: 0.9261 - val_loss: 0.1975 - val_acc: 0.9450\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.19753 to 0.19697, saving model to best.model\n",
      "0s - loss: 0.2392 - acc: 0.9261 - val_loss: 0.1970 - val_acc: 0.9450\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.19697 to 0.19625, saving model to best.model\n",
      "0s - loss: 0.2544 - acc: 0.9261 - val_loss: 0.1963 - val_acc: 0.9450\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.19625 to 0.19549, saving model to best.model\n",
      "0s - loss: 0.2688 - acc: 0.9261 - val_loss: 0.1955 - val_acc: 0.9450\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.19549 to 0.19468, saving model to best.model\n",
      "0s - loss: 0.2600 - acc: 0.9261 - val_loss: 0.1947 - val_acc: 0.9450\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.19468 to 0.19384, saving model to best.model\n",
      "0s - loss: 0.2517 - acc: 0.9261 - val_loss: 0.1938 - val_acc: 0.9450\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.19384 to 0.19294, saving model to best.model\n",
      "0s - loss: 0.2504 - acc: 0.9261 - val_loss: 0.1929 - val_acc: 0.9450\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.19294 to 0.19202, saving model to best.model\n",
      "0s - loss: 0.2659 - acc: 0.9261 - val_loss: 0.1920 - val_acc: 0.9450\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.19202 to 0.19103, saving model to best.model\n",
      "0s - loss: 0.2529 - acc: 0.9261 - val_loss: 0.1910 - val_acc: 0.9450\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.19103 to 0.18996, saving model to best.model\n",
      "0s - loss: 0.2503 - acc: 0.9261 - val_loss: 0.1900 - val_acc: 0.9450\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.18996 to 0.18887, saving model to best.model\n",
      "0s - loss: 0.2478 - acc: 0.9261 - val_loss: 0.1889 - val_acc: 0.9450\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.18887 to 0.18775, saving model to best.model\n",
      "0s - loss: 0.2662 - acc: 0.9261 - val_loss: 0.1877 - val_acc: 0.9450\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.18775 to 0.18662, saving model to best.model\n",
      "0s - loss: 0.2635 - acc: 0.9261 - val_loss: 0.1866 - val_acc: 0.9450\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.18662 to 0.18549, saving model to best.model\n",
      "0s - loss: 0.2567 - acc: 0.9261 - val_loss: 0.1855 - val_acc: 0.9450\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.18549 to 0.18425, saving model to best.model\n",
      "0s - loss: 0.2384 - acc: 0.9261 - val_loss: 0.1843 - val_acc: 0.9450\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.18425 to 0.18290, saving model to best.model\n",
      "0s - loss: 0.2392 - acc: 0.9261 - val_loss: 0.1829 - val_acc: 0.9450\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.18290 to 0.18144, saving model to best.model\n",
      "0s - loss: 0.2293 - acc: 0.9284 - val_loss: 0.1814 - val_acc: 0.9450\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.18144 to 0.17989, saving model to best.model\n",
      "0s - loss: 0.2366 - acc: 0.9261 - val_loss: 0.1799 - val_acc: 0.9450\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.17989 to 0.17837, saving model to best.model\n",
      "0s - loss: 0.2508 - acc: 0.9261 - val_loss: 0.1784 - val_acc: 0.9450\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.17837 to 0.17669, saving model to best.model\n",
      "0s - loss: 0.2576 - acc: 0.9261 - val_loss: 0.1767 - val_acc: 0.9450\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.17669 to 0.17500, saving model to best.model\n",
      "0s - loss: 0.2389 - acc: 0.9284 - val_loss: 0.1750 - val_acc: 0.9450\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.17500 to 0.17325, saving model to best.model\n",
      "0s - loss: 0.2359 - acc: 0.9261 - val_loss: 0.1733 - val_acc: 0.9450\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.17325 to 0.17131, saving model to best.model\n",
      "0s - loss: 0.2349 - acc: 0.9261 - val_loss: 0.1713 - val_acc: 0.9450\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.17131 to 0.16939, saving model to best.model\n",
      "0s - loss: 0.2376 - acc: 0.9261 - val_loss: 0.1694 - val_acc: 0.9450\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.16939 to 0.16741, saving model to best.model\n",
      "0s - loss: 0.2393 - acc: 0.9261 - val_loss: 0.1674 - val_acc: 0.9450\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.16741 to 0.16541, saving model to best.model\n",
      "0s - loss: 0.2404 - acc: 0.9261 - val_loss: 0.1654 - val_acc: 0.9450\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.16541 to 0.16345, saving model to best.model\n",
      "0s - loss: 0.2217 - acc: 0.9261 - val_loss: 0.1635 - val_acc: 0.9450\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.16345 to 0.16141, saving model to best.model\n",
      "0s - loss: 0.2328 - acc: 0.9261 - val_loss: 0.1614 - val_acc: 0.9450\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.16141 to 0.15922, saving model to best.model\n",
      "0s - loss: 0.2132 - acc: 0.9284 - val_loss: 0.1592 - val_acc: 0.9450\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.15922 to 0.15689, saving model to best.model\n",
      "0s - loss: 0.2159 - acc: 0.9261 - val_loss: 0.1569 - val_acc: 0.9450\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.15689 to 0.15450, saving model to best.model\n",
      "0s - loss: 0.2115 - acc: 0.9261 - val_loss: 0.1545 - val_acc: 0.9450\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.15450 to 0.15210, saving model to best.model\n",
      "0s - loss: 0.2111 - acc: 0.9307 - val_loss: 0.1521 - val_acc: 0.9450\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.15210 to 0.14980, saving model to best.model\n",
      "0s - loss: 0.2198 - acc: 0.9238 - val_loss: 0.1498 - val_acc: 0.9450\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.14980 to 0.14747, saving model to best.model\n",
      "0s - loss: 0.2103 - acc: 0.9307 - val_loss: 0.1475 - val_acc: 0.9450\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.14747 to 0.14527, saving model to best.model\n",
      "0s - loss: 0.2227 - acc: 0.9261 - val_loss: 0.1453 - val_acc: 0.9450\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.14527 to 0.14286, saving model to best.model\n",
      "0s - loss: 0.2039 - acc: 0.9284 - val_loss: 0.1429 - val_acc: 0.9450\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.14286 to 0.14032, saving model to best.model\n",
      "0s - loss: 0.2172 - acc: 0.9307 - val_loss: 0.1403 - val_acc: 0.9450\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.14032 to 0.13795, saving model to best.model\n",
      "0s - loss: 0.1998 - acc: 0.9284 - val_loss: 0.1380 - val_acc: 0.9450\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.13795 to 0.13573, saving model to best.model\n",
      "0s - loss: 0.1940 - acc: 0.9284 - val_loss: 0.1357 - val_acc: 0.9450\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.13573 to 0.13337, saving model to best.model\n",
      "0s - loss: 0.1904 - acc: 0.9353 - val_loss: 0.1334 - val_acc: 0.9450\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.13337 to 0.13140, saving model to best.model\n",
      "0s - loss: 0.1840 - acc: 0.9353 - val_loss: 0.1314 - val_acc: 0.9450\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.13140 to 0.12957, saving model to best.model\n",
      "0s - loss: 0.1832 - acc: 0.9307 - val_loss: 0.1296 - val_acc: 0.9450\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.12957 to 0.12740, saving model to best.model\n",
      "0s - loss: 0.1901 - acc: 0.9330 - val_loss: 0.1274 - val_acc: 0.9450\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.12740 to 0.12510, saving model to best.model\n",
      "0s - loss: 0.1939 - acc: 0.9330 - val_loss: 0.1251 - val_acc: 0.9450\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.12510 to 0.12265, saving model to best.model\n",
      "0s - loss: 0.1780 - acc: 0.9376 - val_loss: 0.1226 - val_acc: 0.9450\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.12265 to 0.12051, saving model to best.model\n",
      "0s - loss: 0.1885 - acc: 0.9307 - val_loss: 0.1205 - val_acc: 0.9450\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.12051 to 0.11931, saving model to best.model\n",
      "0s - loss: 0.1810 - acc: 0.9376 - val_loss: 0.1193 - val_acc: 0.9450\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.11931 to 0.11806, saving model to best.model\n",
      "0s - loss: 0.1922 - acc: 0.9307 - val_loss: 0.1181 - val_acc: 0.9450\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.11806 to 0.11640, saving model to best.model\n",
      "0s - loss: 0.1799 - acc: 0.9376 - val_loss: 0.1164 - val_acc: 0.9358\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.11640 to 0.11397, saving model to best.model\n",
      "0s - loss: 0.1685 - acc: 0.9353 - val_loss: 0.1140 - val_acc: 0.9358\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.11397 to 0.11198, saving model to best.model\n",
      "0s - loss: 0.1634 - acc: 0.9446 - val_loss: 0.1120 - val_acc: 0.9358\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.11198 to 0.11011, saving model to best.model\n",
      "0s - loss: 0.1805 - acc: 0.9330 - val_loss: 0.1101 - val_acc: 0.9358\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.11011 to 0.10901, saving model to best.model\n",
      "0s - loss: 0.1584 - acc: 0.9446 - val_loss: 0.1090 - val_acc: 0.9358\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.10901 to 0.10732, saving model to best.model\n",
      "0s - loss: 0.1546 - acc: 0.9469 - val_loss: 0.1073 - val_acc: 0.9358\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.10732 to 0.10655, saving model to best.model\n",
      "0s - loss: 0.1525 - acc: 0.9515 - val_loss: 0.1066 - val_acc: 0.9450\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.10655 to 0.10622, saving model to best.model\n",
      "0s - loss: 0.1382 - acc: 0.9469 - val_loss: 0.1062 - val_acc: 0.9450\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.10622 to 0.10573, saving model to best.model\n",
      "0s - loss: 0.1483 - acc: 0.9515 - val_loss: 0.1057 - val_acc: 0.9450\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.10573 to 0.10398, saving model to best.model\n",
      "0s - loss: 0.1485 - acc: 0.9515 - val_loss: 0.1040 - val_acc: 0.9450\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.10398 to 0.10245, saving model to best.model\n",
      "0s - loss: 0.1538 - acc: 0.9515 - val_loss: 0.1024 - val_acc: 0.9450\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.10245 to 0.10149, saving model to best.model\n",
      "0s - loss: 0.1535 - acc: 0.9561 - val_loss: 0.1015 - val_acc: 0.9541\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss did not improve\n",
      "0s - loss: 0.1332 - acc: 0.9607 - val_loss: 0.1016 - val_acc: 0.9450\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss did not improve\n",
      "0s - loss: 0.1257 - acc: 0.9561 - val_loss: 0.1019 - val_acc: 0.9450\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss did not improve\n",
      "0s - loss: 0.1224 - acc: 0.9492 - val_loss: 0.1022 - val_acc: 0.9450\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss did not improve\n",
      "0s - loss: 0.1323 - acc: 0.9538 - val_loss: 0.1026 - val_acc: 0.9450\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss did not improve\n",
      "0s - loss: 0.1398 - acc: 0.9584 - val_loss: 0.1021 - val_acc: 0.9541\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.10149 to 0.10083, saving model to best.model\n",
      "0s - loss: 0.1618 - acc: 0.9492 - val_loss: 0.1008 - val_acc: 0.9633\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.10083 to 0.09830, saving model to best.model\n",
      "0s - loss: 0.1466 - acc: 0.9492 - val_loss: 0.0983 - val_acc: 0.9633\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.09830 to 0.09710, saving model to best.model\n",
      "0s - loss: 0.1380 - acc: 0.9469 - val_loss: 0.0971 - val_acc: 0.9633\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.1386 - acc: 0.9630 - val_loss: 0.0979 - val_acc: 0.9633\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.1259 - acc: 0.9561 - val_loss: 0.1002 - val_acc: 0.9633\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.1353 - acc: 0.9538 - val_loss: 0.1039 - val_acc: 0.9633\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss did not improve\n",
      "0s - loss: 0.1371 - acc: 0.9607 - val_loss: 0.1052 - val_acc: 0.9633\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.1198 - acc: 0.9607 - val_loss: 0.1053 - val_acc: 0.9633\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.1359 - acc: 0.9515 - val_loss: 0.1042 - val_acc: 0.9633\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.1181 - acc: 0.9492 - val_loss: 0.1005 - val_acc: 0.9633\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.1109 - acc: 0.9723 - val_loss: 0.0995 - val_acc: 0.9633\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.1065 - acc: 0.9700 - val_loss: 0.0998 - val_acc: 0.9633\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.0902 - acc: 0.9630 - val_loss: 0.1023 - val_acc: 0.9633\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.1108 - acc: 0.9654 - val_loss: 0.1054 - val_acc: 0.9633\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.1136 - acc: 0.9607 - val_loss: 0.1060 - val_acc: 0.9633\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss did not improve\n",
      "0s - loss: 0.1095 - acc: 0.9677 - val_loss: 0.1062 - val_acc: 0.9633\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss did not improve\n",
      "0s - loss: 0.1134 - acc: 0.9700 - val_loss: 0.1071 - val_acc: 0.9633\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.1018 - acc: 0.9677 - val_loss: 0.1076 - val_acc: 0.9633\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss did not improve\n",
      "0s - loss: 0.1013 - acc: 0.9630 - val_loss: 0.1074 - val_acc: 0.9633\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.89368, saving model to best.model\n",
      "0s - loss: 1.5617 - acc: 0.1871 - val_loss: 0.8937 - val_acc: 0.0642\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.89368 to 0.49603, saving model to best.model\n",
      "0s - loss: 0.9537 - acc: 0.4365 - val_loss: 0.4960 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.49603 to 0.30799, saving model to best.model\n",
      "0s - loss: 0.6091 - acc: 0.6813 - val_loss: 0.3080 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.30799 to 0.24580, saving model to best.model\n",
      "0s - loss: 0.3775 - acc: 0.8591 - val_loss: 0.2458 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.24580 to 0.23980, saving model to best.model\n",
      "0s - loss: 0.2944 - acc: 0.9169 - val_loss: 0.2398 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2931 - acc: 0.9261 - val_loss: 0.2531 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2729 - acc: 0.9353 - val_loss: 0.2706 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2702 - acc: 0.9353 - val_loss: 0.2864 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3103 - acc: 0.9353 - val_loss: 0.2991 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3008 - acc: 0.9353 - val_loss: 0.3080 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2923 - acc: 0.9330 - val_loss: 0.3134 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3163 - acc: 0.9353 - val_loss: 0.3158 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2856 - acc: 0.9353 - val_loss: 0.3156 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3077 - acc: 0.9353 - val_loss: 0.3138 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3428 - acc: 0.9353 - val_loss: 0.3097 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2756 - acc: 0.9353 - val_loss: 0.3046 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2948 - acc: 0.9353 - val_loss: 0.2986 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3129 - acc: 0.9353 - val_loss: 0.2918 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3137 - acc: 0.9353 - val_loss: 0.2845 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2935 - acc: 0.9353 - val_loss: 0.2773 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2577 - acc: 0.9353 - val_loss: 0.2704 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2722 - acc: 0.9353 - val_loss: 0.2641 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2701 - acc: 0.9353 - val_loss: 0.2588 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2807 - acc: 0.9330 - val_loss: 0.2541 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2604 - acc: 0.9353 - val_loss: 0.2503 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2532 - acc: 0.9353 - val_loss: 0.2474 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2719 - acc: 0.9353 - val_loss: 0.2451 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2875 - acc: 0.9307 - val_loss: 0.2434 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2710 - acc: 0.9330 - val_loss: 0.2422 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2725 - acc: 0.9330 - val_loss: 0.2414 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2823 - acc: 0.9330 - val_loss: 0.2410 - val_acc: 0.9358\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.41393, saving model to best.model\n",
      "0s - loss: 0.7789 - acc: 0.5820 - val_loss: 0.4139 - val_acc: 0.8991\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.41393 to 0.33049, saving model to best.model\n",
      "0s - loss: 0.4567 - acc: 0.7945 - val_loss: 0.3305 - val_acc: 0.8991\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.3250 - acc: 0.9030 - val_loss: 0.3406 - val_acc: 0.8991\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2656 - acc: 0.9353 - val_loss: 0.3818 - val_acc: 0.8991\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2409 - acc: 0.9492 - val_loss: 0.4255 - val_acc: 0.8991\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2384 - acc: 0.9469 - val_loss: 0.4632 - val_acc: 0.8991\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2172 - acc: 0.9469 - val_loss: 0.4927 - val_acc: 0.8991\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2714 - acc: 0.9492 - val_loss: 0.5130 - val_acc: 0.8991\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2585 - acc: 0.9492 - val_loss: 0.5244 - val_acc: 0.8991\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2471 - acc: 0.9492 - val_loss: 0.5290 - val_acc: 0.8991\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2437 - acc: 0.9492 - val_loss: 0.5281 - val_acc: 0.8991\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2788 - acc: 0.9492 - val_loss: 0.5218 - val_acc: 0.8991\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2702 - acc: 0.9492 - val_loss: 0.5124 - val_acc: 0.8991\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2884 - acc: 0.9492 - val_loss: 0.4994 - val_acc: 0.8991\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2450 - acc: 0.9492 - val_loss: 0.4848 - val_acc: 0.8991\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2464 - acc: 0.9492 - val_loss: 0.4684 - val_acc: 0.8991\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2573 - acc: 0.9492 - val_loss: 0.4512 - val_acc: 0.8991\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2487 - acc: 0.9492 - val_loss: 0.4344 - val_acc: 0.8991\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2410 - acc: 0.9492 - val_loss: 0.4182 - val_acc: 0.8991\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2424 - acc: 0.9492 - val_loss: 0.4038 - val_acc: 0.8991\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2410 - acc: 0.9469 - val_loss: 0.3928 - val_acc: 0.8991\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2409 - acc: 0.9469 - val_loss: 0.3842 - val_acc: 0.8991\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2188 - acc: 0.9492 - val_loss: 0.3777 - val_acc: 0.8991\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2323 - acc: 0.9446 - val_loss: 0.3729 - val_acc: 0.8991\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2403 - acc: 0.9469 - val_loss: 0.3695 - val_acc: 0.8991\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2287 - acc: 0.9469 - val_loss: 0.3681 - val_acc: 0.8991\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2228 - acc: 0.9469 - val_loss: 0.3677 - val_acc: 0.8991\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2279 - acc: 0.9446 - val_loss: 0.3693 - val_acc: 0.8991\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.71564, saving model to best.model\n",
      "0s - loss: 2.4280 - acc: 0.0762 - val_loss: 1.7156 - val_acc: 0.0917\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.71564 to 1.08205, saving model to best.model\n",
      "0s - loss: 1.7665 - acc: 0.1270 - val_loss: 1.0820 - val_acc: 0.0917\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.08205 to 0.64298, saving model to best.model\n",
      "0s - loss: 1.1950 - acc: 0.2679 - val_loss: 0.6430 - val_acc: 0.9083\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.64298 to 0.40958, saving model to best.model\n",
      "0s - loss: 0.7272 - acc: 0.5497 - val_loss: 0.4096 - val_acc: 0.9083\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.40958 to 0.32098, saving model to best.model\n",
      "0s - loss: 0.4978 - acc: 0.7829 - val_loss: 0.3210 - val_acc: 0.9083\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.32098 to 0.30766, saving model to best.model\n",
      "0s - loss: 0.3441 - acc: 0.8915 - val_loss: 0.3077 - val_acc: 0.9083\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2787 - acc: 0.9307 - val_loss: 0.3254 - val_acc: 0.9083\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2468 - acc: 0.9446 - val_loss: 0.3524 - val_acc: 0.9083\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2299 - acc: 0.9469 - val_loss: 0.3795 - val_acc: 0.9083\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2294 - acc: 0.9469 - val_loss: 0.4032 - val_acc: 0.9083\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2313 - acc: 0.9469 - val_loss: 0.4229 - val_acc: 0.9083\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2596 - acc: 0.9469 - val_loss: 0.4385 - val_acc: 0.9083\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2310 - acc: 0.9469 - val_loss: 0.4499 - val_acc: 0.9083\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2617 - acc: 0.9469 - val_loss: 0.4581 - val_acc: 0.9083\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2687 - acc: 0.9469 - val_loss: 0.4630 - val_acc: 0.9083\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2460 - acc: 0.9469 - val_loss: 0.4653 - val_acc: 0.9083\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2790 - acc: 0.9469 - val_loss: 0.4652 - val_acc: 0.9083\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2514 - acc: 0.9469 - val_loss: 0.4632 - val_acc: 0.9083\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2776 - acc: 0.9469 - val_loss: 0.4597 - val_acc: 0.9083\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2491 - acc: 0.9469 - val_loss: 0.4551 - val_acc: 0.9083\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2480 - acc: 0.9469 - val_loss: 0.4494 - val_acc: 0.9083\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2780 - acc: 0.9469 - val_loss: 0.4429 - val_acc: 0.9083\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2614 - acc: 0.9469 - val_loss: 0.4358 - val_acc: 0.9083\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2463 - acc: 0.9469 - val_loss: 0.4282 - val_acc: 0.9083\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2589 - acc: 0.9469 - val_loss: 0.4203 - val_acc: 0.9083\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2409 - acc: 0.9469 - val_loss: 0.4124 - val_acc: 0.9083\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2488 - acc: 0.9469 - val_loss: 0.4049 - val_acc: 0.9083\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2383 - acc: 0.9469 - val_loss: 0.3976 - val_acc: 0.9083\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2463 - acc: 0.9469 - val_loss: 0.3906 - val_acc: 0.9083\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2466 - acc: 0.9469 - val_loss: 0.3839 - val_acc: 0.9083\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2636 - acc: 0.9446 - val_loss: 0.3778 - val_acc: 0.9083\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2358 - acc: 0.9469 - val_loss: 0.3718 - val_acc: 0.9083\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.43759, saving model to best.model\n",
      "0s - loss: 0.8328 - acc: 0.4781 - val_loss: 0.4376 - val_acc: 0.9358\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.43759 to 0.28941, saving model to best.model\n",
      "0s - loss: 0.5360 - acc: 0.7390 - val_loss: 0.2894 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.28941 to 0.24290, saving model to best.model\n",
      "0s - loss: 0.3928 - acc: 0.8730 - val_loss: 0.2429 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.24290 to 0.24002, saving model to best.model\n",
      "0s - loss: 0.3133 - acc: 0.9169 - val_loss: 0.2400 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3209 - acc: 0.9215 - val_loss: 0.2514 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3158 - acc: 0.9215 - val_loss: 0.2645 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2907 - acc: 0.9215 - val_loss: 0.2752 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3395 - acc: 0.9215 - val_loss: 0.2818 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3127 - acc: 0.9215 - val_loss: 0.2845 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3701 - acc: 0.9215 - val_loss: 0.2836 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3348 - acc: 0.9215 - val_loss: 0.2803 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3587 - acc: 0.9215 - val_loss: 0.2747 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3252 - acc: 0.9215 - val_loss: 0.2678 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3258 - acc: 0.9215 - val_loss: 0.2606 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2953 - acc: 0.9215 - val_loss: 0.2539 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2846 - acc: 0.9215 - val_loss: 0.2480 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3289 - acc: 0.9215 - val_loss: 0.2433 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2839 - acc: 0.9215 - val_loss: 0.2403 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.24002 to 0.23868, saving model to best.model\n",
      "0s - loss: 0.3047 - acc: 0.9215 - val_loss: 0.2387 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.23868 to 0.23808, saving model to best.model\n",
      "0s - loss: 0.3237 - acc: 0.9192 - val_loss: 0.2381 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss improved from 0.23808 to 0.23801, saving model to best.model\n",
      "0s - loss: 0.3163 - acc: 0.9145 - val_loss: 0.2380 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3159 - acc: 0.9215 - val_loss: 0.2381 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.3017 - acc: 0.9215 - val_loss: 0.2381 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2975 - acc: 0.9192 - val_loss: 0.2380 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.23801 to 0.23793, saving model to best.model\n",
      "0s - loss: 0.2978 - acc: 0.9215 - val_loss: 0.2379 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.3008 - acc: 0.9145 - val_loss: 0.2380 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2945 - acc: 0.9215 - val_loss: 0.2382 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2914 - acc: 0.9215 - val_loss: 0.2385 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.3188 - acc: 0.9215 - val_loss: 0.2389 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.3046 - acc: 0.9215 - val_loss: 0.2394 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2911 - acc: 0.9192 - val_loss: 0.2397 - val_acc: 0.9358\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.3025 - acc: 0.9215 - val_loss: 0.2397 - val_acc: 0.9358\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.3182 - acc: 0.9215 - val_loss: 0.2397 - val_acc: 0.9358\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2755 - acc: 0.9215 - val_loss: 0.2394 - val_acc: 0.9358\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.3151 - acc: 0.9215 - val_loss: 0.2390 - val_acc: 0.9358\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.3295 - acc: 0.9215 - val_loss: 0.2385 - val_acc: 0.9358\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2761 - acc: 0.9192 - val_loss: 0.2383 - val_acc: 0.9358\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.3055 - acc: 0.9238 - val_loss: 0.2380 - val_acc: 0.9358\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.23793 to 0.23781, saving model to best.model\n",
      "0s - loss: 0.3042 - acc: 0.9215 - val_loss: 0.2378 - val_acc: 0.9358\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.23781 to 0.23777, saving model to best.model\n",
      "0s - loss: 0.3078 - acc: 0.9215 - val_loss: 0.2378 - val_acc: 0.9358\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.23777 to 0.23771, saving model to best.model\n",
      "0s - loss: 0.3118 - acc: 0.9215 - val_loss: 0.2377 - val_acc: 0.9358\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.23771 to 0.23763, saving model to best.model\n",
      "0s - loss: 0.3216 - acc: 0.9215 - val_loss: 0.2376 - val_acc: 0.9358\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2790 - acc: 0.9215 - val_loss: 0.2376 - val_acc: 0.9358\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.3027 - acc: 0.9215 - val_loss: 0.2377 - val_acc: 0.9358\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.23763 to 0.23762, saving model to best.model\n",
      "0s - loss: 0.2901 - acc: 0.9215 - val_loss: 0.2376 - val_acc: 0.9358\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.23762 to 0.23755, saving model to best.model\n",
      "0s - loss: 0.2933 - acc: 0.9215 - val_loss: 0.2376 - val_acc: 0.9358\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.23755 to 0.23753, saving model to best.model\n",
      "0s - loss: 0.3109 - acc: 0.9215 - val_loss: 0.2375 - val_acc: 0.9358\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.23753 to 0.23750, saving model to best.model\n",
      "0s - loss: 0.2935 - acc: 0.9215 - val_loss: 0.2375 - val_acc: 0.9358\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.23750 to 0.23740, saving model to best.model\n",
      "0s - loss: 0.2924 - acc: 0.9215 - val_loss: 0.2374 - val_acc: 0.9358\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.23740 to 0.23730, saving model to best.model\n",
      "0s - loss: 0.2970 - acc: 0.9215 - val_loss: 0.2373 - val_acc: 0.9358\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.23730 to 0.23722, saving model to best.model\n",
      "0s - loss: 0.3028 - acc: 0.9192 - val_loss: 0.2372 - val_acc: 0.9358\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.23722 to 0.23709, saving model to best.model\n",
      "0s - loss: 0.3121 - acc: 0.9215 - val_loss: 0.2371 - val_acc: 0.9358\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.23709 to 0.23701, saving model to best.model\n",
      "0s - loss: 0.2956 - acc: 0.9215 - val_loss: 0.2370 - val_acc: 0.9358\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.23701 to 0.23696, saving model to best.model\n",
      "0s - loss: 0.2826 - acc: 0.9215 - val_loss: 0.2370 - val_acc: 0.9358\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.23696 to 0.23692, saving model to best.model\n",
      "0s - loss: 0.3039 - acc: 0.9215 - val_loss: 0.2369 - val_acc: 0.9358\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.23692 to 0.23683, saving model to best.model\n",
      "0s - loss: 0.3137 - acc: 0.9215 - val_loss: 0.2368 - val_acc: 0.9358\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.23683 to 0.23667, saving model to best.model\n",
      "0s - loss: 0.3094 - acc: 0.9192 - val_loss: 0.2367 - val_acc: 0.9358\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.23667 to 0.23651, saving model to best.model\n",
      "0s - loss: 0.3004 - acc: 0.9215 - val_loss: 0.2365 - val_acc: 0.9358\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.23651 to 0.23636, saving model to best.model\n",
      "0s - loss: 0.2978 - acc: 0.9215 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.23636 to 0.23622, saving model to best.model\n",
      "0s - loss: 0.3077 - acc: 0.9215 - val_loss: 0.2362 - val_acc: 0.9358\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.23622 to 0.23608, saving model to best.model\n",
      "0s - loss: 0.2898 - acc: 0.9215 - val_loss: 0.2361 - val_acc: 0.9358\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.23608 to 0.23596, saving model to best.model\n",
      "0s - loss: 0.2857 - acc: 0.9215 - val_loss: 0.2360 - val_acc: 0.9358\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.23596 to 0.23585, saving model to best.model\n",
      "0s - loss: 0.2810 - acc: 0.9215 - val_loss: 0.2359 - val_acc: 0.9358\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.23585 to 0.23573, saving model to best.model\n",
      "0s - loss: 0.2821 - acc: 0.9215 - val_loss: 0.2357 - val_acc: 0.9358\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.23573 to 0.23563, saving model to best.model\n",
      "0s - loss: 0.3062 - acc: 0.9215 - val_loss: 0.2356 - val_acc: 0.9358\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.23563 to 0.23555, saving model to best.model\n",
      "0s - loss: 0.2895 - acc: 0.9238 - val_loss: 0.2356 - val_acc: 0.9358\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.23555 to 0.23547, saving model to best.model\n",
      "0s - loss: 0.2770 - acc: 0.9215 - val_loss: 0.2355 - val_acc: 0.9358\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.23547 to 0.23538, saving model to best.model\n",
      "0s - loss: 0.2908 - acc: 0.9215 - val_loss: 0.2354 - val_acc: 0.9358\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.23538 to 0.23529, saving model to best.model\n",
      "0s - loss: 0.2824 - acc: 0.9215 - val_loss: 0.2353 - val_acc: 0.9358\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.23529 to 0.23519, saving model to best.model\n",
      "0s - loss: 0.2858 - acc: 0.9215 - val_loss: 0.2352 - val_acc: 0.9358\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.23519 to 0.23500, saving model to best.model\n",
      "0s - loss: 0.2883 - acc: 0.9215 - val_loss: 0.2350 - val_acc: 0.9358\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.23500 to 0.23475, saving model to best.model\n",
      "0s - loss: 0.2911 - acc: 0.9215 - val_loss: 0.2347 - val_acc: 0.9358\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.23475 to 0.23452, saving model to best.model\n",
      "0s - loss: 0.2831 - acc: 0.9215 - val_loss: 0.2345 - val_acc: 0.9358\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.23452 to 0.23429, saving model to best.model\n",
      "0s - loss: 0.2816 - acc: 0.9215 - val_loss: 0.2343 - val_acc: 0.9358\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.23429 to 0.23409, saving model to best.model\n",
      "0s - loss: 0.2796 - acc: 0.9215 - val_loss: 0.2341 - val_acc: 0.9358\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.23409 to 0.23392, saving model to best.model\n",
      "0s - loss: 0.2782 - acc: 0.9215 - val_loss: 0.2339 - val_acc: 0.9358\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.23392 to 0.23372, saving model to best.model\n",
      "0s - loss: 0.2809 - acc: 0.9215 - val_loss: 0.2337 - val_acc: 0.9358\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.23372 to 0.23352, saving model to best.model\n",
      "0s - loss: 0.2972 - acc: 0.9215 - val_loss: 0.2335 - val_acc: 0.9358\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.23352 to 0.23329, saving model to best.model\n",
      "0s - loss: 0.2790 - acc: 0.9215 - val_loss: 0.2333 - val_acc: 0.9358\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.23329 to 0.23307, saving model to best.model\n",
      "0s - loss: 0.2808 - acc: 0.9215 - val_loss: 0.2331 - val_acc: 0.9358\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.23307 to 0.23287, saving model to best.model\n",
      "0s - loss: 0.2979 - acc: 0.9215 - val_loss: 0.2329 - val_acc: 0.9358\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.23287 to 0.23263, saving model to best.model\n",
      "0s - loss: 0.2740 - acc: 0.9215 - val_loss: 0.2326 - val_acc: 0.9358\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.23263 to 0.23243, saving model to best.model\n",
      "0s - loss: 0.2859 - acc: 0.9215 - val_loss: 0.2324 - val_acc: 0.9358\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.23243 to 0.23220, saving model to best.model\n",
      "0s - loss: 0.2780 - acc: 0.9215 - val_loss: 0.2322 - val_acc: 0.9358\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.23220 to 0.23195, saving model to best.model\n",
      "0s - loss: 0.2865 - acc: 0.9215 - val_loss: 0.2320 - val_acc: 0.9358\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.23195 to 0.23161, saving model to best.model\n",
      "0s - loss: 0.2686 - acc: 0.9215 - val_loss: 0.2316 - val_acc: 0.9358\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.23161 to 0.23123, saving model to best.model\n",
      "0s - loss: 0.2842 - acc: 0.9215 - val_loss: 0.2312 - val_acc: 0.9358\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.23123 to 0.23086, saving model to best.model\n",
      "0s - loss: 0.2864 - acc: 0.9215 - val_loss: 0.2309 - val_acc: 0.9358\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.23086 to 0.23049, saving model to best.model\n",
      "0s - loss: 0.2755 - acc: 0.9215 - val_loss: 0.2305 - val_acc: 0.9358\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.23049 to 0.23018, saving model to best.model\n",
      "0s - loss: 0.2811 - acc: 0.9215 - val_loss: 0.2302 - val_acc: 0.9358\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.23018 to 0.22986, saving model to best.model\n",
      "0s - loss: 0.2665 - acc: 0.9215 - val_loss: 0.2299 - val_acc: 0.9358\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.22986 to 0.22945, saving model to best.model\n",
      "0s - loss: 0.2813 - acc: 0.9215 - val_loss: 0.2295 - val_acc: 0.9358\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.22945 to 0.22900, saving model to best.model\n",
      "0s - loss: 0.2857 - acc: 0.9215 - val_loss: 0.2290 - val_acc: 0.9358\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.22900 to 0.22846, saving model to best.model\n",
      "0s - loss: 0.2690 - acc: 0.9215 - val_loss: 0.2285 - val_acc: 0.9358\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.22846 to 0.22789, saving model to best.model\n",
      "0s - loss: 0.2721 - acc: 0.9215 - val_loss: 0.2279 - val_acc: 0.9358\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.22789 to 0.22730, saving model to best.model\n",
      "0s - loss: 0.2721 - acc: 0.9215 - val_loss: 0.2273 - val_acc: 0.9358\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.22730 to 0.22673, saving model to best.model\n",
      "0s - loss: 0.2959 - acc: 0.9215 - val_loss: 0.2267 - val_acc: 0.9358\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.22673 to 0.22621, saving model to best.model\n",
      "0s - loss: 0.2843 - acc: 0.9192 - val_loss: 0.2262 - val_acc: 0.9358\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.22621 to 0.22568, saving model to best.model\n",
      "0s - loss: 0.2775 - acc: 0.9215 - val_loss: 0.2257 - val_acc: 0.9358\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.22568 to 0.22516, saving model to best.model\n",
      "0s - loss: 0.2876 - acc: 0.9215 - val_loss: 0.2252 - val_acc: 0.9358\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.22516 to 0.22455, saving model to best.model\n",
      "0s - loss: 0.2677 - acc: 0.9215 - val_loss: 0.2246 - val_acc: 0.9358\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.22455 to 0.22390, saving model to best.model\n",
      "0s - loss: 0.2633 - acc: 0.9215 - val_loss: 0.2239 - val_acc: 0.9358\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.22390 to 0.22311, saving model to best.model\n",
      "0s - loss: 0.2692 - acc: 0.9215 - val_loss: 0.2231 - val_acc: 0.9358\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.22311 to 0.22230, saving model to best.model\n",
      "0s - loss: 0.2723 - acc: 0.9215 - val_loss: 0.2223 - val_acc: 0.9358\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.22230 to 0.22151, saving model to best.model\n",
      "0s - loss: 0.2627 - acc: 0.9215 - val_loss: 0.2215 - val_acc: 0.9358\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.22151 to 0.22068, saving model to best.model\n",
      "0s - loss: 0.2811 - acc: 0.9215 - val_loss: 0.2207 - val_acc: 0.9358\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.22068 to 0.21985, saving model to best.model\n",
      "0s - loss: 0.2404 - acc: 0.9215 - val_loss: 0.2199 - val_acc: 0.9358\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.21985 to 0.21906, saving model to best.model\n",
      "0s - loss: 0.2623 - acc: 0.9215 - val_loss: 0.2191 - val_acc: 0.9358\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.21906 to 0.21836, saving model to best.model\n",
      "0s - loss: 0.2566 - acc: 0.9215 - val_loss: 0.2184 - val_acc: 0.9358\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.21836 to 0.21761, saving model to best.model\n",
      "0s - loss: 0.2635 - acc: 0.9215 - val_loss: 0.2176 - val_acc: 0.9358\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.21761 to 0.21668, saving model to best.model\n",
      "0s - loss: 0.2570 - acc: 0.9215 - val_loss: 0.2167 - val_acc: 0.9358\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.21668 to 0.21543, saving model to best.model\n",
      "0s - loss: 0.2605 - acc: 0.9215 - val_loss: 0.2154 - val_acc: 0.9358\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.21543 to 0.21400, saving model to best.model\n",
      "0s - loss: 0.2597 - acc: 0.9215 - val_loss: 0.2140 - val_acc: 0.9358\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.21400 to 0.21271, saving model to best.model\n",
      "0s - loss: 0.2558 - acc: 0.9215 - val_loss: 0.2127 - val_acc: 0.9358\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.21271 to 0.21138, saving model to best.model\n",
      "0s - loss: 0.2557 - acc: 0.9215 - val_loss: 0.2114 - val_acc: 0.9358\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.21138 to 0.21000, saving model to best.model\n",
      "0s - loss: 0.2535 - acc: 0.9215 - val_loss: 0.2100 - val_acc: 0.9358\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.21000 to 0.20853, saving model to best.model\n",
      "0s - loss: 0.2496 - acc: 0.9215 - val_loss: 0.2085 - val_acc: 0.9358\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.20853 to 0.20716, saving model to best.model\n",
      "0s - loss: 0.2490 - acc: 0.9215 - val_loss: 0.2072 - val_acc: 0.9358\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.20716 to 0.20581, saving model to best.model\n",
      "0s - loss: 0.2432 - acc: 0.9215 - val_loss: 0.2058 - val_acc: 0.9358\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.20581 to 0.20407, saving model to best.model\n",
      "0s - loss: 0.2542 - acc: 0.9215 - val_loss: 0.2041 - val_acc: 0.9358\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.20407 to 0.20222, saving model to best.model\n",
      "0s - loss: 0.2470 - acc: 0.9215 - val_loss: 0.2022 - val_acc: 0.9358\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.20222 to 0.20004, saving model to best.model\n",
      "0s - loss: 0.2428 - acc: 0.9215 - val_loss: 0.2000 - val_acc: 0.9358\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.20004 to 0.19786, saving model to best.model\n",
      "0s - loss: 0.2492 - acc: 0.9215 - val_loss: 0.1979 - val_acc: 0.9358\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.19786 to 0.19582, saving model to best.model\n",
      "0s - loss: 0.2322 - acc: 0.9215 - val_loss: 0.1958 - val_acc: 0.9358\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.19582 to 0.19372, saving model to best.model\n",
      "0s - loss: 0.2362 - acc: 0.9215 - val_loss: 0.1937 - val_acc: 0.9358\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.19372 to 0.19175, saving model to best.model\n",
      "0s - loss: 0.2443 - acc: 0.9192 - val_loss: 0.1917 - val_acc: 0.9358\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.19175 to 0.18972, saving model to best.model\n",
      "0s - loss: 0.2314 - acc: 0.9215 - val_loss: 0.1897 - val_acc: 0.9358\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.18972 to 0.18762, saving model to best.model\n",
      "0s - loss: 0.2279 - acc: 0.9215 - val_loss: 0.1876 - val_acc: 0.9358\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.18762 to 0.18536, saving model to best.model\n",
      "0s - loss: 0.2254 - acc: 0.9192 - val_loss: 0.1854 - val_acc: 0.9358\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.18536 to 0.18340, saving model to best.model\n",
      "0s - loss: 0.2324 - acc: 0.9261 - val_loss: 0.1834 - val_acc: 0.9358\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.18340 to 0.18085, saving model to best.model\n",
      "0s - loss: 0.2191 - acc: 0.9215 - val_loss: 0.1809 - val_acc: 0.9358\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.18085 to 0.17749, saving model to best.model\n",
      "0s - loss: 0.2221 - acc: 0.9238 - val_loss: 0.1775 - val_acc: 0.9358\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.17749 to 0.17422, saving model to best.model\n",
      "0s - loss: 0.2123 - acc: 0.9261 - val_loss: 0.1742 - val_acc: 0.9358\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.17422 to 0.17130, saving model to best.model\n",
      "0s - loss: 0.1965 - acc: 0.9261 - val_loss: 0.1713 - val_acc: 0.9358\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.17130 to 0.16852, saving model to best.model\n",
      "0s - loss: 0.2064 - acc: 0.9307 - val_loss: 0.1685 - val_acc: 0.9358\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.16852 to 0.16604, saving model to best.model\n",
      "0s - loss: 0.2243 - acc: 0.9238 - val_loss: 0.1660 - val_acc: 0.9358\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.16604 to 0.16369, saving model to best.model\n",
      "0s - loss: 0.2093 - acc: 0.9238 - val_loss: 0.1637 - val_acc: 0.9358\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.16369 to 0.16210, saving model to best.model\n",
      "0s - loss: 0.2036 - acc: 0.9238 - val_loss: 0.1621 - val_acc: 0.9358\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.16210 to 0.16011, saving model to best.model\n",
      "0s - loss: 0.2004 - acc: 0.9261 - val_loss: 0.1601 - val_acc: 0.9358\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.16011 to 0.15637, saving model to best.model\n",
      "0s - loss: 0.2007 - acc: 0.9307 - val_loss: 0.1564 - val_acc: 0.9358\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.15637 to 0.15243, saving model to best.model\n",
      "0s - loss: 0.2063 - acc: 0.9238 - val_loss: 0.1524 - val_acc: 0.9358\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.15243 to 0.14925, saving model to best.model\n",
      "0s - loss: 0.1916 - acc: 0.9330 - val_loss: 0.1493 - val_acc: 0.9358\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.14925 to 0.14584, saving model to best.model\n",
      "0s - loss: 0.1917 - acc: 0.9284 - val_loss: 0.1458 - val_acc: 0.9450\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.14584 to 0.14319, saving model to best.model\n",
      "0s - loss: 0.1941 - acc: 0.9353 - val_loss: 0.1432 - val_acc: 0.9450\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.14319 to 0.14178, saving model to best.model\n",
      "0s - loss: 0.2077 - acc: 0.9284 - val_loss: 0.1418 - val_acc: 0.9450\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.14178 to 0.14076, saving model to best.model\n",
      "0s - loss: 0.1739 - acc: 0.9446 - val_loss: 0.1408 - val_acc: 0.9450\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.14076 to 0.13842, saving model to best.model\n",
      "0s - loss: 0.1684 - acc: 0.9446 - val_loss: 0.1384 - val_acc: 0.9450\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.13842 to 0.13465, saving model to best.model\n",
      "0s - loss: 0.1724 - acc: 0.9330 - val_loss: 0.1347 - val_acc: 0.9450\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.13465 to 0.13051, saving model to best.model\n",
      "0s - loss: 0.1709 - acc: 0.9307 - val_loss: 0.1305 - val_acc: 0.9450\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.13051 to 0.12729, saving model to best.model\n",
      "0s - loss: 0.1704 - acc: 0.9400 - val_loss: 0.1273 - val_acc: 0.9450\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.12729 to 0.12538, saving model to best.model\n",
      "0s - loss: 0.1688 - acc: 0.9400 - val_loss: 0.1254 - val_acc: 0.9450\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.12538 to 0.12291, saving model to best.model\n",
      "0s - loss: 0.1881 - acc: 0.9307 - val_loss: 0.1229 - val_acc: 0.9450\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.12291 to 0.12036, saving model to best.model\n",
      "0s - loss: 0.1824 - acc: 0.9330 - val_loss: 0.1204 - val_acc: 0.9450\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.12036 to 0.11839, saving model to best.model\n",
      "0s - loss: 0.1526 - acc: 0.9538 - val_loss: 0.1184 - val_acc: 0.9450\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.11839 to 0.11551, saving model to best.model\n",
      "0s - loss: 0.1720 - acc: 0.9423 - val_loss: 0.1155 - val_acc: 0.9450\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.11551 to 0.11351, saving model to best.model\n",
      "0s - loss: 0.1445 - acc: 0.9469 - val_loss: 0.1135 - val_acc: 0.9450\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.11351 to 0.11167, saving model to best.model\n",
      "0s - loss: 0.1634 - acc: 0.9515 - val_loss: 0.1117 - val_acc: 0.9450\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.11167 to 0.11165, saving model to best.model\n",
      "0s - loss: 0.1522 - acc: 0.9492 - val_loss: 0.1116 - val_acc: 0.9450\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.1558 - acc: 0.9376 - val_loss: 0.1124 - val_acc: 0.9450\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.11165 to 0.10999, saving model to best.model\n",
      "0s - loss: 0.1744 - acc: 0.9376 - val_loss: 0.1100 - val_acc: 0.9450\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.10999 to 0.10563, saving model to best.model\n",
      "0s - loss: 0.1490 - acc: 0.9515 - val_loss: 0.1056 - val_acc: 0.9450\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.10563 to 0.10094, saving model to best.model\n",
      "0s - loss: 0.1553 - acc: 0.9400 - val_loss: 0.1009 - val_acc: 0.9450\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.10094 to 0.09883, saving model to best.model\n",
      "0s - loss: 0.1612 - acc: 0.9492 - val_loss: 0.0988 - val_acc: 0.9541\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.09883 to 0.09765, saving model to best.model\n",
      "0s - loss: 0.1597 - acc: 0.9446 - val_loss: 0.0976 - val_acc: 0.9541\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.09765 to 0.09727, saving model to best.model\n",
      "0s - loss: 0.1579 - acc: 0.9446 - val_loss: 0.0973 - val_acc: 0.9450\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.09727 to 0.09721, saving model to best.model\n",
      "0s - loss: 0.1690 - acc: 0.9446 - val_loss: 0.0972 - val_acc: 0.9450\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.09721 to 0.09721, saving model to best.model\n",
      "0s - loss: 0.1625 - acc: 0.9469 - val_loss: 0.0972 - val_acc: 0.9450\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.09721 to 0.09542, saving model to best.model\n",
      "0s - loss: 0.1494 - acc: 0.9492 - val_loss: 0.0954 - val_acc: 0.9450\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.09542 to 0.09294, saving model to best.model\n",
      "0s - loss: 0.1649 - acc: 0.9492 - val_loss: 0.0929 - val_acc: 0.9541\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.09294 to 0.09237, saving model to best.model\n",
      "0s - loss: 0.1478 - acc: 0.9584 - val_loss: 0.0924 - val_acc: 0.9541\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.09237 to 0.09223, saving model to best.model\n",
      "0s - loss: 0.1331 - acc: 0.9538 - val_loss: 0.0922 - val_acc: 0.9450\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.09223 to 0.09222, saving model to best.model\n",
      "0s - loss: 0.1289 - acc: 0.9677 - val_loss: 0.0922 - val_acc: 0.9450\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.09222 to 0.09064, saving model to best.model\n",
      "0s - loss: 0.1437 - acc: 0.9492 - val_loss: 0.0906 - val_acc: 0.9450\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.09064 to 0.08897, saving model to best.model\n",
      "0s - loss: 0.1300 - acc: 0.9584 - val_loss: 0.0890 - val_acc: 0.9541\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.08897 to 0.08721, saving model to best.model\n",
      "0s - loss: 0.1463 - acc: 0.9538 - val_loss: 0.0872 - val_acc: 0.9633\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.08721 to 0.08536, saving model to best.model\n",
      "0s - loss: 0.1540 - acc: 0.9423 - val_loss: 0.0854 - val_acc: 0.9633\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.08536 to 0.08409, saving model to best.model\n",
      "0s - loss: 0.1467 - acc: 0.9492 - val_loss: 0.0841 - val_acc: 0.9633\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.08409 to 0.08315, saving model to best.model\n",
      "0s - loss: 0.1463 - acc: 0.9469 - val_loss: 0.0831 - val_acc: 0.9633\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.08315 to 0.08305, saving model to best.model\n",
      "0s - loss: 0.1443 - acc: 0.9538 - val_loss: 0.0831 - val_acc: 0.9633\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.08305 to 0.08249, saving model to best.model\n",
      "0s - loss: 0.1224 - acc: 0.9584 - val_loss: 0.0825 - val_acc: 0.9633\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.08249 to 0.08133, saving model to best.model\n",
      "0s - loss: 0.1357 - acc: 0.9492 - val_loss: 0.0813 - val_acc: 0.9633\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.08133 to 0.08063, saving model to best.model\n",
      "0s - loss: 0.1686 - acc: 0.9446 - val_loss: 0.0806 - val_acc: 0.9633\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.08063 to 0.08054, saving model to best.model\n",
      "0s - loss: 0.1322 - acc: 0.9561 - val_loss: 0.0805 - val_acc: 0.9725\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.08054 to 0.08017, saving model to best.model\n",
      "0s - loss: 0.1395 - acc: 0.9538 - val_loss: 0.0802 - val_acc: 0.9633\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.1273 - acc: 0.9584 - val_loss: 0.0803 - val_acc: 0.9633\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.1520 - acc: 0.9469 - val_loss: 0.0811 - val_acc: 0.9633\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.1416 - acc: 0.9561 - val_loss: 0.0815 - val_acc: 0.9541\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.08017 to 0.07977, saving model to best.model\n",
      "0s - loss: 0.1179 - acc: 0.9561 - val_loss: 0.0798 - val_acc: 0.9633\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.07977 to 0.07870, saving model to best.model\n",
      "0s - loss: 0.1504 - acc: 0.9515 - val_loss: 0.0787 - val_acc: 0.9817\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.1413 - acc: 0.9607 - val_loss: 0.0793 - val_acc: 0.9725\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.1181 - acc: 0.9561 - val_loss: 0.0793 - val_acc: 0.9725\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.07870 to 0.07808, saving model to best.model\n",
      "0s - loss: 0.1576 - acc: 0.9492 - val_loss: 0.0781 - val_acc: 0.9817\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.07808 to 0.07743, saving model to best.model\n",
      "0s - loss: 0.1376 - acc: 0.9584 - val_loss: 0.0774 - val_acc: 0.9817\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.07743 to 0.07724, saving model to best.model\n",
      "0s - loss: 0.1288 - acc: 0.9607 - val_loss: 0.0772 - val_acc: 0.9817\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.1310 - acc: 0.9630 - val_loss: 0.0773 - val_acc: 0.9633\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.07724 to 0.07674, saving model to best.model\n",
      "0s - loss: 0.1332 - acc: 0.9607 - val_loss: 0.0767 - val_acc: 0.9633\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.07674 to 0.07551, saving model to best.model\n",
      "0s - loss: 0.1366 - acc: 0.9469 - val_loss: 0.0755 - val_acc: 0.9817\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.07551 to 0.07467, saving model to best.model\n",
      "0s - loss: 0.1191 - acc: 0.9561 - val_loss: 0.0747 - val_acc: 0.9817\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.07467 to 0.07436, saving model to best.model\n",
      "0s - loss: 0.1276 - acc: 0.9630 - val_loss: 0.0744 - val_acc: 0.9817\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.07436 to 0.07389, saving model to best.model\n",
      "0s - loss: 0.1208 - acc: 0.9584 - val_loss: 0.0739 - val_acc: 0.9817\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.62479, saving model to best.model\n",
      "0s - loss: 1.1102 - acc: 0.3072 - val_loss: 0.6248 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.62479 to 0.37763, saving model to best.model\n",
      "0s - loss: 0.7107 - acc: 0.5635 - val_loss: 0.3776 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.37763 to 0.28195, saving model to best.model\n",
      "0s - loss: 0.4971 - acc: 0.7898 - val_loss: 0.2819 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.28195 to 0.26228, saving model to best.model\n",
      "0s - loss: 0.3908 - acc: 0.8637 - val_loss: 0.2623 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3020 - acc: 0.9169 - val_loss: 0.2726 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2834 - acc: 0.9215 - val_loss: 0.2912 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2861 - acc: 0.9215 - val_loss: 0.3093 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3023 - acc: 0.9284 - val_loss: 0.3241 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3341 - acc: 0.9284 - val_loss: 0.3343 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3200 - acc: 0.9284 - val_loss: 0.3404 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3186 - acc: 0.9284 - val_loss: 0.3428 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3370 - acc: 0.9284 - val_loss: 0.3419 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3607 - acc: 0.9284 - val_loss: 0.3383 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3138 - acc: 0.9284 - val_loss: 0.3327 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3270 - acc: 0.9284 - val_loss: 0.3258 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3121 - acc: 0.9284 - val_loss: 0.3182 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3077 - acc: 0.9284 - val_loss: 0.3101 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3125 - acc: 0.9284 - val_loss: 0.3024 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3091 - acc: 0.9284 - val_loss: 0.2950 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3283 - acc: 0.9261 - val_loss: 0.2881 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3385 - acc: 0.9261 - val_loss: 0.2822 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3080 - acc: 0.9284 - val_loss: 0.2771 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2837 - acc: 0.9284 - val_loss: 0.2730 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2738 - acc: 0.9238 - val_loss: 0.2699 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2746 - acc: 0.9284 - val_loss: 0.2677 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2790 - acc: 0.9284 - val_loss: 0.2662 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2954 - acc: 0.9192 - val_loss: 0.2654 - val_acc: 0.9266\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2915 - acc: 0.9261 - val_loss: 0.2651 - val_acc: 0.9266\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2831 - acc: 0.9261 - val_loss: 0.2651 - val_acc: 0.9266\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2892 - acc: 0.9238 - val_loss: 0.2655 - val_acc: 0.9266\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.27136, saving model to best.model\n",
      "0s - loss: 0.4162 - acc: 0.8453 - val_loss: 0.2714 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.27136 to 0.26441, saving model to best.model\n",
      "0s - loss: 0.3237 - acc: 0.9169 - val_loss: 0.2644 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2999 - acc: 0.9261 - val_loss: 0.2825 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2904 - acc: 0.9307 - val_loss: 0.3001 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3042 - acc: 0.9307 - val_loss: 0.3095 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3097 - acc: 0.9307 - val_loss: 0.3109 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2872 - acc: 0.9307 - val_loss: 0.3068 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2946 - acc: 0.9307 - val_loss: 0.2986 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2913 - acc: 0.9307 - val_loss: 0.2892 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2842 - acc: 0.9307 - val_loss: 0.2795 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2741 - acc: 0.9307 - val_loss: 0.2718 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2900 - acc: 0.9307 - val_loss: 0.2667 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.26441 to 0.26375, saving model to best.model\n",
      "0s - loss: 0.2909 - acc: 0.9307 - val_loss: 0.2638 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss improved from 0.26375 to 0.26249, saving model to best.model\n",
      "0s - loss: 0.2869 - acc: 0.9261 - val_loss: 0.2625 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.26249 to 0.26204, saving model to best.model\n",
      "0s - loss: 0.2888 - acc: 0.9284 - val_loss: 0.2620 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2815 - acc: 0.9261 - val_loss: 0.2622 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2769 - acc: 0.9284 - val_loss: 0.2632 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2835 - acc: 0.9307 - val_loss: 0.2652 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2837 - acc: 0.9307 - val_loss: 0.2678 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2931 - acc: 0.9284 - val_loss: 0.2703 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2823 - acc: 0.9307 - val_loss: 0.2717 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2782 - acc: 0.9307 - val_loss: 0.2723 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2649 - acc: 0.9307 - val_loss: 0.2721 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2664 - acc: 0.9307 - val_loss: 0.2709 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2609 - acc: 0.9261 - val_loss: 0.2692 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2651 - acc: 0.9307 - val_loss: 0.2676 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2902 - acc: 0.9307 - val_loss: 0.2657 - val_acc: 0.9266\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2585 - acc: 0.9307 - val_loss: 0.2643 - val_acc: 0.9266\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2702 - acc: 0.9307 - val_loss: 0.2630 - val_acc: 0.9266\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2635 - acc: 0.9307 - val_loss: 0.2625 - val_acc: 0.9266\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2669 - acc: 0.9307 - val_loss: 0.2626 - val_acc: 0.9266\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2589 - acc: 0.9307 - val_loss: 0.2632 - val_acc: 0.9266\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2671 - acc: 0.9284 - val_loss: 0.2636 - val_acc: 0.9266\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2668 - acc: 0.9307 - val_loss: 0.2640 - val_acc: 0.9266\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2805 - acc: 0.9284 - val_loss: 0.2640 - val_acc: 0.9266\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2662 - acc: 0.9307 - val_loss: 0.2641 - val_acc: 0.9266\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2853 - acc: 0.9307 - val_loss: 0.2640 - val_acc: 0.9266\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2607 - acc: 0.9307 - val_loss: 0.2639 - val_acc: 0.9266\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2712 - acc: 0.9307 - val_loss: 0.2639 - val_acc: 0.9266\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2607 - acc: 0.9307 - val_loss: 0.2637 - val_acc: 0.9266\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2710 - acc: 0.9307 - val_loss: 0.2634 - val_acc: 0.9266\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.44655, saving model to best.model\n",
      "0s - loss: 0.7827 - acc: 0.5358 - val_loss: 0.4466 - val_acc: 0.8807\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.44655 to 0.36803, saving model to best.model\n",
      "0s - loss: 0.4896 - acc: 0.7552 - val_loss: 0.3680 - val_acc: 0.8807\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.3737 - acc: 0.8845 - val_loss: 0.3808 - val_acc: 0.8807\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.3063 - acc: 0.9238 - val_loss: 0.4222 - val_acc: 0.8807\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3067 - acc: 0.9284 - val_loss: 0.4648 - val_acc: 0.8807\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2888 - acc: 0.9284 - val_loss: 0.4994 - val_acc: 0.8807\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3153 - acc: 0.9284 - val_loss: 0.5233 - val_acc: 0.8807\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3481 - acc: 0.9284 - val_loss: 0.5371 - val_acc: 0.8807\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3167 - acc: 0.9284 - val_loss: 0.5422 - val_acc: 0.8807\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3283 - acc: 0.9284 - val_loss: 0.5398 - val_acc: 0.8807\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3359 - acc: 0.9284 - val_loss: 0.5314 - val_acc: 0.8807\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3465 - acc: 0.9261 - val_loss: 0.5181 - val_acc: 0.8807\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3139 - acc: 0.9284 - val_loss: 0.5018 - val_acc: 0.8807\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3082 - acc: 0.9261 - val_loss: 0.4837 - val_acc: 0.8807\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2963 - acc: 0.9261 - val_loss: 0.4658 - val_acc: 0.8807\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3141 - acc: 0.9284 - val_loss: 0.4483 - val_acc: 0.8807\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2842 - acc: 0.9284 - val_loss: 0.4321 - val_acc: 0.8807\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3112 - acc: 0.9261 - val_loss: 0.4185 - val_acc: 0.8807\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2972 - acc: 0.9284 - val_loss: 0.4081 - val_acc: 0.8807\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3104 - acc: 0.9238 - val_loss: 0.4012 - val_acc: 0.8807\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2803 - acc: 0.9261 - val_loss: 0.3963 - val_acc: 0.8807\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3056 - acc: 0.9261 - val_loss: 0.3937 - val_acc: 0.8807\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2710 - acc: 0.9238 - val_loss: 0.3929 - val_acc: 0.8807\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.3122 - acc: 0.9192 - val_loss: 0.3930 - val_acc: 0.8807\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2939 - acc: 0.9261 - val_loss: 0.3944 - val_acc: 0.8807\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2992 - acc: 0.9284 - val_loss: 0.3961 - val_acc: 0.8807\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2833 - acc: 0.9238 - val_loss: 0.3987 - val_acc: 0.8807\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.3073 - acc: 0.9261 - val_loss: 0.4016 - val_acc: 0.8807\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.36972, saving model to best.model\n",
      "0s - loss: 0.5758 - acc: 0.7229 - val_loss: 0.3697 - val_acc: 0.8899\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.36972 to 0.34917, saving model to best.model\n",
      "0s - loss: 0.3866 - acc: 0.8684 - val_loss: 0.3492 - val_acc: 0.8899\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2905 - acc: 0.9099 - val_loss: 0.3822 - val_acc: 0.8899\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2670 - acc: 0.9353 - val_loss: 0.4255 - val_acc: 0.8899\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2813 - acc: 0.9330 - val_loss: 0.4623 - val_acc: 0.8899\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2808 - acc: 0.9330 - val_loss: 0.4865 - val_acc: 0.8899\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3111 - acc: 0.9330 - val_loss: 0.4990 - val_acc: 0.8899\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3259 - acc: 0.9330 - val_loss: 0.5011 - val_acc: 0.8899\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3147 - acc: 0.9330 - val_loss: 0.4943 - val_acc: 0.8899\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2989 - acc: 0.9330 - val_loss: 0.4815 - val_acc: 0.8899\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2851 - acc: 0.9330 - val_loss: 0.4652 - val_acc: 0.8899\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2827 - acc: 0.9330 - val_loss: 0.4460 - val_acc: 0.8899\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3084 - acc: 0.9330 - val_loss: 0.4260 - val_acc: 0.8899\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2823 - acc: 0.9330 - val_loss: 0.4085 - val_acc: 0.8899\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2435 - acc: 0.9330 - val_loss: 0.3923 - val_acc: 0.8899\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2599 - acc: 0.9330 - val_loss: 0.3793 - val_acc: 0.8899\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2641 - acc: 0.9307 - val_loss: 0.3710 - val_acc: 0.8899\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2525 - acc: 0.9330 - val_loss: 0.3662 - val_acc: 0.8899\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2639 - acc: 0.9261 - val_loss: 0.3642 - val_acc: 0.8899\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2915 - acc: 0.9307 - val_loss: 0.3645 - val_acc: 0.8899\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2594 - acc: 0.9330 - val_loss: 0.3662 - val_acc: 0.8899\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2877 - acc: 0.9307 - val_loss: 0.3695 - val_acc: 0.8899\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2821 - acc: 0.9307 - val_loss: 0.3734 - val_acc: 0.8899\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2709 - acc: 0.9307 - val_loss: 0.3776 - val_acc: 0.8899\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2857 - acc: 0.9330 - val_loss: 0.3819 - val_acc: 0.8899\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2613 - acc: 0.9353 - val_loss: 0.3846 - val_acc: 0.8899\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2807 - acc: 0.9330 - val_loss: 0.3866 - val_acc: 0.8899\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2621 - acc: 0.9330 - val_loss: 0.3871 - val_acc: 0.8899\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.35007, saving model to best.model\n",
      "0s - loss: 0.6440 - acc: 0.6212 - val_loss: 0.3501 - val_acc: 0.9174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.35007 to 0.28760, saving model to best.model\n",
      "0s - loss: 0.4468 - acc: 0.8176 - val_loss: 0.2876 - val_acc: 0.9174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.3402 - acc: 0.8984 - val_loss: 0.2935 - val_acc: 0.9174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.3281 - acc: 0.9192 - val_loss: 0.3176 - val_acc: 0.9174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3517 - acc: 0.9215 - val_loss: 0.3401 - val_acc: 0.9174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3342 - acc: 0.9215 - val_loss: 0.3555 - val_acc: 0.9174\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3594 - acc: 0.9215 - val_loss: 0.3626 - val_acc: 0.9174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3271 - acc: 0.9215 - val_loss: 0.3626 - val_acc: 0.9174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3449 - acc: 0.9215 - val_loss: 0.3567 - val_acc: 0.9174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3596 - acc: 0.9238 - val_loss: 0.3474 - val_acc: 0.9174\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3380 - acc: 0.9192 - val_loss: 0.3369 - val_acc: 0.9174\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3552 - acc: 0.9215 - val_loss: 0.3252 - val_acc: 0.9174\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3118 - acc: 0.9215 - val_loss: 0.3145 - val_acc: 0.9174\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3342 - acc: 0.9215 - val_loss: 0.3048 - val_acc: 0.9174\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3285 - acc: 0.9145 - val_loss: 0.2971 - val_acc: 0.9174\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3056 - acc: 0.9192 - val_loss: 0.2918 - val_acc: 0.9174\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3018 - acc: 0.9145 - val_loss: 0.2887 - val_acc: 0.9174\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.28760 to 0.28699, saving model to best.model\n",
      "0s - loss: 0.3249 - acc: 0.9192 - val_loss: 0.2870 - val_acc: 0.9174\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.28699 to 0.28644, saving model to best.model\n",
      "0s - loss: 0.3304 - acc: 0.9099 - val_loss: 0.2864 - val_acc: 0.9174\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3062 - acc: 0.9169 - val_loss: 0.2865 - val_acc: 0.9174\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3306 - acc: 0.9122 - val_loss: 0.2870 - val_acc: 0.9174\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2969 - acc: 0.9145 - val_loss: 0.2884 - val_acc: 0.9174\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.3223 - acc: 0.9192 - val_loss: 0.2901 - val_acc: 0.9174\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2840 - acc: 0.9215 - val_loss: 0.2922 - val_acc: 0.9174\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.3197 - acc: 0.9192 - val_loss: 0.2941 - val_acc: 0.9174\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.3136 - acc: 0.9238 - val_loss: 0.2955 - val_acc: 0.9174\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3111 - acc: 0.9169 - val_loss: 0.2958 - val_acc: 0.9174\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.3144 - acc: 0.9192 - val_loss: 0.2957 - val_acc: 0.9174\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2974 - acc: 0.9192 - val_loss: 0.2951 - val_acc: 0.9174\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.3063 - acc: 0.9215 - val_loss: 0.2944 - val_acc: 0.9174\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.3066 - acc: 0.9215 - val_loss: 0.2936 - val_acc: 0.9174\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2998 - acc: 0.9192 - val_loss: 0.2926 - val_acc: 0.9174\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.3075 - acc: 0.9215 - val_loss: 0.2917 - val_acc: 0.9174\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2952 - acc: 0.9215 - val_loss: 0.2911 - val_acc: 0.9174\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2925 - acc: 0.9192 - val_loss: 0.2909 - val_acc: 0.9174\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.3030 - acc: 0.9192 - val_loss: 0.2905 - val_acc: 0.9174\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2869 - acc: 0.9215 - val_loss: 0.2902 - val_acc: 0.9174\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.3231 - acc: 0.9215 - val_loss: 0.2904 - val_acc: 0.9174\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.3017 - acc: 0.9215 - val_loss: 0.2906 - val_acc: 0.9174\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.3006 - acc: 0.9215 - val_loss: 0.2910 - val_acc: 0.9174\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.3090 - acc: 0.9215 - val_loss: 0.2906 - val_acc: 0.9174\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2792 - acc: 0.9238 - val_loss: 0.2903 - val_acc: 0.9174\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2976 - acc: 0.9215 - val_loss: 0.2897 - val_acc: 0.9174\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2878 - acc: 0.9215 - val_loss: 0.2891 - val_acc: 0.9174\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2882 - acc: 0.9169 - val_loss: 0.2886 - val_acc: 0.9174\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.41237, saving model to best.model\n",
      "0s - loss: 0.6811 - acc: 0.6166 - val_loss: 0.4124 - val_acc: 0.8899\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.41237 to 0.34795, saving model to best.model\n",
      "0s - loss: 0.4210 - acc: 0.8245 - val_loss: 0.3480 - val_acc: 0.8899\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.3502 - acc: 0.9053 - val_loss: 0.3623 - val_acc: 0.8899\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2951 - acc: 0.9261 - val_loss: 0.4006 - val_acc: 0.8899\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2569 - acc: 0.9307 - val_loss: 0.4381 - val_acc: 0.8899\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3035 - acc: 0.9307 - val_loss: 0.4668 - val_acc: 0.8899\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2925 - acc: 0.9307 - val_loss: 0.4848 - val_acc: 0.8899\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3092 - acc: 0.9307 - val_loss: 0.4929 - val_acc: 0.8899\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2951 - acc: 0.9307 - val_loss: 0.4942 - val_acc: 0.8899\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3393 - acc: 0.9307 - val_loss: 0.4888 - val_acc: 0.8899\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2812 - acc: 0.9307 - val_loss: 0.4784 - val_acc: 0.8899\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3223 - acc: 0.9307 - val_loss: 0.4648 - val_acc: 0.8899\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3049 - acc: 0.9307 - val_loss: 0.4491 - val_acc: 0.8899\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2827 - acc: 0.9307 - val_loss: 0.4314 - val_acc: 0.8899\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2949 - acc: 0.9307 - val_loss: 0.4143 - val_acc: 0.8899\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2849 - acc: 0.9307 - val_loss: 0.3996 - val_acc: 0.8899\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2671 - acc: 0.9307 - val_loss: 0.3876 - val_acc: 0.8899\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2807 - acc: 0.9307 - val_loss: 0.3774 - val_acc: 0.8899\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2839 - acc: 0.9307 - val_loss: 0.3705 - val_acc: 0.8899\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3096 - acc: 0.9284 - val_loss: 0.3661 - val_acc: 0.8899\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2613 - acc: 0.9261 - val_loss: 0.3641 - val_acc: 0.8899\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2713 - acc: 0.9330 - val_loss: 0.3638 - val_acc: 0.8899\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2919 - acc: 0.9284 - val_loss: 0.3654 - val_acc: 0.8899\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2964 - acc: 0.9284 - val_loss: 0.3675 - val_acc: 0.8899\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2893 - acc: 0.9307 - val_loss: 0.3703 - val_acc: 0.8899\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2800 - acc: 0.9284 - val_loss: 0.3734 - val_acc: 0.8899\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2890 - acc: 0.9284 - val_loss: 0.3765 - val_acc: 0.8899\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2653 - acc: 0.9307 - val_loss: 0.3788 - val_acc: 0.8899\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.92833, saving model to best.model\n",
      "0s - loss: 1.5628 - acc: 0.1894 - val_loss: 0.9283 - val_acc: 0.0642\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.92833 to 0.49984, saving model to best.model\n",
      "0s - loss: 0.9788 - acc: 0.4249 - val_loss: 0.4998 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.49984 to 0.30389, saving model to best.model\n",
      "0s - loss: 0.6269 - acc: 0.6536 - val_loss: 0.3039 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.30389 to 0.24418, saving model to best.model\n",
      "0s - loss: 0.4083 - acc: 0.8476 - val_loss: 0.2442 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.24418 to 0.24077, saving model to best.model\n",
      "0s - loss: 0.3245 - acc: 0.9030 - val_loss: 0.2408 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2828 - acc: 0.9215 - val_loss: 0.2557 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2977 - acc: 0.9261 - val_loss: 0.2741 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2898 - acc: 0.9284 - val_loss: 0.2903 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3017 - acc: 0.9307 - val_loss: 0.3030 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3205 - acc: 0.9307 - val_loss: 0.3117 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3111 - acc: 0.9307 - val_loss: 0.3168 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3464 - acc: 0.9307 - val_loss: 0.3187 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3351 - acc: 0.9307 - val_loss: 0.3179 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3346 - acc: 0.9307 - val_loss: 0.3151 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3328 - acc: 0.9307 - val_loss: 0.3105 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3195 - acc: 0.9307 - val_loss: 0.3047 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3143 - acc: 0.9307 - val_loss: 0.2980 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3261 - acc: 0.9307 - val_loss: 0.2907 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3138 - acc: 0.9307 - val_loss: 0.2834 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3235 - acc: 0.9307 - val_loss: 0.2758 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3105 - acc: 0.9307 - val_loss: 0.2684 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2930 - acc: 0.9307 - val_loss: 0.2619 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2747 - acc: 0.9307 - val_loss: 0.2562 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.3031 - acc: 0.9307 - val_loss: 0.2515 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2927 - acc: 0.9307 - val_loss: 0.2480 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2835 - acc: 0.9330 - val_loss: 0.2454 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2785 - acc: 0.9261 - val_loss: 0.2436 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2687 - acc: 0.9261 - val_loss: 0.2424 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2961 - acc: 0.9192 - val_loss: 0.2417 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2744 - acc: 0.9307 - val_loss: 0.2413 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2912 - acc: 0.9284 - val_loss: 0.2412 - val_acc: 0.9358\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.43452, saving model to best.model\n",
      "0s - loss: 0.8042 - acc: 0.4965 - val_loss: 0.4345 - val_acc: 0.9358\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.43452 to 0.28759, saving model to best.model\n",
      "0s - loss: 0.4900 - acc: 0.7644 - val_loss: 0.2876 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.28759 to 0.24167, saving model to best.model\n",
      "0s - loss: 0.3565 - acc: 0.8799 - val_loss: 0.2417 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.24167 to 0.23910, saving model to best.model\n",
      "0s - loss: 0.2940 - acc: 0.9261 - val_loss: 0.2391 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2892 - acc: 0.9307 - val_loss: 0.2512 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2579 - acc: 0.9353 - val_loss: 0.2655 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2621 - acc: 0.9353 - val_loss: 0.2778 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2727 - acc: 0.9330 - val_loss: 0.2866 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3146 - acc: 0.9353 - val_loss: 0.2919 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3145 - acc: 0.9353 - val_loss: 0.2938 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2995 - acc: 0.9353 - val_loss: 0.2929 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2970 - acc: 0.9353 - val_loss: 0.2898 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3070 - acc: 0.9353 - val_loss: 0.2848 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2830 - acc: 0.9353 - val_loss: 0.2787 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2983 - acc: 0.9330 - val_loss: 0.2718 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2823 - acc: 0.9353 - val_loss: 0.2652 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2694 - acc: 0.9353 - val_loss: 0.2592 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2564 - acc: 0.9353 - val_loss: 0.2536 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2663 - acc: 0.9353 - val_loss: 0.2489 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2667 - acc: 0.9353 - val_loss: 0.2453 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2854 - acc: 0.9353 - val_loss: 0.2429 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2718 - acc: 0.9353 - val_loss: 0.2410 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2691 - acc: 0.9353 - val_loss: 0.2399 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2798 - acc: 0.9307 - val_loss: 0.2392 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.23910 to 0.23892, saving model to best.model\n",
      "0s - loss: 0.2666 - acc: 0.9353 - val_loss: 0.2389 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.23892 to 0.23892, saving model to best.model\n",
      "0s - loss: 0.2702 - acc: 0.9330 - val_loss: 0.2389 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2602 - acc: 0.9353 - val_loss: 0.2390 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2833 - acc: 0.9330 - val_loss: 0.2393 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2702 - acc: 0.9376 - val_loss: 0.2397 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2638 - acc: 0.9353 - val_loss: 0.2401 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2589 - acc: 0.9307 - val_loss: 0.2408 - val_acc: 0.9358\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2694 - acc: 0.9330 - val_loss: 0.2413 - val_acc: 0.9358\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2776 - acc: 0.9353 - val_loss: 0.2417 - val_acc: 0.9358\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2781 - acc: 0.9353 - val_loss: 0.2420 - val_acc: 0.9358\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2838 - acc: 0.9353 - val_loss: 0.2423 - val_acc: 0.9358\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2757 - acc: 0.9353 - val_loss: 0.2423 - val_acc: 0.9358\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2541 - acc: 0.9353 - val_loss: 0.2422 - val_acc: 0.9358\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2443 - acc: 0.9353 - val_loss: 0.2422 - val_acc: 0.9358\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2863 - acc: 0.9330 - val_loss: 0.2420 - val_acc: 0.9358\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2723 - acc: 0.9353 - val_loss: 0.2419 - val_acc: 0.9358\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2560 - acc: 0.9353 - val_loss: 0.2417 - val_acc: 0.9358\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2819 - acc: 0.9330 - val_loss: 0.2416 - val_acc: 0.9358\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2501 - acc: 0.9353 - val_loss: 0.2414 - val_acc: 0.9358\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2613 - acc: 0.9353 - val_loss: 0.2412 - val_acc: 0.9358\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2519 - acc: 0.9353 - val_loss: 0.2410 - val_acc: 0.9358\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2643 - acc: 0.9353 - val_loss: 0.2408 - val_acc: 0.9358\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2503 - acc: 0.9376 - val_loss: 0.2407 - val_acc: 0.9358\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2602 - acc: 0.9353 - val_loss: 0.2404 - val_acc: 0.9358\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2597 - acc: 0.9353 - val_loss: 0.2403 - val_acc: 0.9358\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2706 - acc: 0.9353 - val_loss: 0.2403 - val_acc: 0.9358\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.2698 - acc: 0.9353 - val_loss: 0.2405 - val_acc: 0.9358\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.2510 - acc: 0.9353 - val_loss: 0.2406 - val_acc: 0.9358\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.24344, saving model to best.model\n",
      "0s - loss: 0.3985 - acc: 0.8314 - val_loss: 0.2434 - val_acc: 0.9450\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.24344 to 0.21387, saving model to best.model\n",
      "0s - loss: 0.3013 - acc: 0.9215 - val_loss: 0.2139 - val_acc: 0.9450\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2535 - acc: 0.9469 - val_loss: 0.2203 - val_acc: 0.9450\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2674 - acc: 0.9446 - val_loss: 0.2349 - val_acc: 0.9450\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2495 - acc: 0.9446 - val_loss: 0.2471 - val_acc: 0.9450\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2527 - acc: 0.9446 - val_loss: 0.2545 - val_acc: 0.9450\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2722 - acc: 0.9446 - val_loss: 0.2569 - val_acc: 0.9450\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2858 - acc: 0.9446 - val_loss: 0.2550 - val_acc: 0.9450\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2634 - acc: 0.9446 - val_loss: 0.2501 - val_acc: 0.9450\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2766 - acc: 0.9446 - val_loss: 0.2430 - val_acc: 0.9450\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2507 - acc: 0.9446 - val_loss: 0.2347 - val_acc: 0.9450\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2468 - acc: 0.9446 - val_loss: 0.2273 - val_acc: 0.9450\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2164 - acc: 0.9446 - val_loss: 0.2212 - val_acc: 0.9450\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2390 - acc: 0.9446 - val_loss: 0.2170 - val_acc: 0.9450\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2314 - acc: 0.9469 - val_loss: 0.2147 - val_acc: 0.9450\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.21387 to 0.21376, saving model to best.model\n",
      "0s - loss: 0.2276 - acc: 0.9446 - val_loss: 0.2138 - val_acc: 0.9450\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss improved from 0.21376 to 0.21346, saving model to best.model\n",
      "0s - loss: 0.2435 - acc: 0.9423 - val_loss: 0.2135 - val_acc: 0.9450\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.21346 to 0.21342, saving model to best.model\n",
      "0s - loss: 0.2321 - acc: 0.9446 - val_loss: 0.2134 - val_acc: 0.9450\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2400 - acc: 0.9446 - val_loss: 0.2135 - val_acc: 0.9450\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2373 - acc: 0.9423 - val_loss: 0.2138 - val_acc: 0.9450\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2437 - acc: 0.9423 - val_loss: 0.2143 - val_acc: 0.9450\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2387 - acc: 0.9446 - val_loss: 0.2151 - val_acc: 0.9450\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2340 - acc: 0.9446 - val_loss: 0.2162 - val_acc: 0.9450\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2411 - acc: 0.9446 - val_loss: 0.2174 - val_acc: 0.9450\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2378 - acc: 0.9446 - val_loss: 0.2183 - val_acc: 0.9450\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2461 - acc: 0.9446 - val_loss: 0.2191 - val_acc: 0.9450\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2419 - acc: 0.9446 - val_loss: 0.2192 - val_acc: 0.9450\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2431 - acc: 0.9446 - val_loss: 0.2189 - val_acc: 0.9450\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2454 - acc: 0.9423 - val_loss: 0.2187 - val_acc: 0.9450\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2304 - acc: 0.9446 - val_loss: 0.2180 - val_acc: 0.9450\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2420 - acc: 0.9446 - val_loss: 0.2170 - val_acc: 0.9450\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2239 - acc: 0.9446 - val_loss: 0.2160 - val_acc: 0.9450\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2098 - acc: 0.9446 - val_loss: 0.2155 - val_acc: 0.9450\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2325 - acc: 0.9446 - val_loss: 0.2150 - val_acc: 0.9450\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2427 - acc: 0.9446 - val_loss: 0.2148 - val_acc: 0.9450\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2216 - acc: 0.9446 - val_loss: 0.2147 - val_acc: 0.9450\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2258 - acc: 0.9446 - val_loss: 0.2149 - val_acc: 0.9450\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2147 - acc: 0.9446 - val_loss: 0.2150 - val_acc: 0.9450\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2273 - acc: 0.9446 - val_loss: 0.2152 - val_acc: 0.9450\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2265 - acc: 0.9446 - val_loss: 0.2152 - val_acc: 0.9450\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2360 - acc: 0.9446 - val_loss: 0.2151 - val_acc: 0.9450\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2171 - acc: 0.9446 - val_loss: 0.2150 - val_acc: 0.9450\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2336 - acc: 0.9446 - val_loss: 0.2149 - val_acc: 0.9450\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2384 - acc: 0.9446 - val_loss: 0.2152 - val_acc: 0.9450\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.36764, saving model to best.model\n",
      "0s - loss: 0.6726 - acc: 0.6328 - val_loss: 0.3676 - val_acc: 0.9358\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.36764 to 0.26316, saving model to best.model\n",
      "0s - loss: 0.4456 - acc: 0.8199 - val_loss: 0.2632 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.26316 to 0.23848, saving model to best.model\n",
      "0s - loss: 0.3020 - acc: 0.9238 - val_loss: 0.2385 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.3216 - acc: 0.9261 - val_loss: 0.2456 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3112 - acc: 0.9284 - val_loss: 0.2606 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2815 - acc: 0.9284 - val_loss: 0.2747 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3249 - acc: 0.9284 - val_loss: 0.2843 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3191 - acc: 0.9284 - val_loss: 0.2893 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3057 - acc: 0.9284 - val_loss: 0.2894 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3295 - acc: 0.9284 - val_loss: 0.2861 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2765 - acc: 0.9284 - val_loss: 0.2800 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3281 - acc: 0.9284 - val_loss: 0.2723 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3080 - acc: 0.9284 - val_loss: 0.2639 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2869 - acc: 0.9284 - val_loss: 0.2558 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3074 - acc: 0.9284 - val_loss: 0.2492 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3212 - acc: 0.9261 - val_loss: 0.2441 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3002 - acc: 0.9284 - val_loss: 0.2405 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2854 - acc: 0.9284 - val_loss: 0.2385 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.23848 to 0.23777, saving model to best.model\n",
      "0s - loss: 0.2779 - acc: 0.9261 - val_loss: 0.2378 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss improved from 0.23777 to 0.23771, saving model to best.model\n",
      "0s - loss: 0.3188 - acc: 0.9192 - val_loss: 0.2377 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2944 - acc: 0.9284 - val_loss: 0.2377 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss improved from 0.23771 to 0.23768, saving model to best.model\n",
      "0s - loss: 0.3022 - acc: 0.9192 - val_loss: 0.2377 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss improved from 0.23768 to 0.23756, saving model to best.model\n",
      "0s - loss: 0.2817 - acc: 0.9261 - val_loss: 0.2376 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2994 - acc: 0.9215 - val_loss: 0.2376 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2823 - acc: 0.9261 - val_loss: 0.2379 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2946 - acc: 0.9284 - val_loss: 0.2385 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2878 - acc: 0.9284 - val_loss: 0.2393 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2887 - acc: 0.9284 - val_loss: 0.2399 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2755 - acc: 0.9284 - val_loss: 0.2404 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2684 - acc: 0.9284 - val_loss: 0.2409 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2801 - acc: 0.9284 - val_loss: 0.2410 - val_acc: 0.9358\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2840 - acc: 0.9284 - val_loss: 0.2409 - val_acc: 0.9358\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2741 - acc: 0.9284 - val_loss: 0.2407 - val_acc: 0.9358\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2715 - acc: 0.9284 - val_loss: 0.2402 - val_acc: 0.9358\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2745 - acc: 0.9284 - val_loss: 0.2395 - val_acc: 0.9358\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2839 - acc: 0.9284 - val_loss: 0.2390 - val_acc: 0.9358\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2732 - acc: 0.9284 - val_loss: 0.2386 - val_acc: 0.9358\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2926 - acc: 0.9284 - val_loss: 0.2383 - val_acc: 0.9358\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2815 - acc: 0.9238 - val_loss: 0.2380 - val_acc: 0.9358\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2794 - acc: 0.9284 - val_loss: 0.2378 - val_acc: 0.9358\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.23756 to 0.23748, saving model to best.model\n",
      "0s - loss: 0.2753 - acc: 0.9284 - val_loss: 0.2375 - val_acc: 0.9358\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.23748 to 0.23730, saving model to best.model\n",
      "0s - loss: 0.2794 - acc: 0.9284 - val_loss: 0.2373 - val_acc: 0.9358\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.23730 to 0.23720, saving model to best.model\n",
      "0s - loss: 0.2766 - acc: 0.9284 - val_loss: 0.2372 - val_acc: 0.9358\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.23720 to 0.23712, saving model to best.model\n",
      "0s - loss: 0.2897 - acc: 0.9284 - val_loss: 0.2371 - val_acc: 0.9358\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.23712 to 0.23703, saving model to best.model\n",
      "0s - loss: 0.2944 - acc: 0.9284 - val_loss: 0.2370 - val_acc: 0.9358\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.23703 to 0.23699, saving model to best.model\n",
      "0s - loss: 0.2779 - acc: 0.9284 - val_loss: 0.2370 - val_acc: 0.9358\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.23699 to 0.23687, saving model to best.model\n",
      "0s - loss: 0.2659 - acc: 0.9261 - val_loss: 0.2369 - val_acc: 0.9358\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.23687 to 0.23678, saving model to best.model\n",
      "0s - loss: 0.3079 - acc: 0.9284 - val_loss: 0.2368 - val_acc: 0.9358\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.23678 to 0.23672, saving model to best.model\n",
      "0s - loss: 0.2805 - acc: 0.9284 - val_loss: 0.2367 - val_acc: 0.9358\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.23672 to 0.23657, saving model to best.model\n",
      "0s - loss: 0.2856 - acc: 0.9284 - val_loss: 0.2366 - val_acc: 0.9358\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.23657 to 0.23646, saving model to best.model\n",
      "0s - loss: 0.2928 - acc: 0.9284 - val_loss: 0.2365 - val_acc: 0.9358\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.23646 to 0.23642, saving model to best.model\n",
      "0s - loss: 0.2823 - acc: 0.9284 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.23642 to 0.23637, saving model to best.model\n",
      "0s - loss: 0.2526 - acc: 0.9284 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.23637 to 0.23635, saving model to best.model\n",
      "0s - loss: 0.2736 - acc: 0.9284 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.2655 - acc: 0.9284 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.2650 - acc: 0.9284 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.2845 - acc: 0.9284 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss did not improve\n",
      "0s - loss: 0.2717 - acc: 0.9284 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss did not improve\n",
      "0s - loss: 0.2755 - acc: 0.9284 - val_loss: 0.2364 - val_acc: 0.9358\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.23635 to 0.23635, saving model to best.model\n",
      "0s - loss: 0.2785 - acc: 0.9284 - val_loss: 0.2363 - val_acc: 0.9358\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.23635 to 0.23620, saving model to best.model\n",
      "0s - loss: 0.2736 - acc: 0.9284 - val_loss: 0.2362 - val_acc: 0.9358\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.23620 to 0.23599, saving model to best.model\n",
      "0s - loss: 0.2678 - acc: 0.9284 - val_loss: 0.2360 - val_acc: 0.9358\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.23599 to 0.23582, saving model to best.model\n",
      "0s - loss: 0.2796 - acc: 0.9284 - val_loss: 0.2358 - val_acc: 0.9358\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.23582 to 0.23566, saving model to best.model\n",
      "0s - loss: 0.2759 - acc: 0.9284 - val_loss: 0.2357 - val_acc: 0.9358\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.23566 to 0.23556, saving model to best.model\n",
      "0s - loss: 0.2710 - acc: 0.9284 - val_loss: 0.2356 - val_acc: 0.9358\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.23556 to 0.23551, saving model to best.model\n",
      "0s - loss: 0.2695 - acc: 0.9284 - val_loss: 0.2355 - val_acc: 0.9358\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.23551 to 0.23544, saving model to best.model\n",
      "0s - loss: 0.2536 - acc: 0.9284 - val_loss: 0.2354 - val_acc: 0.9358\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.23544 to 0.23538, saving model to best.model\n",
      "0s - loss: 0.2678 - acc: 0.9284 - val_loss: 0.2354 - val_acc: 0.9358\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.23538 to 0.23528, saving model to best.model\n",
      "0s - loss: 0.2683 - acc: 0.9284 - val_loss: 0.2353 - val_acc: 0.9358\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.23528 to 0.23521, saving model to best.model\n",
      "0s - loss: 0.2548 - acc: 0.9284 - val_loss: 0.2352 - val_acc: 0.9358\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.23521 to 0.23516, saving model to best.model\n",
      "0s - loss: 0.2748 - acc: 0.9284 - val_loss: 0.2352 - val_acc: 0.9358\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.23516 to 0.23512, saving model to best.model\n",
      "0s - loss: 0.2661 - acc: 0.9284 - val_loss: 0.2351 - val_acc: 0.9358\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.23512 to 0.23504, saving model to best.model\n",
      "0s - loss: 0.2754 - acc: 0.9284 - val_loss: 0.2350 - val_acc: 0.9358\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.23504 to 0.23497, saving model to best.model\n",
      "0s - loss: 0.2727 - acc: 0.9284 - val_loss: 0.2350 - val_acc: 0.9358\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.23497 to 0.23490, saving model to best.model\n",
      "0s - loss: 0.2749 - acc: 0.9284 - val_loss: 0.2349 - val_acc: 0.9358\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.23490 to 0.23481, saving model to best.model\n",
      "0s - loss: 0.2706 - acc: 0.9284 - val_loss: 0.2348 - val_acc: 0.9358\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.23481 to 0.23480, saving model to best.model\n",
      "0s - loss: 0.2689 - acc: 0.9284 - val_loss: 0.2348 - val_acc: 0.9358\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.23480 to 0.23474, saving model to best.model\n",
      "0s - loss: 0.2759 - acc: 0.9284 - val_loss: 0.2347 - val_acc: 0.9358\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.23474 to 0.23454, saving model to best.model\n",
      "0s - loss: 0.2675 - acc: 0.9284 - val_loss: 0.2345 - val_acc: 0.9358\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.23454 to 0.23435, saving model to best.model\n",
      "0s - loss: 0.2668 - acc: 0.9284 - val_loss: 0.2344 - val_acc: 0.9358\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.23435 to 0.23421, saving model to best.model\n",
      "0s - loss: 0.2656 - acc: 0.9284 - val_loss: 0.2342 - val_acc: 0.9358\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.23421 to 0.23408, saving model to best.model\n",
      "0s - loss: 0.2889 - acc: 0.9284 - val_loss: 0.2341 - val_acc: 0.9358\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.23408 to 0.23393, saving model to best.model\n",
      "0s - loss: 0.2755 - acc: 0.9284 - val_loss: 0.2339 - val_acc: 0.9358\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.23393 to 0.23380, saving model to best.model\n",
      "0s - loss: 0.2627 - acc: 0.9284 - val_loss: 0.2338 - val_acc: 0.9358\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.23380 to 0.23365, saving model to best.model\n",
      "0s - loss: 0.2703 - acc: 0.9284 - val_loss: 0.2337 - val_acc: 0.9358\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.23365 to 0.23351, saving model to best.model\n",
      "0s - loss: 0.2543 - acc: 0.9284 - val_loss: 0.2335 - val_acc: 0.9358\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.23351 to 0.23342, saving model to best.model\n",
      "0s - loss: 0.2624 - acc: 0.9284 - val_loss: 0.2334 - val_acc: 0.9358\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.23342 to 0.23330, saving model to best.model\n",
      "0s - loss: 0.2493 - acc: 0.9284 - val_loss: 0.2333 - val_acc: 0.9358\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.23330 to 0.23324, saving model to best.model\n",
      "0s - loss: 0.2594 - acc: 0.9284 - val_loss: 0.2332 - val_acc: 0.9358\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.23324 to 0.23315, saving model to best.model\n",
      "0s - loss: 0.2479 - acc: 0.9284 - val_loss: 0.2332 - val_acc: 0.9358\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.23315 to 0.23297, saving model to best.model\n",
      "0s - loss: 0.2719 - acc: 0.9284 - val_loss: 0.2330 - val_acc: 0.9358\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.23297 to 0.23278, saving model to best.model\n",
      "0s - loss: 0.2738 - acc: 0.9284 - val_loss: 0.2328 - val_acc: 0.9358\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.23278 to 0.23257, saving model to best.model\n",
      "0s - loss: 0.2723 - acc: 0.9284 - val_loss: 0.2326 - val_acc: 0.9358\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.23257 to 0.23229, saving model to best.model\n",
      "0s - loss: 0.2648 - acc: 0.9284 - val_loss: 0.2323 - val_acc: 0.9358\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.23229 to 0.23196, saving model to best.model\n",
      "0s - loss: 0.2724 - acc: 0.9284 - val_loss: 0.2320 - val_acc: 0.9358\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.23196 to 0.23168, saving model to best.model\n",
      "0s - loss: 0.2495 - acc: 0.9284 - val_loss: 0.2317 - val_acc: 0.9358\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.23168 to 0.23140, saving model to best.model\n",
      "0s - loss: 0.2572 - acc: 0.9284 - val_loss: 0.2314 - val_acc: 0.9358\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.23140 to 0.23110, saving model to best.model\n",
      "0s - loss: 0.2693 - acc: 0.9284 - val_loss: 0.2311 - val_acc: 0.9358\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.23110 to 0.23086, saving model to best.model\n",
      "0s - loss: 0.2611 - acc: 0.9284 - val_loss: 0.2309 - val_acc: 0.9358\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.23086 to 0.23061, saving model to best.model\n",
      "0s - loss: 0.2590 - acc: 0.9284 - val_loss: 0.2306 - val_acc: 0.9358\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.23061 to 0.23037, saving model to best.model\n",
      "0s - loss: 0.2626 - acc: 0.9284 - val_loss: 0.2304 - val_acc: 0.9358\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.23037 to 0.23008, saving model to best.model\n",
      "0s - loss: 0.2563 - acc: 0.9284 - val_loss: 0.2301 - val_acc: 0.9358\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.23008 to 0.22977, saving model to best.model\n",
      "0s - loss: 0.2656 - acc: 0.9284 - val_loss: 0.2298 - val_acc: 0.9358\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.22977 to 0.22942, saving model to best.model\n",
      "0s - loss: 0.2528 - acc: 0.9284 - val_loss: 0.2294 - val_acc: 0.9358\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.22942 to 0.22906, saving model to best.model\n",
      "0s - loss: 0.2784 - acc: 0.9284 - val_loss: 0.2291 - val_acc: 0.9358\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.22906 to 0.22875, saving model to best.model\n",
      "0s - loss: 0.2571 - acc: 0.9284 - val_loss: 0.2287 - val_acc: 0.9358\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.22875 to 0.22839, saving model to best.model\n",
      "0s - loss: 0.2700 - acc: 0.9284 - val_loss: 0.2284 - val_acc: 0.9358\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.22839 to 0.22798, saving model to best.model\n",
      "0s - loss: 0.2681 - acc: 0.9284 - val_loss: 0.2280 - val_acc: 0.9358\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.22798 to 0.22752, saving model to best.model\n",
      "0s - loss: 0.2565 - acc: 0.9284 - val_loss: 0.2275 - val_acc: 0.9358\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.22752 to 0.22707, saving model to best.model\n",
      "0s - loss: 0.2611 - acc: 0.9284 - val_loss: 0.2271 - val_acc: 0.9358\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.22707 to 0.22663, saving model to best.model\n",
      "0s - loss: 0.2601 - acc: 0.9284 - val_loss: 0.2266 - val_acc: 0.9358\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.22663 to 0.22619, saving model to best.model\n",
      "0s - loss: 0.2574 - acc: 0.9284 - val_loss: 0.2262 - val_acc: 0.9358\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.22619 to 0.22572, saving model to best.model\n",
      "0s - loss: 0.2531 - acc: 0.9284 - val_loss: 0.2257 - val_acc: 0.9358\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.22572 to 0.22517, saving model to best.model\n",
      "0s - loss: 0.2535 - acc: 0.9284 - val_loss: 0.2252 - val_acc: 0.9358\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.22517 to 0.22462, saving model to best.model\n",
      "0s - loss: 0.2574 - acc: 0.9284 - val_loss: 0.2246 - val_acc: 0.9358\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.22462 to 0.22406, saving model to best.model\n",
      "0s - loss: 0.2598 - acc: 0.9284 - val_loss: 0.2241 - val_acc: 0.9358\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.22406 to 0.22344, saving model to best.model\n",
      "0s - loss: 0.2619 - acc: 0.9284 - val_loss: 0.2234 - val_acc: 0.9358\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.22344 to 0.22282, saving model to best.model\n",
      "0s - loss: 0.2594 - acc: 0.9284 - val_loss: 0.2228 - val_acc: 0.9358\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.22282 to 0.22222, saving model to best.model\n",
      "0s - loss: 0.2589 - acc: 0.9284 - val_loss: 0.2222 - val_acc: 0.9358\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.22222 to 0.22162, saving model to best.model\n",
      "0s - loss: 0.2579 - acc: 0.9284 - val_loss: 0.2216 - val_acc: 0.9358\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.22162 to 0.22097, saving model to best.model\n",
      "0s - loss: 0.2538 - acc: 0.9284 - val_loss: 0.2210 - val_acc: 0.9358\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.22097 to 0.22031, saving model to best.model\n",
      "0s - loss: 0.2610 - acc: 0.9284 - val_loss: 0.2203 - val_acc: 0.9358\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.22031 to 0.21968, saving model to best.model\n",
      "0s - loss: 0.2543 - acc: 0.9284 - val_loss: 0.2197 - val_acc: 0.9358\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.21968 to 0.21902, saving model to best.model\n",
      "0s - loss: 0.2430 - acc: 0.9284 - val_loss: 0.2190 - val_acc: 0.9358\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.21902 to 0.21823, saving model to best.model\n",
      "0s - loss: 0.2525 - acc: 0.9284 - val_loss: 0.2182 - val_acc: 0.9358\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.21823 to 0.21744, saving model to best.model\n",
      "0s - loss: 0.2616 - acc: 0.9284 - val_loss: 0.2174 - val_acc: 0.9358\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.21744 to 0.21663, saving model to best.model\n",
      "0s - loss: 0.2437 - acc: 0.9284 - val_loss: 0.2166 - val_acc: 0.9358\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.21663 to 0.21573, saving model to best.model\n",
      "0s - loss: 0.2499 - acc: 0.9284 - val_loss: 0.2157 - val_acc: 0.9358\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.21573 to 0.21473, saving model to best.model\n",
      "0s - loss: 0.2449 - acc: 0.9284 - val_loss: 0.2147 - val_acc: 0.9358\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.21473 to 0.21368, saving model to best.model\n",
      "0s - loss: 0.2551 - acc: 0.9284 - val_loss: 0.2137 - val_acc: 0.9358\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.21368 to 0.21260, saving model to best.model\n",
      "0s - loss: 0.2431 - acc: 0.9284 - val_loss: 0.2126 - val_acc: 0.9358\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.21260 to 0.21145, saving model to best.model\n",
      "0s - loss: 0.2444 - acc: 0.9284 - val_loss: 0.2115 - val_acc: 0.9358\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.21145 to 0.21025, saving model to best.model\n",
      "0s - loss: 0.2414 - acc: 0.9284 - val_loss: 0.2102 - val_acc: 0.9358\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.21025 to 0.20908, saving model to best.model\n",
      "0s - loss: 0.2464 - acc: 0.9284 - val_loss: 0.2091 - val_acc: 0.9358\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.20908 to 0.20780, saving model to best.model\n",
      "0s - loss: 0.2365 - acc: 0.9284 - val_loss: 0.2078 - val_acc: 0.9358\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.20780 to 0.20657, saving model to best.model\n",
      "0s - loss: 0.2419 - acc: 0.9284 - val_loss: 0.2066 - val_acc: 0.9358\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.20657 to 0.20529, saving model to best.model\n",
      "0s - loss: 0.2393 - acc: 0.9284 - val_loss: 0.2053 - val_acc: 0.9358\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.20529 to 0.20395, saving model to best.model\n",
      "0s - loss: 0.2327 - acc: 0.9284 - val_loss: 0.2040 - val_acc: 0.9358\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.20395 to 0.20259, saving model to best.model\n",
      "0s - loss: 0.2474 - acc: 0.9284 - val_loss: 0.2026 - val_acc: 0.9358\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.20259 to 0.20115, saving model to best.model\n",
      "0s - loss: 0.2292 - acc: 0.9284 - val_loss: 0.2011 - val_acc: 0.9358\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.20115 to 0.19962, saving model to best.model\n",
      "0s - loss: 0.2361 - acc: 0.9284 - val_loss: 0.1996 - val_acc: 0.9358\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.19962 to 0.19813, saving model to best.model\n",
      "0s - loss: 0.2401 - acc: 0.9284 - val_loss: 0.1981 - val_acc: 0.9358\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.19813 to 0.19655, saving model to best.model\n",
      "0s - loss: 0.2371 - acc: 0.9284 - val_loss: 0.1965 - val_acc: 0.9358\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.19655 to 0.19471, saving model to best.model\n",
      "0s - loss: 0.2325 - acc: 0.9284 - val_loss: 0.1947 - val_acc: 0.9358\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19471 to 0.19306, saving model to best.model\n",
      "0s - loss: 0.2294 - acc: 0.9284 - val_loss: 0.1931 - val_acc: 0.9358\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.19306 to 0.19129, saving model to best.model\n",
      "0s - loss: 0.2350 - acc: 0.9284 - val_loss: 0.1913 - val_acc: 0.9358\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.19129 to 0.18948, saving model to best.model\n",
      "0s - loss: 0.2306 - acc: 0.9284 - val_loss: 0.1895 - val_acc: 0.9358\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.18948 to 0.18758, saving model to best.model\n",
      "0s - loss: 0.2290 - acc: 0.9284 - val_loss: 0.1876 - val_acc: 0.9358\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.18758 to 0.18581, saving model to best.model\n",
      "0s - loss: 0.2274 - acc: 0.9284 - val_loss: 0.1858 - val_acc: 0.9358\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.18581 to 0.18392, saving model to best.model\n",
      "0s - loss: 0.2252 - acc: 0.9284 - val_loss: 0.1839 - val_acc: 0.9358\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.18392 to 0.18202, saving model to best.model\n",
      "0s - loss: 0.2406 - acc: 0.9284 - val_loss: 0.1820 - val_acc: 0.9358\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.18202 to 0.17970, saving model to best.model\n",
      "0s - loss: 0.2186 - acc: 0.9284 - val_loss: 0.1797 - val_acc: 0.9358\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.17970 to 0.17758, saving model to best.model\n",
      "0s - loss: 0.2245 - acc: 0.9284 - val_loss: 0.1776 - val_acc: 0.9358\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.17758 to 0.17572, saving model to best.model\n",
      "0s - loss: 0.2140 - acc: 0.9284 - val_loss: 0.1757 - val_acc: 0.9358\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.17572 to 0.17402, saving model to best.model\n",
      "0s - loss: 0.2135 - acc: 0.9284 - val_loss: 0.1740 - val_acc: 0.9358\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.17402 to 0.17202, saving model to best.model\n",
      "0s - loss: 0.2081 - acc: 0.9284 - val_loss: 0.1720 - val_acc: 0.9358\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.17202 to 0.16963, saving model to best.model\n",
      "0s - loss: 0.1985 - acc: 0.9307 - val_loss: 0.1696 - val_acc: 0.9358\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.16963 to 0.16619, saving model to best.model\n",
      "0s - loss: 0.1960 - acc: 0.9284 - val_loss: 0.1662 - val_acc: 0.9358\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.16619 to 0.16301, saving model to best.model\n",
      "0s - loss: 0.2086 - acc: 0.9307 - val_loss: 0.1630 - val_acc: 0.9358\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.16301 to 0.16017, saving model to best.model\n",
      "0s - loss: 0.2046 - acc: 0.9261 - val_loss: 0.1602 - val_acc: 0.9358\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.16017 to 0.15816, saving model to best.model\n",
      "0s - loss: 0.1970 - acc: 0.9307 - val_loss: 0.1582 - val_acc: 0.9358\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.15816 to 0.15698, saving model to best.model\n",
      "0s - loss: 0.1861 - acc: 0.9330 - val_loss: 0.1570 - val_acc: 0.9358\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.15698 to 0.15663, saving model to best.model\n",
      "0s - loss: 0.1857 - acc: 0.9330 - val_loss: 0.1566 - val_acc: 0.9358\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.15663 to 0.15321, saving model to best.model\n",
      "0s - loss: 0.1876 - acc: 0.9330 - val_loss: 0.1532 - val_acc: 0.9358\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.15321 to 0.14866, saving model to best.model\n",
      "0s - loss: 0.1957 - acc: 0.9307 - val_loss: 0.1487 - val_acc: 0.9358\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.14866 to 0.14518, saving model to best.model\n",
      "0s - loss: 0.1846 - acc: 0.9376 - val_loss: 0.1452 - val_acc: 0.9358\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.14518 to 0.14161, saving model to best.model\n",
      "0s - loss: 0.1780 - acc: 0.9400 - val_loss: 0.1416 - val_acc: 0.9358\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.14161 to 0.13838, saving model to best.model\n",
      "0s - loss: 0.1790 - acc: 0.9353 - val_loss: 0.1384 - val_acc: 0.9358\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.13838 to 0.13538, saving model to best.model\n",
      "0s - loss: 0.1938 - acc: 0.9330 - val_loss: 0.1354 - val_acc: 0.9358\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.13538 to 0.13343, saving model to best.model\n",
      "0s - loss: 0.1744 - acc: 0.9376 - val_loss: 0.1334 - val_acc: 0.9358\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.13343 to 0.13242, saving model to best.model\n",
      "0s - loss: 0.1859 - acc: 0.9330 - val_loss: 0.1324 - val_acc: 0.9358\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.13242 to 0.13061, saving model to best.model\n",
      "0s - loss: 0.1717 - acc: 0.9376 - val_loss: 0.1306 - val_acc: 0.9358\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.13061 to 0.12712, saving model to best.model\n",
      "0s - loss: 0.1755 - acc: 0.9376 - val_loss: 0.1271 - val_acc: 0.9358\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.12712 to 0.12363, saving model to best.model\n",
      "0s - loss: 0.1806 - acc: 0.9423 - val_loss: 0.1236 - val_acc: 0.9358\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.12363 to 0.12116, saving model to best.model\n",
      "0s - loss: 0.1746 - acc: 0.9376 - val_loss: 0.1212 - val_acc: 0.9358\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.12116 to 0.11962, saving model to best.model\n",
      "0s - loss: 0.1738 - acc: 0.9400 - val_loss: 0.1196 - val_acc: 0.9358\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.11962 to 0.11855, saving model to best.model\n",
      "0s - loss: 0.1926 - acc: 0.9284 - val_loss: 0.1185 - val_acc: 0.9358\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.11855 to 0.11816, saving model to best.model\n",
      "0s - loss: 0.1696 - acc: 0.9376 - val_loss: 0.1182 - val_acc: 0.9358\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.11816 to 0.11786, saving model to best.model\n",
      "0s - loss: 0.1779 - acc: 0.9353 - val_loss: 0.1179 - val_acc: 0.9358\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.11786 to 0.11697, saving model to best.model\n",
      "0s - loss: 0.1502 - acc: 0.9538 - val_loss: 0.1170 - val_acc: 0.9358\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.11697 to 0.11416, saving model to best.model\n",
      "0s - loss: 0.1790 - acc: 0.9353 - val_loss: 0.1142 - val_acc: 0.9541\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.11416 to 0.10964, saving model to best.model\n",
      "0s - loss: 0.1534 - acc: 0.9423 - val_loss: 0.1096 - val_acc: 0.9633\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.10964 to 0.10737, saving model to best.model\n",
      "0s - loss: 0.1693 - acc: 0.9353 - val_loss: 0.1074 - val_acc: 0.9725\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.10737 to 0.10644, saving model to best.model\n",
      "0s - loss: 0.1451 - acc: 0.9423 - val_loss: 0.1064 - val_acc: 0.9725\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.1602 - acc: 0.9330 - val_loss: 0.1083 - val_acc: 0.9541\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.1815 - acc: 0.9330 - val_loss: 0.1083 - val_acc: 0.9541\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.10644 to 0.10547, saving model to best.model\n",
      "0s - loss: 0.1513 - acc: 0.9423 - val_loss: 0.1055 - val_acc: 0.9633\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.10547 to 0.10198, saving model to best.model\n",
      "0s - loss: 0.1618 - acc: 0.9353 - val_loss: 0.1020 - val_acc: 0.9633\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss did not improve\n",
      "0s - loss: 0.1460 - acc: 0.9469 - val_loss: 0.1020 - val_acc: 0.9633\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss did not improve\n",
      "0s - loss: 0.1665 - acc: 0.9423 - val_loss: 0.1029 - val_acc: 0.9633\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss did not improve\n",
      "0s - loss: 0.1679 - acc: 0.9423 - val_loss: 0.1037 - val_acc: 0.9725\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss did not improve\n",
      "0s - loss: 0.1776 - acc: 0.9284 - val_loss: 0.1044 - val_acc: 0.9633\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.10198 to 0.10177, saving model to best.model\n",
      "0s - loss: 0.1532 - acc: 0.9561 - val_loss: 0.1018 - val_acc: 0.9633\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.10177 to 0.09899, saving model to best.model\n",
      "0s - loss: 0.1420 - acc: 0.9423 - val_loss: 0.0990 - val_acc: 0.9633\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.09899 to 0.09783, saving model to best.model\n",
      "0s - loss: 0.1595 - acc: 0.9353 - val_loss: 0.0978 - val_acc: 0.9633\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.09783 to 0.09748, saving model to best.model\n",
      "0s - loss: 0.1525 - acc: 0.9446 - val_loss: 0.0975 - val_acc: 0.9633\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.09748 to 0.09737, saving model to best.model\n",
      "0s - loss: 0.1649 - acc: 0.9353 - val_loss: 0.0974 - val_acc: 0.9633\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.09737 to 0.09712, saving model to best.model\n",
      "0s - loss: 0.1571 - acc: 0.9446 - val_loss: 0.0971 - val_acc: 0.9633\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.09712 to 0.09708, saving model to best.model\n",
      "0s - loss: 0.1547 - acc: 0.9469 - val_loss: 0.0971 - val_acc: 0.9633\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.09708 to 0.09563, saving model to best.model\n",
      "0s - loss: 0.1447 - acc: 0.9400 - val_loss: 0.0956 - val_acc: 0.9633\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.29669, saving model to best.model\n",
      "0s - loss: 0.4321 - acc: 0.8014 - val_loss: 0.2967 - val_acc: 0.9174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.29669 to 0.29055, saving model to best.model\n",
      "0s - loss: 0.3135 - acc: 0.9169 - val_loss: 0.2906 - val_acc: 0.9174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2542 - acc: 0.9330 - val_loss: 0.3208 - val_acc: 0.9174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2475 - acc: 0.9469 - val_loss: 0.3550 - val_acc: 0.9174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2487 - acc: 0.9469 - val_loss: 0.3816 - val_acc: 0.9174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2486 - acc: 0.9469 - val_loss: 0.3980 - val_acc: 0.9174\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2522 - acc: 0.9469 - val_loss: 0.4060 - val_acc: 0.9174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2535 - acc: 0.9469 - val_loss: 0.4056 - val_acc: 0.9174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2688 - acc: 0.9469 - val_loss: 0.3996 - val_acc: 0.9174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2782 - acc: 0.9469 - val_loss: 0.3882 - val_acc: 0.9174\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2574 - acc: 0.9469 - val_loss: 0.3733 - val_acc: 0.9174\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2449 - acc: 0.9469 - val_loss: 0.3576 - val_acc: 0.9174\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2371 - acc: 0.9469 - val_loss: 0.3425 - val_acc: 0.9174\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2471 - acc: 0.9469 - val_loss: 0.3284 - val_acc: 0.9174\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2097 - acc: 0.9469 - val_loss: 0.3166 - val_acc: 0.9174\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2268 - acc: 0.9400 - val_loss: 0.3084 - val_acc: 0.9174\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2418 - acc: 0.9400 - val_loss: 0.3033 - val_acc: 0.9174\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2244 - acc: 0.9423 - val_loss: 0.3006 - val_acc: 0.9174\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2440 - acc: 0.9446 - val_loss: 0.3005 - val_acc: 0.9174\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2210 - acc: 0.9469 - val_loss: 0.3019 - val_acc: 0.9174\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2296 - acc: 0.9446 - val_loss: 0.3051 - val_acc: 0.9174\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2387 - acc: 0.9469 - val_loss: 0.3091 - val_acc: 0.9174\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2580 - acc: 0.9423 - val_loss: 0.3138 - val_acc: 0.9174\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2247 - acc: 0.9469 - val_loss: 0.3179 - val_acc: 0.9174\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2404 - acc: 0.9469 - val_loss: 0.3212 - val_acc: 0.9174\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2359 - acc: 0.9469 - val_loss: 0.3246 - val_acc: 0.9174\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2394 - acc: 0.9469 - val_loss: 0.3258 - val_acc: 0.9174\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2182 - acc: 0.9469 - val_loss: 0.3251 - val_acc: 0.9174\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.47965, saving model to best.model\n",
      "0s - loss: 0.9713 - acc: 0.4203 - val_loss: 0.4797 - val_acc: 0.9358\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.47965 to 0.29347, saving model to best.model\n",
      "0s - loss: 0.6127 - acc: 0.6605 - val_loss: 0.2935 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.29347 to 0.24131, saving model to best.model\n",
      "0s - loss: 0.3654 - acc: 0.8661 - val_loss: 0.2413 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.3064 - acc: 0.9145 - val_loss: 0.2429 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2781 - acc: 0.9353 - val_loss: 0.2606 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2810 - acc: 0.9330 - val_loss: 0.2800 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2963 - acc: 0.9376 - val_loss: 0.2959 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2858 - acc: 0.9376 - val_loss: 0.3071 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2798 - acc: 0.9376 - val_loss: 0.3143 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3295 - acc: 0.9376 - val_loss: 0.3171 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3204 - acc: 0.9376 - val_loss: 0.3163 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3036 - acc: 0.9376 - val_loss: 0.3131 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3116 - acc: 0.9376 - val_loss: 0.3083 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2838 - acc: 0.9376 - val_loss: 0.3013 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2978 - acc: 0.9376 - val_loss: 0.2934 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2875 - acc: 0.9376 - val_loss: 0.2851 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2836 - acc: 0.9376 - val_loss: 0.2763 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3023 - acc: 0.9353 - val_loss: 0.2681 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2694 - acc: 0.9376 - val_loss: 0.2604 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2618 - acc: 0.9376 - val_loss: 0.2541 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3006 - acc: 0.9330 - val_loss: 0.2495 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2380 - acc: 0.9376 - val_loss: 0.2460 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2685 - acc: 0.9353 - val_loss: 0.2434 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2716 - acc: 0.9353 - val_loss: 0.2420 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2617 - acc: 0.9330 - val_loss: 0.2414 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.24131 to 0.24099, saving model to best.model\n",
      "0s - loss: 0.2694 - acc: 0.9353 - val_loss: 0.2410 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2613 - acc: 0.9284 - val_loss: 0.2411 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2615 - acc: 0.9353 - val_loss: 0.2413 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2632 - acc: 0.9376 - val_loss: 0.2418 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2724 - acc: 0.9400 - val_loss: 0.2426 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2663 - acc: 0.9376 - val_loss: 0.2432 - val_acc: 0.9358\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2499 - acc: 0.9353 - val_loss: 0.2440 - val_acc: 0.9358\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2695 - acc: 0.9353 - val_loss: 0.2447 - val_acc: 0.9358\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2513 - acc: 0.9376 - val_loss: 0.2451 - val_acc: 0.9358\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2819 - acc: 0.9376 - val_loss: 0.2452 - val_acc: 0.9358\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2693 - acc: 0.9400 - val_loss: 0.2457 - val_acc: 0.9358\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2352 - acc: 0.9376 - val_loss: 0.2458 - val_acc: 0.9358\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2610 - acc: 0.9353 - val_loss: 0.2460 - val_acc: 0.9358\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2576 - acc: 0.9353 - val_loss: 0.2460 - val_acc: 0.9358\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2588 - acc: 0.9353 - val_loss: 0.2459 - val_acc: 0.9358\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2585 - acc: 0.9376 - val_loss: 0.2458 - val_acc: 0.9358\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2868 - acc: 0.9376 - val_loss: 0.2456 - val_acc: 0.9358\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2677 - acc: 0.9376 - val_loss: 0.2453 - val_acc: 0.9358\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2628 - acc: 0.9353 - val_loss: 0.2449 - val_acc: 0.9358\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2440 - acc: 0.9353 - val_loss: 0.2447 - val_acc: 0.9358\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2769 - acc: 0.9376 - val_loss: 0.2447 - val_acc: 0.9358\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2733 - acc: 0.9376 - val_loss: 0.2445 - val_acc: 0.9358\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2244 - acc: 0.9376 - val_loss: 0.2442 - val_acc: 0.9358\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2598 - acc: 0.9376 - val_loss: 0.2437 - val_acc: 0.9358\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2611 - acc: 0.9376 - val_loss: 0.2432 - val_acc: 0.9358\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.2812 - acc: 0.9353 - val_loss: 0.2427 - val_acc: 0.9358\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.2626 - acc: 0.9353 - val_loss: 0.2427 - val_acc: 0.9358\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.29994, saving model to best.model\n",
      "0s - loss: 0.3163 - acc: 0.8938 - val_loss: 0.2999 - val_acc: 0.9174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.2994 - acc: 0.9284 - val_loss: 0.3177 - val_acc: 0.9174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.3164 - acc: 0.9261 - val_loss: 0.3215 - val_acc: 0.9174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2773 - acc: 0.9284 - val_loss: 0.3149 - val_acc: 0.9174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3048 - acc: 0.9284 - val_loss: 0.3054 - val_acc: 0.9174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.29994 to 0.29683, saving model to best.model\n",
      "0s - loss: 0.3114 - acc: 0.9215 - val_loss: 0.2968 - val_acc: 0.9174\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.29683 to 0.29155, saving model to best.model\n",
      "0s - loss: 0.3107 - acc: 0.9215 - val_loss: 0.2915 - val_acc: 0.9174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss improved from 0.29155 to 0.28918, saving model to best.model\n",
      "0s - loss: 0.2908 - acc: 0.9215 - val_loss: 0.2892 - val_acc: 0.9174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2965 - acc: 0.9261 - val_loss: 0.2895 - val_acc: 0.9174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2926 - acc: 0.9215 - val_loss: 0.2903 - val_acc: 0.9174\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2880 - acc: 0.9238 - val_loss: 0.2928 - val_acc: 0.9174\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2879 - acc: 0.9284 - val_loss: 0.2952 - val_acc: 0.9174\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2999 - acc: 0.9284 - val_loss: 0.2972 - val_acc: 0.9174\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2995 - acc: 0.9261 - val_loss: 0.2988 - val_acc: 0.9174\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3026 - acc: 0.9238 - val_loss: 0.2978 - val_acc: 0.9174\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2818 - acc: 0.9261 - val_loss: 0.2971 - val_acc: 0.9174\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2876 - acc: 0.9284 - val_loss: 0.2960 - val_acc: 0.9174\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3089 - acc: 0.9284 - val_loss: 0.2928 - val_acc: 0.9174\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3001 - acc: 0.9284 - val_loss: 0.2917 - val_acc: 0.9174\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2779 - acc: 0.9284 - val_loss: 0.2909 - val_acc: 0.9174\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3186 - acc: 0.9284 - val_loss: 0.2901 - val_acc: 0.9174\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2913 - acc: 0.9261 - val_loss: 0.2905 - val_acc: 0.9174\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2790 - acc: 0.9284 - val_loss: 0.2919 - val_acc: 0.9174\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2767 - acc: 0.9284 - val_loss: 0.2937 - val_acc: 0.9174\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2858 - acc: 0.9284 - val_loss: 0.2941 - val_acc: 0.9174\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2782 - acc: 0.9284 - val_loss: 0.2936 - val_acc: 0.9174\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2703 - acc: 0.9284 - val_loss: 0.2932 - val_acc: 0.9174\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2856 - acc: 0.9284 - val_loss: 0.2922 - val_acc: 0.9174\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2845 - acc: 0.9284 - val_loss: 0.2905 - val_acc: 0.9174\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.28918 to 0.28910, saving model to best.model\n",
      "0s - loss: 0.2908 - acc: 0.9284 - val_loss: 0.2891 - val_acc: 0.9174\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.28910 to 0.28735, saving model to best.model\n",
      "0s - loss: 0.2742 - acc: 0.9284 - val_loss: 0.2873 - val_acc: 0.9174\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.28735 to 0.28636, saving model to best.model\n",
      "0s - loss: 0.2707 - acc: 0.9284 - val_loss: 0.2864 - val_acc: 0.9174\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.28636 to 0.28628, saving model to best.model\n",
      "0s - loss: 0.2723 - acc: 0.9284 - val_loss: 0.2863 - val_acc: 0.9174\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2656 - acc: 0.9284 - val_loss: 0.2867 - val_acc: 0.9174\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2785 - acc: 0.9284 - val_loss: 0.2867 - val_acc: 0.9174\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2676 - acc: 0.9284 - val_loss: 0.2871 - val_acc: 0.9174\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2684 - acc: 0.9284 - val_loss: 0.2870 - val_acc: 0.9174\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2582 - acc: 0.9284 - val_loss: 0.2877 - val_acc: 0.9174\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2644 - acc: 0.9284 - val_loss: 0.2883 - val_acc: 0.9174\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2811 - acc: 0.9284 - val_loss: 0.2886 - val_acc: 0.9174\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2708 - acc: 0.9284 - val_loss: 0.2878 - val_acc: 0.9174\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2814 - acc: 0.9284 - val_loss: 0.2870 - val_acc: 0.9174\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.28628 to 0.28627, saving model to best.model\n",
      "0s - loss: 0.2447 - acc: 0.9284 - val_loss: 0.2863 - val_acc: 0.9174\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2885 - acc: 0.9284 - val_loss: 0.2863 - val_acc: 0.9174\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.28627 to 0.28554, saving model to best.model\n",
      "0s - loss: 0.2600 - acc: 0.9284 - val_loss: 0.2855 - val_acc: 0.9174\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.28554 to 0.28515, saving model to best.model\n",
      "0s - loss: 0.2454 - acc: 0.9284 - val_loss: 0.2851 - val_acc: 0.9174\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.28515 to 0.28493, saving model to best.model\n",
      "0s - loss: 0.2828 - acc: 0.9284 - val_loss: 0.2849 - val_acc: 0.9174\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.28493 to 0.28454, saving model to best.model\n",
      "0s - loss: 0.2656 - acc: 0.9284 - val_loss: 0.2845 - val_acc: 0.9174\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.28454 to 0.28395, saving model to best.model\n",
      "0s - loss: 0.2693 - acc: 0.9284 - val_loss: 0.2840 - val_acc: 0.9174\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.28395 to 0.28373, saving model to best.model\n",
      "0s - loss: 0.2697 - acc: 0.9284 - val_loss: 0.2837 - val_acc: 0.9174\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.28373 to 0.28352, saving model to best.model\n",
      "0s - loss: 0.2652 - acc: 0.9284 - val_loss: 0.2835 - val_acc: 0.9174\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.28352 to 0.28313, saving model to best.model\n",
      "0s - loss: 0.2620 - acc: 0.9284 - val_loss: 0.2831 - val_acc: 0.9174\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.2688 - acc: 0.9284 - val_loss: 0.2832 - val_acc: 0.9174\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.2549 - acc: 0.9284 - val_loss: 0.2839 - val_acc: 0.9174\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss did not improve\n",
      "0s - loss: 0.2550 - acc: 0.9284 - val_loss: 0.2843 - val_acc: 0.9174\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss did not improve\n",
      "0s - loss: 0.2696 - acc: 0.9284 - val_loss: 0.2842 - val_acc: 0.9174\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss did not improve\n",
      "0s - loss: 0.2624 - acc: 0.9284 - val_loss: 0.2835 - val_acc: 0.9174\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.28313 to 0.28222, saving model to best.model\n",
      "0s - loss: 0.2658 - acc: 0.9284 - val_loss: 0.2822 - val_acc: 0.9174\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.28222 to 0.28076, saving model to best.model\n",
      "0s - loss: 0.2746 - acc: 0.9284 - val_loss: 0.2808 - val_acc: 0.9174\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.28076 to 0.27989, saving model to best.model\n",
      "0s - loss: 0.2654 - acc: 0.9284 - val_loss: 0.2799 - val_acc: 0.9174\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.27989 to 0.27944, saving model to best.model\n",
      "0s - loss: 0.2662 - acc: 0.9284 - val_loss: 0.2794 - val_acc: 0.9174\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.27944 to 0.27902, saving model to best.model\n",
      "0s - loss: 0.2643 - acc: 0.9284 - val_loss: 0.2790 - val_acc: 0.9174\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.27902 to 0.27880, saving model to best.model\n",
      "0s - loss: 0.2536 - acc: 0.9284 - val_loss: 0.2788 - val_acc: 0.9174\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.27880 to 0.27860, saving model to best.model\n",
      "0s - loss: 0.2544 - acc: 0.9284 - val_loss: 0.2786 - val_acc: 0.9174\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.27860 to 0.27856, saving model to best.model\n",
      "0s - loss: 0.2638 - acc: 0.9284 - val_loss: 0.2786 - val_acc: 0.9174\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.27856 to 0.27850, saving model to best.model\n",
      "0s - loss: 0.2602 - acc: 0.9284 - val_loss: 0.2785 - val_acc: 0.9174\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.27850 to 0.27802, saving model to best.model\n",
      "0s - loss: 0.2524 - acc: 0.9284 - val_loss: 0.2780 - val_acc: 0.9174\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.27802 to 0.27734, saving model to best.model\n",
      "0s - loss: 0.2608 - acc: 0.9284 - val_loss: 0.2773 - val_acc: 0.9174\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.27734 to 0.27645, saving model to best.model\n",
      "0s - loss: 0.2743 - acc: 0.9284 - val_loss: 0.2765 - val_acc: 0.9174\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.27645 to 0.27607, saving model to best.model\n",
      "0s - loss: 0.2571 - acc: 0.9284 - val_loss: 0.2761 - val_acc: 0.9174\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.27607 to 0.27564, saving model to best.model\n",
      "0s - loss: 0.2696 - acc: 0.9284 - val_loss: 0.2756 - val_acc: 0.9174\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.27564 to 0.27537, saving model to best.model\n",
      "0s - loss: 0.2638 - acc: 0.9284 - val_loss: 0.2754 - val_acc: 0.9174\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.27537 to 0.27495, saving model to best.model\n",
      "0s - loss: 0.2710 - acc: 0.9284 - val_loss: 0.2750 - val_acc: 0.9174\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.27495 to 0.27433, saving model to best.model\n",
      "0s - loss: 0.2679 - acc: 0.9284 - val_loss: 0.2743 - val_acc: 0.9174\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.27433 to 0.27399, saving model to best.model\n",
      "0s - loss: 0.2568 - acc: 0.9284 - val_loss: 0.2740 - val_acc: 0.9174\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.27399 to 0.27337, saving model to best.model\n",
      "0s - loss: 0.2541 - acc: 0.9284 - val_loss: 0.2734 - val_acc: 0.9174\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.27337 to 0.27222, saving model to best.model\n",
      "0s - loss: 0.2610 - acc: 0.9284 - val_loss: 0.2722 - val_acc: 0.9174\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.27222 to 0.27107, saving model to best.model\n",
      "0s - loss: 0.2591 - acc: 0.9284 - val_loss: 0.2711 - val_acc: 0.9174\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.27107 to 0.27018, saving model to best.model\n",
      "0s - loss: 0.2495 - acc: 0.9284 - val_loss: 0.2702 - val_acc: 0.9174\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.27018 to 0.26955, saving model to best.model\n",
      "0s - loss: 0.2473 - acc: 0.9284 - val_loss: 0.2695 - val_acc: 0.9174\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.26955 to 0.26932, saving model to best.model\n",
      "0s - loss: 0.2562 - acc: 0.9284 - val_loss: 0.2693 - val_acc: 0.9174\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.26932 to 0.26921, saving model to best.model\n",
      "0s - loss: 0.2540 - acc: 0.9284 - val_loss: 0.2692 - val_acc: 0.9174\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.26921 to 0.26860, saving model to best.model\n",
      "0s - loss: 0.2572 - acc: 0.9284 - val_loss: 0.2686 - val_acc: 0.9174\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.26860 to 0.26843, saving model to best.model\n",
      "0s - loss: 0.2516 - acc: 0.9284 - val_loss: 0.2684 - val_acc: 0.9174\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.26843 to 0.26768, saving model to best.model\n",
      "0s - loss: 0.2572 - acc: 0.9284 - val_loss: 0.2677 - val_acc: 0.9174\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.26768 to 0.26674, saving model to best.model\n",
      "0s - loss: 0.2360 - acc: 0.9284 - val_loss: 0.2667 - val_acc: 0.9174\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.26674 to 0.26602, saving model to best.model\n",
      "0s - loss: 0.2595 - acc: 0.9284 - val_loss: 0.2660 - val_acc: 0.9174\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.26602 to 0.26455, saving model to best.model\n",
      "0s - loss: 0.2610 - acc: 0.9284 - val_loss: 0.2645 - val_acc: 0.9174\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.26455 to 0.26282, saving model to best.model\n",
      "0s - loss: 0.2502 - acc: 0.9284 - val_loss: 0.2628 - val_acc: 0.9174\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.26282 to 0.26153, saving model to best.model\n",
      "0s - loss: 0.2461 - acc: 0.9284 - val_loss: 0.2615 - val_acc: 0.9174\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.26153 to 0.26034, saving model to best.model\n",
      "0s - loss: 0.2367 - acc: 0.9284 - val_loss: 0.2603 - val_acc: 0.9174\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.26034 to 0.25913, saving model to best.model\n",
      "0s - loss: 0.2306 - acc: 0.9284 - val_loss: 0.2591 - val_acc: 0.9174\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.25913 to 0.25804, saving model to best.model\n",
      "0s - loss: 0.2346 - acc: 0.9284 - val_loss: 0.2580 - val_acc: 0.9174\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.25804 to 0.25724, saving model to best.model\n",
      "0s - loss: 0.2433 - acc: 0.9284 - val_loss: 0.2572 - val_acc: 0.9174\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.25724 to 0.25593, saving model to best.model\n",
      "0s - loss: 0.2433 - acc: 0.9284 - val_loss: 0.2559 - val_acc: 0.9174\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.25593 to 0.25415, saving model to best.model\n",
      "0s - loss: 0.2414 - acc: 0.9284 - val_loss: 0.2541 - val_acc: 0.9174\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.25415 to 0.25288, saving model to best.model\n",
      "0s - loss: 0.2353 - acc: 0.9284 - val_loss: 0.2529 - val_acc: 0.9174\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.25288 to 0.25215, saving model to best.model\n",
      "0s - loss: 0.2470 - acc: 0.9284 - val_loss: 0.2522 - val_acc: 0.9174\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.25215 to 0.25065, saving model to best.model\n",
      "0s - loss: 0.2299 - acc: 0.9284 - val_loss: 0.2507 - val_acc: 0.9174\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.25065 to 0.24820, saving model to best.model\n",
      "0s - loss: 0.2435 - acc: 0.9284 - val_loss: 0.2482 - val_acc: 0.9174\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.24820 to 0.24648, saving model to best.model\n",
      "0s - loss: 0.2308 - acc: 0.9261 - val_loss: 0.2465 - val_acc: 0.9174\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.24648 to 0.24456, saving model to best.model\n",
      "0s - loss: 0.2437 - acc: 0.9284 - val_loss: 0.2446 - val_acc: 0.9174\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.24456 to 0.24357, saving model to best.model\n",
      "0s - loss: 0.2326 - acc: 0.9284 - val_loss: 0.2436 - val_acc: 0.9174\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.24357 to 0.24235, saving model to best.model\n",
      "0s - loss: 0.2242 - acc: 0.9284 - val_loss: 0.2423 - val_acc: 0.9174\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.24235 to 0.24128, saving model to best.model\n",
      "0s - loss: 0.2104 - acc: 0.9284 - val_loss: 0.2413 - val_acc: 0.9174\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.24128 to 0.23991, saving model to best.model\n",
      "0s - loss: 0.2299 - acc: 0.9284 - val_loss: 0.2399 - val_acc: 0.9174\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.23991 to 0.23855, saving model to best.model\n",
      "0s - loss: 0.2108 - acc: 0.9284 - val_loss: 0.2386 - val_acc: 0.9174\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.23855 to 0.23670, saving model to best.model\n",
      "0s - loss: 0.2156 - acc: 0.9307 - val_loss: 0.2367 - val_acc: 0.9174\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.23670 to 0.23414, saving model to best.model\n",
      "0s - loss: 0.2209 - acc: 0.9284 - val_loss: 0.2341 - val_acc: 0.9174\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.23414 to 0.23228, saving model to best.model\n",
      "0s - loss: 0.2310 - acc: 0.9261 - val_loss: 0.2323 - val_acc: 0.9174\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.23228 to 0.23052, saving model to best.model\n",
      "0s - loss: 0.2217 - acc: 0.9284 - val_loss: 0.2305 - val_acc: 0.9174\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.23052 to 0.22876, saving model to best.model\n",
      "0s - loss: 0.2125 - acc: 0.9353 - val_loss: 0.2288 - val_acc: 0.9174\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.22876 to 0.22783, saving model to best.model\n",
      "0s - loss: 0.2219 - acc: 0.9284 - val_loss: 0.2278 - val_acc: 0.9174\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.22783 to 0.22534, saving model to best.model\n",
      "0s - loss: 0.2069 - acc: 0.9307 - val_loss: 0.2253 - val_acc: 0.9174\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.22534 to 0.22275, saving model to best.model\n",
      "0s - loss: 0.2110 - acc: 0.9284 - val_loss: 0.2227 - val_acc: 0.9174\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.22275 to 0.22086, saving model to best.model\n",
      "0s - loss: 0.2264 - acc: 0.9307 - val_loss: 0.2209 - val_acc: 0.9174\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.22086 to 0.21991, saving model to best.model\n",
      "0s - loss: 0.2031 - acc: 0.9284 - val_loss: 0.2199 - val_acc: 0.9174\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.21991 to 0.21876, saving model to best.model\n",
      "0s - loss: 0.2130 - acc: 0.9284 - val_loss: 0.2188 - val_acc: 0.9174\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.21876 to 0.21872, saving model to best.model\n",
      "0s - loss: 0.2004 - acc: 0.9353 - val_loss: 0.2187 - val_acc: 0.9174\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.21872 to 0.21809, saving model to best.model\n",
      "0s - loss: 0.2091 - acc: 0.9330 - val_loss: 0.2181 - val_acc: 0.9174\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.21809 to 0.21492, saving model to best.model\n",
      "0s - loss: 0.1951 - acc: 0.9307 - val_loss: 0.2149 - val_acc: 0.9174\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.21492 to 0.21096, saving model to best.model\n",
      "0s - loss: 0.2062 - acc: 0.9330 - val_loss: 0.2110 - val_acc: 0.9174\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.21096 to 0.20825, saving model to best.model\n",
      "0s - loss: 0.1869 - acc: 0.9400 - val_loss: 0.2083 - val_acc: 0.9174\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.20825 to 0.20682, saving model to best.model\n",
      "0s - loss: 0.2079 - acc: 0.9400 - val_loss: 0.2068 - val_acc: 0.9266\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.20682 to 0.20553, saving model to best.model\n",
      "0s - loss: 0.1792 - acc: 0.9353 - val_loss: 0.2055 - val_acc: 0.9266\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.20553 to 0.20477, saving model to best.model\n",
      "0s - loss: 0.2022 - acc: 0.9446 - val_loss: 0.2048 - val_acc: 0.9266\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.1926 - acc: 0.9376 - val_loss: 0.2052 - val_acc: 0.9266\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.1982 - acc: 0.9307 - val_loss: 0.2057 - val_acc: 0.9266\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.20477 to 0.20279, saving model to best.model\n",
      "0s - loss: 0.1787 - acc: 0.9376 - val_loss: 0.2028 - val_acc: 0.9174\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.20279 to 0.20105, saving model to best.model\n",
      "0s - loss: 0.1750 - acc: 0.9376 - val_loss: 0.2010 - val_acc: 0.9358\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.20105 to 0.19980, saving model to best.model\n",
      "0s - loss: 0.1854 - acc: 0.9469 - val_loss: 0.1998 - val_acc: 0.9358\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.19980 to 0.19902, saving model to best.model\n",
      "0s - loss: 0.1756 - acc: 0.9400 - val_loss: 0.1990 - val_acc: 0.9358\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss did not improve\n",
      "0s - loss: 0.1648 - acc: 0.9469 - val_loss: 0.1995 - val_acc: 0.9358\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss did not improve\n",
      "0s - loss: 0.1651 - acc: 0.9538 - val_loss: 0.2017 - val_acc: 0.9358\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.1845 - acc: 0.9423 - val_loss: 0.2017 - val_acc: 0.9358\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.1843 - acc: 0.9423 - val_loss: 0.2015 - val_acc: 0.9358\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.1678 - acc: 0.9353 - val_loss: 0.1993 - val_acc: 0.9358\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.19902 to 0.19878, saving model to best.model\n",
      "0s - loss: 0.1838 - acc: 0.9400 - val_loss: 0.1988 - val_acc: 0.9358\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.19878 to 0.19646, saving model to best.model\n",
      "0s - loss: 0.1777 - acc: 0.9376 - val_loss: 0.1965 - val_acc: 0.9358\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.19646 to 0.19618, saving model to best.model\n",
      "0s - loss: 0.1504 - acc: 0.9469 - val_loss: 0.1962 - val_acc: 0.9358\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.19618 to 0.19608, saving model to best.model\n",
      "0s - loss: 0.1556 - acc: 0.9469 - val_loss: 0.1961 - val_acc: 0.9358\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.1763 - acc: 0.9423 - val_loss: 0.1966 - val_acc: 0.9358\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.1544 - acc: 0.9538 - val_loss: 0.1968 - val_acc: 0.9358\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.19608 to 0.19496, saving model to best.model\n",
      "0s - loss: 0.1679 - acc: 0.9469 - val_loss: 0.1950 - val_acc: 0.9266\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19496 to 0.19443, saving model to best.model\n",
      "0s - loss: 0.1715 - acc: 0.9469 - val_loss: 0.1944 - val_acc: 0.9266\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.1566 - acc: 0.9469 - val_loss: 0.1953 - val_acc: 0.9266\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss did not improve\n",
      "0s - loss: 0.1786 - acc: 0.9538 - val_loss: 0.1966 - val_acc: 0.9266\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss did not improve\n",
      "0s - loss: 0.1670 - acc: 0.9469 - val_loss: 0.1957 - val_acc: 0.9266\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.1662 - acc: 0.9538 - val_loss: 0.1953 - val_acc: 0.9266\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.1653 - acc: 0.9446 - val_loss: 0.1955 - val_acc: 0.9266\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.1418 - acc: 0.9561 - val_loss: 0.1960 - val_acc: 0.9266\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.1614 - acc: 0.9607 - val_loss: 0.1975 - val_acc: 0.9266\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.1414 - acc: 0.9561 - val_loss: 0.1994 - val_acc: 0.9266\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.1632 - acc: 0.9492 - val_loss: 0.1994 - val_acc: 0.9266\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.1674 - acc: 0.9446 - val_loss: 0.1980 - val_acc: 0.9266\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.1467 - acc: 0.9515 - val_loss: 0.1969 - val_acc: 0.9266\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.1507 - acc: 0.9492 - val_loss: 0.1966 - val_acc: 0.9266\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.1565 - acc: 0.9492 - val_loss: 0.1959 - val_acc: 0.9358\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.1457 - acc: 0.9584 - val_loss: 0.1958 - val_acc: 0.9358\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.1625 - acc: 0.9538 - val_loss: 0.1965 - val_acc: 0.9358\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.1491 - acc: 0.9584 - val_loss: 0.1962 - val_acc: 0.9358\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.1512 - acc: 0.9469 - val_loss: 0.1966 - val_acc: 0.9358\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.1577 - acc: 0.9492 - val_loss: 0.1972 - val_acc: 0.9358\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.1423 - acc: 0.9561 - val_loss: 0.1976 - val_acc: 0.9358\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.1690 - acc: 0.9584 - val_loss: 0.1992 - val_acc: 0.9266\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.1441 - acc: 0.9584 - val_loss: 0.2001 - val_acc: 0.9266\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.1440 - acc: 0.9492 - val_loss: 0.1989 - val_acc: 0.9266\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.1451 - acc: 0.9561 - val_loss: 0.1971 - val_acc: 0.9358\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.1512 - acc: 0.9446 - val_loss: 0.1969 - val_acc: 0.9358\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.1414 - acc: 0.9492 - val_loss: 0.1965 - val_acc: 0.9358\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.1595 - acc: 0.9469 - val_loss: 0.1970 - val_acc: 0.9358\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.03634, saving model to best.model\n",
      "0s - loss: 1.5562 - acc: 0.1663 - val_loss: 1.0363 - val_acc: 0.0642\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.03634 to 0.56627, saving model to best.model\n",
      "0s - loss: 1.1234 - acc: 0.3048 - val_loss: 0.5663 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.56627 to 0.33051, saving model to best.model\n",
      "0s - loss: 0.6858 - acc: 0.6236 - val_loss: 0.3305 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.33051 to 0.24973, saving model to best.model\n",
      "0s - loss: 0.4176 - acc: 0.8245 - val_loss: 0.2497 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.24973 to 0.23912, saving model to best.model\n",
      "0s - loss: 0.2954 - acc: 0.9261 - val_loss: 0.2391 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2797 - acc: 0.9376 - val_loss: 0.2533 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2467 - acc: 0.9400 - val_loss: 0.2733 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2878 - acc: 0.9423 - val_loss: 0.2922 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2568 - acc: 0.9400 - val_loss: 0.3077 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2889 - acc: 0.9423 - val_loss: 0.3196 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2803 - acc: 0.9423 - val_loss: 0.3277 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2786 - acc: 0.9423 - val_loss: 0.3325 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2808 - acc: 0.9423 - val_loss: 0.3343 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3163 - acc: 0.9423 - val_loss: 0.3336 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2850 - acc: 0.9423 - val_loss: 0.3310 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3283 - acc: 0.9423 - val_loss: 0.3269 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2774 - acc: 0.9423 - val_loss: 0.3217 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2993 - acc: 0.9423 - val_loss: 0.3155 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3038 - acc: 0.9400 - val_loss: 0.3088 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2946 - acc: 0.9423 - val_loss: 0.3016 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2774 - acc: 0.9423 - val_loss: 0.2946 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2868 - acc: 0.9423 - val_loss: 0.2877 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2523 - acc: 0.9423 - val_loss: 0.2813 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2691 - acc: 0.9400 - val_loss: 0.2752 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2773 - acc: 0.9423 - val_loss: 0.2695 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2455 - acc: 0.9400 - val_loss: 0.2646 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2789 - acc: 0.9423 - val_loss: 0.2601 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2302 - acc: 0.9446 - val_loss: 0.2565 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2506 - acc: 0.9423 - val_loss: 0.2538 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2669 - acc: 0.9423 - val_loss: 0.2516 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2590 - acc: 0.9423 - val_loss: 0.2500 - val_acc: 0.9358\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.20596, saving model to best.model\n",
      "0s - loss: 0.3896 - acc: 0.8545 - val_loss: 0.2060 - val_acc: 0.9541\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.20596 to 0.18609, saving model to best.model\n",
      "0s - loss: 0.3027 - acc: 0.9169 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2873 - acc: 0.9330 - val_loss: 0.1904 - val_acc: 0.9541\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2843 - acc: 0.9330 - val_loss: 0.1979 - val_acc: 0.9541\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2942 - acc: 0.9330 - val_loss: 0.2014 - val_acc: 0.9541\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2910 - acc: 0.9330 - val_loss: 0.2008 - val_acc: 0.9541\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3089 - acc: 0.9330 - val_loss: 0.1977 - val_acc: 0.9541\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3080 - acc: 0.9330 - val_loss: 0.1937 - val_acc: 0.9541\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2650 - acc: 0.9330 - val_loss: 0.1895 - val_acc: 0.9541\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2644 - acc: 0.9330 - val_loss: 0.1868 - val_acc: 0.9541\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.18609 to 0.18593, saving model to best.model\n",
      "0s - loss: 0.2790 - acc: 0.9330 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2757 - acc: 0.9330 - val_loss: 0.1868 - val_acc: 0.9541\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2795 - acc: 0.9284 - val_loss: 0.1880 - val_acc: 0.9541\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2688 - acc: 0.9284 - val_loss: 0.1889 - val_acc: 0.9541\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2613 - acc: 0.9330 - val_loss: 0.1887 - val_acc: 0.9541\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2772 - acc: 0.9284 - val_loss: 0.1879 - val_acc: 0.9541\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2740 - acc: 0.9307 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2811 - acc: 0.9330 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.18593 to 0.18588, saving model to best.model\n",
      "0s - loss: 0.2722 - acc: 0.9307 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2735 - acc: 0.9330 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2729 - acc: 0.9330 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2884 - acc: 0.9330 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2796 - acc: 0.9330 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2488 - acc: 0.9330 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.18588 to 0.18587, saving model to best.model\n",
      "0s - loss: 0.2716 - acc: 0.9330 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.18587 to 0.18583, saving model to best.model\n",
      "0s - loss: 0.2825 - acc: 0.9330 - val_loss: 0.1858 - val_acc: 0.9541\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2618 - acc: 0.9330 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2888 - acc: 0.9330 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2748 - acc: 0.9330 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2559 - acc: 0.9330 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2699 - acc: 0.9307 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2678 - acc: 0.9330 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2706 - acc: 0.9330 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2563 - acc: 0.9330 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2761 - acc: 0.9330 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2626 - acc: 0.9330 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2683 - acc: 0.9330 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2720 - acc: 0.9330 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2539 - acc: 0.9330 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2539 - acc: 0.9330 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2507 - acc: 0.9330 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2491 - acc: 0.9330 - val_loss: 0.1871 - val_acc: 0.9541\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2605 - acc: 0.9330 - val_loss: 0.1872 - val_acc: 0.9541\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2540 - acc: 0.9330 - val_loss: 0.1869 - val_acc: 0.9541\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2546 - acc: 0.9330 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2659 - acc: 0.9330 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2752 - acc: 0.9330 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2546 - acc: 0.9330 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2649 - acc: 0.9330 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2500 - acc: 0.9330 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.2604 - acc: 0.9330 - val_loss: 0.1868 - val_acc: 0.9541\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.2550 - acc: 0.9330 - val_loss: 0.1873 - val_acc: 0.9541\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.18070, saving model to best.model\n",
      "0s - loss: 0.3966 - acc: 0.8522 - val_loss: 0.1807 - val_acc: 0.9725\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.18070 to 0.13503, saving model to best.model\n",
      "0s - loss: 0.2718 - acc: 0.9330 - val_loss: 0.1350 - val_acc: 0.9725\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.13503 to 0.12598, saving model to best.model\n",
      "0s - loss: 0.2594 - acc: 0.9492 - val_loss: 0.1260 - val_acc: 0.9725\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2277 - acc: 0.9492 - val_loss: 0.1281 - val_acc: 0.9725\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2395 - acc: 0.9469 - val_loss: 0.1322 - val_acc: 0.9725\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2527 - acc: 0.9492 - val_loss: 0.1351 - val_acc: 0.9725\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2428 - acc: 0.9492 - val_loss: 0.1364 - val_acc: 0.9725\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2566 - acc: 0.9492 - val_loss: 0.1361 - val_acc: 0.9725\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2264 - acc: 0.9492 - val_loss: 0.1347 - val_acc: 0.9725\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2654 - acc: 0.9492 - val_loss: 0.1326 - val_acc: 0.9725\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2171 - acc: 0.9492 - val_loss: 0.1303 - val_acc: 0.9725\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2227 - acc: 0.9492 - val_loss: 0.1281 - val_acc: 0.9725\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2263 - acc: 0.9492 - val_loss: 0.1267 - val_acc: 0.9725\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2129 - acc: 0.9492 - val_loss: 0.1260 - val_acc: 0.9725\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2249 - acc: 0.9492 - val_loss: 0.1262 - val_acc: 0.9725\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2441 - acc: 0.9446 - val_loss: 0.1271 - val_acc: 0.9725\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2239 - acc: 0.9492 - val_loss: 0.1283 - val_acc: 0.9725\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2204 - acc: 0.9492 - val_loss: 0.1291 - val_acc: 0.9725\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2190 - acc: 0.9446 - val_loss: 0.1297 - val_acc: 0.9725\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2222 - acc: 0.9469 - val_loss: 0.1293 - val_acc: 0.9725\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2452 - acc: 0.9492 - val_loss: 0.1287 - val_acc: 0.9725\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2335 - acc: 0.9492 - val_loss: 0.1280 - val_acc: 0.9725\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2182 - acc: 0.9469 - val_loss: 0.1272 - val_acc: 0.9725\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2374 - acc: 0.9492 - val_loss: 0.1266 - val_acc: 0.9725\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2214 - acc: 0.9492 - val_loss: 0.1263 - val_acc: 0.9725\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2130 - acc: 0.9492 - val_loss: 0.1262 - val_acc: 0.9725\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2338 - acc: 0.9492 - val_loss: 0.1261 - val_acc: 0.9725\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2248 - acc: 0.9492 - val_loss: 0.1261 - val_acc: 0.9725\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2180 - acc: 0.9492 - val_loss: 0.1261 - val_acc: 0.9725\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.63070, saving model to best.model\n",
      "0s - loss: 1.1778 - acc: 0.2702 - val_loss: 0.6307 - val_acc: 0.9725\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.63070 to 0.34447, saving model to best.model\n",
      "0s - loss: 0.7420 - acc: 0.5566 - val_loss: 0.3445 - val_acc: 0.9725\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.34447 to 0.20632, saving model to best.model\n",
      "0s - loss: 0.4694 - acc: 0.7991 - val_loss: 0.2063 - val_acc: 0.9725\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.20632 to 0.14915, saving model to best.model\n",
      "0s - loss: 0.3456 - acc: 0.8799 - val_loss: 0.1492 - val_acc: 0.9725\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.14915 to 0.12947, saving model to best.model\n",
      "0s - loss: 0.2964 - acc: 0.9192 - val_loss: 0.1295 - val_acc: 0.9725\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.12947 to 0.12524, saving model to best.model\n",
      "0s - loss: 0.2735 - acc: 0.9307 - val_loss: 0.1252 - val_acc: 0.9725\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2573 - acc: 0.9307 - val_loss: 0.1266 - val_acc: 0.9725\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2904 - acc: 0.9330 - val_loss: 0.1295 - val_acc: 0.9725\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3118 - acc: 0.9330 - val_loss: 0.1322 - val_acc: 0.9725\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3089 - acc: 0.9330 - val_loss: 0.1341 - val_acc: 0.9725\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3406 - acc: 0.9330 - val_loss: 0.1351 - val_acc: 0.9725\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3228 - acc: 0.9330 - val_loss: 0.1354 - val_acc: 0.9725\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3307 - acc: 0.9330 - val_loss: 0.1347 - val_acc: 0.9725\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3290 - acc: 0.9330 - val_loss: 0.1335 - val_acc: 0.9725\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3183 - acc: 0.9330 - val_loss: 0.1319 - val_acc: 0.9725\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2886 - acc: 0.9330 - val_loss: 0.1303 - val_acc: 0.9725\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2866 - acc: 0.9330 - val_loss: 0.1288 - val_acc: 0.9725\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3159 - acc: 0.9330 - val_loss: 0.1273 - val_acc: 0.9725\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2819 - acc: 0.9330 - val_loss: 0.1263 - val_acc: 0.9725\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2849 - acc: 0.9330 - val_loss: 0.1257 - val_acc: 0.9725\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3019 - acc: 0.9330 - val_loss: 0.1257 - val_acc: 0.9725\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2687 - acc: 0.9330 - val_loss: 0.1262 - val_acc: 0.9725\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2985 - acc: 0.9330 - val_loss: 0.1272 - val_acc: 0.9725\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2745 - acc: 0.9330 - val_loss: 0.1286 - val_acc: 0.9725\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2683 - acc: 0.9330 - val_loss: 0.1300 - val_acc: 0.9725\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2866 - acc: 0.9284 - val_loss: 0.1311 - val_acc: 0.9725\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3057 - acc: 0.9307 - val_loss: 0.1318 - val_acc: 0.9725\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2801 - acc: 0.9307 - val_loss: 0.1319 - val_acc: 0.9725\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2786 - acc: 0.9284 - val_loss: 0.1317 - val_acc: 0.9725\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2593 - acc: 0.9330 - val_loss: 0.1313 - val_acc: 0.9725\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2910 - acc: 0.9284 - val_loss: 0.1306 - val_acc: 0.9725\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2956 - acc: 0.9330 - val_loss: 0.1301 - val_acc: 0.9725\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.28644, saving model to best.model\n",
      "0s - loss: 0.3624 - acc: 0.8891 - val_loss: 0.2864 - val_acc: 0.9174\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.2454 - acc: 0.9446 - val_loss: 0.2967 - val_acc: 0.9174\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2265 - acc: 0.9515 - val_loss: 0.3290 - val_acc: 0.9174\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2059 - acc: 0.9538 - val_loss: 0.3607 - val_acc: 0.9174\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2137 - acc: 0.9538 - val_loss: 0.3849 - val_acc: 0.9174\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2395 - acc: 0.9538 - val_loss: 0.3984 - val_acc: 0.9174\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2257 - acc: 0.9538 - val_loss: 0.4040 - val_acc: 0.9174\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2240 - acc: 0.9538 - val_loss: 0.4019 - val_acc: 0.9174\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2491 - acc: 0.9538 - val_loss: 0.3930 - val_acc: 0.9174\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2171 - acc: 0.9538 - val_loss: 0.3798 - val_acc: 0.9174\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2215 - acc: 0.9538 - val_loss: 0.3636 - val_acc: 0.9174\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2081 - acc: 0.9538 - val_loss: 0.3479 - val_acc: 0.9174\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2157 - acc: 0.9538 - val_loss: 0.3336 - val_acc: 0.9174\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2088 - acc: 0.9538 - val_loss: 0.3217 - val_acc: 0.9174\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2040 - acc: 0.9515 - val_loss: 0.3122 - val_acc: 0.9174\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2147 - acc: 0.9515 - val_loss: 0.3066 - val_acc: 0.9174\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2066 - acc: 0.9538 - val_loss: 0.3033 - val_acc: 0.9174\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2249 - acc: 0.9538 - val_loss: 0.3027 - val_acc: 0.9174\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2122 - acc: 0.9515 - val_loss: 0.3035 - val_acc: 0.9174\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2066 - acc: 0.9515 - val_loss: 0.3061 - val_acc: 0.9174\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2012 - acc: 0.9538 - val_loss: 0.3090 - val_acc: 0.9174\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2162 - acc: 0.9538 - val_loss: 0.3129 - val_acc: 0.9174\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.1894 - acc: 0.9538 - val_loss: 0.3159 - val_acc: 0.9174\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2228 - acc: 0.9538 - val_loss: 0.3178 - val_acc: 0.9174\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2151 - acc: 0.9538 - val_loss: 0.3187 - val_acc: 0.9174\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2103 - acc: 0.9538 - val_loss: 0.3190 - val_acc: 0.9174\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.1938 - acc: 0.9538 - val_loss: 0.3191 - val_acc: 0.9174\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.28959, saving model to best.model\n",
      "0s - loss: 0.4765 - acc: 0.7968 - val_loss: 0.2896 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.28959 to 0.26240, saving model to best.model\n",
      "0s - loss: 0.3192 - acc: 0.9007 - val_loss: 0.2624 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2267 - acc: 0.9376 - val_loss: 0.2816 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2119 - acc: 0.9515 - val_loss: 0.3115 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2392 - acc: 0.9515 - val_loss: 0.3375 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2251 - acc: 0.9538 - val_loss: 0.3558 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2339 - acc: 0.9538 - val_loss: 0.3669 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2420 - acc: 0.9538 - val_loss: 0.3709 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2169 - acc: 0.9538 - val_loss: 0.3702 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2246 - acc: 0.9538 - val_loss: 0.3658 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2316 - acc: 0.9538 - val_loss: 0.3572 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2350 - acc: 0.9538 - val_loss: 0.3459 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2168 - acc: 0.9538 - val_loss: 0.3337 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2222 - acc: 0.9538 - val_loss: 0.3214 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2452 - acc: 0.9538 - val_loss: 0.3090 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.1860 - acc: 0.9538 - val_loss: 0.2986 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2153 - acc: 0.9538 - val_loss: 0.2905 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2276 - acc: 0.9538 - val_loss: 0.2836 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2229 - acc: 0.9515 - val_loss: 0.2789 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2220 - acc: 0.9538 - val_loss: 0.2755 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2046 - acc: 0.9538 - val_loss: 0.2737 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.1943 - acc: 0.9538 - val_loss: 0.2738 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2222 - acc: 0.9538 - val_loss: 0.2750 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2106 - acc: 0.9515 - val_loss: 0.2772 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2082 - acc: 0.9515 - val_loss: 0.2797 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2166 - acc: 0.9538 - val_loss: 0.2830 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2114 - acc: 0.9538 - val_loss: 0.2866 - val_acc: 0.9266\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2008 - acc: 0.9538 - val_loss: 0.2887 - val_acc: 0.9266\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 1.76044, saving model to best.model\n",
      "0s - loss: 2.3730 - acc: 0.0947 - val_loss: 1.7604 - val_acc: 0.0550\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 1.76044 to 1.11186, saving model to best.model\n",
      "0s - loss: 1.6791 - acc: 0.1409 - val_loss: 1.1119 - val_acc: 0.0550\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 1.11186 to 0.64850, saving model to best.model\n",
      "0s - loss: 1.1782 - acc: 0.2841 - val_loss: 0.6485 - val_acc: 0.9450\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.64850 to 0.38327, saving model to best.model\n",
      "0s - loss: 0.7453 - acc: 0.5358 - val_loss: 0.3833 - val_acc: 0.9450\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.38327 to 0.26250, saving model to best.model\n",
      "0s - loss: 0.4793 - acc: 0.7783 - val_loss: 0.2625 - val_acc: 0.9450\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss improved from 0.26250 to 0.22023, saving model to best.model\n",
      "0s - loss: 0.3795 - acc: 0.8545 - val_loss: 0.2202 - val_acc: 0.9450\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss improved from 0.22023 to 0.21296, saving model to best.model\n",
      "0s - loss: 0.3353 - acc: 0.8961 - val_loss: 0.2130 - val_acc: 0.9450\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3345 - acc: 0.9076 - val_loss: 0.2190 - val_acc: 0.9450\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3227 - acc: 0.9169 - val_loss: 0.2287 - val_acc: 0.9450\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3382 - acc: 0.9192 - val_loss: 0.2384 - val_acc: 0.9450\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3384 - acc: 0.9192 - val_loss: 0.2461 - val_acc: 0.9450\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3812 - acc: 0.9192 - val_loss: 0.2517 - val_acc: 0.9450\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3597 - acc: 0.9192 - val_loss: 0.2549 - val_acc: 0.9450\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3575 - acc: 0.9192 - val_loss: 0.2561 - val_acc: 0.9450\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3546 - acc: 0.9192 - val_loss: 0.2557 - val_acc: 0.9450\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3900 - acc: 0.9192 - val_loss: 0.2540 - val_acc: 0.9450\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3428 - acc: 0.9192 - val_loss: 0.2513 - val_acc: 0.9450\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3678 - acc: 0.9192 - val_loss: 0.2479 - val_acc: 0.9450\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3569 - acc: 0.9192 - val_loss: 0.2439 - val_acc: 0.9450\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3799 - acc: 0.9192 - val_loss: 0.2393 - val_acc: 0.9450\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3632 - acc: 0.9192 - val_loss: 0.2347 - val_acc: 0.9450\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3352 - acc: 0.9192 - val_loss: 0.2301 - val_acc: 0.9450\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.3397 - acc: 0.9192 - val_loss: 0.2259 - val_acc: 0.9450\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.3345 - acc: 0.9145 - val_loss: 0.2223 - val_acc: 0.9450\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.3106 - acc: 0.9192 - val_loss: 0.2193 - val_acc: 0.9450\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.3118 - acc: 0.9192 - val_loss: 0.2169 - val_acc: 0.9450\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2980 - acc: 0.9192 - val_loss: 0.2150 - val_acc: 0.9450\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.3059 - acc: 0.9192 - val_loss: 0.2137 - val_acc: 0.9450\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.21296 to 0.21287, saving model to best.model\n",
      "0s - loss: 0.3108 - acc: 0.9169 - val_loss: 0.2129 - val_acc: 0.9450\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.21287 to 0.21244, saving model to best.model\n",
      "0s - loss: 0.2939 - acc: 0.9169 - val_loss: 0.2124 - val_acc: 0.9450\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.21244 to 0.21227, saving model to best.model\n",
      "0s - loss: 0.3058 - acc: 0.9192 - val_loss: 0.2123 - val_acc: 0.9450\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.21227 to 0.21222, saving model to best.model\n",
      "0s - loss: 0.3251 - acc: 0.9192 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.3265 - acc: 0.9076 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.3114 - acc: 0.9169 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss improved from 0.21222 to 0.21219, saving model to best.model\n",
      "0s - loss: 0.3149 - acc: 0.9169 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss improved from 0.21219 to 0.21215, saving model to best.model\n",
      "0s - loss: 0.3179 - acc: 0.9122 - val_loss: 0.2121 - val_acc: 0.9450\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss improved from 0.21215 to 0.21208, saving model to best.model\n",
      "0s - loss: 0.3220 - acc: 0.9099 - val_loss: 0.2121 - val_acc: 0.9450\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss improved from 0.21208 to 0.21201, saving model to best.model\n",
      "0s - loss: 0.3262 - acc: 0.9099 - val_loss: 0.2120 - val_acc: 0.9450\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.21201 to 0.21196, saving model to best.model\n",
      "0s - loss: 0.3264 - acc: 0.9145 - val_loss: 0.2120 - val_acc: 0.9450\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.21196 to 0.21193, saving model to best.model\n",
      "0s - loss: 0.3001 - acc: 0.9169 - val_loss: 0.2119 - val_acc: 0.9450\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.3063 - acc: 0.9192 - val_loss: 0.2119 - val_acc: 0.9450\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.3155 - acc: 0.9192 - val_loss: 0.2120 - val_acc: 0.9450\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.3209 - acc: 0.9169 - val_loss: 0.2120 - val_acc: 0.9450\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.3395 - acc: 0.9169 - val_loss: 0.2121 - val_acc: 0.9450\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.3444 - acc: 0.9192 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.3127 - acc: 0.9145 - val_loss: 0.2123 - val_acc: 0.9450\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.3135 - acc: 0.9192 - val_loss: 0.2123 - val_acc: 0.9450\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.3038 - acc: 0.9169 - val_loss: 0.2123 - val_acc: 0.9450\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.3276 - acc: 0.9192 - val_loss: 0.2123 - val_acc: 0.9450\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2895 - acc: 0.9192 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.3368 - acc: 0.9169 - val_loss: 0.2122 - val_acc: 0.9450\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.3294 - acc: 0.9169 - val_loss: 0.2120 - val_acc: 0.9450\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.21193 to 0.21191, saving model to best.model\n",
      "0s - loss: 0.3068 - acc: 0.9192 - val_loss: 0.2119 - val_acc: 0.9450\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.21191 to 0.21177, saving model to best.model\n",
      "0s - loss: 0.3102 - acc: 0.9169 - val_loss: 0.2118 - val_acc: 0.9450\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.21177 to 0.21160, saving model to best.model\n",
      "0s - loss: 0.2921 - acc: 0.9192 - val_loss: 0.2116 - val_acc: 0.9450\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.21160 to 0.21145, saving model to best.model\n",
      "0s - loss: 0.3141 - acc: 0.9169 - val_loss: 0.2114 - val_acc: 0.9450\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.21145 to 0.21133, saving model to best.model\n",
      "0s - loss: 0.3252 - acc: 0.9192 - val_loss: 0.2113 - val_acc: 0.9450\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.21133 to 0.21123, saving model to best.model\n",
      "0s - loss: 0.3242 - acc: 0.9169 - val_loss: 0.2112 - val_acc: 0.9450\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.21123 to 0.21114, saving model to best.model\n",
      "0s - loss: 0.3106 - acc: 0.9169 - val_loss: 0.2111 - val_acc: 0.9450\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.21114 to 0.21108, saving model to best.model\n",
      "0s - loss: 0.3142 - acc: 0.9192 - val_loss: 0.2111 - val_acc: 0.9450\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.21108 to 0.21102, saving model to best.model\n",
      "0s - loss: 0.2938 - acc: 0.9215 - val_loss: 0.2110 - val_acc: 0.9450\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.21102 to 0.21097, saving model to best.model\n",
      "0s - loss: 0.3183 - acc: 0.9192 - val_loss: 0.2110 - val_acc: 0.9450\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.21097 to 0.21091, saving model to best.model\n",
      "0s - loss: 0.3107 - acc: 0.9192 - val_loss: 0.2109 - val_acc: 0.9450\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.21091 to 0.21085, saving model to best.model\n",
      "0s - loss: 0.3179 - acc: 0.9145 - val_loss: 0.2108 - val_acc: 0.9450\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.21085 to 0.21078, saving model to best.model\n",
      "0s - loss: 0.3140 - acc: 0.9192 - val_loss: 0.2108 - val_acc: 0.9450\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.21078 to 0.21073, saving model to best.model\n",
      "0s - loss: 0.3113 - acc: 0.9192 - val_loss: 0.2107 - val_acc: 0.9450\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.21073 to 0.21067, saving model to best.model\n",
      "0s - loss: 0.3133 - acc: 0.9169 - val_loss: 0.2107 - val_acc: 0.9450\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.21067 to 0.21061, saving model to best.model\n",
      "0s - loss: 0.3040 - acc: 0.9145 - val_loss: 0.2106 - val_acc: 0.9450\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.21061 to 0.21055, saving model to best.model\n",
      "0s - loss: 0.3082 - acc: 0.9192 - val_loss: 0.2106 - val_acc: 0.9450\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.21055 to 0.21049, saving model to best.model\n",
      "0s - loss: 0.3024 - acc: 0.9192 - val_loss: 0.2105 - val_acc: 0.9450\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.21049 to 0.21043, saving model to best.model\n",
      "0s - loss: 0.3110 - acc: 0.9169 - val_loss: 0.2104 - val_acc: 0.9450\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.21043 to 0.21036, saving model to best.model\n",
      "0s - loss: 0.2790 - acc: 0.9169 - val_loss: 0.2104 - val_acc: 0.9450\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.21036 to 0.21029, saving model to best.model\n",
      "0s - loss: 0.2947 - acc: 0.9215 - val_loss: 0.2103 - val_acc: 0.9450\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.21029 to 0.21023, saving model to best.model\n",
      "0s - loss: 0.2963 - acc: 0.9169 - val_loss: 0.2102 - val_acc: 0.9450\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.21023 to 0.21017, saving model to best.model\n",
      "0s - loss: 0.2994 - acc: 0.9192 - val_loss: 0.2102 - val_acc: 0.9450\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.21017 to 0.21011, saving model to best.model\n",
      "0s - loss: 0.3128 - acc: 0.9192 - val_loss: 0.2101 - val_acc: 0.9450\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.21011 to 0.21004, saving model to best.model\n",
      "0s - loss: 0.2857 - acc: 0.9192 - val_loss: 0.2100 - val_acc: 0.9450\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.21004 to 0.20993, saving model to best.model\n",
      "0s - loss: 0.2883 - acc: 0.9192 - val_loss: 0.2099 - val_acc: 0.9450\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.20993 to 0.20982, saving model to best.model\n",
      "0s - loss: 0.3085 - acc: 0.9192 - val_loss: 0.2098 - val_acc: 0.9450\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.20982 to 0.20971, saving model to best.model\n",
      "0s - loss: 0.2975 - acc: 0.9192 - val_loss: 0.2097 - val_acc: 0.9450\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.20971 to 0.20958, saving model to best.model\n",
      "0s - loss: 0.3138 - acc: 0.9192 - val_loss: 0.2096 - val_acc: 0.9450\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.20958 to 0.20944, saving model to best.model\n",
      "0s - loss: 0.2872 - acc: 0.9192 - val_loss: 0.2094 - val_acc: 0.9450\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.20944 to 0.20932, saving model to best.model\n",
      "0s - loss: 0.3164 - acc: 0.9192 - val_loss: 0.2093 - val_acc: 0.9450\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.20932 to 0.20921, saving model to best.model\n",
      "0s - loss: 0.2832 - acc: 0.9192 - val_loss: 0.2092 - val_acc: 0.9450\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.20921 to 0.20910, saving model to best.model\n",
      "0s - loss: 0.3166 - acc: 0.9192 - val_loss: 0.2091 - val_acc: 0.9450\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.20910 to 0.20899, saving model to best.model\n",
      "0s - loss: 0.2868 - acc: 0.9169 - val_loss: 0.2090 - val_acc: 0.9450\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.20899 to 0.20888, saving model to best.model\n",
      "0s - loss: 0.3211 - acc: 0.9192 - val_loss: 0.2089 - val_acc: 0.9450\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.20888 to 0.20877, saving model to best.model\n",
      "0s - loss: 0.2889 - acc: 0.9215 - val_loss: 0.2088 - val_acc: 0.9450\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.20877 to 0.20864, saving model to best.model\n",
      "0s - loss: 0.3077 - acc: 0.9192 - val_loss: 0.2086 - val_acc: 0.9450\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.20864 to 0.20853, saving model to best.model\n",
      "0s - loss: 0.2965 - acc: 0.9192 - val_loss: 0.2085 - val_acc: 0.9450\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.20853 to 0.20844, saving model to best.model\n",
      "0s - loss: 0.3075 - acc: 0.9192 - val_loss: 0.2084 - val_acc: 0.9450\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.20844 to 0.20834, saving model to best.model\n",
      "0s - loss: 0.3002 - acc: 0.9169 - val_loss: 0.2083 - val_acc: 0.9450\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.20834 to 0.20822, saving model to best.model\n",
      "0s - loss: 0.3151 - acc: 0.9169 - val_loss: 0.2082 - val_acc: 0.9450\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.20822 to 0.20805, saving model to best.model\n",
      "0s - loss: 0.2771 - acc: 0.9192 - val_loss: 0.2080 - val_acc: 0.9450\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.20805 to 0.20784, saving model to best.model\n",
      "0s - loss: 0.2817 - acc: 0.9192 - val_loss: 0.2078 - val_acc: 0.9450\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.20784 to 0.20765, saving model to best.model\n",
      "0s - loss: 0.2817 - acc: 0.9192 - val_loss: 0.2076 - val_acc: 0.9450\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.20765 to 0.20744, saving model to best.model\n",
      "0s - loss: 0.3143 - acc: 0.9192 - val_loss: 0.2074 - val_acc: 0.9450\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.20744 to 0.20724, saving model to best.model\n",
      "0s - loss: 0.2805 - acc: 0.9192 - val_loss: 0.2072 - val_acc: 0.9450\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.20724 to 0.20704, saving model to best.model\n",
      "0s - loss: 0.3215 - acc: 0.9192 - val_loss: 0.2070 - val_acc: 0.9450\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.20704 to 0.20683, saving model to best.model\n",
      "0s - loss: 0.2992 - acc: 0.9192 - val_loss: 0.2068 - val_acc: 0.9450\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.20683 to 0.20666, saving model to best.model\n",
      "0s - loss: 0.3009 - acc: 0.9192 - val_loss: 0.2067 - val_acc: 0.9450\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.20666 to 0.20649, saving model to best.model\n",
      "0s - loss: 0.3039 - acc: 0.9192 - val_loss: 0.2065 - val_acc: 0.9450\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.20649 to 0.20631, saving model to best.model\n",
      "0s - loss: 0.2958 - acc: 0.9192 - val_loss: 0.2063 - val_acc: 0.9450\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.20631 to 0.20607, saving model to best.model\n",
      "0s - loss: 0.2715 - acc: 0.9169 - val_loss: 0.2061 - val_acc: 0.9450\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.20607 to 0.20583, saving model to best.model\n",
      "0s - loss: 0.2943 - acc: 0.9192 - val_loss: 0.2058 - val_acc: 0.9450\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.20583 to 0.20560, saving model to best.model\n",
      "0s - loss: 0.3002 - acc: 0.9192 - val_loss: 0.2056 - val_acc: 0.9450\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.20560 to 0.20537, saving model to best.model\n",
      "0s - loss: 0.2980 - acc: 0.9192 - val_loss: 0.2054 - val_acc: 0.9450\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.20537 to 0.20516, saving model to best.model\n",
      "0s - loss: 0.3032 - acc: 0.9192 - val_loss: 0.2052 - val_acc: 0.9450\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.20516 to 0.20497, saving model to best.model\n",
      "0s - loss: 0.2792 - acc: 0.9192 - val_loss: 0.2050 - val_acc: 0.9450\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.20497 to 0.20477, saving model to best.model\n",
      "0s - loss: 0.2738 - acc: 0.9192 - val_loss: 0.2048 - val_acc: 0.9450\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.20477 to 0.20455, saving model to best.model\n",
      "0s - loss: 0.2912 - acc: 0.9169 - val_loss: 0.2045 - val_acc: 0.9450\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.20455 to 0.20432, saving model to best.model\n",
      "0s - loss: 0.2874 - acc: 0.9192 - val_loss: 0.2043 - val_acc: 0.9450\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.20432 to 0.20408, saving model to best.model\n",
      "0s - loss: 0.2788 - acc: 0.9192 - val_loss: 0.2041 - val_acc: 0.9450\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.20408 to 0.20384, saving model to best.model\n",
      "0s - loss: 0.2857 - acc: 0.9169 - val_loss: 0.2038 - val_acc: 0.9450\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.20384 to 0.20363, saving model to best.model\n",
      "0s - loss: 0.2893 - acc: 0.9192 - val_loss: 0.2036 - val_acc: 0.9450\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.20363 to 0.20340, saving model to best.model\n",
      "0s - loss: 0.2876 - acc: 0.9192 - val_loss: 0.2034 - val_acc: 0.9450\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.20340 to 0.20317, saving model to best.model\n",
      "0s - loss: 0.2843 - acc: 0.9192 - val_loss: 0.2032 - val_acc: 0.9450\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.20317 to 0.20295, saving model to best.model\n",
      "0s - loss: 0.2834 - acc: 0.9192 - val_loss: 0.2030 - val_acc: 0.9450\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.20295 to 0.20274, saving model to best.model\n",
      "0s - loss: 0.2792 - acc: 0.9192 - val_loss: 0.2027 - val_acc: 0.9450\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.20274 to 0.20253, saving model to best.model\n",
      "0s - loss: 0.2827 - acc: 0.9192 - val_loss: 0.2025 - val_acc: 0.9450\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.20253 to 0.20231, saving model to best.model\n",
      "0s - loss: 0.2837 - acc: 0.9192 - val_loss: 0.2023 - val_acc: 0.9450\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.20231 to 0.20206, saving model to best.model\n",
      "0s - loss: 0.2932 - acc: 0.9192 - val_loss: 0.2021 - val_acc: 0.9450\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.20206 to 0.20181, saving model to best.model\n",
      "0s - loss: 0.2796 - acc: 0.9192 - val_loss: 0.2018 - val_acc: 0.9450\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.20181 to 0.20156, saving model to best.model\n",
      "0s - loss: 0.2689 - acc: 0.9192 - val_loss: 0.2016 - val_acc: 0.9450\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.20156 to 0.20129, saving model to best.model\n",
      "0s - loss: 0.2927 - acc: 0.9192 - val_loss: 0.2013 - val_acc: 0.9450\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.20129 to 0.20100, saving model to best.model\n",
      "0s - loss: 0.2687 - acc: 0.9192 - val_loss: 0.2010 - val_acc: 0.9450\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.20100 to 0.20067, saving model to best.model\n",
      "0s - loss: 0.2661 - acc: 0.9192 - val_loss: 0.2007 - val_acc: 0.9450\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.20067 to 0.20031, saving model to best.model\n",
      "0s - loss: 0.2898 - acc: 0.9192 - val_loss: 0.2003 - val_acc: 0.9450\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.20031 to 0.19993, saving model to best.model\n",
      "0s - loss: 0.2788 - acc: 0.9192 - val_loss: 0.1999 - val_acc: 0.9450\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.19993 to 0.19956, saving model to best.model\n",
      "0s - loss: 0.2811 - acc: 0.9192 - val_loss: 0.1996 - val_acc: 0.9450\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.19956 to 0.19920, saving model to best.model\n",
      "0s - loss: 0.2824 - acc: 0.9192 - val_loss: 0.1992 - val_acc: 0.9450\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.19920 to 0.19881, saving model to best.model\n",
      "0s - loss: 0.2798 - acc: 0.9192 - val_loss: 0.1988 - val_acc: 0.9450\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.19881 to 0.19842, saving model to best.model\n",
      "0s - loss: 0.2866 - acc: 0.9192 - val_loss: 0.1984 - val_acc: 0.9450\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.19842 to 0.19801, saving model to best.model\n",
      "0s - loss: 0.2728 - acc: 0.9192 - val_loss: 0.1980 - val_acc: 0.9450\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.19801 to 0.19760, saving model to best.model\n",
      "0s - loss: 0.2688 - acc: 0.9192 - val_loss: 0.1976 - val_acc: 0.9450\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.19760 to 0.19716, saving model to best.model\n",
      "0s - loss: 0.2743 - acc: 0.9192 - val_loss: 0.1972 - val_acc: 0.9450\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.19716 to 0.19679, saving model to best.model\n",
      "0s - loss: 0.2876 - acc: 0.9192 - val_loss: 0.1968 - val_acc: 0.9450\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.19679 to 0.19635, saving model to best.model\n",
      "0s - loss: 0.2715 - acc: 0.9192 - val_loss: 0.1964 - val_acc: 0.9450\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.19635 to 0.19582, saving model to best.model\n",
      "0s - loss: 0.2698 - acc: 0.9192 - val_loss: 0.1958 - val_acc: 0.9450\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.19582 to 0.19535, saving model to best.model\n",
      "0s - loss: 0.2830 - acc: 0.9192 - val_loss: 0.1953 - val_acc: 0.9450\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.19535 to 0.19490, saving model to best.model\n",
      "0s - loss: 0.2715 - acc: 0.9192 - val_loss: 0.1949 - val_acc: 0.9450\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.19490 to 0.19448, saving model to best.model\n",
      "0s - loss: 0.2829 - acc: 0.9192 - val_loss: 0.1945 - val_acc: 0.9450\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.19448 to 0.19406, saving model to best.model\n",
      "0s - loss: 0.2952 - acc: 0.9192 - val_loss: 0.1941 - val_acc: 0.9450\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.19406 to 0.19365, saving model to best.model\n",
      "0s - loss: 0.2782 - acc: 0.9192 - val_loss: 0.1936 - val_acc: 0.9450\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19365 to 0.19329, saving model to best.model\n",
      "0s - loss: 0.2762 - acc: 0.9192 - val_loss: 0.1933 - val_acc: 0.9450\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.19329 to 0.19292, saving model to best.model\n",
      "0s - loss: 0.2648 - acc: 0.9192 - val_loss: 0.1929 - val_acc: 0.9450\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.19292 to 0.19251, saving model to best.model\n",
      "0s - loss: 0.2665 - acc: 0.9192 - val_loss: 0.1925 - val_acc: 0.9450\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.19251 to 0.19200, saving model to best.model\n",
      "0s - loss: 0.2599 - acc: 0.9192 - val_loss: 0.1920 - val_acc: 0.9450\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.19200 to 0.19144, saving model to best.model\n",
      "0s - loss: 0.2564 - acc: 0.9192 - val_loss: 0.1914 - val_acc: 0.9450\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.19144 to 0.19089, saving model to best.model\n",
      "0s - loss: 0.2571 - acc: 0.9192 - val_loss: 0.1909 - val_acc: 0.9450\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.19089 to 0.19036, saving model to best.model\n",
      "0s - loss: 0.2811 - acc: 0.9192 - val_loss: 0.1904 - val_acc: 0.9450\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.19036 to 0.18979, saving model to best.model\n",
      "0s - loss: 0.2688 - acc: 0.9192 - val_loss: 0.1898 - val_acc: 0.9450\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.18979 to 0.18924, saving model to best.model\n",
      "0s - loss: 0.2642 - acc: 0.9192 - val_loss: 0.1892 - val_acc: 0.9450\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.18924 to 0.18866, saving model to best.model\n",
      "0s - loss: 0.2670 - acc: 0.9192 - val_loss: 0.1887 - val_acc: 0.9450\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.18866 to 0.18805, saving model to best.model\n",
      "0s - loss: 0.2507 - acc: 0.9192 - val_loss: 0.1880 - val_acc: 0.9450\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.18805 to 0.18743, saving model to best.model\n",
      "0s - loss: 0.2651 - acc: 0.9192 - val_loss: 0.1874 - val_acc: 0.9450\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.18743 to 0.18678, saving model to best.model\n",
      "0s - loss: 0.2628 - acc: 0.9169 - val_loss: 0.1868 - val_acc: 0.9450\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.18678 to 0.18608, saving model to best.model\n",
      "0s - loss: 0.2544 - acc: 0.9215 - val_loss: 0.1861 - val_acc: 0.9450\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.18608 to 0.18537, saving model to best.model\n",
      "0s - loss: 0.2642 - acc: 0.9192 - val_loss: 0.1854 - val_acc: 0.9450\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.18537 to 0.18466, saving model to best.model\n",
      "0s - loss: 0.2659 - acc: 0.9215 - val_loss: 0.1847 - val_acc: 0.9450\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.18466 to 0.18396, saving model to best.model\n",
      "0s - loss: 0.2638 - acc: 0.9192 - val_loss: 0.1840 - val_acc: 0.9450\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.18396 to 0.18319, saving model to best.model\n",
      "0s - loss: 0.2588 - acc: 0.9192 - val_loss: 0.1832 - val_acc: 0.9450\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.18319 to 0.18245, saving model to best.model\n",
      "0s - loss: 0.2571 - acc: 0.9215 - val_loss: 0.1825 - val_acc: 0.9450\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.18245 to 0.18176, saving model to best.model\n",
      "0s - loss: 0.2551 - acc: 0.9215 - val_loss: 0.1818 - val_acc: 0.9450\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.18176 to 0.18098, saving model to best.model\n",
      "0s - loss: 0.2710 - acc: 0.9192 - val_loss: 0.1810 - val_acc: 0.9450\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.18098 to 0.18011, saving model to best.model\n",
      "0s - loss: 0.2514 - acc: 0.9192 - val_loss: 0.1801 - val_acc: 0.9450\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.18011 to 0.17935, saving model to best.model\n",
      "0s - loss: 0.2599 - acc: 0.9192 - val_loss: 0.1794 - val_acc: 0.9450\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.17935 to 0.17864, saving model to best.model\n",
      "0s - loss: 0.2311 - acc: 0.9192 - val_loss: 0.1786 - val_acc: 0.9450\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.17864 to 0.17785, saving model to best.model\n",
      "0s - loss: 0.2347 - acc: 0.9192 - val_loss: 0.1778 - val_acc: 0.9450\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.17785 to 0.17702, saving model to best.model\n",
      "0s - loss: 0.2470 - acc: 0.9215 - val_loss: 0.1770 - val_acc: 0.9450\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.17702 to 0.17612, saving model to best.model\n",
      "0s - loss: 0.2617 - acc: 0.9192 - val_loss: 0.1761 - val_acc: 0.9450\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.17612 to 0.17509, saving model to best.model\n",
      "0s - loss: 0.2393 - acc: 0.9215 - val_loss: 0.1751 - val_acc: 0.9450\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.17509 to 0.17404, saving model to best.model\n",
      "0s - loss: 0.2399 - acc: 0.9192 - val_loss: 0.1740 - val_acc: 0.9450\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.17404 to 0.17305, saving model to best.model\n",
      "0s - loss: 0.2504 - acc: 0.9169 - val_loss: 0.1731 - val_acc: 0.9450\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.17305 to 0.17199, saving model to best.model\n",
      "0s - loss: 0.2533 - acc: 0.9192 - val_loss: 0.1720 - val_acc: 0.9450\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.17199 to 0.17093, saving model to best.model\n",
      "0s - loss: 0.2313 - acc: 0.9238 - val_loss: 0.1709 - val_acc: 0.9450\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.17093 to 0.16986, saving model to best.model\n",
      "0s - loss: 0.2380 - acc: 0.9238 - val_loss: 0.1699 - val_acc: 0.9450\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.16986 to 0.16879, saving model to best.model\n",
      "0s - loss: 0.2486 - acc: 0.9192 - val_loss: 0.1688 - val_acc: 0.9450\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.16879 to 0.16803, saving model to best.model\n",
      "0s - loss: 0.2506 - acc: 0.9192 - val_loss: 0.1680 - val_acc: 0.9450\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.16803 to 0.16762, saving model to best.model\n",
      "0s - loss: 0.2433 - acc: 0.9215 - val_loss: 0.1676 - val_acc: 0.9450\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.16762 to 0.16686, saving model to best.model\n",
      "0s - loss: 0.2236 - acc: 0.9238 - val_loss: 0.1669 - val_acc: 0.9450\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.16686 to 0.16589, saving model to best.model\n",
      "0s - loss: 0.2440 - acc: 0.9238 - val_loss: 0.1659 - val_acc: 0.9450\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.16589 to 0.16450, saving model to best.model\n",
      "0s - loss: 0.2386 - acc: 0.9192 - val_loss: 0.1645 - val_acc: 0.9450\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.16450 to 0.16294, saving model to best.model\n",
      "0s - loss: 0.2203 - acc: 0.9307 - val_loss: 0.1629 - val_acc: 0.9450\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss improved from 0.16294 to 0.16152, saving model to best.model\n",
      "0s - loss: 0.2177 - acc: 0.9238 - val_loss: 0.1615 - val_acc: 0.9450\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss improved from 0.16152 to 0.16022, saving model to best.model\n",
      "0s - loss: 0.2181 - acc: 0.9238 - val_loss: 0.1602 - val_acc: 0.9450\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss improved from 0.16022 to 0.15911, saving model to best.model\n",
      "0s - loss: 0.2339 - acc: 0.9215 - val_loss: 0.1591 - val_acc: 0.9450\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.15911 to 0.15805, saving model to best.model\n",
      "0s - loss: 0.2199 - acc: 0.9238 - val_loss: 0.1581 - val_acc: 0.9450\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.15805 to 0.15745, saving model to best.model\n",
      "0s - loss: 0.2134 - acc: 0.9215 - val_loss: 0.1574 - val_acc: 0.9450\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.15745 to 0.15695, saving model to best.model\n",
      "0s - loss: 0.2161 - acc: 0.9284 - val_loss: 0.1569 - val_acc: 0.9450\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.15695 to 0.15632, saving model to best.model\n",
      "0s - loss: 0.2369 - acc: 0.9145 - val_loss: 0.1563 - val_acc: 0.9450\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.15632 to 0.15559, saving model to best.model\n",
      "0s - loss: 0.2236 - acc: 0.9238 - val_loss: 0.1556 - val_acc: 0.9450\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss improved from 0.15559 to 0.15471, saving model to best.model\n",
      "0s - loss: 0.2402 - acc: 0.9192 - val_loss: 0.1547 - val_acc: 0.9450\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss improved from 0.15471 to 0.15320, saving model to best.model\n",
      "0s - loss: 0.2102 - acc: 0.9238 - val_loss: 0.1532 - val_acc: 0.9450\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss improved from 0.15320 to 0.15123, saving model to best.model\n",
      "0s - loss: 0.2196 - acc: 0.9261 - val_loss: 0.1512 - val_acc: 0.9450\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss improved from 0.15123 to 0.14983, saving model to best.model\n",
      "0s - loss: 0.2061 - acc: 0.9238 - val_loss: 0.1498 - val_acc: 0.9450\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.14983 to 0.14855, saving model to best.model\n",
      "0s - loss: 0.2109 - acc: 0.9215 - val_loss: 0.1485 - val_acc: 0.9450\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.14855 to 0.14749, saving model to best.model\n",
      "0s - loss: 0.2154 - acc: 0.9192 - val_loss: 0.1475 - val_acc: 0.9450\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss improved from 0.14749 to 0.14727, saving model to best.model\n",
      "0s - loss: 0.2060 - acc: 0.9261 - val_loss: 0.1473 - val_acc: 0.9450\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.14727 to 0.14725, saving model to best.model\n",
      "0s - loss: 0.1997 - acc: 0.9330 - val_loss: 0.1472 - val_acc: 0.9450\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.35149, saving model to best.model\n",
      "0s - loss: 0.5878 - acc: 0.6813 - val_loss: 0.3515 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.35149 to 0.27256, saving model to best.model\n",
      "0s - loss: 0.4067 - acc: 0.8499 - val_loss: 0.2726 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.27256 to 0.26416, saving model to best.model\n",
      "0s - loss: 0.3052 - acc: 0.9261 - val_loss: 0.2642 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2762 - acc: 0.9376 - val_loss: 0.2809 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2772 - acc: 0.9376 - val_loss: 0.3013 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2839 - acc: 0.9376 - val_loss: 0.3184 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2550 - acc: 0.9376 - val_loss: 0.3293 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3052 - acc: 0.9376 - val_loss: 0.3346 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2908 - acc: 0.9376 - val_loss: 0.3359 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2817 - acc: 0.9376 - val_loss: 0.3324 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2930 - acc: 0.9376 - val_loss: 0.3258 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3136 - acc: 0.9376 - val_loss: 0.3172 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3015 - acc: 0.9376 - val_loss: 0.3077 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2705 - acc: 0.9353 - val_loss: 0.2984 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2781 - acc: 0.9353 - val_loss: 0.2898 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2773 - acc: 0.9376 - val_loss: 0.2818 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2667 - acc: 0.9376 - val_loss: 0.2755 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2788 - acc: 0.9376 - val_loss: 0.2709 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2311 - acc: 0.9376 - val_loss: 0.2680 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2716 - acc: 0.9353 - val_loss: 0.2665 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2666 - acc: 0.9376 - val_loss: 0.2659 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2515 - acc: 0.9353 - val_loss: 0.2660 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2586 - acc: 0.9376 - val_loss: 0.2664 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2548 - acc: 0.9376 - val_loss: 0.2673 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2520 - acc: 0.9376 - val_loss: 0.2685 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2850 - acc: 0.9353 - val_loss: 0.2701 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2815 - acc: 0.9353 - val_loss: 0.2717 - val_acc: 0.9266\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2475 - acc: 0.9376 - val_loss: 0.2733 - val_acc: 0.9266\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2829 - acc: 0.9376 - val_loss: 0.2743 - val_acc: 0.9266\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.29941, saving model to best.model\n",
      "0s - loss: 0.6256 - acc: 0.6628 - val_loss: 0.2994 - val_acc: 0.9450\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.29941 to 0.22383, saving model to best.model\n",
      "0s - loss: 0.3580 - acc: 0.8637 - val_loss: 0.2238 - val_acc: 0.9450\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.22383 to 0.21384, saving model to best.model\n",
      "0s - loss: 0.2971 - acc: 0.9238 - val_loss: 0.2138 - val_acc: 0.9450\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2501 - acc: 0.9423 - val_loss: 0.2264 - val_acc: 0.9450\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2213 - acc: 0.9469 - val_loss: 0.2430 - val_acc: 0.9450\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2678 - acc: 0.9469 - val_loss: 0.2566 - val_acc: 0.9450\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2683 - acc: 0.9469 - val_loss: 0.2663 - val_acc: 0.9450\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2539 - acc: 0.9469 - val_loss: 0.2720 - val_acc: 0.9450\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2748 - acc: 0.9469 - val_loss: 0.2736 - val_acc: 0.9450\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2686 - acc: 0.9469 - val_loss: 0.2718 - val_acc: 0.9450\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2826 - acc: 0.9469 - val_loss: 0.2680 - val_acc: 0.9450\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2555 - acc: 0.9469 - val_loss: 0.2632 - val_acc: 0.9450\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2589 - acc: 0.9469 - val_loss: 0.2567 - val_acc: 0.9450\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2618 - acc: 0.9469 - val_loss: 0.2497 - val_acc: 0.9450\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2477 - acc: 0.9469 - val_loss: 0.2428 - val_acc: 0.9450\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2384 - acc: 0.9469 - val_loss: 0.2359 - val_acc: 0.9450\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2121 - acc: 0.9469 - val_loss: 0.2299 - val_acc: 0.9450\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2216 - acc: 0.9469 - val_loss: 0.2250 - val_acc: 0.9450\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2142 - acc: 0.9469 - val_loss: 0.2215 - val_acc: 0.9450\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2429 - acc: 0.9446 - val_loss: 0.2193 - val_acc: 0.9450\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2329 - acc: 0.9446 - val_loss: 0.2181 - val_acc: 0.9450\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2376 - acc: 0.9423 - val_loss: 0.2174 - val_acc: 0.9450\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2511 - acc: 0.9446 - val_loss: 0.2170 - val_acc: 0.9450\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2331 - acc: 0.9469 - val_loss: 0.2169 - val_acc: 0.9450\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2445 - acc: 0.9446 - val_loss: 0.2169 - val_acc: 0.9450\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2233 - acc: 0.9469 - val_loss: 0.2174 - val_acc: 0.9450\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2263 - acc: 0.9469 - val_loss: 0.2180 - val_acc: 0.9450\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2382 - acc: 0.9446 - val_loss: 0.2189 - val_acc: 0.9450\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2190 - acc: 0.9469 - val_loss: 0.2199 - val_acc: 0.9450\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.42163, saving model to best.model\n",
      "0s - loss: 0.7995 - acc: 0.4896 - val_loss: 0.4216 - val_acc: 0.9541\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.42163 to 0.25621, saving model to best.model\n",
      "0s - loss: 0.4706 - acc: 0.7898 - val_loss: 0.2562 - val_acc: 0.9541\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.25621 to 0.19741, saving model to best.model\n",
      "0s - loss: 0.3447 - acc: 0.9076 - val_loss: 0.1974 - val_acc: 0.9541\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.19741 to 0.18607, saving model to best.model\n",
      "0s - loss: 0.2701 - acc: 0.9353 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2770 - acc: 0.9376 - val_loss: 0.1918 - val_acc: 0.9541\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2381 - acc: 0.9423 - val_loss: 0.2015 - val_acc: 0.9541\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2803 - acc: 0.9423 - val_loss: 0.2109 - val_acc: 0.9541\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2702 - acc: 0.9423 - val_loss: 0.2178 - val_acc: 0.9541\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2806 - acc: 0.9423 - val_loss: 0.2222 - val_acc: 0.9541\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2810 - acc: 0.9423 - val_loss: 0.2239 - val_acc: 0.9541\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2811 - acc: 0.9423 - val_loss: 0.2233 - val_acc: 0.9541\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2756 - acc: 0.9423 - val_loss: 0.2210 - val_acc: 0.9541\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2563 - acc: 0.9423 - val_loss: 0.2178 - val_acc: 0.9541\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3019 - acc: 0.9423 - val_loss: 0.2137 - val_acc: 0.9541\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2591 - acc: 0.9423 - val_loss: 0.2090 - val_acc: 0.9541\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2583 - acc: 0.9423 - val_loss: 0.2042 - val_acc: 0.9541\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2337 - acc: 0.9423 - val_loss: 0.1997 - val_acc: 0.9541\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2366 - acc: 0.9423 - val_loss: 0.1957 - val_acc: 0.9541\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2434 - acc: 0.9423 - val_loss: 0.1924 - val_acc: 0.9541\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2309 - acc: 0.9400 - val_loss: 0.1899 - val_acc: 0.9541\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2520 - acc: 0.9423 - val_loss: 0.1882 - val_acc: 0.9541\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2423 - acc: 0.9400 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2314 - acc: 0.9446 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss improved from 0.18607 to 0.18606, saving model to best.model\n",
      "0s - loss: 0.2405 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.18606 to 0.18594, saving model to best.model\n",
      "0s - loss: 0.2491 - acc: 0.9400 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.18594 to 0.18589, saving model to best.model\n",
      "0s - loss: 0.2541 - acc: 0.9400 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.18589 to 0.18587, saving model to best.model\n",
      "0s - loss: 0.2481 - acc: 0.9423 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2411 - acc: 0.9423 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2387 - acc: 0.9423 - val_loss: 0.1859 - val_acc: 0.9541\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2344 - acc: 0.9446 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2499 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2450 - acc: 0.9423 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2565 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2462 - acc: 0.9423 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2423 - acc: 0.9423 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2582 - acc: 0.9423 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2325 - acc: 0.9423 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2497 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2360 - acc: 0.9423 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2486 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2520 - acc: 0.9423 - val_loss: 0.1860 - val_acc: 0.9541\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2343 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2564 - acc: 0.9423 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2372 - acc: 0.9400 - val_loss: 0.1861 - val_acc: 0.9541\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2291 - acc: 0.9423 - val_loss: 0.1862 - val_acc: 0.9541\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2380 - acc: 0.9423 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2248 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2308 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2449 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2376 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.2363 - acc: 0.9423 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.2172 - acc: 0.9423 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.2386 - acc: 0.9423 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.52215, saving model to best.model\n",
      "0s - loss: 0.9674 - acc: 0.3995 - val_loss: 0.5222 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.52215 to 0.32114, saving model to best.model\n",
      "0s - loss: 0.5833 - acc: 0.6859 - val_loss: 0.3211 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.32114 to 0.26521, saving model to best.model\n",
      "0s - loss: 0.4369 - acc: 0.8314 - val_loss: 0.2652 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.3764 - acc: 0.8799 - val_loss: 0.2674 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3804 - acc: 0.8961 - val_loss: 0.2855 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.4087 - acc: 0.9030 - val_loss: 0.3027 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3921 - acc: 0.9053 - val_loss: 0.3152 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3955 - acc: 0.9053 - val_loss: 0.3220 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.4070 - acc: 0.9053 - val_loss: 0.3237 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.4096 - acc: 0.9030 - val_loss: 0.3211 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.4138 - acc: 0.9053 - val_loss: 0.3154 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3925 - acc: 0.9030 - val_loss: 0.3076 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3810 - acc: 0.9053 - val_loss: 0.2980 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3667 - acc: 0.9030 - val_loss: 0.2882 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.4109 - acc: 0.9053 - val_loss: 0.2793 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3723 - acc: 0.9030 - val_loss: 0.2717 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3676 - acc: 0.8984 - val_loss: 0.2663 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss improved from 0.26521 to 0.26305, saving model to best.model\n",
      "0s - loss: 0.3527 - acc: 0.9007 - val_loss: 0.2631 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss improved from 0.26305 to 0.26170, saving model to best.model\n",
      "0s - loss: 0.3681 - acc: 0.8961 - val_loss: 0.2617 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3451 - acc: 0.9007 - val_loss: 0.2618 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3606 - acc: 0.8891 - val_loss: 0.2622 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3784 - acc: 0.8776 - val_loss: 0.2625 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.3326 - acc: 0.8984 - val_loss: 0.2624 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.3917 - acc: 0.8707 - val_loss: 0.2619 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss improved from 0.26170 to 0.26146, saving model to best.model\n",
      "0s - loss: 0.3697 - acc: 0.8868 - val_loss: 0.2615 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.26146 to 0.26145, saving model to best.model\n",
      "0s - loss: 0.3546 - acc: 0.8961 - val_loss: 0.2615 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3683 - acc: 0.8891 - val_loss: 0.2620 - val_acc: 0.9266\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.3802 - acc: 0.9030 - val_loss: 0.2631 - val_acc: 0.9266\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.3601 - acc: 0.9030 - val_loss: 0.2648 - val_acc: 0.9266\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.3419 - acc: 0.9007 - val_loss: 0.2665 - val_acc: 0.9266\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.3614 - acc: 0.8984 - val_loss: 0.2677 - val_acc: 0.9266\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.3700 - acc: 0.9030 - val_loss: 0.2678 - val_acc: 0.9266\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.3613 - acc: 0.9053 - val_loss: 0.2671 - val_acc: 0.9266\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.3655 - acc: 0.9030 - val_loss: 0.2657 - val_acc: 0.9266\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.3706 - acc: 0.9007 - val_loss: 0.2642 - val_acc: 0.9266\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.3520 - acc: 0.9030 - val_loss: 0.2631 - val_acc: 0.9266\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.3214 - acc: 0.9030 - val_loss: 0.2621 - val_acc: 0.9266\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.3487 - acc: 0.8984 - val_loss: 0.2616 - val_acc: 0.9266\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss improved from 0.26145 to 0.26114, saving model to best.model\n",
      "0s - loss: 0.3305 - acc: 0.9076 - val_loss: 0.2611 - val_acc: 0.9266\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss improved from 0.26114 to 0.26095, saving model to best.model\n",
      "0s - loss: 0.3445 - acc: 0.8984 - val_loss: 0.2610 - val_acc: 0.9266\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.26095 to 0.26089, saving model to best.model\n",
      "0s - loss: 0.3526 - acc: 0.8868 - val_loss: 0.2609 - val_acc: 0.9266\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.26089 to 0.26083, saving model to best.model\n",
      "0s - loss: 0.3508 - acc: 0.9053 - val_loss: 0.2608 - val_acc: 0.9266\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.26083 to 0.26075, saving model to best.model\n",
      "0s - loss: 0.3730 - acc: 0.9030 - val_loss: 0.2607 - val_acc: 0.9266\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.26075 to 0.26061, saving model to best.model\n",
      "0s - loss: 0.3515 - acc: 0.9030 - val_loss: 0.2606 - val_acc: 0.9266\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.26061 to 0.26055, saving model to best.model\n",
      "0s - loss: 0.3565 - acc: 0.8984 - val_loss: 0.2605 - val_acc: 0.9266\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.26055 to 0.26046, saving model to best.model\n",
      "0s - loss: 0.3508 - acc: 0.9053 - val_loss: 0.2605 - val_acc: 0.9266\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.26046 to 0.26045, saving model to best.model\n",
      "0s - loss: 0.3716 - acc: 0.9007 - val_loss: 0.2604 - val_acc: 0.9266\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.26045 to 0.26044, saving model to best.model\n",
      "0s - loss: 0.3487 - acc: 0.9053 - val_loss: 0.2604 - val_acc: 0.9266\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.26044 to 0.26033, saving model to best.model\n",
      "0s - loss: 0.3539 - acc: 0.9030 - val_loss: 0.2603 - val_acc: 0.9266\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.26033 to 0.26020, saving model to best.model\n",
      "0s - loss: 0.3412 - acc: 0.9053 - val_loss: 0.2602 - val_acc: 0.9266\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.26020 to 0.26009, saving model to best.model\n",
      "0s - loss: 0.3622 - acc: 0.9007 - val_loss: 0.2601 - val_acc: 0.9266\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.26009 to 0.26003, saving model to best.model\n",
      "0s - loss: 0.3246 - acc: 0.9076 - val_loss: 0.2600 - val_acc: 0.9266\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.26003 to 0.26001, saving model to best.model\n",
      "0s - loss: 0.3340 - acc: 0.9007 - val_loss: 0.2600 - val_acc: 0.9266\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.26001 to 0.25992, saving model to best.model\n",
      "0s - loss: 0.3389 - acc: 0.9053 - val_loss: 0.2599 - val_acc: 0.9266\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.25992 to 0.25985, saving model to best.model\n",
      "0s - loss: 0.3231 - acc: 0.9076 - val_loss: 0.2598 - val_acc: 0.9266\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.25985 to 0.25973, saving model to best.model\n",
      "0s - loss: 0.3547 - acc: 0.9030 - val_loss: 0.2597 - val_acc: 0.9266\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.25973 to 0.25953, saving model to best.model\n",
      "0s - loss: 0.3642 - acc: 0.9053 - val_loss: 0.2595 - val_acc: 0.9266\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.25953 to 0.25939, saving model to best.model\n",
      "0s - loss: 0.3326 - acc: 0.9053 - val_loss: 0.2594 - val_acc: 0.9266\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.25939 to 0.25921, saving model to best.model\n",
      "0s - loss: 0.3265 - acc: 0.9007 - val_loss: 0.2592 - val_acc: 0.9266\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.25921 to 0.25909, saving model to best.model\n",
      "0s - loss: 0.3541 - acc: 0.9053 - val_loss: 0.2591 - val_acc: 0.9266\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.25909 to 0.25889, saving model to best.model\n",
      "0s - loss: 0.3371 - acc: 0.9053 - val_loss: 0.2589 - val_acc: 0.9266\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.25889 to 0.25875, saving model to best.model\n",
      "0s - loss: 0.3331 - acc: 0.9053 - val_loss: 0.2587 - val_acc: 0.9266\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.25875 to 0.25864, saving model to best.model\n",
      "0s - loss: 0.3389 - acc: 0.9030 - val_loss: 0.2586 - val_acc: 0.9266\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.25864 to 0.25854, saving model to best.model\n",
      "0s - loss: 0.3584 - acc: 0.9030 - val_loss: 0.2585 - val_acc: 0.9266\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.25854 to 0.25840, saving model to best.model\n",
      "0s - loss: 0.3527 - acc: 0.9030 - val_loss: 0.2584 - val_acc: 0.9266\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.25840 to 0.25819, saving model to best.model\n",
      "0s - loss: 0.3337 - acc: 0.9053 - val_loss: 0.2582 - val_acc: 0.9266\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.25819 to 0.25798, saving model to best.model\n",
      "0s - loss: 0.3286 - acc: 0.9030 - val_loss: 0.2580 - val_acc: 0.9266\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.25798 to 0.25780, saving model to best.model\n",
      "0s - loss: 0.3223 - acc: 0.9030 - val_loss: 0.2578 - val_acc: 0.9266\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.25780 to 0.25765, saving model to best.model\n",
      "0s - loss: 0.3385 - acc: 0.9030 - val_loss: 0.2576 - val_acc: 0.9266\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.25765 to 0.25749, saving model to best.model\n",
      "0s - loss: 0.3244 - acc: 0.9053 - val_loss: 0.2575 - val_acc: 0.9266\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.25749 to 0.25727, saving model to best.model\n",
      "0s - loss: 0.3409 - acc: 0.9030 - val_loss: 0.2573 - val_acc: 0.9266\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.25727 to 0.25705, saving model to best.model\n",
      "0s - loss: 0.3274 - acc: 0.9030 - val_loss: 0.2571 - val_acc: 0.9266\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.25705 to 0.25682, saving model to best.model\n",
      "0s - loss: 0.3314 - acc: 0.9053 - val_loss: 0.2568 - val_acc: 0.9266\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.25682 to 0.25660, saving model to best.model\n",
      "0s - loss: 0.3450 - acc: 0.9007 - val_loss: 0.2566 - val_acc: 0.9266\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.25660 to 0.25638, saving model to best.model\n",
      "0s - loss: 0.3128 - acc: 0.9030 - val_loss: 0.2564 - val_acc: 0.9266\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.25638 to 0.25617, saving model to best.model\n",
      "0s - loss: 0.3337 - acc: 0.9053 - val_loss: 0.2562 - val_acc: 0.9266\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.25617 to 0.25595, saving model to best.model\n",
      "0s - loss: 0.3265 - acc: 0.9053 - val_loss: 0.2559 - val_acc: 0.9266\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.25595 to 0.25565, saving model to best.model\n",
      "0s - loss: 0.3275 - acc: 0.9076 - val_loss: 0.2557 - val_acc: 0.9266\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.25565 to 0.25529, saving model to best.model\n",
      "0s - loss: 0.3292 - acc: 0.9053 - val_loss: 0.2553 - val_acc: 0.9266\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.25529 to 0.25495, saving model to best.model\n",
      "0s - loss: 0.3414 - acc: 0.9053 - val_loss: 0.2549 - val_acc: 0.9266\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.25495 to 0.25471, saving model to best.model\n",
      "0s - loss: 0.3252 - acc: 0.9053 - val_loss: 0.2547 - val_acc: 0.9266\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.25471 to 0.25455, saving model to best.model\n",
      "0s - loss: 0.3242 - acc: 0.9053 - val_loss: 0.2546 - val_acc: 0.9266\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.25455 to 0.25447, saving model to best.model\n",
      "0s - loss: 0.3208 - acc: 0.9053 - val_loss: 0.2545 - val_acc: 0.9266\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss improved from 0.25447 to 0.25423, saving model to best.model\n",
      "0s - loss: 0.3180 - acc: 0.9030 - val_loss: 0.2542 - val_acc: 0.9266\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.25423 to 0.25392, saving model to best.model\n",
      "0s - loss: 0.3149 - acc: 0.9053 - val_loss: 0.2539 - val_acc: 0.9266\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.25392 to 0.25355, saving model to best.model\n",
      "0s - loss: 0.3434 - acc: 0.9053 - val_loss: 0.2536 - val_acc: 0.9266\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.25355 to 0.25320, saving model to best.model\n",
      "0s - loss: 0.3314 - acc: 0.9053 - val_loss: 0.2532 - val_acc: 0.9266\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.25320 to 0.25288, saving model to best.model\n",
      "0s - loss: 0.3307 - acc: 0.9030 - val_loss: 0.2529 - val_acc: 0.9266\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.25288 to 0.25249, saving model to best.model\n",
      "0s - loss: 0.3451 - acc: 0.9030 - val_loss: 0.2525 - val_acc: 0.9266\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.25249 to 0.25193, saving model to best.model\n",
      "0s - loss: 0.3334 - acc: 0.9053 - val_loss: 0.2519 - val_acc: 0.9266\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.25193 to 0.25127, saving model to best.model\n",
      "0s - loss: 0.3205 - acc: 0.9053 - val_loss: 0.2513 - val_acc: 0.9266\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.25127 to 0.25074, saving model to best.model\n",
      "0s - loss: 0.3316 - acc: 0.9053 - val_loss: 0.2507 - val_acc: 0.9266\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.25074 to 0.25028, saving model to best.model\n",
      "0s - loss: 0.3184 - acc: 0.9076 - val_loss: 0.2503 - val_acc: 0.9266\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.25028 to 0.24987, saving model to best.model\n",
      "0s - loss: 0.3325 - acc: 0.9053 - val_loss: 0.2499 - val_acc: 0.9266\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.24987 to 0.24944, saving model to best.model\n",
      "0s - loss: 0.3296 - acc: 0.9053 - val_loss: 0.2494 - val_acc: 0.9266\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.24944 to 0.24898, saving model to best.model\n",
      "0s - loss: 0.3025 - acc: 0.9053 - val_loss: 0.2490 - val_acc: 0.9266\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.24898 to 0.24855, saving model to best.model\n",
      "0s - loss: 0.3145 - acc: 0.9053 - val_loss: 0.2486 - val_acc: 0.9266\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.24855 to 0.24814, saving model to best.model\n",
      "0s - loss: 0.2968 - acc: 0.9076 - val_loss: 0.2481 - val_acc: 0.9266\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.24814 to 0.24773, saving model to best.model\n",
      "0s - loss: 0.3307 - acc: 0.9053 - val_loss: 0.2477 - val_acc: 0.9266\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.24773 to 0.24725, saving model to best.model\n",
      "0s - loss: 0.3138 - acc: 0.9053 - val_loss: 0.2472 - val_acc: 0.9266\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.24725 to 0.24667, saving model to best.model\n",
      "0s - loss: 0.3098 - acc: 0.9053 - val_loss: 0.2467 - val_acc: 0.9266\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.24667 to 0.24612, saving model to best.model\n",
      "0s - loss: 0.3031 - acc: 0.9053 - val_loss: 0.2461 - val_acc: 0.9266\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.24612 to 0.24546, saving model to best.model\n",
      "0s - loss: 0.3146 - acc: 0.9076 - val_loss: 0.2455 - val_acc: 0.9266\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.24546 to 0.24484, saving model to best.model\n",
      "0s - loss: 0.3135 - acc: 0.9053 - val_loss: 0.2448 - val_acc: 0.9266\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.24484 to 0.24422, saving model to best.model\n",
      "0s - loss: 0.3086 - acc: 0.9053 - val_loss: 0.2442 - val_acc: 0.9266\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.24422 to 0.24351, saving model to best.model\n",
      "0s - loss: 0.2823 - acc: 0.9053 - val_loss: 0.2435 - val_acc: 0.9266\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.24351 to 0.24284, saving model to best.model\n",
      "0s - loss: 0.3166 - acc: 0.9053 - val_loss: 0.2428 - val_acc: 0.9266\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.24284 to 0.24234, saving model to best.model\n",
      "0s - loss: 0.3103 - acc: 0.9053 - val_loss: 0.2423 - val_acc: 0.9266\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.24234 to 0.24180, saving model to best.model\n",
      "0s - loss: 0.3109 - acc: 0.9053 - val_loss: 0.2418 - val_acc: 0.9266\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.24180 to 0.24117, saving model to best.model\n",
      "0s - loss: 0.3093 - acc: 0.9053 - val_loss: 0.2412 - val_acc: 0.9266\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.24117 to 0.24018, saving model to best.model\n",
      "0s - loss: 0.2900 - acc: 0.9076 - val_loss: 0.2402 - val_acc: 0.9266\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.24018 to 0.23922, saving model to best.model\n",
      "0s - loss: 0.2912 - acc: 0.9053 - val_loss: 0.2392 - val_acc: 0.9266\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.23922 to 0.23829, saving model to best.model\n",
      "0s - loss: 0.2920 - acc: 0.9076 - val_loss: 0.2383 - val_acc: 0.9266\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.23829 to 0.23729, saving model to best.model\n",
      "0s - loss: 0.3034 - acc: 0.9053 - val_loss: 0.2373 - val_acc: 0.9266\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.23729 to 0.23645, saving model to best.model\n",
      "0s - loss: 0.3027 - acc: 0.9030 - val_loss: 0.2365 - val_acc: 0.9266\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.23645 to 0.23566, saving model to best.model\n",
      "0s - loss: 0.2996 - acc: 0.9053 - val_loss: 0.2357 - val_acc: 0.9266\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.23566 to 0.23484, saving model to best.model\n",
      "0s - loss: 0.3002 - acc: 0.9053 - val_loss: 0.2348 - val_acc: 0.9266\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.23484 to 0.23381, saving model to best.model\n",
      "0s - loss: 0.2943 - acc: 0.9076 - val_loss: 0.2338 - val_acc: 0.9266\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.23381 to 0.23248, saving model to best.model\n",
      "0s - loss: 0.2843 - acc: 0.9053 - val_loss: 0.2325 - val_acc: 0.9266\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.23248 to 0.23122, saving model to best.model\n",
      "0s - loss: 0.2931 - acc: 0.9053 - val_loss: 0.2312 - val_acc: 0.9266\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.23122 to 0.23000, saving model to best.model\n",
      "0s - loss: 0.3046 - acc: 0.9076 - val_loss: 0.2300 - val_acc: 0.9266\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.23000 to 0.22888, saving model to best.model\n",
      "0s - loss: 0.2926 - acc: 0.9076 - val_loss: 0.2289 - val_acc: 0.9266\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.22888 to 0.22768, saving model to best.model\n",
      "0s - loss: 0.2738 - acc: 0.9053 - val_loss: 0.2277 - val_acc: 0.9266\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.22768 to 0.22659, saving model to best.model\n",
      "0s - loss: 0.2822 - acc: 0.9053 - val_loss: 0.2266 - val_acc: 0.9266\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.22659 to 0.22540, saving model to best.model\n",
      "0s - loss: 0.2951 - acc: 0.9053 - val_loss: 0.2254 - val_acc: 0.9266\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss improved from 0.22540 to 0.22391, saving model to best.model\n",
      "0s - loss: 0.2957 - acc: 0.9053 - val_loss: 0.2239 - val_acc: 0.9266\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss improved from 0.22391 to 0.22234, saving model to best.model\n",
      "0s - loss: 0.3026 - acc: 0.9053 - val_loss: 0.2223 - val_acc: 0.9266\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss improved from 0.22234 to 0.22087, saving model to best.model\n",
      "0s - loss: 0.2860 - acc: 0.9053 - val_loss: 0.2209 - val_acc: 0.9266\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss improved from 0.22087 to 0.21925, saving model to best.model\n",
      "0s - loss: 0.2926 - acc: 0.9053 - val_loss: 0.2193 - val_acc: 0.9266\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss improved from 0.21925 to 0.21774, saving model to best.model\n",
      "0s - loss: 0.2975 - acc: 0.9007 - val_loss: 0.2177 - val_acc: 0.9266\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.21774 to 0.21612, saving model to best.model\n",
      "0s - loss: 0.2667 - acc: 0.9076 - val_loss: 0.2161 - val_acc: 0.9266\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.21612 to 0.21441, saving model to best.model\n",
      "0s - loss: 0.2672 - acc: 0.9076 - val_loss: 0.2144 - val_acc: 0.9266\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.21441 to 0.21270, saving model to best.model\n",
      "0s - loss: 0.2766 - acc: 0.9007 - val_loss: 0.2127 - val_acc: 0.9266\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.21270 to 0.21105, saving model to best.model\n",
      "0s - loss: 0.2659 - acc: 0.9053 - val_loss: 0.2111 - val_acc: 0.9266\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss improved from 0.21105 to 0.20921, saving model to best.model\n",
      "0s - loss: 0.2530 - acc: 0.9099 - val_loss: 0.2092 - val_acc: 0.9266\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss improved from 0.20921 to 0.20710, saving model to best.model\n",
      "0s - loss: 0.2654 - acc: 0.9053 - val_loss: 0.2071 - val_acc: 0.9266\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss improved from 0.20710 to 0.20529, saving model to best.model\n",
      "0s - loss: 0.2811 - acc: 0.9053 - val_loss: 0.2053 - val_acc: 0.9266\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.20529 to 0.20363, saving model to best.model\n",
      "0s - loss: 0.2601 - acc: 0.9076 - val_loss: 0.2036 - val_acc: 0.9266\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.20363 to 0.20168, saving model to best.model\n",
      "0s - loss: 0.2410 - acc: 0.9122 - val_loss: 0.2017 - val_acc: 0.9266\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.20168 to 0.19911, saving model to best.model\n",
      "0s - loss: 0.2490 - acc: 0.9076 - val_loss: 0.1991 - val_acc: 0.9266\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.19911 to 0.19629, saving model to best.model\n",
      "0s - loss: 0.2545 - acc: 0.9053 - val_loss: 0.1963 - val_acc: 0.9266\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss improved from 0.19629 to 0.19442, saving model to best.model\n",
      "0s - loss: 0.2651 - acc: 0.9076 - val_loss: 0.1944 - val_acc: 0.9266\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss improved from 0.19442 to 0.19288, saving model to best.model\n",
      "0s - loss: 0.2575 - acc: 0.9145 - val_loss: 0.1929 - val_acc: 0.9266\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss improved from 0.19288 to 0.19017, saving model to best.model\n",
      "0s - loss: 0.2346 - acc: 0.9076 - val_loss: 0.1902 - val_acc: 0.9266\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss improved from 0.19017 to 0.18690, saving model to best.model\n",
      "0s - loss: 0.2501 - acc: 0.9099 - val_loss: 0.1869 - val_acc: 0.9266\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss improved from 0.18690 to 0.18480, saving model to best.model\n",
      "0s - loss: 0.2414 - acc: 0.9122 - val_loss: 0.1848 - val_acc: 0.9266\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.18480 to 0.18285, saving model to best.model\n",
      "0s - loss: 0.2542 - acc: 0.9053 - val_loss: 0.1829 - val_acc: 0.9358\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.18285 to 0.18013, saving model to best.model\n",
      "0s - loss: 0.2619 - acc: 0.9076 - val_loss: 0.1801 - val_acc: 0.9358\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss improved from 0.18013 to 0.17763, saving model to best.model\n",
      "0s - loss: 0.2396 - acc: 0.9053 - val_loss: 0.1776 - val_acc: 0.9358\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss improved from 0.17763 to 0.17651, saving model to best.model\n",
      "0s - loss: 0.2356 - acc: 0.9169 - val_loss: 0.1765 - val_acc: 0.9358\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss improved from 0.17651 to 0.17514, saving model to best.model\n",
      "0s - loss: 0.2458 - acc: 0.9076 - val_loss: 0.1751 - val_acc: 0.9358\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss improved from 0.17514 to 0.17286, saving model to best.model\n",
      "0s - loss: 0.2305 - acc: 0.9145 - val_loss: 0.1729 - val_acc: 0.9358\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss improved from 0.17286 to 0.16971, saving model to best.model\n",
      "0s - loss: 0.2300 - acc: 0.9169 - val_loss: 0.1697 - val_acc: 0.9358\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss improved from 0.16971 to 0.16776, saving model to best.model\n",
      "0s - loss: 0.2167 - acc: 0.9099 - val_loss: 0.1678 - val_acc: 0.9358\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss improved from 0.16776 to 0.16604, saving model to best.model\n",
      "0s - loss: 0.2214 - acc: 0.9122 - val_loss: 0.1660 - val_acc: 0.9266\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss improved from 0.16604 to 0.16395, saving model to best.model\n",
      "0s - loss: 0.2323 - acc: 0.9099 - val_loss: 0.1640 - val_acc: 0.9358\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss improved from 0.16395 to 0.16272, saving model to best.model\n",
      "0s - loss: 0.2140 - acc: 0.9215 - val_loss: 0.1627 - val_acc: 0.9358\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss improved from 0.16272 to 0.16203, saving model to best.model\n",
      "0s - loss: 0.2309 - acc: 0.9192 - val_loss: 0.1620 - val_acc: 0.9358\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss improved from 0.16203 to 0.16095, saving model to best.model\n",
      "0s - loss: 0.2126 - acc: 0.9192 - val_loss: 0.1610 - val_acc: 0.9358\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss improved from 0.16095 to 0.15837, saving model to best.model\n",
      "0s - loss: 0.2109 - acc: 0.9192 - val_loss: 0.1584 - val_acc: 0.9358\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss improved from 0.15837 to 0.15513, saving model to best.model\n",
      "0s - loss: 0.2350 - acc: 0.9053 - val_loss: 0.1551 - val_acc: 0.9266\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss improved from 0.15513 to 0.15320, saving model to best.model\n",
      "0s - loss: 0.2146 - acc: 0.9169 - val_loss: 0.1532 - val_acc: 0.9266\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss improved from 0.15320 to 0.15155, saving model to best.model\n",
      "0s - loss: 0.2256 - acc: 0.9169 - val_loss: 0.1515 - val_acc: 0.9266\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss improved from 0.15155 to 0.15013, saving model to best.model\n",
      "0s - loss: 0.2045 - acc: 0.9330 - val_loss: 0.1501 - val_acc: 0.9266\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss improved from 0.15013 to 0.14963, saving model to best.model\n",
      "0s - loss: 0.2027 - acc: 0.9423 - val_loss: 0.1496 - val_acc: 0.9266\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss improved from 0.14963 to 0.14910, saving model to best.model\n",
      "0s - loss: 0.1968 - acc: 0.9169 - val_loss: 0.1491 - val_acc: 0.9266\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss improved from 0.14910 to 0.14664, saving model to best.model\n",
      "0s - loss: 0.2099 - acc: 0.9215 - val_loss: 0.1466 - val_acc: 0.9266\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss improved from 0.14664 to 0.14401, saving model to best.model\n",
      "0s - loss: 0.2053 - acc: 0.9307 - val_loss: 0.1440 - val_acc: 0.9266\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss improved from 0.14401 to 0.14214, saving model to best.model\n",
      "0s - loss: 0.1815 - acc: 0.9192 - val_loss: 0.1421 - val_acc: 0.9266\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss improved from 0.14214 to 0.14053, saving model to best.model\n",
      "0s - loss: 0.1730 - acc: 0.9376 - val_loss: 0.1405 - val_acc: 0.9266\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss improved from 0.14053 to 0.13970, saving model to best.model\n",
      "0s - loss: 0.2095 - acc: 0.9238 - val_loss: 0.1397 - val_acc: 0.9266\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss improved from 0.13970 to 0.13871, saving model to best.model\n",
      "0s - loss: 0.1824 - acc: 0.9284 - val_loss: 0.1387 - val_acc: 0.9266\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss improved from 0.13871 to 0.13716, saving model to best.model\n",
      "0s - loss: 0.1805 - acc: 0.9307 - val_loss: 0.1372 - val_acc: 0.9266\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss improved from 0.13716 to 0.13457, saving model to best.model\n",
      "0s - loss: 0.1831 - acc: 0.9261 - val_loss: 0.1346 - val_acc: 0.9450\n",
      "Epoch 175/200\n",
      "Epoch 00174: val_loss improved from 0.13457 to 0.13263, saving model to best.model\n",
      "0s - loss: 0.2127 - acc: 0.9284 - val_loss: 0.1326 - val_acc: 0.9450\n",
      "Epoch 176/200\n",
      "Epoch 00175: val_loss improved from 0.13263 to 0.13169, saving model to best.model\n",
      "0s - loss: 0.1970 - acc: 0.9376 - val_loss: 0.1317 - val_acc: 0.9450\n",
      "Epoch 177/200\n",
      "Epoch 00176: val_loss improved from 0.13169 to 0.13142, saving model to best.model\n",
      "0s - loss: 0.1812 - acc: 0.9376 - val_loss: 0.1314 - val_acc: 0.9450\n",
      "Epoch 178/200\n",
      "Epoch 00177: val_loss improved from 0.13142 to 0.13137, saving model to best.model\n",
      "0s - loss: 0.1596 - acc: 0.9307 - val_loss: 0.1314 - val_acc: 0.9450\n",
      "Epoch 179/200\n",
      "Epoch 00178: val_loss improved from 0.13137 to 0.12989, saving model to best.model\n",
      "0s - loss: 0.1861 - acc: 0.9284 - val_loss: 0.1299 - val_acc: 0.9450\n",
      "Epoch 180/200\n",
      "Epoch 00179: val_loss improved from 0.12989 to 0.12785, saving model to best.model\n",
      "0s - loss: 0.1825 - acc: 0.9238 - val_loss: 0.1278 - val_acc: 0.9450\n",
      "Epoch 181/200\n",
      "Epoch 00180: val_loss improved from 0.12785 to 0.12655, saving model to best.model\n",
      "0s - loss: 0.1714 - acc: 0.9307 - val_loss: 0.1266 - val_acc: 0.9358\n",
      "Epoch 182/200\n",
      "Epoch 00181: val_loss improved from 0.12655 to 0.12576, saving model to best.model\n",
      "0s - loss: 0.1925 - acc: 0.9330 - val_loss: 0.1258 - val_acc: 0.9358\n",
      "Epoch 183/200\n",
      "Epoch 00182: val_loss improved from 0.12576 to 0.12486, saving model to best.model\n",
      "0s - loss: 0.1633 - acc: 0.9446 - val_loss: 0.1249 - val_acc: 0.9358\n",
      "Epoch 184/200\n",
      "Epoch 00183: val_loss improved from 0.12486 to 0.12447, saving model to best.model\n",
      "0s - loss: 0.1722 - acc: 0.9330 - val_loss: 0.1245 - val_acc: 0.9358\n",
      "Epoch 185/200\n",
      "Epoch 00184: val_loss did not improve\n",
      "0s - loss: 0.1688 - acc: 0.9376 - val_loss: 0.1257 - val_acc: 0.9450\n",
      "Epoch 186/200\n",
      "Epoch 00185: val_loss did not improve\n",
      "0s - loss: 0.1740 - acc: 0.9353 - val_loss: 0.1258 - val_acc: 0.9450\n",
      "Epoch 187/200\n",
      "Epoch 00186: val_loss did not improve\n",
      "0s - loss: 0.1683 - acc: 0.9353 - val_loss: 0.1247 - val_acc: 0.9450\n",
      "Epoch 188/200\n",
      "Epoch 00187: val_loss improved from 0.12447 to 0.12293, saving model to best.model\n",
      "0s - loss: 0.1635 - acc: 0.9469 - val_loss: 0.1229 - val_acc: 0.9358\n",
      "Epoch 189/200\n",
      "Epoch 00188: val_loss improved from 0.12293 to 0.12066, saving model to best.model\n",
      "0s - loss: 0.1592 - acc: 0.9376 - val_loss: 0.1207 - val_acc: 0.9358\n",
      "Epoch 190/200\n",
      "Epoch 00189: val_loss improved from 0.12066 to 0.11829, saving model to best.model\n",
      "0s - loss: 0.1555 - acc: 0.9376 - val_loss: 0.1183 - val_acc: 0.9450\n",
      "Epoch 191/200\n",
      "Epoch 00190: val_loss improved from 0.11829 to 0.11742, saving model to best.model\n",
      "0s - loss: 0.1711 - acc: 0.9515 - val_loss: 0.1174 - val_acc: 0.9450\n",
      "Epoch 192/200\n",
      "Epoch 00191: val_loss improved from 0.11742 to 0.11690, saving model to best.model\n",
      "0s - loss: 0.1507 - acc: 0.9469 - val_loss: 0.1169 - val_acc: 0.9450\n",
      "Epoch 193/200\n",
      "Epoch 00192: val_loss did not improve\n",
      "0s - loss: 0.1672 - acc: 0.9307 - val_loss: 0.1179 - val_acc: 0.9450\n",
      "Epoch 194/200\n",
      "Epoch 00193: val_loss did not improve\n",
      "0s - loss: 0.1701 - acc: 0.9353 - val_loss: 0.1223 - val_acc: 0.9450\n",
      "Epoch 195/200\n",
      "Epoch 00194: val_loss did not improve\n",
      "0s - loss: 0.1860 - acc: 0.9284 - val_loss: 0.1251 - val_acc: 0.9450\n",
      "Epoch 196/200\n",
      "Epoch 00195: val_loss did not improve\n",
      "0s - loss: 0.1890 - acc: 0.9353 - val_loss: 0.1225 - val_acc: 0.9450\n",
      "Epoch 197/200\n",
      "Epoch 00196: val_loss improved from 0.11690 to 0.11679, saving model to best.model\n",
      "0s - loss: 0.1617 - acc: 0.9400 - val_loss: 0.1168 - val_acc: 0.9450\n",
      "Epoch 198/200\n",
      "Epoch 00197: val_loss improved from 0.11679 to 0.11569, saving model to best.model\n",
      "0s - loss: 0.1647 - acc: 0.9400 - val_loss: 0.1157 - val_acc: 0.9450\n",
      "Epoch 199/200\n",
      "Epoch 00198: val_loss did not improve\n",
      "0s - loss: 0.1392 - acc: 0.9446 - val_loss: 0.1158 - val_acc: 0.9450\n",
      "Epoch 200/200\n",
      "Epoch 00199: val_loss improved from 0.11569 to 0.11527, saving model to best.model\n",
      "0s - loss: 0.1757 - acc: 0.9400 - val_loss: 0.1153 - val_acc: 0.9450\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.26604, saving model to best.model\n",
      "0s - loss: 0.4119 - acc: 0.8406 - val_loss: 0.2660 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.3354 - acc: 0.9122 - val_loss: 0.2667 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.3133 - acc: 0.9261 - val_loss: 0.2814 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2896 - acc: 0.9261 - val_loss: 0.2928 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3173 - acc: 0.9261 - val_loss: 0.2960 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3312 - acc: 0.9261 - val_loss: 0.2932 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2941 - acc: 0.9261 - val_loss: 0.2865 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3251 - acc: 0.9215 - val_loss: 0.2791 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2928 - acc: 0.9261 - val_loss: 0.2725 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2801 - acc: 0.9284 - val_loss: 0.2670 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss improved from 0.26604 to 0.26415, saving model to best.model\n",
      "0s - loss: 0.2825 - acc: 0.9238 - val_loss: 0.2641 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss improved from 0.26415 to 0.26305, saving model to best.model\n",
      "0s - loss: 0.2707 - acc: 0.9238 - val_loss: 0.2630 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss improved from 0.26305 to 0.26294, saving model to best.model\n",
      "0s - loss: 0.3071 - acc: 0.9215 - val_loss: 0.2629 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2888 - acc: 0.9238 - val_loss: 0.2631 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3138 - acc: 0.9215 - val_loss: 0.2637 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2988 - acc: 0.9238 - val_loss: 0.2645 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2928 - acc: 0.9261 - val_loss: 0.2653 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2904 - acc: 0.9261 - val_loss: 0.2662 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3068 - acc: 0.9284 - val_loss: 0.2664 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3212 - acc: 0.9261 - val_loss: 0.2665 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2934 - acc: 0.9261 - val_loss: 0.2661 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2956 - acc: 0.9261 - val_loss: 0.2659 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2891 - acc: 0.9261 - val_loss: 0.2651 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2882 - acc: 0.9261 - val_loss: 0.2647 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2828 - acc: 0.9238 - val_loss: 0.2644 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2875 - acc: 0.9261 - val_loss: 0.2639 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.3178 - acc: 0.9261 - val_loss: 0.2635 - val_acc: 0.9266\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2933 - acc: 0.9261 - val_loss: 0.2632 - val_acc: 0.9266\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss improved from 0.26294 to 0.26289, saving model to best.model\n",
      "0s - loss: 0.2909 - acc: 0.9261 - val_loss: 0.2629 - val_acc: 0.9266\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss improved from 0.26289 to 0.26252, saving model to best.model\n",
      "0s - loss: 0.2934 - acc: 0.9261 - val_loss: 0.2625 - val_acc: 0.9266\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss improved from 0.26252 to 0.26212, saving model to best.model\n",
      "0s - loss: 0.2794 - acc: 0.9261 - val_loss: 0.2621 - val_acc: 0.9266\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss improved from 0.26212 to 0.26165, saving model to best.model\n",
      "0s - loss: 0.2930 - acc: 0.9261 - val_loss: 0.2616 - val_acc: 0.9266\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss improved from 0.26165 to 0.26151, saving model to best.model\n",
      "0s - loss: 0.2763 - acc: 0.9238 - val_loss: 0.2615 - val_acc: 0.9266\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss improved from 0.26151 to 0.26150, saving model to best.model\n",
      "0s - loss: 0.2835 - acc: 0.9261 - val_loss: 0.2615 - val_acc: 0.9266\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2775 - acc: 0.9261 - val_loss: 0.2616 - val_acc: 0.9266\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2905 - acc: 0.9261 - val_loss: 0.2620 - val_acc: 0.9266\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2739 - acc: 0.9261 - val_loss: 0.2621 - val_acc: 0.9266\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2813 - acc: 0.9261 - val_loss: 0.2622 - val_acc: 0.9266\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2868 - acc: 0.9261 - val_loss: 0.2620 - val_acc: 0.9266\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2761 - acc: 0.9261 - val_loss: 0.2616 - val_acc: 0.9266\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss improved from 0.26150 to 0.26127, saving model to best.model\n",
      "0s - loss: 0.2743 - acc: 0.9261 - val_loss: 0.2613 - val_acc: 0.9266\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss improved from 0.26127 to 0.26078, saving model to best.model\n",
      "0s - loss: 0.2675 - acc: 0.9261 - val_loss: 0.2608 - val_acc: 0.9266\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss improved from 0.26078 to 0.26028, saving model to best.model\n",
      "0s - loss: 0.2876 - acc: 0.9238 - val_loss: 0.2603 - val_acc: 0.9266\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss improved from 0.26028 to 0.26000, saving model to best.model\n",
      "0s - loss: 0.2732 - acc: 0.9261 - val_loss: 0.2600 - val_acc: 0.9266\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss improved from 0.26000 to 0.25974, saving model to best.model\n",
      "0s - loss: 0.2835 - acc: 0.9261 - val_loss: 0.2597 - val_acc: 0.9266\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss improved from 0.25974 to 0.25957, saving model to best.model\n",
      "0s - loss: 0.2765 - acc: 0.9261 - val_loss: 0.2596 - val_acc: 0.9266\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss improved from 0.25957 to 0.25955, saving model to best.model\n",
      "0s - loss: 0.2747 - acc: 0.9261 - val_loss: 0.2595 - val_acc: 0.9266\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss improved from 0.25955 to 0.25901, saving model to best.model\n",
      "0s - loss: 0.2780 - acc: 0.9261 - val_loss: 0.2590 - val_acc: 0.9266\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss improved from 0.25901 to 0.25874, saving model to best.model\n",
      "0s - loss: 0.2734 - acc: 0.9261 - val_loss: 0.2587 - val_acc: 0.9266\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss improved from 0.25874 to 0.25869, saving model to best.model\n",
      "0s - loss: 0.2851 - acc: 0.9261 - val_loss: 0.2587 - val_acc: 0.9266\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss improved from 0.25869 to 0.25859, saving model to best.model\n",
      "0s - loss: 0.2704 - acc: 0.9261 - val_loss: 0.2586 - val_acc: 0.9266\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss improved from 0.25859 to 0.25846, saving model to best.model\n",
      "0s - loss: 0.2869 - acc: 0.9261 - val_loss: 0.2585 - val_acc: 0.9266\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss improved from 0.25846 to 0.25817, saving model to best.model\n",
      "0s - loss: 0.2778 - acc: 0.9261 - val_loss: 0.2582 - val_acc: 0.9266\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss improved from 0.25817 to 0.25766, saving model to best.model\n",
      "0s - loss: 0.2602 - acc: 0.9261 - val_loss: 0.2577 - val_acc: 0.9266\n",
      "Epoch 55/200\n",
      "Epoch 00054: val_loss improved from 0.25766 to 0.25726, saving model to best.model\n",
      "0s - loss: 0.2758 - acc: 0.9261 - val_loss: 0.2573 - val_acc: 0.9266\n",
      "Epoch 56/200\n",
      "Epoch 00055: val_loss improved from 0.25726 to 0.25681, saving model to best.model\n",
      "0s - loss: 0.2794 - acc: 0.9261 - val_loss: 0.2568 - val_acc: 0.9266\n",
      "Epoch 57/200\n",
      "Epoch 00056: val_loss improved from 0.25681 to 0.25640, saving model to best.model\n",
      "0s - loss: 0.2760 - acc: 0.9261 - val_loss: 0.2564 - val_acc: 0.9266\n",
      "Epoch 58/200\n",
      "Epoch 00057: val_loss improved from 0.25640 to 0.25600, saving model to best.model\n",
      "0s - loss: 0.2659 - acc: 0.9261 - val_loss: 0.2560 - val_acc: 0.9266\n",
      "Epoch 59/200\n",
      "Epoch 00058: val_loss improved from 0.25600 to 0.25573, saving model to best.model\n",
      "0s - loss: 0.2701 - acc: 0.9261 - val_loss: 0.2557 - val_acc: 0.9266\n",
      "Epoch 60/200\n",
      "Epoch 00059: val_loss improved from 0.25573 to 0.25548, saving model to best.model\n",
      "0s - loss: 0.2681 - acc: 0.9261 - val_loss: 0.2555 - val_acc: 0.9266\n",
      "Epoch 61/200\n",
      "Epoch 00060: val_loss improved from 0.25548 to 0.25526, saving model to best.model\n",
      "0s - loss: 0.2770 - acc: 0.9261 - val_loss: 0.2553 - val_acc: 0.9266\n",
      "Epoch 62/200\n",
      "Epoch 00061: val_loss improved from 0.25526 to 0.25523, saving model to best.model\n",
      "0s - loss: 0.2701 - acc: 0.9261 - val_loss: 0.2552 - val_acc: 0.9266\n",
      "Epoch 63/200\n",
      "Epoch 00062: val_loss improved from 0.25523 to 0.25485, saving model to best.model\n",
      "0s - loss: 0.2778 - acc: 0.9261 - val_loss: 0.2548 - val_acc: 0.9266\n",
      "Epoch 64/200\n",
      "Epoch 00063: val_loss improved from 0.25485 to 0.25442, saving model to best.model\n",
      "0s - loss: 0.2704 - acc: 0.9261 - val_loss: 0.2544 - val_acc: 0.9266\n",
      "Epoch 65/200\n",
      "Epoch 00064: val_loss improved from 0.25442 to 0.25387, saving model to best.model\n",
      "0s - loss: 0.2580 - acc: 0.9261 - val_loss: 0.2539 - val_acc: 0.9266\n",
      "Epoch 66/200\n",
      "Epoch 00065: val_loss improved from 0.25387 to 0.25355, saving model to best.model\n",
      "0s - loss: 0.2668 - acc: 0.9261 - val_loss: 0.2535 - val_acc: 0.9266\n",
      "Epoch 67/200\n",
      "Epoch 00066: val_loss improved from 0.25355 to 0.25284, saving model to best.model\n",
      "0s - loss: 0.2667 - acc: 0.9261 - val_loss: 0.2528 - val_acc: 0.9266\n",
      "Epoch 68/200\n",
      "Epoch 00067: val_loss improved from 0.25284 to 0.25210, saving model to best.model\n",
      "0s - loss: 0.2766 - acc: 0.9261 - val_loss: 0.2521 - val_acc: 0.9266\n",
      "Epoch 69/200\n",
      "Epoch 00068: val_loss improved from 0.25210 to 0.25162, saving model to best.model\n",
      "0s - loss: 0.2599 - acc: 0.9261 - val_loss: 0.2516 - val_acc: 0.9266\n",
      "Epoch 70/200\n",
      "Epoch 00069: val_loss improved from 0.25162 to 0.25126, saving model to best.model\n",
      "0s - loss: 0.2604 - acc: 0.9261 - val_loss: 0.2513 - val_acc: 0.9266\n",
      "Epoch 71/200\n",
      "Epoch 00070: val_loss improved from 0.25126 to 0.25105, saving model to best.model\n",
      "0s - loss: 0.2619 - acc: 0.9261 - val_loss: 0.2510 - val_acc: 0.9266\n",
      "Epoch 72/200\n",
      "Epoch 00071: val_loss improved from 0.25105 to 0.25082, saving model to best.model\n",
      "0s - loss: 0.2606 - acc: 0.9261 - val_loss: 0.2508 - val_acc: 0.9266\n",
      "Epoch 73/200\n",
      "Epoch 00072: val_loss improved from 0.25082 to 0.25031, saving model to best.model\n",
      "0s - loss: 0.2473 - acc: 0.9261 - val_loss: 0.2503 - val_acc: 0.9266\n",
      "Epoch 74/200\n",
      "Epoch 00073: val_loss improved from 0.25031 to 0.24933, saving model to best.model\n",
      "0s - loss: 0.2670 - acc: 0.9261 - val_loss: 0.2493 - val_acc: 0.9266\n",
      "Epoch 75/200\n",
      "Epoch 00074: val_loss improved from 0.24933 to 0.24870, saving model to best.model\n",
      "0s - loss: 0.2504 - acc: 0.9261 - val_loss: 0.2487 - val_acc: 0.9266\n",
      "Epoch 76/200\n",
      "Epoch 00075: val_loss improved from 0.24870 to 0.24795, saving model to best.model\n",
      "0s - loss: 0.2586 - acc: 0.9261 - val_loss: 0.2479 - val_acc: 0.9266\n",
      "Epoch 77/200\n",
      "Epoch 00076: val_loss improved from 0.24795 to 0.24723, saving model to best.model\n",
      "0s - loss: 0.2506 - acc: 0.9261 - val_loss: 0.2472 - val_acc: 0.9266\n",
      "Epoch 78/200\n",
      "Epoch 00077: val_loss improved from 0.24723 to 0.24631, saving model to best.model\n",
      "0s - loss: 0.2710 - acc: 0.9261 - val_loss: 0.2463 - val_acc: 0.9266\n",
      "Epoch 79/200\n",
      "Epoch 00078: val_loss improved from 0.24631 to 0.24531, saving model to best.model\n",
      "0s - loss: 0.2489 - acc: 0.9261 - val_loss: 0.2453 - val_acc: 0.9266\n",
      "Epoch 80/200\n",
      "Epoch 00079: val_loss improved from 0.24531 to 0.24420, saving model to best.model\n",
      "0s - loss: 0.2576 - acc: 0.9261 - val_loss: 0.2442 - val_acc: 0.9266\n",
      "Epoch 81/200\n",
      "Epoch 00080: val_loss improved from 0.24420 to 0.24324, saving model to best.model\n",
      "0s - loss: 0.2490 - acc: 0.9261 - val_loss: 0.2432 - val_acc: 0.9266\n",
      "Epoch 82/200\n",
      "Epoch 00081: val_loss improved from 0.24324 to 0.24253, saving model to best.model\n",
      "0s - loss: 0.2535 - acc: 0.9261 - val_loss: 0.2425 - val_acc: 0.9266\n",
      "Epoch 83/200\n",
      "Epoch 00082: val_loss improved from 0.24253 to 0.24224, saving model to best.model\n",
      "0s - loss: 0.2444 - acc: 0.9261 - val_loss: 0.2422 - val_acc: 0.9266\n",
      "Epoch 84/200\n",
      "Epoch 00083: val_loss did not improve\n",
      "0s - loss: 0.2414 - acc: 0.9261 - val_loss: 0.2424 - val_acc: 0.9266\n",
      "Epoch 85/200\n",
      "Epoch 00084: val_loss improved from 0.24224 to 0.24177, saving model to best.model\n",
      "0s - loss: 0.2586 - acc: 0.9261 - val_loss: 0.2418 - val_acc: 0.9266\n",
      "Epoch 86/200\n",
      "Epoch 00085: val_loss improved from 0.24177 to 0.24053, saving model to best.model\n",
      "0s - loss: 0.2542 - acc: 0.9261 - val_loss: 0.2405 - val_acc: 0.9266\n",
      "Epoch 87/200\n",
      "Epoch 00086: val_loss improved from 0.24053 to 0.23879, saving model to best.model\n",
      "0s - loss: 0.2429 - acc: 0.9261 - val_loss: 0.2388 - val_acc: 0.9266\n",
      "Epoch 88/200\n",
      "Epoch 00087: val_loss improved from 0.23879 to 0.23728, saving model to best.model\n",
      "0s - loss: 0.2629 - acc: 0.9261 - val_loss: 0.2373 - val_acc: 0.9266\n",
      "Epoch 89/200\n",
      "Epoch 00088: val_loss improved from 0.23728 to 0.23586, saving model to best.model\n",
      "0s - loss: 0.2419 - acc: 0.9261 - val_loss: 0.2359 - val_acc: 0.9266\n",
      "Epoch 90/200\n",
      "Epoch 00089: val_loss improved from 0.23586 to 0.23455, saving model to best.model\n",
      "0s - loss: 0.2523 - acc: 0.9261 - val_loss: 0.2346 - val_acc: 0.9266\n",
      "Epoch 91/200\n",
      "Epoch 00090: val_loss improved from 0.23455 to 0.23330, saving model to best.model\n",
      "0s - loss: 0.2531 - acc: 0.9261 - val_loss: 0.2333 - val_acc: 0.9266\n",
      "Epoch 92/200\n",
      "Epoch 00091: val_loss improved from 0.23330 to 0.23257, saving model to best.model\n",
      "0s - loss: 0.2486 - acc: 0.9261 - val_loss: 0.2326 - val_acc: 0.9266\n",
      "Epoch 93/200\n",
      "Epoch 00092: val_loss improved from 0.23257 to 0.23134, saving model to best.model\n",
      "0s - loss: 0.2393 - acc: 0.9261 - val_loss: 0.2313 - val_acc: 0.9266\n",
      "Epoch 94/200\n",
      "Epoch 00093: val_loss improved from 0.23134 to 0.23061, saving model to best.model\n",
      "0s - loss: 0.2382 - acc: 0.9261 - val_loss: 0.2306 - val_acc: 0.9266\n",
      "Epoch 95/200\n",
      "Epoch 00094: val_loss improved from 0.23061 to 0.23033, saving model to best.model\n",
      "0s - loss: 0.2415 - acc: 0.9261 - val_loss: 0.2303 - val_acc: 0.9266\n",
      "Epoch 96/200\n",
      "Epoch 00095: val_loss improved from 0.23033 to 0.22994, saving model to best.model\n",
      "0s - loss: 0.2235 - acc: 0.9284 - val_loss: 0.2299 - val_acc: 0.9266\n",
      "Epoch 97/200\n",
      "Epoch 00096: val_loss improved from 0.22994 to 0.22905, saving model to best.model\n",
      "0s - loss: 0.2307 - acc: 0.9261 - val_loss: 0.2290 - val_acc: 0.9266\n",
      "Epoch 98/200\n",
      "Epoch 00097: val_loss improved from 0.22905 to 0.22751, saving model to best.model\n",
      "0s - loss: 0.2275 - acc: 0.9261 - val_loss: 0.2275 - val_acc: 0.9266\n",
      "Epoch 99/200\n",
      "Epoch 00098: val_loss improved from 0.22751 to 0.22572, saving model to best.model\n",
      "0s - loss: 0.2518 - acc: 0.9261 - val_loss: 0.2257 - val_acc: 0.9266\n",
      "Epoch 100/200\n",
      "Epoch 00099: val_loss improved from 0.22572 to 0.22356, saving model to best.model\n",
      "0s - loss: 0.2386 - acc: 0.9261 - val_loss: 0.2236 - val_acc: 0.9266\n",
      "Epoch 101/200\n",
      "Epoch 00100: val_loss improved from 0.22356 to 0.22126, saving model to best.model\n",
      "0s - loss: 0.2343 - acc: 0.9261 - val_loss: 0.2213 - val_acc: 0.9266\n",
      "Epoch 102/200\n",
      "Epoch 00101: val_loss improved from 0.22126 to 0.21935, saving model to best.model\n",
      "0s - loss: 0.2354 - acc: 0.9261 - val_loss: 0.2193 - val_acc: 0.9266\n",
      "Epoch 103/200\n",
      "Epoch 00102: val_loss improved from 0.21935 to 0.21813, saving model to best.model\n",
      "0s - loss: 0.2315 - acc: 0.9284 - val_loss: 0.2181 - val_acc: 0.9266\n",
      "Epoch 104/200\n",
      "Epoch 00103: val_loss improved from 0.21813 to 0.21715, saving model to best.model\n",
      "0s - loss: 0.2181 - acc: 0.9261 - val_loss: 0.2172 - val_acc: 0.9266\n",
      "Epoch 105/200\n",
      "Epoch 00104: val_loss improved from 0.21715 to 0.21622, saving model to best.model\n",
      "0s - loss: 0.2252 - acc: 0.9261 - val_loss: 0.2162 - val_acc: 0.9266\n",
      "Epoch 106/200\n",
      "Epoch 00105: val_loss improved from 0.21622 to 0.21603, saving model to best.model\n",
      "0s - loss: 0.2090 - acc: 0.9284 - val_loss: 0.2160 - val_acc: 0.9266\n",
      "Epoch 107/200\n",
      "Epoch 00106: val_loss improved from 0.21603 to 0.21431, saving model to best.model\n",
      "0s - loss: 0.2248 - acc: 0.9284 - val_loss: 0.2143 - val_acc: 0.9266\n",
      "Epoch 108/200\n",
      "Epoch 00107: val_loss improved from 0.21431 to 0.21196, saving model to best.model\n",
      "0s - loss: 0.2077 - acc: 0.9307 - val_loss: 0.2120 - val_acc: 0.9266\n",
      "Epoch 109/200\n",
      "Epoch 00108: val_loss improved from 0.21196 to 0.21048, saving model to best.model\n",
      "0s - loss: 0.2066 - acc: 0.9261 - val_loss: 0.2105 - val_acc: 0.9266\n",
      "Epoch 110/200\n",
      "Epoch 00109: val_loss improved from 0.21048 to 0.20887, saving model to best.model\n",
      "0s - loss: 0.2218 - acc: 0.9330 - val_loss: 0.2089 - val_acc: 0.9266\n",
      "Epoch 111/200\n",
      "Epoch 00110: val_loss improved from 0.20887 to 0.20665, saving model to best.model\n",
      "0s - loss: 0.2070 - acc: 0.9330 - val_loss: 0.2066 - val_acc: 0.9266\n",
      "Epoch 112/200\n",
      "Epoch 00111: val_loss improved from 0.20665 to 0.20426, saving model to best.model\n",
      "0s - loss: 0.1951 - acc: 0.9376 - val_loss: 0.2043 - val_acc: 0.9266\n",
      "Epoch 113/200\n",
      "Epoch 00112: val_loss improved from 0.20426 to 0.20268, saving model to best.model\n",
      "0s - loss: 0.2005 - acc: 0.9353 - val_loss: 0.2027 - val_acc: 0.9266\n",
      "Epoch 114/200\n",
      "Epoch 00113: val_loss improved from 0.20268 to 0.20191, saving model to best.model\n",
      "0s - loss: 0.1916 - acc: 0.9353 - val_loss: 0.2019 - val_acc: 0.9266\n",
      "Epoch 115/200\n",
      "Epoch 00114: val_loss improved from 0.20191 to 0.20130, saving model to best.model\n",
      "0s - loss: 0.2126 - acc: 0.9330 - val_loss: 0.2013 - val_acc: 0.9266\n",
      "Epoch 116/200\n",
      "Epoch 00115: val_loss improved from 0.20130 to 0.20043, saving model to best.model\n",
      "0s - loss: 0.1917 - acc: 0.9330 - val_loss: 0.2004 - val_acc: 0.9266\n",
      "Epoch 117/200\n",
      "Epoch 00116: val_loss improved from 0.20043 to 0.19931, saving model to best.model\n",
      "0s - loss: 0.1881 - acc: 0.9353 - val_loss: 0.1993 - val_acc: 0.9266\n",
      "Epoch 118/200\n",
      "Epoch 00117: val_loss improved from 0.19931 to 0.19806, saving model to best.model\n",
      "0s - loss: 0.2012 - acc: 0.9376 - val_loss: 0.1981 - val_acc: 0.9266\n",
      "Epoch 119/200\n",
      "Epoch 00118: val_loss improved from 0.19806 to 0.19725, saving model to best.model\n",
      "0s - loss: 0.1728 - acc: 0.9400 - val_loss: 0.1973 - val_acc: 0.9358\n",
      "Epoch 120/200\n",
      "Epoch 00119: val_loss improved from 0.19725 to 0.19673, saving model to best.model\n",
      "0s - loss: 0.1871 - acc: 0.9400 - val_loss: 0.1967 - val_acc: 0.9358\n",
      "Epoch 121/200\n",
      "Epoch 00120: val_loss improved from 0.19673 to 0.19612, saving model to best.model\n",
      "0s - loss: 0.1924 - acc: 0.9353 - val_loss: 0.1961 - val_acc: 0.9358\n",
      "Epoch 122/200\n",
      "Epoch 00121: val_loss improved from 0.19612 to 0.19413, saving model to best.model\n",
      "0s - loss: 0.2015 - acc: 0.9307 - val_loss: 0.1941 - val_acc: 0.9358\n",
      "Epoch 123/200\n",
      "Epoch 00122: val_loss improved from 0.19413 to 0.19106, saving model to best.model\n",
      "0s - loss: 0.1848 - acc: 0.9423 - val_loss: 0.1911 - val_acc: 0.9358\n",
      "Epoch 124/200\n",
      "Epoch 00123: val_loss improved from 0.19106 to 0.18768, saving model to best.model\n",
      "0s - loss: 0.1780 - acc: 0.9446 - val_loss: 0.1877 - val_acc: 0.9358\n",
      "Epoch 125/200\n",
      "Epoch 00124: val_loss improved from 0.18768 to 0.18685, saving model to best.model\n",
      "0s - loss: 0.1596 - acc: 0.9423 - val_loss: 0.1869 - val_acc: 0.9358\n",
      "Epoch 126/200\n",
      "Epoch 00125: val_loss did not improve\n",
      "0s - loss: 0.1780 - acc: 0.9400 - val_loss: 0.1876 - val_acc: 0.9358\n",
      "Epoch 127/200\n",
      "Epoch 00126: val_loss did not improve\n",
      "0s - loss: 0.1653 - acc: 0.9307 - val_loss: 0.1893 - val_acc: 0.9358\n",
      "Epoch 128/200\n",
      "Epoch 00127: val_loss did not improve\n",
      "0s - loss: 0.1734 - acc: 0.9330 - val_loss: 0.1888 - val_acc: 0.9358\n",
      "Epoch 129/200\n",
      "Epoch 00128: val_loss did not improve\n",
      "0s - loss: 0.1859 - acc: 0.9284 - val_loss: 0.1884 - val_acc: 0.9358\n",
      "Epoch 130/200\n",
      "Epoch 00129: val_loss did not improve\n",
      "0s - loss: 0.1463 - acc: 0.9515 - val_loss: 0.1887 - val_acc: 0.9358\n",
      "Epoch 131/200\n",
      "Epoch 00130: val_loss improved from 0.18685 to 0.18628, saving model to best.model\n",
      "0s - loss: 0.1520 - acc: 0.9469 - val_loss: 0.1863 - val_acc: 0.9266\n",
      "Epoch 132/200\n",
      "Epoch 00131: val_loss improved from 0.18628 to 0.18460, saving model to best.model\n",
      "0s - loss: 0.1772 - acc: 0.9446 - val_loss: 0.1846 - val_acc: 0.9358\n",
      "Epoch 133/200\n",
      "Epoch 00132: val_loss improved from 0.18460 to 0.18267, saving model to best.model\n",
      "0s - loss: 0.1633 - acc: 0.9423 - val_loss: 0.1827 - val_acc: 0.9358\n",
      "Epoch 134/200\n",
      "Epoch 00133: val_loss improved from 0.18267 to 0.18172, saving model to best.model\n",
      "0s - loss: 0.1518 - acc: 0.9330 - val_loss: 0.1817 - val_acc: 0.9358\n",
      "Epoch 135/200\n",
      "Epoch 00134: val_loss did not improve\n",
      "0s - loss: 0.1419 - acc: 0.9446 - val_loss: 0.1832 - val_acc: 0.9358\n",
      "Epoch 136/200\n",
      "Epoch 00135: val_loss did not improve\n",
      "0s - loss: 0.1630 - acc: 0.9376 - val_loss: 0.1839 - val_acc: 0.9358\n",
      "Epoch 137/200\n",
      "Epoch 00136: val_loss did not improve\n",
      "0s - loss: 0.1670 - acc: 0.9469 - val_loss: 0.1821 - val_acc: 0.9358\n",
      "Epoch 138/200\n",
      "Epoch 00137: val_loss improved from 0.18172 to 0.17917, saving model to best.model\n",
      "0s - loss: 0.1406 - acc: 0.9515 - val_loss: 0.1792 - val_acc: 0.9266\n",
      "Epoch 139/200\n",
      "Epoch 00138: val_loss improved from 0.17917 to 0.17744, saving model to best.model\n",
      "0s - loss: 0.1279 - acc: 0.9515 - val_loss: 0.1774 - val_acc: 0.9266\n",
      "Epoch 140/200\n",
      "Epoch 00139: val_loss improved from 0.17744 to 0.17697, saving model to best.model\n",
      "0s - loss: 0.1355 - acc: 0.9469 - val_loss: 0.1770 - val_acc: 0.9266\n",
      "Epoch 141/200\n",
      "Epoch 00140: val_loss improved from 0.17697 to 0.17645, saving model to best.model\n",
      "0s - loss: 0.1485 - acc: 0.9492 - val_loss: 0.1765 - val_acc: 0.9266\n",
      "Epoch 142/200\n",
      "Epoch 00141: val_loss did not improve\n",
      "0s - loss: 0.1348 - acc: 0.9469 - val_loss: 0.1807 - val_acc: 0.9266\n",
      "Epoch 143/200\n",
      "Epoch 00142: val_loss did not improve\n",
      "0s - loss: 0.1321 - acc: 0.9492 - val_loss: 0.1862 - val_acc: 0.9266\n",
      "Epoch 144/200\n",
      "Epoch 00143: val_loss did not improve\n",
      "0s - loss: 0.1384 - acc: 0.9492 - val_loss: 0.1856 - val_acc: 0.9266\n",
      "Epoch 145/200\n",
      "Epoch 00144: val_loss did not improve\n",
      "0s - loss: 0.1374 - acc: 0.9538 - val_loss: 0.1825 - val_acc: 0.9266\n",
      "Epoch 146/200\n",
      "Epoch 00145: val_loss did not improve\n",
      "0s - loss: 0.1429 - acc: 0.9446 - val_loss: 0.1785 - val_acc: 0.9266\n",
      "Epoch 147/200\n",
      "Epoch 00146: val_loss improved from 0.17645 to 0.17571, saving model to best.model\n",
      "0s - loss: 0.1498 - acc: 0.9492 - val_loss: 0.1757 - val_acc: 0.9266\n",
      "Epoch 148/200\n",
      "Epoch 00147: val_loss improved from 0.17571 to 0.17491, saving model to best.model\n",
      "0s - loss: 0.1456 - acc: 0.9469 - val_loss: 0.1749 - val_acc: 0.9358\n",
      "Epoch 149/200\n",
      "Epoch 00148: val_loss did not improve\n",
      "0s - loss: 0.1180 - acc: 0.9584 - val_loss: 0.1776 - val_acc: 0.9266\n",
      "Epoch 150/200\n",
      "Epoch 00149: val_loss did not improve\n",
      "0s - loss: 0.1461 - acc: 0.9515 - val_loss: 0.1816 - val_acc: 0.9266\n",
      "Epoch 151/200\n",
      "Epoch 00150: val_loss did not improve\n",
      "0s - loss: 0.1426 - acc: 0.9469 - val_loss: 0.1860 - val_acc: 0.9266\n",
      "Epoch 152/200\n",
      "Epoch 00151: val_loss did not improve\n",
      "0s - loss: 0.1525 - acc: 0.9492 - val_loss: 0.1850 - val_acc: 0.9266\n",
      "Epoch 153/200\n",
      "Epoch 00152: val_loss did not improve\n",
      "0s - loss: 0.1270 - acc: 0.9515 - val_loss: 0.1797 - val_acc: 0.9266\n",
      "Epoch 154/200\n",
      "Epoch 00153: val_loss did not improve\n",
      "0s - loss: 0.1300 - acc: 0.9515 - val_loss: 0.1776 - val_acc: 0.9358\n",
      "Epoch 155/200\n",
      "Epoch 00154: val_loss did not improve\n",
      "0s - loss: 0.1347 - acc: 0.9469 - val_loss: 0.1778 - val_acc: 0.9358\n",
      "Epoch 156/200\n",
      "Epoch 00155: val_loss did not improve\n",
      "0s - loss: 0.1284 - acc: 0.9423 - val_loss: 0.1794 - val_acc: 0.9358\n",
      "Epoch 157/200\n",
      "Epoch 00156: val_loss did not improve\n",
      "0s - loss: 0.1417 - acc: 0.9423 - val_loss: 0.1817 - val_acc: 0.9358\n",
      "Epoch 158/200\n",
      "Epoch 00157: val_loss did not improve\n",
      "0s - loss: 0.1223 - acc: 0.9515 - val_loss: 0.1846 - val_acc: 0.9266\n",
      "Epoch 159/200\n",
      "Epoch 00158: val_loss did not improve\n",
      "0s - loss: 0.1251 - acc: 0.9538 - val_loss: 0.1841 - val_acc: 0.9266\n",
      "Epoch 160/200\n",
      "Epoch 00159: val_loss did not improve\n",
      "0s - loss: 0.1302 - acc: 0.9469 - val_loss: 0.1824 - val_acc: 0.9358\n",
      "Epoch 161/200\n",
      "Epoch 00160: val_loss did not improve\n",
      "0s - loss: 0.1149 - acc: 0.9538 - val_loss: 0.1799 - val_acc: 0.9358\n",
      "Epoch 162/200\n",
      "Epoch 00161: val_loss did not improve\n",
      "0s - loss: 0.1358 - acc: 0.9538 - val_loss: 0.1786 - val_acc: 0.9358\n",
      "Epoch 163/200\n",
      "Epoch 00162: val_loss did not improve\n",
      "0s - loss: 0.1290 - acc: 0.9538 - val_loss: 0.1795 - val_acc: 0.9358\n",
      "Epoch 164/200\n",
      "Epoch 00163: val_loss did not improve\n",
      "0s - loss: 0.1053 - acc: 0.9584 - val_loss: 0.1818 - val_acc: 0.9358\n",
      "Epoch 165/200\n",
      "Epoch 00164: val_loss did not improve\n",
      "0s - loss: 0.1467 - acc: 0.9492 - val_loss: 0.1845 - val_acc: 0.9358\n",
      "Epoch 166/200\n",
      "Epoch 00165: val_loss did not improve\n",
      "0s - loss: 0.1161 - acc: 0.9561 - val_loss: 0.1830 - val_acc: 0.9358\n",
      "Epoch 167/200\n",
      "Epoch 00166: val_loss did not improve\n",
      "0s - loss: 0.1315 - acc: 0.9561 - val_loss: 0.1802 - val_acc: 0.9358\n",
      "Epoch 168/200\n",
      "Epoch 00167: val_loss did not improve\n",
      "0s - loss: 0.1224 - acc: 0.9492 - val_loss: 0.1785 - val_acc: 0.9358\n",
      "Epoch 169/200\n",
      "Epoch 00168: val_loss did not improve\n",
      "0s - loss: 0.1380 - acc: 0.9538 - val_loss: 0.1789 - val_acc: 0.9358\n",
      "Epoch 170/200\n",
      "Epoch 00169: val_loss did not improve\n",
      "0s - loss: 0.1254 - acc: 0.9584 - val_loss: 0.1788 - val_acc: 0.9358\n",
      "Epoch 171/200\n",
      "Epoch 00170: val_loss did not improve\n",
      "0s - loss: 0.1154 - acc: 0.9492 - val_loss: 0.1790 - val_acc: 0.9358\n",
      "Epoch 172/200\n",
      "Epoch 00171: val_loss did not improve\n",
      "0s - loss: 0.1088 - acc: 0.9584 - val_loss: 0.1798 - val_acc: 0.9358\n",
      "Epoch 173/200\n",
      "Epoch 00172: val_loss did not improve\n",
      "0s - loss: 0.1199 - acc: 0.9630 - val_loss: 0.1788 - val_acc: 0.9358\n",
      "Epoch 174/200\n",
      "Epoch 00173: val_loss did not improve\n",
      "0s - loss: 0.1247 - acc: 0.9584 - val_loss: 0.1774 - val_acc: 0.9358\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.37838, saving model to best.model\n",
      "0s - loss: 0.5442 - acc: 0.6998 - val_loss: 0.3784 - val_acc: 0.8807\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.37838 to 0.37217, saving model to best.model\n",
      "0s - loss: 0.3466 - acc: 0.8938 - val_loss: 0.3722 - val_acc: 0.8807\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.3060 - acc: 0.9122 - val_loss: 0.4156 - val_acc: 0.8807\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2809 - acc: 0.9307 - val_loss: 0.4641 - val_acc: 0.8807\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2915 - acc: 0.9330 - val_loss: 0.5030 - val_acc: 0.8807\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2983 - acc: 0.9330 - val_loss: 0.5270 - val_acc: 0.8807\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3192 - acc: 0.9330 - val_loss: 0.5369 - val_acc: 0.8807\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3109 - acc: 0.9330 - val_loss: 0.5354 - val_acc: 0.8807\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3042 - acc: 0.9330 - val_loss: 0.5254 - val_acc: 0.8807\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2838 - acc: 0.9330 - val_loss: 0.5086 - val_acc: 0.8807\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2986 - acc: 0.9330 - val_loss: 0.4878 - val_acc: 0.8807\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2905 - acc: 0.9330 - val_loss: 0.4657 - val_acc: 0.8807\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3143 - acc: 0.9330 - val_loss: 0.4442 - val_acc: 0.8807\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2853 - acc: 0.9330 - val_loss: 0.4243 - val_acc: 0.8807\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3104 - acc: 0.9330 - val_loss: 0.4090 - val_acc: 0.8807\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2741 - acc: 0.9330 - val_loss: 0.3969 - val_acc: 0.8807\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2591 - acc: 0.9330 - val_loss: 0.3895 - val_acc: 0.8807\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2902 - acc: 0.9307 - val_loss: 0.3854 - val_acc: 0.8807\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2813 - acc: 0.9307 - val_loss: 0.3840 - val_acc: 0.8807\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2602 - acc: 0.9307 - val_loss: 0.3846 - val_acc: 0.8807\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2732 - acc: 0.9330 - val_loss: 0.3871 - val_acc: 0.8807\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2739 - acc: 0.9307 - val_loss: 0.3908 - val_acc: 0.8807\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2769 - acc: 0.9330 - val_loss: 0.3966 - val_acc: 0.8807\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2756 - acc: 0.9330 - val_loss: 0.4026 - val_acc: 0.8807\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2777 - acc: 0.9330 - val_loss: 0.4073 - val_acc: 0.8807\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2646 - acc: 0.9330 - val_loss: 0.4119 - val_acc: 0.8807\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2670 - acc: 0.9307 - val_loss: 0.4149 - val_acc: 0.8807\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2951 - acc: 0.9330 - val_loss: 0.4157 - val_acc: 0.8807\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.72302, saving model to best.model\n",
      "0s - loss: 1.1876 - acc: 0.2379 - val_loss: 0.7230 - val_acc: 0.1009\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.72302 to 0.46720, saving model to best.model\n",
      "0s - loss: 0.7741 - acc: 0.4988 - val_loss: 0.4672 - val_acc: 0.8991\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.46720 to 0.35472, saving model to best.model\n",
      "0s - loss: 0.5315 - acc: 0.7344 - val_loss: 0.3547 - val_acc: 0.8991\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.35472 to 0.32683, saving model to best.model\n",
      "0s - loss: 0.3868 - acc: 0.8684 - val_loss: 0.3268 - val_acc: 0.8991\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.3126 - acc: 0.9099 - val_loss: 0.3375 - val_acc: 0.8991\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2824 - acc: 0.9238 - val_loss: 0.3609 - val_acc: 0.8991\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3059 - acc: 0.9238 - val_loss: 0.3855 - val_acc: 0.8991\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3121 - acc: 0.9261 - val_loss: 0.4070 - val_acc: 0.8991\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.3100 - acc: 0.9261 - val_loss: 0.4234 - val_acc: 0.8991\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3098 - acc: 0.9261 - val_loss: 0.4341 - val_acc: 0.8991\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.3162 - acc: 0.9261 - val_loss: 0.4399 - val_acc: 0.8991\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3304 - acc: 0.9261 - val_loss: 0.4405 - val_acc: 0.8991\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3109 - acc: 0.9261 - val_loss: 0.4379 - val_acc: 0.8991\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3033 - acc: 0.9261 - val_loss: 0.4332 - val_acc: 0.8991\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3163 - acc: 0.9261 - val_loss: 0.4263 - val_acc: 0.8991\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3127 - acc: 0.9261 - val_loss: 0.4178 - val_acc: 0.8991\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2973 - acc: 0.9261 - val_loss: 0.4084 - val_acc: 0.8991\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3190 - acc: 0.9261 - val_loss: 0.3978 - val_acc: 0.8991\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3305 - acc: 0.9261 - val_loss: 0.3877 - val_acc: 0.8991\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2905 - acc: 0.9261 - val_loss: 0.3782 - val_acc: 0.8991\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3040 - acc: 0.9238 - val_loss: 0.3695 - val_acc: 0.8991\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3130 - acc: 0.9261 - val_loss: 0.3620 - val_acc: 0.8991\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2986 - acc: 0.9261 - val_loss: 0.3553 - val_acc: 0.8991\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.3061 - acc: 0.9261 - val_loss: 0.3500 - val_acc: 0.8991\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.3018 - acc: 0.9238 - val_loss: 0.3458 - val_acc: 0.8991\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2955 - acc: 0.9261 - val_loss: 0.3426 - val_acc: 0.8991\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2861 - acc: 0.9261 - val_loss: 0.3404 - val_acc: 0.8991\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2990 - acc: 0.9192 - val_loss: 0.3391 - val_acc: 0.8991\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2766 - acc: 0.9238 - val_loss: 0.3383 - val_acc: 0.8991\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2744 - acc: 0.9261 - val_loss: 0.3384 - val_acc: 0.8991\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.26681, saving model to best.model\n",
      "0s - loss: 0.4170 - acc: 0.8291 - val_loss: 0.2668 - val_acc: 0.9266\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.2945 - acc: 0.9261 - val_loss: 0.2705 - val_acc: 0.9266\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2502 - acc: 0.9400 - val_loss: 0.2952 - val_acc: 0.9266\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2903 - acc: 0.9376 - val_loss: 0.3193 - val_acc: 0.9266\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2947 - acc: 0.9400 - val_loss: 0.3347 - val_acc: 0.9266\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2778 - acc: 0.9400 - val_loss: 0.3389 - val_acc: 0.9266\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2855 - acc: 0.9400 - val_loss: 0.3366 - val_acc: 0.9266\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2881 - acc: 0.9376 - val_loss: 0.3282 - val_acc: 0.9266\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2670 - acc: 0.9400 - val_loss: 0.3167 - val_acc: 0.9266\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2912 - acc: 0.9400 - val_loss: 0.3035 - val_acc: 0.9266\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2707 - acc: 0.9400 - val_loss: 0.2909 - val_acc: 0.9266\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2791 - acc: 0.9400 - val_loss: 0.2811 - val_acc: 0.9266\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2554 - acc: 0.9400 - val_loss: 0.2748 - val_acc: 0.9266\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2729 - acc: 0.9400 - val_loss: 0.2710 - val_acc: 0.9266\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2484 - acc: 0.9376 - val_loss: 0.2690 - val_acc: 0.9266\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2707 - acc: 0.9376 - val_loss: 0.2688 - val_acc: 0.9266\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2481 - acc: 0.9376 - val_loss: 0.2694 - val_acc: 0.9266\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2686 - acc: 0.9376 - val_loss: 0.2709 - val_acc: 0.9266\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2743 - acc: 0.9400 - val_loss: 0.2728 - val_acc: 0.9266\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2584 - acc: 0.9400 - val_loss: 0.2746 - val_acc: 0.9266\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2501 - acc: 0.9376 - val_loss: 0.2756 - val_acc: 0.9266\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2566 - acc: 0.9400 - val_loss: 0.2764 - val_acc: 0.9266\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2688 - acc: 0.9400 - val_loss: 0.2769 - val_acc: 0.9266\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2416 - acc: 0.9400 - val_loss: 0.2768 - val_acc: 0.9266\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2535 - acc: 0.9400 - val_loss: 0.2761 - val_acc: 0.9266\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2508 - acc: 0.9400 - val_loss: 0.2756 - val_acc: 0.9266\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2583 - acc: 0.9376 - val_loss: 0.2751 - val_acc: 0.9266\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.42738, saving model to best.model\n",
      "0s - loss: 0.8419 - acc: 0.5058 - val_loss: 0.4274 - val_acc: 0.9541\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.42738 to 0.25314, saving model to best.model\n",
      "0s - loss: 0.4950 - acc: 0.7344 - val_loss: 0.2531 - val_acc: 0.9541\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.25314 to 0.19530, saving model to best.model\n",
      "0s - loss: 0.3366 - acc: 0.8776 - val_loss: 0.1953 - val_acc: 0.9541\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.19530 to 0.18646, saving model to best.model\n",
      "0s - loss: 0.2434 - acc: 0.9330 - val_loss: 0.1865 - val_acc: 0.9541\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2408 - acc: 0.9446 - val_loss: 0.1940 - val_acc: 0.9541\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2511 - acc: 0.9469 - val_loss: 0.2053 - val_acc: 0.9541\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2529 - acc: 0.9469 - val_loss: 0.2156 - val_acc: 0.9541\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2441 - acc: 0.9469 - val_loss: 0.2233 - val_acc: 0.9541\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2787 - acc: 0.9469 - val_loss: 0.2286 - val_acc: 0.9541\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2478 - acc: 0.9469 - val_loss: 0.2313 - val_acc: 0.9541\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2706 - acc: 0.9469 - val_loss: 0.2317 - val_acc: 0.9541\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2621 - acc: 0.9469 - val_loss: 0.2302 - val_acc: 0.9541\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2573 - acc: 0.9469 - val_loss: 0.2270 - val_acc: 0.9541\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2650 - acc: 0.9469 - val_loss: 0.2229 - val_acc: 0.9541\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.2564 - acc: 0.9469 - val_loss: 0.2182 - val_acc: 0.9541\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.2608 - acc: 0.9469 - val_loss: 0.2131 - val_acc: 0.9541\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2460 - acc: 0.9469 - val_loss: 0.2079 - val_acc: 0.9541\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2533 - acc: 0.9469 - val_loss: 0.2031 - val_acc: 0.9541\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2338 - acc: 0.9469 - val_loss: 0.1987 - val_acc: 0.9541\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2281 - acc: 0.9446 - val_loss: 0.1949 - val_acc: 0.9541\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2408 - acc: 0.9469 - val_loss: 0.1918 - val_acc: 0.9541\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2471 - acc: 0.9469 - val_loss: 0.1895 - val_acc: 0.9541\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2429 - acc: 0.9469 - val_loss: 0.1880 - val_acc: 0.9541\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2391 - acc: 0.9469 - val_loss: 0.1872 - val_acc: 0.9541\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2216 - acc: 0.9469 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss improved from 0.18646 to 0.18643, saving model to best.model\n",
      "0s - loss: 0.2263 - acc: 0.9446 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss improved from 0.18643 to 0.18633, saving model to best.model\n",
      "0s - loss: 0.2430 - acc: 0.9446 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss improved from 0.18633 to 0.18632, saving model to best.model\n",
      "0s - loss: 0.2190 - acc: 0.9469 - val_loss: 0.1863 - val_acc: 0.9541\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2278 - acc: 0.9469 - val_loss: 0.1864 - val_acc: 0.9541\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2337 - acc: 0.9446 - val_loss: 0.1866 - val_acc: 0.9541\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2514 - acc: 0.9469 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2520 - acc: 0.9446 - val_loss: 0.1874 - val_acc: 0.9541\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2205 - acc: 0.9469 - val_loss: 0.1878 - val_acc: 0.9541\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2381 - acc: 0.9446 - val_loss: 0.1884 - val_acc: 0.9541\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2283 - acc: 0.9446 - val_loss: 0.1888 - val_acc: 0.9541\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2363 - acc: 0.9469 - val_loss: 0.1891 - val_acc: 0.9541\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2221 - acc: 0.9469 - val_loss: 0.1894 - val_acc: 0.9541\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2222 - acc: 0.9469 - val_loss: 0.1894 - val_acc: 0.9541\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2297 - acc: 0.9469 - val_loss: 0.1895 - val_acc: 0.9541\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2315 - acc: 0.9469 - val_loss: 0.1894 - val_acc: 0.9541\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2479 - acc: 0.9469 - val_loss: 0.1891 - val_acc: 0.9541\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2361 - acc: 0.9469 - val_loss: 0.1888 - val_acc: 0.9541\n",
      "Epoch 43/200\n",
      "Epoch 00042: val_loss did not improve\n",
      "0s - loss: 0.2353 - acc: 0.9469 - val_loss: 0.1883 - val_acc: 0.9541\n",
      "Epoch 44/200\n",
      "Epoch 00043: val_loss did not improve\n",
      "0s - loss: 0.2320 - acc: 0.9469 - val_loss: 0.1879 - val_acc: 0.9541\n",
      "Epoch 45/200\n",
      "Epoch 00044: val_loss did not improve\n",
      "0s - loss: 0.2297 - acc: 0.9446 - val_loss: 0.1876 - val_acc: 0.9541\n",
      "Epoch 46/200\n",
      "Epoch 00045: val_loss did not improve\n",
      "0s - loss: 0.2262 - acc: 0.9469 - val_loss: 0.1874 - val_acc: 0.9541\n",
      "Epoch 47/200\n",
      "Epoch 00046: val_loss did not improve\n",
      "0s - loss: 0.2430 - acc: 0.9469 - val_loss: 0.1873 - val_acc: 0.9541\n",
      "Epoch 48/200\n",
      "Epoch 00047: val_loss did not improve\n",
      "0s - loss: 0.2269 - acc: 0.9446 - val_loss: 0.1871 - val_acc: 0.9541\n",
      "Epoch 49/200\n",
      "Epoch 00048: val_loss did not improve\n",
      "0s - loss: 0.2234 - acc: 0.9469 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 50/200\n",
      "Epoch 00049: val_loss did not improve\n",
      "0s - loss: 0.2451 - acc: 0.9469 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 51/200\n",
      "Epoch 00050: val_loss did not improve\n",
      "0s - loss: 0.2221 - acc: 0.9469 - val_loss: 0.1870 - val_acc: 0.9541\n",
      "Epoch 52/200\n",
      "Epoch 00051: val_loss did not improve\n",
      "0s - loss: 0.2519 - acc: 0.9469 - val_loss: 0.1869 - val_acc: 0.9541\n",
      "Epoch 53/200\n",
      "Epoch 00052: val_loss did not improve\n",
      "0s - loss: 0.2167 - acc: 0.9446 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Epoch 54/200\n",
      "Epoch 00053: val_loss did not improve\n",
      "0s - loss: 0.2270 - acc: 0.9469 - val_loss: 0.1867 - val_acc: 0.9541\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.90005, saving model to best.model\n",
      "0s - loss: 1.3727 - acc: 0.2032 - val_loss: 0.9000 - val_acc: 0.1009\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss improved from 0.90005 to 0.57346, saving model to best.model\n",
      "0s - loss: 0.9518 - acc: 0.3926 - val_loss: 0.5735 - val_acc: 0.8991\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss improved from 0.57346 to 0.40215, saving model to best.model\n",
      "0s - loss: 0.6428 - acc: 0.6397 - val_loss: 0.4022 - val_acc: 0.8991\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss improved from 0.40215 to 0.33669, saving model to best.model\n",
      "0s - loss: 0.4356 - acc: 0.8199 - val_loss: 0.3367 - val_acc: 0.8991\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss improved from 0.33669 to 0.32776, saving model to best.model\n",
      "0s - loss: 0.3504 - acc: 0.8984 - val_loss: 0.3278 - val_acc: 0.8991\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.3191 - acc: 0.9099 - val_loss: 0.3426 - val_acc: 0.8991\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.3113 - acc: 0.9215 - val_loss: 0.3643 - val_acc: 0.8991\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.3049 - acc: 0.9261 - val_loss: 0.3856 - val_acc: 0.8991\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2797 - acc: 0.9261 - val_loss: 0.4037 - val_acc: 0.8991\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.3102 - acc: 0.9261 - val_loss: 0.4176 - val_acc: 0.8991\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2985 - acc: 0.9261 - val_loss: 0.4274 - val_acc: 0.8991\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.3095 - acc: 0.9261 - val_loss: 0.4329 - val_acc: 0.8991\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.3250 - acc: 0.9261 - val_loss: 0.4348 - val_acc: 0.8991\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.3145 - acc: 0.9261 - val_loss: 0.4338 - val_acc: 0.8991\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.3263 - acc: 0.9261 - val_loss: 0.4304 - val_acc: 0.8991\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.3132 - acc: 0.9261 - val_loss: 0.4252 - val_acc: 0.8991\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.3323 - acc: 0.9238 - val_loss: 0.4186 - val_acc: 0.8991\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.3060 - acc: 0.9261 - val_loss: 0.4109 - val_acc: 0.8991\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.3138 - acc: 0.9261 - val_loss: 0.4030 - val_acc: 0.8991\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.3293 - acc: 0.9261 - val_loss: 0.3945 - val_acc: 0.8991\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.3235 - acc: 0.9261 - val_loss: 0.3859 - val_acc: 0.8991\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.3003 - acc: 0.9261 - val_loss: 0.3777 - val_acc: 0.8991\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.3052 - acc: 0.9261 - val_loss: 0.3700 - val_acc: 0.8991\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.3130 - acc: 0.9261 - val_loss: 0.3630 - val_acc: 0.8991\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.3076 - acc: 0.9238 - val_loss: 0.3567 - val_acc: 0.8991\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2974 - acc: 0.9261 - val_loss: 0.3514 - val_acc: 0.8991\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2816 - acc: 0.9261 - val_loss: 0.3471 - val_acc: 0.8991\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2918 - acc: 0.9261 - val_loss: 0.3438 - val_acc: 0.8991\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.3041 - acc: 0.9261 - val_loss: 0.3414 - val_acc: 0.8991\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2901 - acc: 0.9261 - val_loss: 0.3397 - val_acc: 0.8991\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2808 - acc: 0.9261 - val_loss: 0.3385 - val_acc: 0.8991\n",
      "Train on 433 samples, validate on 109 samples\n",
      "Epoch 1/200\n",
      "Epoch 00000: val_loss improved from inf to 0.24217, saving model to best.model\n",
      "0s - loss: 0.3849 - acc: 0.8522 - val_loss: 0.2422 - val_acc: 0.9358\n",
      "Epoch 2/200\n",
      "Epoch 00001: val_loss did not improve\n",
      "0s - loss: 0.2585 - acc: 0.9353 - val_loss: 0.2452 - val_acc: 0.9358\n",
      "Epoch 3/200\n",
      "Epoch 00002: val_loss did not improve\n",
      "0s - loss: 0.2677 - acc: 0.9423 - val_loss: 0.2665 - val_acc: 0.9358\n",
      "Epoch 4/200\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 0.2540 - acc: 0.9423 - val_loss: 0.2847 - val_acc: 0.9358\n",
      "Epoch 5/200\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 0.2435 - acc: 0.9446 - val_loss: 0.2957 - val_acc: 0.9358\n",
      "Epoch 6/200\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 0.2733 - acc: 0.9446 - val_loss: 0.2983 - val_acc: 0.9358\n",
      "Epoch 7/200\n",
      "Epoch 00006: val_loss did not improve\n",
      "0s - loss: 0.2579 - acc: 0.9446 - val_loss: 0.2952 - val_acc: 0.9358\n",
      "Epoch 8/200\n",
      "Epoch 00007: val_loss did not improve\n",
      "0s - loss: 0.2858 - acc: 0.9446 - val_loss: 0.2881 - val_acc: 0.9358\n",
      "Epoch 9/200\n",
      "Epoch 00008: val_loss did not improve\n",
      "0s - loss: 0.2870 - acc: 0.9446 - val_loss: 0.2784 - val_acc: 0.9358\n",
      "Epoch 10/200\n",
      "Epoch 00009: val_loss did not improve\n",
      "0s - loss: 0.2563 - acc: 0.9446 - val_loss: 0.2686 - val_acc: 0.9358\n",
      "Epoch 11/200\n",
      "Epoch 00010: val_loss did not improve\n",
      "0s - loss: 0.2586 - acc: 0.9446 - val_loss: 0.2588 - val_acc: 0.9358\n",
      "Epoch 12/200\n",
      "Epoch 00011: val_loss did not improve\n",
      "0s - loss: 0.2564 - acc: 0.9446 - val_loss: 0.2508 - val_acc: 0.9358\n",
      "Epoch 13/200\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.2423 - acc: 0.9446 - val_loss: 0.2453 - val_acc: 0.9358\n",
      "Epoch 14/200\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.2254 - acc: 0.9446 - val_loss: 0.2423 - val_acc: 0.9358\n",
      "Epoch 15/200\n",
      "Epoch 00014: val_loss improved from 0.24217 to 0.24086, saving model to best.model\n",
      "0s - loss: 0.2430 - acc: 0.9423 - val_loss: 0.2409 - val_acc: 0.9358\n",
      "Epoch 16/200\n",
      "Epoch 00015: val_loss improved from 0.24086 to 0.24060, saving model to best.model\n",
      "0s - loss: 0.2260 - acc: 0.9423 - val_loss: 0.2406 - val_acc: 0.9358\n",
      "Epoch 17/200\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.2616 - acc: 0.9446 - val_loss: 0.2414 - val_acc: 0.9358\n",
      "Epoch 18/200\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.2343 - acc: 0.9446 - val_loss: 0.2432 - val_acc: 0.9358\n",
      "Epoch 19/200\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.2407 - acc: 0.9446 - val_loss: 0.2453 - val_acc: 0.9358\n",
      "Epoch 20/200\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.2488 - acc: 0.9446 - val_loss: 0.2474 - val_acc: 0.9358\n",
      "Epoch 21/200\n",
      "Epoch 00020: val_loss did not improve\n",
      "0s - loss: 0.2561 - acc: 0.9423 - val_loss: 0.2494 - val_acc: 0.9358\n",
      "Epoch 22/200\n",
      "Epoch 00021: val_loss did not improve\n",
      "0s - loss: 0.2394 - acc: 0.9446 - val_loss: 0.2502 - val_acc: 0.9358\n",
      "Epoch 23/200\n",
      "Epoch 00022: val_loss did not improve\n",
      "0s - loss: 0.2571 - acc: 0.9423 - val_loss: 0.2509 - val_acc: 0.9358\n",
      "Epoch 24/200\n",
      "Epoch 00023: val_loss did not improve\n",
      "0s - loss: 0.2334 - acc: 0.9446 - val_loss: 0.2508 - val_acc: 0.9358\n",
      "Epoch 25/200\n",
      "Epoch 00024: val_loss did not improve\n",
      "0s - loss: 0.2331 - acc: 0.9446 - val_loss: 0.2508 - val_acc: 0.9358\n",
      "Epoch 26/200\n",
      "Epoch 00025: val_loss did not improve\n",
      "0s - loss: 0.2441 - acc: 0.9446 - val_loss: 0.2500 - val_acc: 0.9358\n",
      "Epoch 27/200\n",
      "Epoch 00026: val_loss did not improve\n",
      "0s - loss: 0.2402 - acc: 0.9446 - val_loss: 0.2493 - val_acc: 0.9358\n",
      "Epoch 28/200\n",
      "Epoch 00027: val_loss did not improve\n",
      "0s - loss: 0.2251 - acc: 0.9446 - val_loss: 0.2481 - val_acc: 0.9358\n",
      "Epoch 29/200\n",
      "Epoch 00028: val_loss did not improve\n",
      "0s - loss: 0.2428 - acc: 0.9446 - val_loss: 0.2467 - val_acc: 0.9358\n",
      "Epoch 30/200\n",
      "Epoch 00029: val_loss did not improve\n",
      "0s - loss: 0.2395 - acc: 0.9446 - val_loss: 0.2452 - val_acc: 0.9358\n",
      "Epoch 31/200\n",
      "Epoch 00030: val_loss did not improve\n",
      "0s - loss: 0.2143 - acc: 0.9446 - val_loss: 0.2439 - val_acc: 0.9358\n",
      "Epoch 32/200\n",
      "Epoch 00031: val_loss did not improve\n",
      "0s - loss: 0.2364 - acc: 0.9423 - val_loss: 0.2432 - val_acc: 0.9358\n",
      "Epoch 33/200\n",
      "Epoch 00032: val_loss did not improve\n",
      "0s - loss: 0.2390 - acc: 0.9446 - val_loss: 0.2425 - val_acc: 0.9358\n",
      "Epoch 34/200\n",
      "Epoch 00033: val_loss did not improve\n",
      "0s - loss: 0.2353 - acc: 0.9446 - val_loss: 0.2421 - val_acc: 0.9358\n",
      "Epoch 35/200\n",
      "Epoch 00034: val_loss did not improve\n",
      "0s - loss: 0.2410 - acc: 0.9446 - val_loss: 0.2422 - val_acc: 0.9358\n",
      "Epoch 36/200\n",
      "Epoch 00035: val_loss did not improve\n",
      "0s - loss: 0.2472 - acc: 0.9446 - val_loss: 0.2425 - val_acc: 0.9358\n",
      "Epoch 37/200\n",
      "Epoch 00036: val_loss did not improve\n",
      "0s - loss: 0.2381 - acc: 0.9446 - val_loss: 0.2427 - val_acc: 0.9358\n",
      "Epoch 38/200\n",
      "Epoch 00037: val_loss did not improve\n",
      "0s - loss: 0.2464 - acc: 0.9446 - val_loss: 0.2431 - val_acc: 0.9358\n",
      "Epoch 39/200\n",
      "Epoch 00038: val_loss did not improve\n",
      "0s - loss: 0.2397 - acc: 0.9446 - val_loss: 0.2435 - val_acc: 0.9358\n",
      "Epoch 40/200\n",
      "Epoch 00039: val_loss did not improve\n",
      "0s - loss: 0.2318 - acc: 0.9446 - val_loss: 0.2436 - val_acc: 0.9358\n",
      "Epoch 41/200\n",
      "Epoch 00040: val_loss did not improve\n",
      "0s - loss: 0.2161 - acc: 0.9446 - val_loss: 0.2437 - val_acc: 0.9358\n",
      "Epoch 42/200\n",
      "Epoch 00041: val_loss did not improve\n",
      "0s - loss: 0.2412 - acc: 0.9446 - val_loss: 0.2436 - val_acc: 0.9358\n"
     ]
    }
   ],
   "source": [
    "result=[]\n",
    "for i in range(50):\n",
    "    y_pred=train_nn_simple(train,X_val,y_val)\n",
    "    result.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 9, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 7, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_new=np.array(result)\n",
    "result_new1=result_new.sum(axis=0)\n",
    "re=result_new1.tolist()\n",
    "result_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[204   0]\n",
      " [ 11   0]]\n",
      "94.8837209302\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97       204\n",
      "          1       0.00      0.00      0.00        11\n",
      "\n",
      "avg / total       0.90      0.95      0.92       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=[]\n",
    "for each in re:\n",
    "    if each>=25:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cf = confusion_matrix(y_val, y_pred)\n",
    "print(cf)\n",
    "print(accuracy_score(y_val, y_pred) * 100) \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
